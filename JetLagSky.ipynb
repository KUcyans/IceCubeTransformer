{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "import logging\n",
    "# 20 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Zen of Python, by Tim Peters  \n",
    "  \n",
    "Beautiful is better than ugly.  \n",
    "Explicit is better than implicit.  \n",
    "Simple is better than complex.  \n",
    "Complex is better than complicated.  \n",
    "Flat is better than nested.  \n",
    "Sparse is better than dense.  \n",
    "Readability counts.  \n",
    "Special cases aren't special enough to break the rules.  \n",
    "Although practicality beats purity.  \n",
    "Errors should never pass silently.  \n",
    "Unless explicitly silenced.  \n",
    "In the face of ambiguity, refuse the temptation to guess.  \n",
    "There should be one-- and preferably only one --obvious way to do it.  \n",
    "Although that way may not be obvious at first unless you're Dutch.  \n",
    "Now is better than never.  \n",
    "Although never is often better than *right* now.  \n",
    "If the implementation is hard to explain, it's a bad idea.  \n",
    "If the implementation is easy to explain, it may be a good idea.  \n",
    "Namespaces are one honking great idea -- let's do more of those!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, LightningDataModule, LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# fails after 1min 10 sec and reruns within 10 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/groups/icecube/cyan/Utils')\n",
    "from PlotUtils import setMplParam, getColour, getHistoParam \n",
    "# getHistoParam:\n",
    "# Nbins, binwidth, bins, counts, bin_centers  = \n",
    "from DB_lister import list_content, list_tables\n",
    "from ExternalFunctions import nice_string_output, add_text_to_ax\n",
    "setMplParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyRange(Enum):\n",
    "    ER_100_GEV_10_TEV = (0, [\"22010\", \"22013\", \"22016\"])\n",
    "    ER_10_TEV_1_PEV   = (1, [\"22011\", \"22014\", \"22017\"])\n",
    "    ER_1_PEV_100_PEV  = (2, [\"22012\", \"22015\", \"22018\"])\n",
    "\n",
    "    def __init__(self, value, subdirs):\n",
    "        self._value_ = value\n",
    "        self._subdirs = subdirs\n",
    "\n",
    "    def get_subdirs(self):\n",
    "        return self._subdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/lustre/hpc/project/icecube/HE_Nu_Aske_Oct2024/PMTfied/Snowstorm/\"\n",
    "NuE_PeV_root = root_dir + \"22015/\"\n",
    "NuMu_PeV_root = root_dir + \"22012/\"\n",
    "NuTau_PeV_root = root_dir + \"22018/\"\n",
    "\n",
    "truth_NuE_PeV_1 = NuE_PeV_root + \"truth_1.parquet\"\n",
    "truth_NuMu_PeV_1 = NuMu_PeV_root + \"truth_1.parquet\"\n",
    "truth_NuTau_PeV_1 = NuTau_PeV_root + \"truth_1.parquet\"\n",
    "\n",
    "PMTfied_NuE_PeV_1 = NuE_PeV_root + \"1/\"\n",
    "PMTfied_NuE_PeV_1_1 = PMTfied_NuE_PeV_1 + \"PMTfied_1.parquet\"\n",
    "\n",
    "PMTfied_NuMu_PeV_1 = NuMu_PeV_root + \"1/\"\n",
    "PMTfied_NuMu_PeV_1_1 = PMTfied_NuMu_PeV_1 + \"PMTfied_1.parquet\"\n",
    "\n",
    "PMTfied_NuTau_PeV_1 = NuTau_PeV_root + \"1/\"\n",
    "PMTfied_NuTau_PeV_1_1 = PMTfied_NuTau_PeV_1 + \"PMTfied_1.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/lustre/hpc/project/icecube/HE_Nu_Aske_Oct2024/PMTfied/Snowstorm/22015/1/PMTfied_1.parquet'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMTfied_NuE_PeV_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_dir(directory, extension='.parquet'):\n",
    "    return [f for f in os.listdir(directory) if f.endswith(extension)]\n",
    "def get_subdir_in_dir(directory):\n",
    "    return [name for name in os.listdir(directory) if os.path.isdir(os.path.join(directory, name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', '11', '9', '3', '7', '12', '2', '8', '5', '10', '1', '6']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_subdir_in_dir(NuE_PeV_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertParquetToDF(file:str) -> pd.DataFrame:\n",
    "    table = pq.read_table(file)\n",
    "    df = table.to_pandas()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PMTfied_NuE_PeV_1_1 = convertParquetToDF(PMTfied_NuE_PeV_1_1)\n",
    "# 16 sec\n",
    "# 695403 rows Ã— 24 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isClean(df:pd.DataFrame) -> bool:\n",
    "    return not df.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(isClean(df_PMTfied_NuE_PeV_1_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoNormaliser:\n",
    "    def __init__(self):\n",
    "        self.position_scaler = 2e-3  # 1/500\n",
    "        self.t_scaler = 3e-4         # 1/30000\n",
    "        self.t_shifter = 1e4         # (-) 10000\n",
    "        self.Q_shifter = 2           # (-) 2 in log10\n",
    "\n",
    "    def __call__(self, table: pa.Table) -> pa.Table:\n",
    "        \"\"\"\n",
    "        Apply the normalisation steps to the given PyArrow table.\n",
    "        \"\"\"\n",
    "        table = self._log10_charge(table)\n",
    "        table = self._pseudo_normalise_dom_pos(table)\n",
    "        table = self._pseudo_normalise_time(table)\n",
    "        return table\n",
    "\n",
    "    def _log10_charge(self, table: pa.Table) -> pa.Table:\n",
    "        \"\"\"\n",
    "        Apply log10 transformation and shift on charge-related columns.\n",
    "        \"\"\"\n",
    "        q_columns = ['q1', 'q2', 'q3', 'q4', 'q5', 'Q25', 'Q75', 'Qtotal']\n",
    "        for col in q_columns:\n",
    "            if col in table.column_names:\n",
    "                col_array = table[col].to_pandas()\n",
    "                new_col = np.where(col_array > 0, np.log10(col_array), 0) - self.Q_shifter\n",
    "                idx = table.column_names.index(col)\n",
    "                table = table.set_column(idx, col, pa.array(new_col))\n",
    "        return table\n",
    "\n",
    "    def _pseudo_normalise_dom_pos(self, table: pa.Table) -> pa.Table:\n",
    "        \"\"\"\n",
    "        Apply scaling to DOM position columns.\n",
    "        \"\"\"\n",
    "        pos_columns = ['dom_x', 'dom_y', 'dom_z', 'dom_x_rel', 'dom_y_rel', 'dom_z_rel']\n",
    "        for col in pos_columns:\n",
    "            if col in table.column_names:\n",
    "                new_col = table[col].to_pandas() * self.position_scaler\n",
    "                idx = table.column_names.index(col)\n",
    "                table = table.set_column(idx, col, pa.array(new_col))\n",
    "        return table\n",
    "\n",
    "    def _pseudo_normalise_time(self, table: pa.Table) -> pa.Table:\n",
    "        \"\"\"\n",
    "        Apply shifting and scaling to time-related columns.\n",
    "        \"\"\"\n",
    "        t_columns = ['t1', 't2', 't3', 'T10', 'T50', 'sigmaT']\n",
    "        t_columns_shift = ['t1', 't2', 't3']\n",
    "\n",
    "        # Time shifting\n",
    "        for col in t_columns_shift:\n",
    "            if col in table.column_names:\n",
    "                shifted = table[col].to_pandas() - self.t_shifter\n",
    "                idx = table.column_names.index(col)\n",
    "                table = table.set_column(idx, col, pa.array(shifted))\n",
    "\n",
    "        # Time scaling\n",
    "        for col in t_columns:\n",
    "            if col in table.column_names:\n",
    "                scaled = table[col].to_pandas() * self.t_scaler\n",
    "                idx = table.column_names.index(col)\n",
    "                table = table.set_column(idx, col, pa.array(scaled))\n",
    "\n",
    "        return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'22012'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(os.path.normpath(NuMu_PeV_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxNDOMFinder:\n",
    "    def __init__(self, root_dir: str, energy_band: EnergyRange, part: int = None, shard: int = None, verbosity: int = 0):\n",
    "        self.root_dir = root_dir\n",
    "        self.energy_band = energy_band\n",
    "        self.part = part\n",
    "        self.shard = shard\n",
    "        self.verbosity = verbosity\n",
    "        self.subdirectories = self.energy_band.get_subdirs()\n",
    "\n",
    "    def __call__(self) -> int:\n",
    "        max_n_doms_list = [self._get_max_n_doms_for_subdirectory(subdir) for subdir in self.subdirectories]\n",
    "        max_n_doms_list = [value for value in max_n_doms_list if value is not None]\n",
    "\n",
    "        global_max_n_doms = max(max_n_doms_list, default=0)\n",
    "        if self.verbosity > 0:\n",
    "            print(f\"Global max_n_doms across all data: {global_max_n_doms}\")\n",
    "        return global_max_n_doms\n",
    "\n",
    "    def _get_max_n_doms_for_subdirectory(self, subdirectory: str) -> int:\n",
    "        if self.part is not None: # only specific parts\n",
    "            part_path = os.path.join(self.root_dir, subdirectory, str(self.part))\n",
    "            truth_path = os.path.join(self.root_dir, subdirectory, f\"truth_{self.part}.parquet\")\n",
    "\n",
    "            return self._get_max_n_doms_for_part(part_path, truth_path)\n",
    "        else: # across all parts in the subdirectory\n",
    "            return self._get_max_n_doms_for_entire_subdirectory(subdirectory)\n",
    "\n",
    "    def _get_max_n_doms_for_part(self, part_path: str, truth_path: str) -> int:\n",
    "        # Get the maximum `n_doms` across all shards in a part.\n",
    "        if not os.path.exists(truth_path):\n",
    "            if self.verbosity > 0:\n",
    "                print(f\"Truth file missing for {truth_path}. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        truth_data = pq.read_table(truth_path)\n",
    "\n",
    "        if self.shard is not None: # only specific shards\n",
    "            \n",
    "            shard_filter = self._filter_shard_data(truth_data)\n",
    "            return self._compute_max_n_doms(shard_filter)\n",
    "        else: # across all shards in the part\n",
    "            shard_files = [\n",
    "                f for f in os.listdir(part_path) if f.startswith(\"PMTfied_\") and f.endswith(\".parquet\")\n",
    "            ]\n",
    "            max_n_doms_list = []\n",
    "            for shard_file in shard_files:\n",
    "                shard_no = int(shard_file.split(\"_\")[1].split(\".\")[0])\n",
    "                self.shard = shard_no\n",
    "                max_n_doms_list.append(self._get_max_n_doms_for_shard(truth_data))\n",
    "            return max(max_n_doms_list, default=None)\n",
    "\n",
    "    def _get_max_n_doms_for_shard(self, truth_data: pa.Table) -> int:\n",
    "        shard_filter = self._filter_shard_data(truth_data)\n",
    "        return self._compute_max_n_doms(shard_filter)\n",
    "\n",
    "    def _get_max_n_doms_for_entire_subdirectory(self, subdirectory: str) -> int:\n",
    "        # Get the maximum `n_doms` across all parts and shards in a subdirectory.\n",
    "        part_dirs = [\n",
    "            d for d in os.listdir(os.path.join(self.root_dir, subdirectory)) \n",
    "            if os.path.isdir(os.path.join(self.root_dir, subdirectory, d)) and d.isdigit()\n",
    "        ]\n",
    "\n",
    "        max_n_doms_list = []\n",
    "        for part in part_dirs:\n",
    "            self.part = int(part)\n",
    "            part_path = os.path.join(self.root_dir, subdirectory, part)\n",
    "            truth_path = os.path.join(self.root_dir, subdirectory, f\"truth_{part}.parquet\")\n",
    "            max_n_doms_list.append(self._get_max_n_doms_for_part(part_path, truth_path))\n",
    "\n",
    "        return max(max_n_doms_list, default=None)\n",
    "\n",
    "    def _filter_shard_data(self, truth_data: pa.Table) -> pa.Table:\n",
    "        shard_mask = pc.equal(truth_data.column(\"shard_no\"), self.shard)\n",
    "        return truth_data.filter(shard_mask)\n",
    "\n",
    "    def _compute_max_n_doms(self, shard_filter: pa.Table) -> int:\n",
    "        return shard_filter.column(\"N_doms\").combine_chunks().to_numpy().max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global max_n_doms across all data: 2674\n",
      "Global max_n_doms: 2674\n"
     ]
    }
   ],
   "source": [
    "maxNDOMFinder_PeV_1 = MaxNDOMFinder(\n",
    "    root_dir=root_dir,\n",
    "    energy_band=EnergyRange.ER_1_PEV_100_PEV,\n",
    "    part=1,\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "global_max_n_doms = maxNDOMFinder_PeV_1()\n",
    "print(f\"Global max_n_doms: {global_max_n_doms}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMonoFlavourShard(Dataset):\n",
    "    def __init__(self, root_dir: str, \n",
    "                 subdirectory_no: int,\n",
    "                 part: int, \n",
    "                 shard: int, \n",
    "                 max_n_doms: int,\n",
    "                 verbosity: int = 0,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): The root directory of the flavour.\n",
    "            part (int): The part of the dataset.\n",
    "            shard (int): The shard number.\n",
    "            verbosity (int): The verbosity level.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.subdirectory_no = subdirectory_no\n",
    "        self.part = part\n",
    "        self.shard = shard\n",
    "        self.verbosity = verbosity\n",
    "        self.max_n_doms = max_n_doms\n",
    "        self.transform = PseudoNormaliser()\n",
    " \n",
    "        self.feature_file = os.path.join(self.root_dir, f\"{self.subdirectory_no}\", f\"{self.part}\", f\"PMTfied_{self.shard}.parquet\")\n",
    "        self.truth_file = os.path.join(self.root_dir, f\"{self.subdirectory_no}\", f\"truth_{self.part}.parquet\")\n",
    "\n",
    "        self.truth_data = self._load_truth_data()\n",
    "        self.feature_data = self._load_feature_data()\n",
    "\n",
    "        if verbosity > 0:\n",
    "            self._show_info()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.truth_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Retrieve truth data\n",
    "        truth_row = self.truth_data.slice(idx, 1)\n",
    "        event_no = truth_row.column(\"event_no\").to_pylist()[0]\n",
    "        original_event_no = truth_row.column(\"original_event_no\").to_pylist()[0]\n",
    "        offset = truth_row.column(\"offset\").to_pylist()[0]\n",
    "        n_doms = truth_row.column(\"N_doms\").to_pylist()[0]\n",
    "        flavour = truth_row.column(\"flavour\").to_pylist()[0]\n",
    "\n",
    "        # Extract and pad features\n",
    "        features = self._extract_features(offset, n_doms)\n",
    "        features_padded = np.zeros((self.max_n_doms, features.shape[1]), dtype=np.float32)\n",
    "        features_padded[:features.shape[0], :] = features\n",
    "\n",
    "        # Create the mask\n",
    "        mask = np.zeros((self.max_n_doms,), dtype=np.float32)\n",
    "        mask[:features.shape[0]] = 1.0\n",
    "\n",
    "        # Convert to tensors\n",
    "        event_no_tensor = torch.tensor([event_no, original_event_no], dtype=torch.int64)\n",
    "        features_tensor = torch.tensor(features_padded, dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(flavour, dtype=torch.int64)\n",
    "        mask_tensor = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"event_no\": event_no_tensor,\n",
    "            \"features\": features_tensor,\n",
    "            \"target\": target_tensor,\n",
    "            \"mask\": mask_tensor,\n",
    "        }\n",
    "\n",
    "    def _load_feature_data(self):\n",
    "        table = pq.read_table(self.feature_file)\n",
    "        table = self.transform(table)\n",
    "        return table\n",
    "    \n",
    "    def _load_truth_data(self):\n",
    "        \"\"\"\n",
    "        Load and filter the truth data for the specific shard.\n",
    "        Dynamically create the 'flavour' column if it is missing.\n",
    "        \"\"\"\n",
    "        # Read the truth data\n",
    "        truth_table = pq.read_table(self.truth_file)\n",
    "\n",
    "        # Filter rows matching the shard number\n",
    "        shard_mask = pc.equal(truth_table.column(\"shard_no\").combine_chunks(), self.shard)\n",
    "        shard_filter = truth_table.filter(shard_mask)\n",
    "\n",
    "        # Check if 'flavour' column exists; if not, create it\n",
    "        if 'flavour' not in shard_filter.column_names:\n",
    "            if 'pid' not in shard_filter.column_names:\n",
    "                raise ValueError(\"The truth data is missing both 'flavour' and 'pid' columns. Cannot determine flavours.\")\n",
    "\n",
    "            # Define PID to flavour mapping\n",
    "            UNKNOWN_FLAVOUR = -1\n",
    "            pid_to_class = {\n",
    "                12: 0,   # NuE\n",
    "                -12: 0,  # NuE\n",
    "                14: 1,   # NuMu\n",
    "                -14: 1,  # NuMu\n",
    "                16: 2,   # NuTau\n",
    "                -16: 2,  # NuTau\n",
    "            }\n",
    "\n",
    "            # Create 'flavour' column based on 'pid'\n",
    "            pid_column = shard_filter.column(\"pid\").combine_chunks().to_numpy()\n",
    "            flavour_array = [\n",
    "                pid_to_class.get(pid, UNKNOWN_FLAVOUR) \n",
    "                for pid in pid_column\n",
    "            ]\n",
    "\n",
    "            # Convert to PyArrow Array and append as 'flavour'\n",
    "            flavour_arrow_array = pa.array(flavour_array, type=pa.int64())\n",
    "            shard_filter = shard_filter.append_column(\"flavour\", flavour_arrow_array)\n",
    "\n",
    "        # Validate 'flavour' values\n",
    "        flavour_column = shard_filter.column(\"flavour\").combine_chunks().to_numpy()\n",
    "        if not np.all(np.isin(flavour_column, [0, 1, 2])):\n",
    "            raise ValueError(\"The 'flavour' column contains invalid values. Expected 0, 1, or 2.\")\n",
    "\n",
    "        return shard_filter\n",
    "\n",
    "    def _extract_features(self, offset, n_rows):\n",
    "        \"\"\"\n",
    "        Extract a specific slice of features based on offset and number of rows using PyArrow.\n",
    "        \"\"\"\n",
    "        features_slice = self.feature_data.slice(offset, n_rows)\n",
    "\n",
    "        # Drop columns \"event_no\" and \"original_event_no\" if present\n",
    "        columns_to_keep = [\n",
    "            col for col in features_slice.column_names if col not in [\"event_no\", \"original_event_no\"]\n",
    "        ]\n",
    "        features_slice = features_slice.select(columns_to_keep)\n",
    "\n",
    "        # Convert PyArrow table to a NumPy array efficiently\n",
    "        features_slice = np.stack([features_slice.column(col).to_numpy() for col in columns_to_keep], axis=1)\n",
    "        return features_slice\n",
    "\n",
    "    def _show_info(self):\n",
    "        print(f\"------------- Statistics (subdirectory {self.subdirectory_no}, part {self.part}, shard {self.shard}) -------------\")\n",
    "        num_events = len(self.truth_data)\n",
    "        print(f\"Total {num_events} events from shard {self.shard}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/groups/icecube/cyan/.local/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Statistics (subdirectory 22015, part 1, shard 1) -------------\n",
      "Total 2000 events from shard 1\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 1) -------------\n",
      "Total 2000 events from shard 1\n"
     ]
    }
   ],
   "source": [
    "dataset_NuMu_PeV_1_1 = DatasetMonoFlavourShard(root_dir=root_dir,\n",
    "                                            subdirectory_no=22012,\n",
    "                                            part=1, \n",
    "                                            shard=1, \n",
    "                                            max_n_doms=3000,\n",
    "                                            verbosity=0)\n",
    "\n",
    "dataset_NuE_PeV_1_1 = DatasetMonoFlavourShard(root_dir=root_dir,\n",
    "                                            subdirectory_no=22015,\n",
    "                                            part=1, \n",
    "                                            shard=1, \n",
    "                                            max_n_doms=3000,\n",
    "                                            verbosity=1)\n",
    "\n",
    "dataset_NuTau_PeV_1_1 = DatasetMonoFlavourShard(root_dir=root_dir,\n",
    "                                            subdirectory_no=22018,\n",
    "                                            part=1, \n",
    "                                            shard=1, \n",
    "                                            max_n_doms=3000,\n",
    "                                            verbosity=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxNDOMFinder_PeV_1_1 = MaxNDOMFinder(root_dir, EnergyRange.PEV_1_TO_PEV_100, 1, 1, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global max_n_doms across all data: 2421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2421"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxNdomFinder_PeV_1_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMultiFlavourShard(Dataset):\n",
    "    def __init__(self, root_dir: str, \n",
    "                 energy_band: EnergyRange, \n",
    "                 part: int, \n",
    "                 shard: int = None,\n",
    "                 max_n_doms: int = None,\n",
    "                 verbosity: int = 0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): The root directory of the dataset.\n",
    "            energy_band (EnergyRange): The energy band (enum) defining the subdirectories.\n",
    "            part (int): The part number to collect.\n",
    "            shard (int): The shard number to collect.\n",
    "            max_n_doms (int): The maximum number of DOMs in the dataset.\n",
    "            verbosity (int): The verbosity level.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.energy_band = energy_band\n",
    "        self.part = part\n",
    "        self.shard = shard\n",
    "        self.verbosity = verbosity\n",
    "        \n",
    "        if max_n_doms is None:\n",
    "            max_n_doms_finder = MaxNDOMFinder(root_dir, energy_band, part, shard, verbosity=self.verbosity)\n",
    "            self.max_n_doms = max_n_doms_finder()\n",
    "        else:\n",
    "            self.max_n_doms = max_n_doms\n",
    "        \n",
    "        self.datasets = self._collect_shards()\n",
    "        self.cumulative_lengths = self._compute_cumulative_lengths()\n",
    "        \n",
    "        if verbosity > 0:\n",
    "            self._show_info()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(len(dataset) for dataset in self.datasets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dataset_idx, local_idx = self._global_to_local_index(idx)\n",
    "        return self.datasets[dataset_idx][local_idx]\n",
    "    \n",
    "    def _collect_shards(self):\n",
    "        datasets = []\n",
    "        \n",
    "        for subdir in self.energy_band.get_subdirs():\n",
    "            dataset = DatasetMonoFlavourShard(\n",
    "                root_dir = self.root_dir, \n",
    "                subdirectory_no = int(subdir),\n",
    "                part = self.part,\n",
    "                shard = self.shard,\n",
    "                max_n_doms = self.max_n_doms,\n",
    "                verbosity = self.verbosity - 1\n",
    "            )\n",
    "            datasets.append(dataset)\n",
    "            \n",
    "        return datasets\n",
    "    \n",
    "    def _global_to_local_index(self, idx):\n",
    "        for dataset_idx, start in enumerate(self.cumulative_lengths[:-1]):\n",
    "            if start <= idx < self.cumulative_lengths[dataset_idx + 1]:\n",
    "                local_idx = idx - start\n",
    "                return dataset_idx, local_idx\n",
    "        raise IndexError(f\"Index {idx} is out of range.\")\n",
    "    \n",
    "    def _compute_cumulative_lengths(self):\n",
    "        lengths = [len(dataset) for dataset in self.datasets]\n",
    "        return [0] + list(np.cumsum(lengths))\n",
    "    \n",
    "    def _show_info(self):\n",
    "        print(f\"------------- Multi-Flavour Shard (Energy Band: {self.energy_band.name}, Part: {self.part}, Shard: {self.shard}) -------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global max_n_doms across all data: 2421\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 1) -------------\n",
      "Total 2000 events from shard 1\n"
     ]
    }
   ],
   "source": [
    "dataset_PeV_1_1 = DatasetMultiFlavourShard(root_dir=root_dir,\n",
    "                                            subdirectory_no=22012,\n",
    "                                            part=1, \n",
    "                                            shard=1, \n",
    "                                            max_n_doms=maxNDOMFinder_PeV_1_1(),\n",
    "                                            verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_PeV_1_1.__getitem__(0)['target'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMultiFlavourPart(Dataset):\n",
    "    def __init__(self, root_dir: str, \n",
    "                 energy_band: EnergyRange, \n",
    "                 part: int, \n",
    "                 max_n_doms: int = None,\n",
    "                 verbosity: int = 0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): The root directory of the dataset.\n",
    "            energy_band (EnergyRange): The energy band (enum) defining the subdirectories.\n",
    "            part (int): The part number to collect.\n",
    "            verbosity (int): The verbosity level.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.energy_band = energy_band\n",
    "        self.part = part\n",
    "        self.verbosity = verbosity\n",
    "        \n",
    "        if max_n_doms is None:\n",
    "            max_n_doms_finder = MaxNDOMFinder(root_dir, energy_band, part, verbosity=verbosity)\n",
    "            self.max_n_doms = max_n_doms_finder()\n",
    "        else:\n",
    "            self.max_n_doms = max_n_doms\n",
    "\n",
    "        # Collect all shards for the part from each flavour\n",
    "        self.datasets = self._collect_shards()\n",
    "\n",
    "        # Compute cumulative lengths for indexing\n",
    "        self.cumulative_lengths = self._compute_cumulative_lengths()\n",
    "\n",
    "        if verbosity > 0:\n",
    "            self._show_info()\n",
    "\n",
    "    def _collect_shards(self):\n",
    "        datasets = []\n",
    "        subdirectories = self.energy_band.get_subdirs()\n",
    "        common_shards, unique_shards = self._get_common_and_unique_shard_numbers(subdirectories)\n",
    "        \n",
    "        for subdir in subdirectories:\n",
    "            for shard in common_shards:\n",
    "                datasets.append(\n",
    "                    DatasetMultiFlavourShard(\n",
    "                        root_dir=self.root_dir,\n",
    "                        energy_band=self.energy_band,\n",
    "                        part=self.part,\n",
    "                        shard=shard,\n",
    "                        max_n_doms=self.max_n_doms,\n",
    "                        verbosity=self.verbosity - 1\n",
    "                    )\n",
    "                )\n",
    "            for shard in unique_shards[subdir]:\n",
    "                datasets.append(\n",
    "                    DatasetMonoFlavourShard(\n",
    "                        root_dir=self.root_dir,\n",
    "                        subdirectory_no=int(subdir),\n",
    "                        part=self.part,\n",
    "                        shard=shard,\n",
    "                        max_n_doms=self.max_n_doms,\n",
    "                        verbosity=self.verbosity - 1\n",
    "                    )\n",
    "                )\n",
    "        return datasets\n",
    "        \n",
    "    def _get_common_and_unique_shard_numbers(self, subdirectories):\n",
    "        shard_sets = []\n",
    "        all_shard_numbers = {}\n",
    "        \n",
    "        for subdir in subdirectories:\n",
    "            shard_dir = os.path.join(self.root_dir, subdir, str(self.part))\n",
    "            shard_numbers = {\n",
    "                int(f.split('_')[1].split('.')[0])\n",
    "                for f in os.listdir(shard_dir) if f.startswith(\"PMTfied_\") and f.endswith(\".parquet\")\n",
    "            }\n",
    "            shard_sets.append(shard_numbers)\n",
    "            all_shard_numbers[subdir] = shard_numbers\n",
    "\n",
    "        common_shards = sorted(set.intersection(*shard_sets))\n",
    "\n",
    "        unique_shards = {\n",
    "            subdir: sorted(shard_numbers - set(common_shards))\n",
    "            for subdir, shard_numbers in all_shard_numbers.items()\n",
    "        }\n",
    "\n",
    "        return common_shards, unique_shards\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(dataset) for dataset in self.datasets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dataset_idx, local_idx = self._global_to_local_index(idx)\n",
    "        return self.datasets[dataset_idx][local_idx]\n",
    "\n",
    "    def _compute_cumulative_lengths(self):\n",
    "        lengths = [len(dataset) for dataset in self.datasets]\n",
    "        return [0] + list(np.cumsum(lengths))\n",
    "\n",
    "    def _global_to_local_index(self, idx):\n",
    "        for dataset_idx, start in enumerate(self.cumulative_lengths[:-1]):\n",
    "            if start <= idx < self.cumulative_lengths[dataset_idx + 1]:\n",
    "                local_idx = idx - start\n",
    "                return dataset_idx, local_idx\n",
    "        raise IndexError(f\"Index {idx} is out of range.\")\n",
    "\n",
    "    def _show_info(self):\n",
    "        print(f\"------------- Multi-Flavour Part (Energy Band: {self.energy_band.name}, Part: {self.part}) -------------\")\n",
    "        for dataset in self.datasets:\n",
    "            dataset._show_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxNDOMFinder_PeV_1 = MaxNDOMFinder(\n",
    "    root_dir = root_dir,\n",
    "    energy_band = EnergyRange.PEV_1_TO_PEV_100,\n",
    "    part=1,\n",
    "    verbosity=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global max_n_doms across all data: 2674\n",
      "------------- Multi-Flavour Part Statistics (Energy Band: PEV_1_TO_PEV_100, Part: 1) -------------\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 1) -------------\n",
      "Total 2000 events from shard 1\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 2) -------------\n",
      "Total 2000 events from shard 2\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 3) -------------\n",
      "Total 2000 events from shard 3\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 4) -------------\n",
      "Total 2000 events from shard 4\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 5) -------------\n",
      "Total 2000 events from shard 5\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 6) -------------\n",
      "Total 2000 events from shard 6\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 7) -------------\n",
      "Total 2000 events from shard 7\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 8) -------------\n",
      "Total 2000 events from shard 8\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 9) -------------\n",
      "Total 2000 events from shard 9\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 10) -------------\n",
      "Total 2000 events from shard 10\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 11) -------------\n",
      "Total 2000 events from shard 11\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 12) -------------\n",
      "Total 2000 events from shard 12\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 13) -------------\n",
      "Total 2000 events from shard 13\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 14) -------------\n",
      "Total 2000 events from shard 14\n",
      "------------- Statistics (subdirectory 22012, part 1, shard 15) -------------\n",
      "Total 1386 events from shard 15\n",
      "------------- Statistics (subdirectory 22015, part 1, shard 1) -------------\n",
      "Total 2000 events from shard 1\n",
      "------------- Statistics (subdirectory 22015, part 1, shard 2) -------------\n",
      "Total 2000 events from shard 2\n",
      "------------- Statistics (subdirectory 22015, part 1, shard 3) -------------\n",
      "Total 2000 events from shard 3\n",
      "------------- Statistics (subdirectory 22015, part 1, shard 4) -------------\n",
      "Total 2000 events from shard 4\n",
      "------------- Statistics (subdirectory 22015, part 1, shard 5) -------------\n",
      "Total 2000 events from shard 5\n",
      "------------- Statistics (subdirectory 22015, part 1, shard 6) -------------\n",
      "Total 2000 events from shard 6\n",
      "------------- Statistics (subdirectory 22015, part 1, shard 7) -------------\n",
      "Total 2000 events from shard 7\n",
      "------------- Statistics (subdirectory 22015, part 1, shard 8) -------------\n",
      "Total 2000 events from shard 8\n",
      "------------- Statistics (subdirectory 22015, part 1, shard 9) -------------\n",
      "Total 2000 events from shard 9\n",
      "------------- Statistics (subdirectory 22015, part 1, shard 10) -------------\n",
      "Total 2000 events from shard 10\n",
      "------------- Statistics (subdirectory 22015, part 1, shard 11) -------------\n",
      "Total 2000 events from shard 11\n",
      "------------- Statistics (subdirectory 22015, part 1, shard 12) -------------\n",
      "Total 2000 events from shard 12\n",
      "------------- Statistics (subdirectory 22015, part 1, shard 13) -------------\n",
      "Total 2000 events from shard 13\n",
      "------------- Statistics (subdirectory 22015, part 1, shard 14) -------------\n",
      "Total 2000 events from shard 14\n",
      "------------- Statistics (subdirectory 22015, part 1, shard 15) -------------\n",
      "Total 258 events from shard 15\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 1) -------------\n",
      "Total 2000 events from shard 1\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 2) -------------\n",
      "Total 2000 events from shard 2\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 3) -------------\n",
      "Total 2000 events from shard 3\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 4) -------------\n",
      "Total 2000 events from shard 4\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 5) -------------\n",
      "Total 2000 events from shard 5\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 6) -------------\n",
      "Total 2000 events from shard 6\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 7) -------------\n",
      "Total 2000 events from shard 7\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 8) -------------\n",
      "Total 2000 events from shard 8\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 9) -------------\n",
      "Total 2000 events from shard 9\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 10) -------------\n",
      "Total 2000 events from shard 10\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 11) -------------\n",
      "Total 2000 events from shard 11\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 12) -------------\n",
      "Total 2000 events from shard 12\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 13) -------------\n",
      "Total 2000 events from shard 13\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 14) -------------\n",
      "Total 2000 events from shard 14\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 15) -------------\n",
      "Total 2000 events from shard 15\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 16) -------------\n",
      "Total 2000 events from shard 16\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 17) -------------\n",
      "Total 2000 events from shard 17\n",
      "------------- Statistics (subdirectory 22018, part 1, shard 18) -------------\n",
      "Total 502 events from shard 18\n"
     ]
    }
   ],
   "source": [
    "dataset_PeV_1 = DatasetMultiFlavourPart(root_dir=root_dir,\n",
    "                                            energy_band=EnergyRange.PEV_1_TO_PEV_100,\n",
    "                                            part=1, \n",
    "                                            max_n_doms=maxNDOMFinder_PeV_1(),\n",
    "                                            verbosity=1)\n",
    "# 30 sec -> 15 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23min 46s, sys: 16.7 s, total: 24min 3s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in dataset_PeV_1:\n",
    "    pass\n",
    "# ~ 2 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92146"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_PeV_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1245.2162162162163"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_PeV_1)/74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMTfiedDataModule(LightningDataModule):\n",
    "    def __init__(self, root_dir: str, \n",
    "                 energy_band: EnergyRange, \n",
    "                 dataset: Dataset, \n",
    "                 batch_size: int = 32, \n",
    "                 num_workers: int = 4, \n",
    "                 split_ratios=(0.8, 0.1, 0.1), verbosity=0):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.energy_band = energy_band\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.split_ratios = split_ratios\n",
    "        self.verbosity = verbosity\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        total_len = len(self.dataset)\n",
    "        train_len = int(total_len * self.split_ratios[0])\n",
    "        val_len = int(total_len * self.split_ratios[1])\n",
    "        test_len = total_len - train_len - val_len\n",
    "\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
    "            self.dataset,\n",
    "            [train_len, val_len, test_len],\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "\n",
    "        # Compute class weights based on targets in train dataset\n",
    "        targets = [sample[\"target\"].item() for sample in self.train_dataset]\n",
    "        class_counts = torch.bincount(torch.tensor(targets), minlength=4)  # Assuming 4 classes (0, 1, 2, 3)\n",
    "        self.class_weights = 1.0 / class_counts.float()\n",
    "\n",
    "        if self.verbosity > 0:\n",
    "            print(f\"Dataset split into train ({train_len}), val ({val_len}), and test ({test_len})\")\n",
    "            print(f\"Class weights: {self.class_weights}\")\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        features = [item[\"features\"] for item in batch]\n",
    "        targets = [item[\"target\"] for item in batch]\n",
    "        masks = [item[\"mask\"] for item in batch]\n",
    "\n",
    "        max_seq_length = max(f.shape[0] for f in features)\n",
    "        padded_features = torch.zeros((len(features), max_seq_length, features[0].shape[1]), dtype=torch.float32)\n",
    "        padded_masks = torch.zeros((len(masks), max_seq_length), dtype=torch.float32)\n",
    "\n",
    "        for i, (feature, mask) in enumerate(zip(features, masks)):\n",
    "            seq_length = feature.shape[0]\n",
    "            padded_features[i, :seq_length, :] = feature\n",
    "            padded_masks[i, :seq_length] = mask\n",
    "\n",
    "        targets = torch.stack(targets)\n",
    "        return {\n",
    "            \"features\": padded_features,\n",
    "            \"target\": targets,\n",
    "            \"mask\": padded_masks,\n",
    "        }\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, \n",
    "                          batch_size=self.batch_size, \n",
    "                          num_workers=self.num_workers, \n",
    "                          shuffle=True, \n",
    "                        #   collate_fn=self._collate_fn, \n",
    "                          pin_memory=True,\n",
    "                          # skip the last batch if it is not full\n",
    "                          drop_last=False)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, \n",
    "                          batch_size=self.batch_size, \n",
    "                          num_workers=self.num_workers, \n",
    "                          shuffle=True, \n",
    "                        #   collate_fn=self._collate_fn, \n",
    "                          pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, \n",
    "                          batch_size=self.batch_size, \n",
    "                          num_workers=self.num_workers, \n",
    "                          shuffle=True, \n",
    "                        #   collate_fn=self._collate_fn, \n",
    "                          pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule_PeV_1_1 = PMTfiedDataModule(root_dir=root_dir,\n",
    "                                        energy_band=EnergyRange.ER_1_PEV_100_PEV,\n",
    "                                        dataset=dataset_PeV_1_1,\n",
    "                                        batch_size=32,\n",
    "                                        num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule_PeV_1_1.setup()\n",
    "dl  = datamodule_PeV_1_1.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 111 ms, sys: 510 ms, total: 620 ms\n",
      "Wall time: 2.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "for batch in dl:\n",
    "    pass\n",
    "# ~ 2.5 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl)/2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x = [\\text{batch size}, \\text{sequence length (or } N_{\\text{doms}}\\text{)}, \\text{input size (or } N_{\\text{features}}\\text{)}] \\\\\n",
    "\n",
    "\\frac{1}{N_{\\text{DOM}}}  \\sum_{i} W_{\\gamma\\beta}^{\\text{output}} \\text{ReLU}\\left( W_{\\beta\\alpha}^{\\text{input}} x_{\\beta i \\alpha} \\right) \\\\\n",
    "\\gamma \\text{ is the number of classes: 3,} \\\\\n",
    "x \\text{ is normalised.}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will tell more about the data. the PMTfied data has the features that I will feed the model. and one of the columns is 'event_no' which the truth data also has. I may want to do these: regression on columns: 'azimuth', 'zenith', or 'energy' or multiclass classification for 'pid'.\n",
    "\n",
    "'azimuth', 'zenith', 'energy' and 'pid' are all columns of truth data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head Attention with ALiBi \n",
    "$\\boxed{\\text{input layer}} \\rightarrow \\boxed{\\text{query, key, value projections}} \\rightarrow \\boxed{\\text{scaled dot-product attention}} \\rightarrow \\boxed{\\text{ALiBi bias addition}} \\rightarrow \\boxed{\\text{attention scores}} \\rightarrow \\boxed{\\text{weighted output}} \\rightarrow \\boxed{\\text{output layer}}$\n",
    "\n",
    "$$\n",
    "\\text{batch\\_size, } d_{\\text{model}} \\text{, and } N_{\\text{head}} \\text{ are pre-defined by model builder, thus, so is } d_{\\text{head}} = \\frac{d_{\\text{model}}}{N_{\\text{head}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\text{embed\\_dim  is } d_{\\text{model}}) \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{similarity measure } s_{ij} = q_i \\cdot k_j^T \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{attention score } S_{ij} = \\frac{s_{ij}}{\\sqrt{d_k}} + m_l \\cdot (j-i) + \\text{mask}_{ij} \\in \\mathbb{R}^{\\text{batch\\_size} \\times N_{\\text{head}} \\times N_{\\text{dom}} \\times N_{\\text{dom}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{softmax }(S_{ij}) = \\frac{\\exp(S_{ij})}{\\sum_{j=1}^{N_{\\text{dom}}} \\exp(S_{ij})} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{attention}_i = \\text{softmax}(S_{ij}) \\cdot v_j \\in \\mathbb{R}^{\\text{batch\\_size} \\times N_{\\text{dom}} \\times N_{\\text{head}} \\times d_{\\text{head}}}\n",
    "$$\n",
    "\n",
    "* `nn.Linear` works for input tensor $x$, already containing both the weight matrix and the bias term: $y = x\\cdot W^T + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALiBiAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int, # the size of an embedded vector\n",
    "                #  d_qk: int,\n",
    "                #  d_v: int,\n",
    "                 n_heads: int,\n",
    "                 dropout: float = 0.1,\n",
    "                nan_logger=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # d_model\n",
    "        self.d_qk = self.d_model\n",
    "        self.d_v = self.d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim_qk = self.d_qk // self.n_heads\n",
    "        self.head_dim_v = self.d_v // self.n_heads\n",
    "        self.scale = self.head_dim_qk ** -0.5\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # a layer is a linear transformation\n",
    "        self.q_proj = nn.Linear(self.d_model, self.d_qk) # Projects input vectors of size d_model into query vectors of size d_qk using a weight matrix W_q of size (d_model x d_qk)\n",
    "        self.k_proj = nn.Linear(self.d_model, self.d_qk)\n",
    "        self.v_proj = nn.Linear(self.d_model, self.d_v)\n",
    "        self.out_proj = nn.Linear(self.d_v, self.d_model)\n",
    "\n",
    "    # forward is invoked when calling the model\n",
    "    # x is the input tensor\n",
    "    # batch_size is the number of data samples in the batch\n",
    "    # seq_length is the number of elements in the sequence(N_dom_max)\n",
    "    # embed_dim is the dimension of the embedding\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        # print(f\"batch_size: {batch_size}, seq_length: {seq_length}, embed_dim: {embed_dim}\")\n",
    "        # print(f\"embed_dim: {embed_dim}, d_model: {self.d_model}\")\n",
    "        # assert embed_dim == self.d_model\n",
    "        \n",
    "        V = self.v_proj(x).view(batch_size, seq_length, self.n_heads, self.head_dim_v)\n",
    "        attention_scores = self._get_attention_pure_score(x, batch_size, seq_length)\n",
    "        alibi_bias = self._get_ALiBi_bias(x, seq_length)\n",
    "        \n",
    "        attention_scores += alibi_bias\n",
    "\n",
    "        # Mask attention scores\n",
    "        # masked_fill() fills elements of the tensor with a value where mask is True\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask[:, None, None, :] == 0, float(\"-inf\"))\n",
    "\n",
    "        attention = F.softmax(attention_scores, dim=-1)\n",
    "        attention_output = torch.einsum(\"bhqk,bkhd->bqhd\", attention, V).reshape(batch_size, seq_length, embed_dim)\n",
    "        \n",
    "        self.nan_logger.info(f\"---------- attention(ALiBi) ---------- \")\n",
    "        self.nan_logger.info(f\"attn_output hasn't nan: {not torch.isnan(attention_output).any()}\")\n",
    "        \n",
    "        return self.out_proj(attention_output)\n",
    "    \n",
    "    def _get_attention_pure_score(self, x, batch_size, seq_length):\n",
    "        Q = self.q_proj(x).view(batch_size, seq_length, self.n_heads, self.head_dim_qk)\n",
    "        K = self.k_proj(x).view(batch_size, seq_length, self.n_heads, self.head_dim_qk)\n",
    "        attention_scores = torch.einsum(\"bqhd,bkhd->bhqk\", Q, K) * self.scale\n",
    "        return attention_scores\n",
    "    \n",
    "    # HACK consider movig this outside this class. That would only be possible N_dom_max is constant for all parts...?\n",
    "    def _get_ALiBi_bias(self, x, seq_length):\n",
    "        # arange(n) returns a 1-D tensor of size n with values from 0 to n - 1\n",
    "        slopes = 1.0 / (2 ** (torch.arange(self.n_heads).float() / self.n_heads))\n",
    "        # to() moves the tensor to the device\n",
    "        slopes = slopes.to(x.device)\n",
    "\n",
    "        # view() reshapes the tensor\n",
    "        relative_positions = torch.arange(seq_length).view(1, 1, seq_length) - torch.arange(seq_length).view(1, seq_length, 1)\n",
    "        relative_positions = relative_positions.to(x.device)\n",
    "        \n",
    "        alibi_bias = slopes.view(self.n_heads, 1, 1) * relative_positions\n",
    "        # unsqueeze() adds a dimension to the tensor\n",
    "        alibi_bias = alibi_bias.unsqueeze(0)\n",
    "        return alibi_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnocentAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int, # the size of an embedded vector\n",
    "                 # d_qk: int,\n",
    "                    # d_v: int,\n",
    "                    n_heads: int,\n",
    "                    dropout: float = 0.1,\n",
    "                    nan_logger=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_qk = self.d_model\n",
    "        self.d_v = self.d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim_qk = self.d_qk // self.n_heads\n",
    "        self.head_dim_v = self.d_v // self.n_heads\n",
    "        self.scale = self.head_dim_qk ** -0.5\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.q_proj = nn.Linear(self.d_model, self.d_qk)\n",
    "        self.k_proj = nn.Linear(self.d_model, self.d_qk)\n",
    "        self.v_proj = nn.Linear(self.d_model, self.d_v)\n",
    "        self.out_proj = nn.Linear(self.d_v, self.d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        V = self.v_proj(x).view(batch_size, seq_length, self.n_heads, self.head_dim_v)\n",
    "        attn_scores = self._get_attention_pure_score(x, batch_size, seq_length)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask[:, None, None, :] == 0, float(\"-inf\"))\n",
    "        \n",
    "        attention = F.softmax(attn_scores, dim=-1)\n",
    "        attention_output = torch.einsum(\"bhqk,bkhd->bqhd\", attention, V).reshape(batch_size, seq_length, embed_dim)\n",
    "        \n",
    "        self.nan_logger.info(f\"---------- attention(Innocent) ---------- \")\n",
    "        self.nan_logger.info(f\"attn_output hasn't nan: {not torch.isnan(attention_output).any()}\")\n",
    "        return self.out_proj(attention_output)\n",
    "    \n",
    "    def _get_attention_pure_score(self, x, batch_size, seq_length):\n",
    "        Q = self.q_proj(x).view(batch_size, seq_length, self.n_heads, self.head_dim_qk)\n",
    "        K = self.k_proj(x).view(batch_size, seq_length, self.n_heads, self.head_dim_qk)\n",
    "        attention_scores = torch.einsum(\"bqhd,bkhd->bhqk\", Q, K) * self.scale\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, \n",
    "                    # d_qk: int,\n",
    "                    # d_v: int,\n",
    "                    n_heads: int, \n",
    "                    dropout: float = 0.1,\n",
    "                    nan_logger=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_qk = self.d_model\n",
    "        self.d_v = self.d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim_qk = self.d_qk // self.n_heads\n",
    "        self.head_dim_v = self.d_v // self.n_heads\n",
    "        self.scale = self.head_dim_qk ** -0.5\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.q_proj = nn.Linear(self.d_model, self.d_qk)\n",
    "        self.k_proj = nn.Linear(self.d_model, self.d_qk)\n",
    "        self.v_proj = nn.Linear(self.d_model, self.d_v)\n",
    "        self.out_proj = nn.Linear(self.d_v, self.d_model)\n",
    "        self.nan_logger = nan_logger\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        print(f\"x shape: {x.shape}\")\n",
    "\n",
    "        # Compute Q, K, V\n",
    "        Q = self.q_proj(x).view(batch_size, seq_length, self.n_heads, self.head_dim_qk).transpose(1, 2)\n",
    "        K = self.k_proj(x).view(batch_size, seq_length, self.n_heads, self.head_dim_qk).transpose(1, 2)\n",
    "        V = self.v_proj(x).view(batch_size, seq_length, self.n_heads, self.head_dim_v).transpose(1, 2)\n",
    "        \n",
    "        # Q : [32, 8, 2421, 16] â€” batch_size, n_heads, seq_length, head_dim_qk\n",
    "        # K : [32, 8, 2421, 16] â€” batch_size, n_heads, seq_length, head_dim_qk\n",
    "        # V : [32, 8, 2421, 16] â€” batch_size, n_heads, seq_length, head_dim_v\n",
    "\n",
    "        assert Q.shape == K.shape, \"Q and K must have the same shape\"\n",
    "        \n",
    "        if mask is not None:\n",
    "            # [batch_size, 1, 1, seq_length]\n",
    "            mask = mask[:, None, None, :]  # Broadcast along heads and query dimensions\n",
    "        \n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            query=Q, key=K, value=V,\n",
    "            attn_mask=mask,\n",
    "            scale=self.scale,\n",
    "            dropout_p=self.dropout\n",
    "        )# Q @ K^T\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_length, embed_dim)\n",
    "\n",
    "        self.nan_logger.info(f\"---------- attention(scaled dot-product) ---------- \")\n",
    "        self.nan_logger.info(f\"attn_output hasn't nan: {not torch.isnan(attn_output).any()}\")\n",
    "        return self.out_proj(attn_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalisation $\\boxed{\\text{input layer}} \\rightarrow \\boxed{\\text{normalised input layer}}$\n",
    "$$\n",
    "d_n \\text{ is the dimension of the normalisation layer, here, } d_n = d_{\\text{model}} = \n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\text{embed\\_dim  is } d_n)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu^l = \\frac{\\sum_{i=1}^{d_n} x_i}{d_n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^l = \\sqrt{\\frac{\\sum_{i=1}^{d_n} (x_i - \\mu^l)^2}{d_n}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_i^l, b_i^l \\in \\mathbb{R}^{d_n} : \\text{learnable parameters}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{LayerNormalisation}_i(x) = g_i^l \\cdot \\frac{x_i - \\mu^l}{\\sigma^l} + b_i^l \\text{  (of feature } i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalisation(nn.Module):\n",
    "    def __init__(self, d_n, eps=1e-10, nan_logger = None):\n",
    "        # d_n (int): dimension of the normalisation layer, \n",
    "        # here d_n = d_model = embed_dim\n",
    "        # eps: epsilon, a small number to avoid division by zero\n",
    "        super().__init__()\n",
    "        self.d_n = d_n\n",
    "        self.eps = eps\n",
    "\n",
    "        self.g = nn.Parameter(torch.ones(d_n)) # gain\n",
    "        self.b = nn.Parameter(torch.zeros(d_n)) # bias\n",
    "        self.nan_logger = nan_logger\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_length, 1)\n",
    "        mu = x.mean(dim=-1, keepdim=True)\n",
    "\n",
    "        # (batch_size, seq_length, 1)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "\n",
    "        x_normalised = (x - mu) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        # (batch_size, seq_length, d_n)\n",
    "        x_tilde = self.g * x_normalised + self.b\n",
    "        \n",
    "        self.nan_logger.info(f\"---------Layer Normalisation-----------\")\n",
    "        self.nan_logger.info(f\"x hasn't nan: {not torch.isnan(x).any()}\")\n",
    "        self.nan_logger.info(f\"mu hasn't nan: {not torch.isnan(mu).any()}\")\n",
    "        self.nan_logger.info(f\"var hasn't nan: {not torch.isnan(var).any()}\")\n",
    "        self.nan_logger.info(f\"x_normalised hasn't nan: {not torch.isnan(x_normalised).any()}\")\n",
    "        self.nan_logger.info(f\"x_tilde hasn't nan: {not torch.isnan(x_tilde).any()}\")\n",
    "        \n",
    "        return x_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Network $\\boxed{\\text{input layer}} \\rightarrow \\boxed{\\text{hidden layer}} \\rightarrow \\boxed{\\text{output layer}}$ \n",
    "\n",
    "$$\n",
    "x_i^l \\text{ is the input of the } i^{th} token in l^{th} \\text{ layer}\n",
    "$$\n",
    "\n",
    "$$\n",
    "d_{\\text{f}} \\text{ is the dimension of the feed forward network}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_{h,i}^l, b_i^l \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{f}}} \\text{ are learnable parameters}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{summed input } s_i^l = x_i^l W_{h,i}^l + b_{h,i}^l\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{hidden layer } h_i^l = \\text{ReLU}(s_i^l)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{summed output } x_i^{l+1} = h_i^l W_{f,i}^l + b_{f,i}^l\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model, d_f, dropout=0.1, nan_logger=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_f = d_f\n",
    "        self.W_h = nn.Linear(self.d_model, self.d_f)\n",
    "        self.W_f = nn.Linear(self.d_f, self.d_model)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.nan_logger = nan_logger\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_length, d_f)\n",
    "        s_i = self.W_h(x)# summed input\n",
    "        # (batch_size, seq_length, d_f)\n",
    "        h_i = self.activation(s_i)\n",
    "        h_i = self.dropout(h_i)\n",
    "        \n",
    "        # (batch_size, seq_length, d_model)\n",
    "        x_next = self.W_f(h_i) # summed output\n",
    "        \n",
    "        self.nan_logger.info(f\"---------FFN-----------\")\n",
    "        self.nan_logger.info(f\"s_i hasn't nan : {not torch.isnan(s_i).any()}\")\n",
    "        self.nan_logger.info(f\"h_i hasm't nan : {not torch.isnan(h_i).any()}\")\n",
    "        self.nan_logger.info(f\"x_next hasn't nan : {not torch.isnan(x_next).any()}\")\n",
    "        \n",
    "        return x_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_f, dropout=0.1, layer_idx=0, nan_logger=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_f = d_f\n",
    "        self.layer_idx = layer_idx\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.nan_logger = nan_logger\n",
    "        \n",
    "        # self.attention = InnocentAttention(self.d_model, self.n_heads, dropout)\n",
    "        self.attention = ScaledDotProductAttention(self.d_model, self.n_heads, dropout, self.nan_logger)\n",
    "        self.norm_attention = LayerNormalisation(self.d_model, self.nan_logger)\n",
    "        \n",
    "        self.ffn = FFN(self.d_model, self.d_f, dropout, self.nan_logger)\n",
    "        self.norm_ffn = LayerNormalisation(self.d_model, self.nan_logger)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        self.nan_logger.info(f\"==============Entering Encoder Block {self.layer_idx}==============\")\n",
    "        \n",
    "        # Attention\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.norm_attention(x + self.dropout(attn_output))\n",
    "        \n",
    "        # FFN\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm_ffn(x + self.dropout(ffn_output))\n",
    "        \n",
    "        self.nan_logger.info(f\"Encoder Block {self.layer_idx} output hasn't nan: {not torch.isnan(x).any()}\")\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModelPMT_Classification(LightningModule):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 n_heads, \n",
    "                 d_f, \n",
    "                 num_layers, \n",
    "                 d_input,\n",
    "                 num_classes, \n",
    "                 dropout=0.1, \n",
    "                 learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_f = d_f\n",
    "        self.num_layers = num_layers\n",
    "        self.d_input = d_input\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.nan_logger = logging.getLogger(\"nan_logger\")\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Input projection layer\n",
    "        self.input_projection = nn.Linear(self.d_input, self.d_model)\n",
    "\n",
    "        # Stacked encoder blocks\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [EncoderBlock(self.d_model, self.n_heads, self.d_f, self.dropout, layer_idx=i, nan_logger=self.nan_logger) for i in range(self.num_layers)]\n",
    "        )\n",
    "\n",
    "        # Classification head\n",
    "        self.classification_output_layer = nn.Linear(self.d_model, self.num_classes)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Input projection\n",
    "        x = self.input_projection(x)\n",
    "\n",
    "        # Encoder blocks\n",
    "        for encoder in self.encoder_blocks:\n",
    "            x = encoder(x, mask)\n",
    "\n",
    "        # Classification head: Mean pooling across sequence and output logits\n",
    "        x = x.mean(dim=1)\n",
    "        logits = self.classification_output_layer(x)\n",
    "        nan_logger.info(f\"---------Classification Head-----------\")\n",
    "        nan_logger.info(f\"logits hasn't nan: {not torch.isnan(logits).any()}\")\n",
    "        return logits\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[\"features\"]\n",
    "        y = batch[\"target\"]  # For classification, this is the class index\n",
    "        mask = batch[\"mask\"]\n",
    "\n",
    "        logits = self(x, mask)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        train_logger.info(f\"Epoch {self.current_epoch}: train_loss={loss.item():.4f}\")\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[\"features\"]\n",
    "        y = batch[\"target\"]\n",
    "        mask = batch[\"mask\"]\n",
    "\n",
    "        logits = self(x, mask)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "\n",
    "        train_logger.info(f\"Epoch {self.current_epoch}: val_loss={loss.item():.4f}, val_acc={acc.item() * 100:.2f}%\")\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch[\"features\"]\n",
    "        y = batch[\"target\"]\n",
    "        mask = batch[\"mask\"]\n",
    "\n",
    "        logits = self(x, mask)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "\n",
    "        train_logger.info(f\"Epoch {self.current_epoch}: test_loss={loss.item():.4f}, test_acc={acc.item() * 100:.2f}%\")\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_acc\", acc)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        nan_logger.info(f\"####################Training epoch {self.current_epoch}####################\")\n",
    "        train_logger.info(f\"####################Training epoch {self.current_epoch}####################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample_root = \"/lustre/hpc/project/icecube/HE_Nu_Aske_Oct2024/PMTfied/Snowstorm/99999\"\n",
    "# NuMu_PeV_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = TransformerModelPMT_Classification(\n",
    "    d_model=128,\n",
    "    n_heads=8,\n",
    "    d_f=256,\n",
    "    num_layers=3,\n",
    "    d_input=32, # the number of PMTfied features\n",
    "    num_classes=3, # the number of flavours: NuE, NuMu, NuTau\n",
    "    dropout=0.1,\n",
    "    learning_rate=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/lustre/hpc/icecube/cyan/factory/IceCubeTransformer/logs/20250128/100922_training.log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m train_logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m train_logger\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[0;32m---> 13\u001b[0m train_handler \u001b[38;5;241m=\u001b[39m \u001b[43mlogging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFileHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_log_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m train_formatter \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mFormatter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m train_handler\u001b[38;5;241m.\u001b[39msetFormatter(train_formatter)\n",
      "File \u001b[0;32m/usr/lib64/python3.9/logging/__init__.py:1146\u001b[0m, in \u001b[0;36mFileHandler.__init__\u001b[0;34m(self, filename, mode, encoding, delay, errors)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1146\u001b[0m     StreamHandler\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib64/python3.9/logging/__init__.py:1175\u001b[0m, in \u001b[0;36mFileHandler._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03m    Open the current base file with the (original) mode and encoding.\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m    Return the resulting stream.\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbaseFilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/lustre/hpc/icecube/cyan/factory/IceCubeTransformer/logs/20250128/100922_training.log'"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "current_time = datetime.now().strftime(\"%H%M%S\")\n",
    "\n",
    "base_log_dir = os.path.join(\"logs\", current_date)\n",
    "\n",
    "base_checkpoint_dir = os.path.join(\"checkpoints\", current_date)\n",
    "os.makedirs(base_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Training log\n",
    "train_log_filename = os.path.join(base_log_dir, f\"{current_time}_training.log\")\n",
    "train_logger = logging.getLogger(\"training\")\n",
    "train_logger.setLevel(logging.INFO)\n",
    "train_handler = logging.FileHandler(train_log_filename)\n",
    "train_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "train_handler.setFormatter(train_formatter)\n",
    "train_logger.addHandler(train_handler)\n",
    "\n",
    "# NaN log\n",
    "nan_log_filename = os.path.join(base_log_dir, f\"{current_time}_nan_checks.log\")\n",
    "nan_logger = logging.getLogger(\"nan_checks\")\n",
    "nan_logger.setLevel(logging.INFO)\n",
    "nan_handler = logging.FileHandler(nan_log_filename)\n",
    "nan_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "nan_handler.setFormatter(nan_formatter)\n",
    "nan_logger.addHandler(nan_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_logger = TensorBoardLogger(\n",
    "    save_dir=base_log_dir,\n",
    "    name=f\"{current_time}\",  # Add time to the logger name\n",
    ")\n",
    "\n",
    "# Set up the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=base_checkpoint_dir, \n",
    "    filename=f\"{current_time}_transformer-epoch{{epoch:02d}}-val_loss{{val_loss:.2f}}\",  # Add time to filename\n",
    "    save_top_k=3,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# Set up the early stopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTraining(model_class: nn.Module):\n",
    "    wandb.init(\n",
    "        project=f\"[{current_date}_{current_time}]Neutrino Flavour Classification\",\n",
    "        config={\n",
    "            \"d_model\": model_class.d_model,\n",
    "            \"n_heads\": model_class.n_heads,\n",
    "            \"d_f\": model_class.d_f,\n",
    "            \"num_layers\": model_class.num_layers,\n",
    "            \"d_input\": model_class.d_input,\n",
    "            \"num_classes\": model_class.num_classes,\n",
    "            \"dropout\": model_class.dropout,\n",
    "            \"learning_rate\": model_class.learning_rate,\n",
    "            \"epochs\": 5,\n",
    "            \"attention\": \"Scaled Dot-Product\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    train_logger.info(\n",
    "    \"| Parameter       | Value               |\\n\"\n",
    "    \"|-----------------|---------------------|\\n\"\n",
    "    f\"| attention       | Scaled Dot-Product |\\n\"\n",
    "    f\"| d_model         | {model_class.d_model:<15}|\\n\"\n",
    "    f\"| n_heads         | {model_class.n_heads:<15}|\\n\"\n",
    "    f\"| d_f             | {model_class.d_f:<15}|\\n\"\n",
    "    f\"| num_layers      | {model_class.num_layers:<15}|\\n\"\n",
    "    f\"| d_input         | {model_class.d_input:<15}|\\n\"\n",
    "    f\"| num_classes     | {model_class.num_classes:<15}|\\n\"\n",
    "    f\"| dropout         | {model_class.dropout:<15}|\\n\"\n",
    "    f\"| learning_rate   | {model_class.learning_rate:<15}|\\n\\n\"\n",
    "    )\n",
    "    train_logger.info(\"Starting training...\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=5,\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1,  # Use 2 GPUs or CPUs\n",
    "        gradient_clip_val=1.0,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "        log_every_n_steps=1,  # Log metrics after every batch\n",
    "        logger=tb_logger\n",
    "    )\n",
    "    trainer.fit(model_class, datamodule=datamodule_PeV_1_1)\n",
    "    \n",
    "    # Finalize wandb\n",
    "    wandb.finish()\n",
    "    return trainer\n",
    "\n",
    "# trainer = runTraining(model_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have build a transformer encoder for multi class classification. I checked the dataset and the dataloader manytimes so I am pretty bit sure that the dataset, dataloader and data module are excellent and almost perfect. And I just finished the model build which seems fine in terms of logical structure. However, after few epochs of training I got NaN. I found this out by adding printing lines after each layer of the transformer. So I would like to go back to the stage before the model building while keeping the current model, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_weights(m):\n",
    "#     if isinstance(m, nn.Linear):  # Check if the layer is Linear\n",
    "#         nn.init.xavier_uniform_(m.weight)  # Xavier initialisation for weights\n",
    "#         if m.bias is not None:\n",
    "#             nn.init.zeros_(m.bias)  # Zero initialisation for biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(model, datamodule=data_module_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(model, datamodule=data_module_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
