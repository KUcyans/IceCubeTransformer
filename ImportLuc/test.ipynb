{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def feature_preprocessing(col_name, value) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess the input features when creating the dataset.\n",
    "    \n",
    "    Args:\n",
    "    - col_name: The name of the column to preprocess.\n",
    "    - value: The value array of the column to preprocess [numpy array].\n",
    "\n",
    "    Returns:\n",
    "    - The preprocessed value array [numpy array].\n",
    "    \"\"\"\n",
    "  \n",
    "    if col_name in ['dom_x', 'dom_y', 'dom_z', 'dom_x_rel', 'dom_y_rel', 'dom_z_rel']:\n",
    "        value = value / 500\n",
    "    elif col_name in ['rde']:\n",
    "        value = (value - 1.25) / 0.25\n",
    "    elif col_name in ['pmt_area']:\n",
    "        value = value / 0.05\n",
    "    elif col_name in ['q1', 'q2', 'q3', 'q4', 'q5', 'Q25', 'Q75', 'Qtotal']:\n",
    "        mask = value > 0\n",
    "        value[mask] = np.log10(value[mask])\n",
    "    elif col_name in ['t1', 't2', 't3','t4', 't5']:\n",
    "        mask = value > 0\n",
    "        value[mask] = (value[mask] - 1.0e04) / 3.0e04\n",
    "    elif col_name in ['T10', 'T50', 'sigmaT']:\n",
    "        mask = value > 0\n",
    "        value[mask] = value[mask] / 1.0e04\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "class PMTfiedDatasetPyArrow(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            truth_paths_1,\n",
    "            truth_paths_2,\n",
    "            truth_paths_3,\n",
    "            sample_weights = [1, 1, 1],\n",
    "            selection=None,\n",
    "            transform=feature_preprocessing,\n",
    "    ):\n",
    "        '''\n",
    "        Args:\n",
    "        - truth_paths_1: List of paths to the truth files of type 1\n",
    "        - truth_paths_2: List of paths to the truth files of type 2\n",
    "        - truth_paths_3: List of paths to the truth files of type 3\n",
    "        - sample_weights: Ratio to sample from file type 1, 2, 3 respectively\n",
    "        - selection: List of event numbers to select from the corresponding truth files\n",
    "        - transform: Function to apply to the features as preprocessing\n",
    "        '''\n",
    "\n",
    "        self.truth_paths_1 = truth_paths_1\n",
    "        self.truth_paths_2 = truth_paths_2\n",
    "        self.truth_paths_3 = truth_paths_3\n",
    "        self.selection = selection\n",
    "        self.transform = transform\n",
    "\n",
    "        # Metadata variables\n",
    "        self.event_counts = [0,0,0]\n",
    "        self.cumulative_event_counts_1 = []\n",
    "        self.cumulative_event_counts_2 = []\n",
    "        self.cumulative_event_counts_3 = []\n",
    "\n",
    "        self.current_file_idx_1 = None\n",
    "        self.current_file_idx_2 = None\n",
    "        self.current_file_idx_3 = None\n",
    "\n",
    "        self.current_truth_1 = None\n",
    "        self.current_truth_2 = None\n",
    "        self.current_truth_3 = None\n",
    "\n",
    "        self.current_feature_path_1 = None\n",
    "        self.current_feature_path_2 = None\n",
    "        self.current_feature_path_3 = None\n",
    "     \n",
    "        self.current_features_1 = None\n",
    "        self.current_features_2 = None\n",
    "        self.current_features_3 = None\n",
    "\n",
    "        # Scan the truth files to get the event \n",
    "        total_events = 0\n",
    "        for path in self.truth_paths_1:\n",
    "            truth = pq.read_table(path)\n",
    "            if self.selection is not None:\n",
    "                mask = pc.is_in(truth['event_no'], value_set=pa.array(self.selection))\n",
    "                truth = truth.filter(mask)\n",
    "            n_events = len(truth)\n",
    "            self.event_counts[0] += n_events\n",
    "            total_events += n_events\n",
    "            self.cumulative_event_counts_1.append(total_events)\n",
    "\n",
    "\n",
    "        total_events = 0\n",
    "        for path in self.truth_paths_2:\n",
    "            truth = pq.read_table(path)\n",
    "            if self.selection is not None:\n",
    "                mask = pc.is_in(truth['event_no'], value_set=pa.array(self.selection))\n",
    "                truth = truth.filter(mask)\n",
    "            n_events = len(truth)\n",
    "            self.event_counts[1] += n_events\n",
    "            total_events += n_events\n",
    "            self.cumulative_event_counts_2.append(total_events)\n",
    "\n",
    "\n",
    "        total_events = 0\n",
    "        for path in self.truth_paths_3:\n",
    "            truth = pq.read_table(path)\n",
    "            if self.selection is not None:\n",
    "                mask = pc.is_in(truth['event_no'], value_set=pa.array(self.selection))\n",
    "                truth = truth.filter(mask)\n",
    "            n_events = len(truth)\n",
    "            self.event_counts[2] += n_events\n",
    "            total_events += n_events\n",
    "            self.cumulative_event_counts_3.append(total_events)\n",
    "\n",
    "        self.sample_weights = sample_weights\n",
    "        self.total_weights = sum(self.sample_weights)\n",
    "\n",
    "        print('Total events:', self.event_counts)\n",
    "        print('Cumulative event counts 1:', self.cumulative_event_counts_1)\n",
    "        print('Cumulative event counts 2:', self.cumulative_event_counts_2)\n",
    "        print('Cumulative event counts 3:', self.cumulative_event_counts_3)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Devide the event counts per file type by the sample weights, take the minimum times the sample weights\n",
    "        return min([count // weight for count, weight in zip(self.event_counts, self.sample_weights)]) * self.total_weights\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Find the file index for the given event index, sampling from different truth lists\n",
    "        \n",
    "        set_idx = idx // self.total_weights\n",
    "        mod_idx = idx % self.total_weights\n",
    "\n",
    "        if mod_idx < self.sample_weights[0]:\n",
    "            file_idx = np.searchsorted(self.cumulative_event_counts_1, self.sample_weights[0]*set_idx + mod_idx, side='right')\n",
    "            local_idx = self.sample_weights[0]*set_idx + mod_idx if file_idx == 0 else self.sample_weights[0]*set_idx + mod_idx - self.cumulative_event_counts_1[file_idx - 1]\n",
    "            truth_path = self.truth_paths_1[file_idx]\n",
    "\n",
    "            if file_idx != self.current_file_idx_1:\n",
    "                self.current_file_idx_1 = file_idx\n",
    "\n",
    "                truth = pq.read_table(truth_path)\n",
    "                if self.selection is not None:\n",
    "                    mask = pc.is_in(truth['event_no'], value_set=pa.array(self.selection))\n",
    "                    self.current_truth_1 = truth.filter(mask)\n",
    "                else:\n",
    "                    self.current_truth_1 = truth\n",
    "\n",
    "            truth = self.current_truth_1\n",
    "\n",
    "\n",
    "            # Get the event details\n",
    "            event_no = torch.tensor(int(truth.column('event_no')[local_idx].as_py()), dtype=torch.long)\n",
    "            energy = torch.tensor(truth.column('energy')[local_idx].as_py(), dtype=torch.float32)\n",
    "            # azimuth = torch.tensor(truth.column('azimuth')[local_idx].as_py(), dtype=torch.float32)\n",
    "            # zenith = torch.tensor(truth.column('zenith')[local_idx].as_py(), dtype=torch.float32)\n",
    "            pid = torch.tensor(truth.column('pid')[local_idx].as_py(), dtype=torch.float32)\n",
    "            \n",
    "            abs_pid = int(torch.abs(pid))\n",
    "            \n",
    "            if abs_pid == 12:\n",
    "                one_hot_pid = torch.tensor([1, 0, 0], dtype=torch.float32)\n",
    "            elif abs_pid == 14:\n",
    "                one_hot_pid = torch.tensor([0, 1, 0], dtype=torch.float32)\n",
    "            elif abs_pid == 16:\n",
    "                one_hot_pid = torch.tensor([0, 0, 1], dtype=torch.float32)\n",
    "            else:\n",
    "                one_hot_pid = torch.tensor([-1, -1, -1], dtype=torch.float32)\n",
    "\n",
    "            # Calculate a 3D unit-vector from the zenith and azimuth angles\n",
    "            # x_dir = torch.sin(zenith) * torch.cos(azimuth)\n",
    "            # y_dir = torch.sin(zenith) * torch.sin(azimuth)\n",
    "            # z_dir = torch.cos(zenith)\n",
    "\n",
    "\n",
    "            offset = int(truth.column('offset')[local_idx].as_py())\n",
    "            n_doms = int(truth.column('N_doms')[local_idx].as_py())\n",
    "            part_no = int(truth.column('part_no')[local_idx].as_py())\n",
    "            shard_no = int(truth.column('shard_no')[local_idx].as_py())\n",
    "\n",
    "            # Define the feature path based on the truth path\n",
    "            feature_path = truth_path.replace('truth_{}.parquet'.format(part_no), '' + str(part_no) + '/PMTfied_{}.parquet'.format(shard_no))\n",
    "\n",
    "            # x from rows (offset-n_doms) to offset\n",
    "            start_row = offset - n_doms\n",
    "\n",
    "            # Load the features and apply preprocessing\n",
    "            if feature_path != self.current_feature_path_1:\n",
    "                self.current_feature_path_1 = feature_path\n",
    "                self.current_features_1 = pq.read_table(feature_path)\n",
    "\n",
    "            features = self.current_features_1\n",
    "\n",
    "            x = features.slice(start_row, n_doms)\n",
    "            # drop the first two columns (event_no and original_event_no)\n",
    "            x = x.drop_columns(['event_no', 'original_event_no'])\n",
    "            num_columns = x.num_columns\n",
    "\n",
    "            x_tensor = torch.full((n_doms, num_columns), fill_value=torch.nan, dtype=torch.float32)\n",
    "\n",
    "            for i, col_name in enumerate(x.column_names):\n",
    "                value = x.column(i).to_numpy()\n",
    "                value = value.copy()\n",
    "                value = self.transform(col_name, value)\n",
    "                # convert to torch tensor\n",
    "                value_tensor = torch.from_numpy(value)\n",
    "                x_tensor[:, i] = value_tensor\n",
    "\n",
    "            return Data(x=x_tensor, n_doms=n_doms, event_no=event_no, feature_path=feature_path, energy=energy, pid=pid, one_hot_pid=one_hot_pid)\n",
    "\n",
    "        elif mod_idx < self.sample_weights[0] + self.sample_weights[1]:\n",
    "            file_idx = np.searchsorted(self.cumulative_event_counts_2, self.sample_weights[1]*set_idx + mod_idx - self.sample_weights[0], side='right')\n",
    "            local_idx = self.sample_weights[1]*set_idx + mod_idx - self.sample_weights[0] if file_idx == 0 else self.sample_weights[1]*set_idx + mod_idx - self.sample_weights[0] - self.cumulative_event_counts_2[file_idx - 1]\n",
    "            truth_path = self.truth_paths_2[file_idx]\n",
    "\n",
    "            if file_idx != self.current_file_idx_2:\n",
    "                self.current_file_idx_2 = file_idx\n",
    "\n",
    "                truth = pq.read_table(truth_path)\n",
    "                if self.selection is not None:\n",
    "                    mask = pc.is_in(truth['event_no'], value_set=pa.array(self.selection))\n",
    "                    self.current_truth_2 = truth.filter(mask)\n",
    "                else:\n",
    "                    self.current_truth_2 = truth\n",
    "\n",
    "            truth = self.current_truth_2\n",
    "\n",
    "            # Get the event details\n",
    "            event_no = torch.tensor(int(truth.column('event_no')[local_idx].as_py()), dtype=torch.long)\n",
    "            energy = torch.tensor(truth.column('energy')[local_idx].as_py(), dtype=torch.float32)\n",
    "            # azimuth = torch.tensor(truth.column('azimuth')[local_idx].as_py(), dtype=torch.float32)\n",
    "            # zenith = torch.tensor(truth.column('zenith')[local_idx].as_py(), dtype=torch.float32)\n",
    "            pid = torch.tensor(truth.column('pid')[local_idx].as_py(), dtype=torch.float32)\n",
    "\n",
    "            # Calculate a 3D unit-vector from the zenith and azimuth angles\n",
    "            # x_dir = torch.sin(zenith) * torch.cos(azimuth)\n",
    "            # y_dir = torch.sin(zenith) * torch.sin(azimuth)\n",
    "            # z_dir = torch.cos(zenith)\n",
    "\n",
    "            # Stack to dir3vec tensor\n",
    "            # dir3vec = torch.stack([x_dir, y_dir, z_dir], dim=-1)\n",
    "            \n",
    "            abs_pid = int(torch.abs(pid))\n",
    "            \n",
    "            if abs_pid == 12:\n",
    "                one_hot_pid = torch.tensor([1, 0, 0], dtype=torch.float32)\n",
    "            elif abs_pid == 14:\n",
    "                one_hot_pid = torch.tensor([0, 1, 0], dtype=torch.float32)\n",
    "            elif abs_pid == 16:\n",
    "                one_hot_pid = torch.tensor([0, 0, 1], dtype=torch.float32)\n",
    "            else:\n",
    "                one_hot_pid = torch.tensor([-1, -1, -1], dtype=torch.float32)\n",
    "\n",
    "            offset = int(truth.column('offset')[local_idx].as_py())\n",
    "            n_doms = int(truth.column('N_doms')[local_idx].as_py())\n",
    "            part_no = int(truth.column('part_no')[local_idx].as_py())\n",
    "            shard_no = int(truth.column('shard_no')[local_idx].as_py())\n",
    "\n",
    "            # Define the feature path based on the truth path\n",
    "            feature_path = truth_path.replace('truth_{}.parquet'.format(part_no), '' + str(part_no) + '/PMTfied_{}.parquet'.format(shard_no))\n",
    "\n",
    "            # x from rows (offset-n_doms) to offset\n",
    "            start_row = offset - n_doms\n",
    "\n",
    "            # Load the features and apply preprocessing\n",
    "            if feature_path != self.current_feature_path_2:\n",
    "                self.current_feature_path_2 = feature_path\n",
    "                self.current_features_2 = pq.read_table(feature_path)\n",
    "\n",
    "            features = self.current_features_2\n",
    "\n",
    "            x = features.slice(start_row, n_doms)\n",
    "            # drop the first two columns (event_no and original_event_no)\n",
    "            x = x.drop_columns(['event_no', 'original_event_no'])\n",
    "            num_columns = x.num_columns\n",
    "\n",
    "            x_tensor = torch.full((n_doms, num_columns), fill_value=torch.nan, dtype=torch.float32)\n",
    "\n",
    "            for i, col_name in enumerate(x.column_names):\n",
    "                value = x.column(i).to_numpy()\n",
    "                value = value.copy()\n",
    "                value = self.transform(col_name, value)\n",
    "                # convert to torch tensor\n",
    "                value_tensor = torch.from_numpy(value)\n",
    "                x_tensor[:, i] = value_tensor\n",
    "\n",
    "            return Data(x=x_tensor, n_doms=n_doms, event_no=event_no, feature_path=feature_path, energy=energy, pid=pid, one_hot_pid=one_hot_pid)\n",
    "\n",
    "        elif mod_idx < self.sample_weights[0] + self.sample_weights[1] + self.sample_weights[2]:\n",
    "            file_idx = np.searchsorted(self.cumulative_event_counts_3, set_idx*self.sample_weights[2] + mod_idx - self.sample_weights[0] - self.sample_weights[1], side='right')\n",
    "            local_idx = self.sample_weights[2]*set_idx + mod_idx - self.sample_weights[0] - self.sample_weights[1] if file_idx == 0 else self.sample_weights[2]*set_idx + mod_idx - self.sample_weights[0] - self.sample_weights[1] - self.cumulative_event_counts_3[file_idx - 1]\n",
    "            truth_path = self.truth_paths_3[file_idx]\n",
    "\n",
    "            if file_idx != self.current_file_idx_3:\n",
    "                self.current_file_idx_3 = file_idx\n",
    "\n",
    "                truth = pq.read_table(truth_path)\n",
    "                if self.selection is not None:\n",
    "                    mask = pc.is_in(truth['event_no'], value_set=pa.array(self.selection))\n",
    "                    self.current_truth_3 = truth.filter(mask)\n",
    "                else:\n",
    "                    self.current_truth_3 = truth\n",
    "\n",
    "            truth = self.current_truth_3\n",
    "\n",
    "            # Get the event details\n",
    "            event_no = torch.tensor(int(truth.column('event_no')[local_idx].as_py()), dtype=torch.long)\n",
    "            energy = torch.tensor(truth.column('energy')[local_idx].as_py(), dtype=torch.float32)\n",
    "            # azimuth = torch.tensor(truth.column('azimuth')[local_idx].as_py(), dtype=torch.float32)\n",
    "            # zenith = torch.tensor(truth.column('zenith')[local_idx].as_py(), dtype=torch.float32)\n",
    "            pid = torch.tensor(truth.column('pid')[local_idx].as_py(), dtype=torch.float32)\n",
    "\n",
    "            # Calculate a 3D unit-vector from the zenith and azimuth angles\n",
    "            # x_dir = torch.sin(zenith) * torch.cos(azimuth)\n",
    "            # y_dir = torch.sin(zenith) * torch.sin(azimuth)\n",
    "            # z_dir = torch.cos(zenith)\n",
    "\n",
    "            # Stack to dir3vec tensor\n",
    "            # dir3vec = torch.stack([x_dir, y_dir, z_dir], dim=-1)\n",
    "            abs_pid = int(torch.abs(pid))\n",
    "            \n",
    "            if abs_pid == 12:\n",
    "                one_hot_pid = torch.tensor([1, 0, 0], dtype=torch.float32)\n",
    "            elif abs_pid == 14:\n",
    "                one_hot_pid = torch.tensor([0, 1, 0], dtype=torch.float32)\n",
    "            elif abs_pid == 16:\n",
    "                one_hot_pid = torch.tensor([0, 0, 1], dtype=torch.float32)\n",
    "            else:\n",
    "                one_hot_pid = torch.tensor([-1, -1, -1], dtype=torch.float32)\n",
    "\n",
    "            offset = int(truth.column('offset')[local_idx].as_py())\n",
    "            n_doms = int(truth.column('N_doms')[local_idx].as_py())\n",
    "            part_no = int(truth.column('part_no')[local_idx].as_py())\n",
    "            shard_no = int(truth.column('shard_no')[local_idx].as_py())\n",
    "\n",
    "            # Define the feature path based on the truth path\n",
    "            feature_path = truth_path.replace('truth_{}.parquet'.format(part_no), '' + str(part_no) + '/PMTfied_{}.parquet'.format(shard_no))\n",
    "\n",
    "            # x from rows (offset-n_doms) to offset\n",
    "            start_row = offset - n_doms\n",
    "\n",
    "            # Load the features and apply preprocessing\n",
    "            if feature_path != self.current_feature_path_3:\n",
    "                self.current_feature_path_3 = feature_path\n",
    "                self.current_features_3 = pq.read_table(feature_path)\n",
    "\n",
    "            features = self.current_features_3\n",
    "\n",
    "            x = features.slice(start_row, n_doms)\n",
    "            # drop the first two columns (event_no and original_event_no)\n",
    "\n",
    "            x = x.drop_columns(['event_no', 'original_event_no'])\n",
    "            num_columns = x.num_columns\n",
    "\n",
    "            x_tensor = torch.full((n_doms, num_columns), fill_value=torch.nan, dtype=torch.float32)\n",
    "\n",
    "            for i, col_name in enumerate(x.column_names):\n",
    "                value = x.column(i).to_numpy()\n",
    "                value = value.copy()\n",
    "                value = self.transform(col_name, value)\n",
    "                # convert to torch tensor\n",
    "                value_tensor = torch.from_numpy(value)\n",
    "                x_tensor[:, i] = value_tensor\n",
    "\n",
    "            return Data(x=x_tensor, n_doms=n_doms, event_no=event_no, feature_path=feature_path, energy=energy, pid=pid, one_hot_pid=one_hot_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_1 = [\"/lustre/hpc/project/icecube/HE_Nu_Aske_Oct2024/PMTfied/Snowstorm/22011/truth_1.parquet\"]\n",
    "truth_2 = [\"/lustre/hpc/project/icecube/HE_Nu_Aske_Oct2024/PMTfied/Snowstorm/22014/truth_1.parquet\"]\n",
    "truth_3 = [\"/lustre/hpc/project/icecube/HE_Nu_Aske_Oct2024/PMTfied/Snowstorm/22017/truth_1.parquet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total events: [400561, 222514, 331040]\n",
      "Cumulative event counts 1: [400561]\n",
      "Cumulative event counts 2: [222514]\n",
      "Cumulative event counts 3: [331040]\n"
     ]
    }
   ],
   "source": [
    "dataset = PMTfiedDatasetPyArrow(\n",
    "    truth_paths_1=truth_1,\n",
    "    truth_paths_2=truth_2,\n",
    "    truth_paths_3=truth_3,\n",
    "    sample_weights=[1, 1, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " len(dataset) = 667542\n"
     ]
    }
   ],
   "source": [
    "print(f\" len(dataset) = {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-14.0 tensor([0., 1., 0.])\n",
      "12.0 tensor([1., 0., 0.])\n",
      "16.0 tensor([0., 0., 1.])\n",
      "14.0 tensor([0., 1., 0.])\n",
      "-12.0 tensor([1., 0., 0.])\n",
      "-16.0 tensor([0., 0., 1.])\n",
      "14.0 tensor([0., 1., 0.])\n",
      "12.0 tensor([1., 0., 0.])\n",
      "-16.0 tensor([0., 0., 1.])\n",
      "14.0 tensor([0., 1., 0.])\n",
      "12.0 tensor([1., 0., 0.])\n",
      "16.0 tensor([0., 0., 1.])\n",
      "14.0 tensor([0., 1., 0.])\n",
      "12.0 tensor([1., 0., 0.])\n",
      "-16.0 tensor([0., 0., 1.])\n",
      "14.0 tensor([0., 1., 0.])\n",
      "-12.0 tensor([1., 0., 0.])\n",
      "16.0 tensor([0., 0., 1.])\n",
      "14.0 tensor([0., 1., 0.])\n",
      "12.0 tensor([1., 0., 0.])\n",
      "16.0 tensor([0., 0., 1.])\n",
      "-14.0 tensor([0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset)):\n",
    "    # print(dataset[i])\n",
    "    print(f\"{dataset[i].pid} {dataset[i].one_hot_pid}\")\n",
    "    if i > 20:\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 45045/667542 [01:28<20:28, 506.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset))):\n\u001b[1;32m      4\u001b[0m     dataset[i]\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mi\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m100_000\u001b[39m:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "for i in tqdm.tqdm(range(len(dataset))):\n",
    "    dataset[i]\n",
    "    if i > 100_000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import custom_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    collate_fn = custom_collate_fn,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    persistent_workers=True, # necessary for caching \n",
    "    pin_memory=True, # necessary for caching\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 32]), torch.Size([64, 3]), tensor([105,  92,  29,  22,  98,  42,  32,  44,  70, 114,  40,  34,  56,  14,\n",
      "         19,  19, 256,  65, 222,  62,  38,  24, 208,  46, 104,  33,  34,  25,\n",
      "         89,  34, 128, 256, 163,  28,  42,  29, 179,  19,  25,  44, 241,  37,\n",
      "         48,  46,  57,  19,  91,  14, 232, 242, 117,  16,  90, 232,  89,  85,\n",
      "         12,  41,  17, 111, 128, 140,  33,  55])\n",
      "torch.Size([64, 256, 32]), torch.Size([64, 3]), tensor([ 83,  28,  73,  37,  39, 177,  60, 164,  17,  30,  30,  36,  93,  68,\n",
      "         41, 256, 127, 152,  92,  10,  18,  46,  24,  69, 256,  24, 128,  23,\n",
      "        180,  39,  58, 103,  16, 256,  72, 256,  37, 256,  14,  49,  12, 101,\n",
      "         41,  17, 256,  14,  14,  48,  78,  96,  18,  78, 256,  17,  48,  27,\n",
      "        148,  66, 256, 201,  32,  29,  95,  56])\n",
      "torch.Size([64, 256, 32]), torch.Size([64, 3]), tensor([  8,  75,  30,   9,  46,  66,  57, 256,  59, 167,   7,  71,  44,  93,\n",
      "         44, 101, 256,  44,  53, 204,  48,  89,  65,  26,  22,  56,   8,  89,\n",
      "         33,  93,  31,  34, 184, 135,  93,  78, 109,  81,  46, 166,  67,  28,\n",
      "        256,  21,  99,  56,  18, 109,  52, 251,  37, 232,   9, 166, 106, 115,\n",
      "        108,  36,  22, 143, 189,  32, 127,  35])\n",
      "torch.Size([64, 256, 32]), torch.Size([64, 3]), tensor([ 49,  12, 102, 104,  77,  24,  32,  23,  46, 256, 256,  18,  35,  36,\n",
      "         15,  26,  15,  34,  18, 256,  64, 201,  13,  31,  18,  44,  53,  53,\n",
      "        256,  17,  58,  13,  27,  56, 256,  17,  16,  33,  81,  53, 175,  83,\n",
      "         27,  15,  81,  84, 129,  44,  63,  29,  13,  17, 254,  44,  17, 146,\n",
      "        237,  38,  90,  24, 256,  13,  60,  14])\n",
      "torch.Size([64, 256, 32]), torch.Size([64, 3]), tensor([ 31,  26,  42, 175,  31,  87, 256,  35,  98,  12,  16,  34, 237,  10,\n",
      "        113, 103,  34,  19,  27,  47,  21, 256,  43, 205,  24,  99,  26, 104,\n",
      "         41,  48, 256, 106,  17,  34,  99,  18, 152,  81,  28,  72,  18, 240,\n",
      "        128,  75,  16, 154,  31,  52,  16,  13,  41,  84, 171, 138, 115, 131,\n",
      "         56,  10,  18,  16,  13,  18, 256, 256])\n",
      "torch.Size([64, 256, 32]), torch.Size([64, 3]), tensor([ 34, 217, 237,  27, 129, 256, 117,  16,  16,  63,  43,  61,  10, 182,\n",
      "        102,  64, 256,  30,  19,  22,  25,  90, 202, 256,  25,  24,  17,  89,\n",
      "        150,  79, 152,  27,  37,  65,   9, 109,  21,  20,  72,  28,  39,  14,\n",
      "         18,  34, 256, 116,  36,  21, 111, 236,  32,  48,  15, 256,   9, 148,\n",
      "         11, 133,  57, 127,  22, 152,  45,  59])\n",
      "torch.Size([64, 256, 32]), torch.Size([64, 3]), tensor([ 75,  64, 256,  28, 256,  34,  19,  34,  13,  82,  34,  54,  39,  29,\n",
      "         56,  28, 231, 249,  44, 256,  17, 155,  25, 198, 192, 208,  10,  59,\n",
      "         26,  11, 256,  41,  31, 125,  26,  82,  27, 246,  17, 201,  45,  12,\n",
      "        129,  23,  46,  10,  15, 170,  61,  58,  30, 198, 256, 256,  21,  71,\n",
      "         36,  35,  15, 256,  75,  16,  95,  15])\n",
      "torch.Size([64, 256, 32]), torch.Size([64, 3]), tensor([256,  39, 256, 102,  20,  52,  40, 103,  11,  33,  88,  26,  33,  21,\n",
      "         76, 256,  71,  29, 241,  14,  27,  85,  23,  33,  63,  13, 256,  51,\n",
      "         21,  30, 175,  19, 103,  54, 118,  43, 134,  46, 256,  85,  24,  88,\n",
      "         95,  41,  93,  59, 256, 114,  90, 113,  31,  19,  14, 114, 256, 194,\n",
      "         53,   8,  47,  22,  81,  38, 245,  13])\n",
      "torch.Size([64, 256, 32]), torch.Size([64, 3]), tensor([ 89,  30, 183, 115,  32, 154,  38,  10,  46,  79,  20, 140,  12,  29,\n",
      "         20, 109, 102,  73, 112,  31,  66,  64, 175,  51,  28, 124,  38,  48,\n",
      "         18,  33,  38,  26,  16,  18,  19,  12, 145,  36, 256,  38,  73, 256,\n",
      "         48,  20, 256, 256, 192, 115, 145, 164, 256,  57, 256,  20, 110,  87,\n",
      "        256,  23,  16, 121,  76,  26,  68, 146])\n",
      "torch.Size([64, 256, 32]), torch.Size([64, 3]), tensor([ 26,  87,   9,  69,  70,  34,  17, 109,  98, 256,  83,  91,  83,  90,\n",
      "         92,  29,  34,  19, 256, 206, 162,  43,  36,  14, 171, 120,  50,  43,\n",
      "        256, 102, 153, 134,  69, 133,  14,  26,  73,  45,  82,  47, 256,  15,\n",
      "         36, 256,  12,  66,  31, 256, 256, 132,  31,  77,  10,  27,  62,  78,\n",
      "        256,  33,  16,  54,  23, 236,  58, 256])\n",
      "torch.Size([64, 256, 32]), torch.Size([64, 3]), tensor([111,  41, 211,  28,  96,  47, 204, 256,  31, 150,  13,  13,  26,  80,\n",
      "         34,  24,  29, 103,  15, 166,  11,  33, 114,  54,  45, 256,  59,  82,\n",
      "        113, 256,  36, 121,  28, 232, 126, 183, 256,  63,  10, 122,  28,  46,\n",
      "         24,  40, 125, 144,  21,  32, 156,  42,  84, 256,  44,  83,  19,  30,\n",
      "         35,  24,  25,  20,  47,  39, 256, 146])\n",
      "torch.Size([64, 256, 32]), torch.Size([64, 3]), tensor([ 31,  54,  23,  35,  16, 153,  83,  17,  32, 242,  76,  66, 235,  45,\n",
      "         91,  28, 256,  17,  19,  71, 107,  86,  30, 155,  51, 234,  26,  26,\n",
      "         19, 132,  36,  45, 158,  19, 256, 116, 152,  25,  10,  63,  24,  11,\n",
      "         37,  32,  19, 256,  40,  70,  38,  35,  14,  28, 147, 160, 109,   8,\n",
      "        134, 100,  13,  29, 206,  62, 133,  78])\n"
     ]
    }
   ],
   "source": [
    "# for i, batch in enumerate(train_dataloader):\n",
    "#     print(f\"{batch[0].shape}, {batch[1].shape}, {batch[2]}\")\n",
    "#     if i > 10:\n",
    "#         break\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
