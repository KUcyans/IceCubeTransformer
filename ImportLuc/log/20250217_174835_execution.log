wandb: Currently logged in as: cyans (cyans-k-benhavns-universitet). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in ./wandb/run-20250217_174847-k0x1bg6z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-pond-8
wandb: ⭐️ View project at https://wandb.ai/cyans-k-benhavns-universitet/%5B20250217%5D%20Flavour%20Classification
wandb: 🚀 View run at https://wandb.ai/cyans-k-benhavns-universitet/%5B20250217%5D%20Flavour%20Classification/runs/k0x1bg6z
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /lustre/hpc/icecube/cyan/IceCube_project_share/icecube-projects/luc/lightning_logs/version_1 exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                   | Params | Mode 
---------------------------------------------------------
0 | model | regression_Transformer | 2.4 M  | train
---------------------------------------------------------
2.4 M     Trainable params
0         Non-trainable params
2.4 M     Total params
9.655     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_loss improved. New best score: 0.188
Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.182
Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.180
Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.178
Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.176
Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.174
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.173
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.173
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.173
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.172
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.172
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.172
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.172
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.171
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.170
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.169
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.169
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.169
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.168
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.168
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.168
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.168
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.167
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.167
`Trainer.fit` stopped: `max_epochs=50` reached.
wandb: - 74.273 MB of 74.273 MB uploadedwandb: \ 74.273 MB of 74.273 MB uploadedwandb: | 74.273 MB of 74.273 MB uploadedwandb: / 74.273 MB of 74.273 MB uploadedwandb: - 74.273 MB of 77.729 MB uploadedwandb: \ 77.729 MB of 77.729 MB uploadedwandb: | 77.729 MB of 77.729 MB uploadedwandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:       learning_rate ▁▁▂▂▃▄▅▆▇▇███████▇▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁
wandb:   median_train_loss █▇▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁
wandb:     median_val_loss █▆▆▅▃▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄
wandb:          train_loss ▇▆▂▇▄▃▃█▄▆▄▅▅▄▅▅▄▄▅▅▅▄▄▄▃▅▃▄▃▃▂▂▃▃▃▄▂▁▁▂
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            val_loss █▆▆▅▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄
wandb: 
wandb: Run summary:
wandb:               epoch 49
wandb:       learning_rate 0.0
wandb:   median_train_loss 0.14509
wandb:     median_val_loss 0.1744
wandb:          train_loss 0.13769
wandb: trainer/global_step 396599
wandb:            val_loss 0.17475
wandb: 
wandb: 🚀 View run legendary-pond-8 at: https://wandb.ai/cyans-k-benhavns-universitet/%5B20250217%5D%20Flavour%20Classification/runs/k0x1bg6z
wandb: ⭐️ View project at: https://wandb.ai/cyans-k-benhavns-universitet/%5B20250217%5D%20Flavour%20Classification
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250217_174847-k0x1bg6z/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
