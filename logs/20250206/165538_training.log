2025-02-06 16:56:21,850 - INFO - Starting training with the following parameters:
2025-02-06 16:56:21,850 - INFO - | Parameter       | Value               |
|-----------------|---------------------|
| attention       | Scaled Dot-Product|
| d_model         | 128            |
| n_heads         | 8              |
| d_f             | 128            |
| num_layers      | 3              |
| d_input         | 32             |
| num_classes     | 3              |
| dropout         | 0.1            |
| learning_rate   | 0.001          |
| epochs          | 200            |
| batch_size      | 16             |

2025-02-06 16:56:25,132 - INFO - Epoch 0: val_loss=0.9615, val_acc=33.33%
2025-02-06 16:56:25,262 - INFO - #################### Training epoch 0 ####################
2025-02-06 16:56:25,262 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 16:56:25,437 - INFO - Epoch 0: train_loss=1.1038
2025-02-06 16:56:26,886 - INFO - Epoch 0: train_loss=2.2126
2025-02-06 16:56:27,158 - INFO - Epoch 0: val_loss=1.9451, val_acc=0.00%
2025-02-06 16:56:27,161 - INFO - Epoch 0: EPOCH_AVG_TRAIN_LOSS=1.6582
2025-02-06 16:56:27,243 - INFO - #################### Training epoch 1 ####################
2025-02-06 16:56:27,243 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 16:56:27,531 - INFO - Epoch 1: train_loss=0.9755
2025-02-06 16:56:27,772 - INFO - Epoch 1: train_loss=1.2684
2025-02-06 16:56:28,063 - INFO - Epoch 1: val_loss=1.5333, val_acc=33.33%
2025-02-06 16:56:28,066 - INFO - Epoch 1: EPOCH_AVG_TRAIN_LOSS=1.1220
2025-02-06 16:56:28,091 - INFO - #################### Training epoch 2 ####################
2025-02-06 16:56:28,091 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 16:56:28,383 - INFO - Epoch 2: train_loss=1.4453
2025-02-06 16:56:28,625 - INFO - Epoch 2: train_loss=1.1410
2025-02-06 16:56:28,919 - INFO - Epoch 2: val_loss=1.6037, val_acc=0.00%
2025-02-06 16:56:28,923 - INFO - Epoch 2: EPOCH_AVG_TRAIN_LOSS=1.2931
2025-02-06 16:56:28,948 - INFO - #################### Training epoch 3 ####################
2025-02-06 16:56:28,948 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 16:56:29,244 - INFO - Epoch 3: train_loss=1.0987
2025-02-06 16:56:29,486 - INFO - Epoch 3: train_loss=1.2680
2025-02-06 16:56:29,773 - INFO - Epoch 3: val_loss=1.7798, val_acc=33.33%
2025-02-06 16:56:29,777 - INFO - Epoch 3: EPOCH_AVG_TRAIN_LOSS=1.1833
2025-02-06 16:56:29,804 - INFO - #################### Training epoch 4 ####################
2025-02-06 16:56:29,804 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 16:56:30,095 - INFO - Epoch 4: train_loss=0.9046
2025-02-06 16:56:30,338 - INFO - Epoch 4: train_loss=1.0361
2025-02-06 16:56:30,628 - INFO - Epoch 4: val_loss=2.2594, val_acc=0.00%
2025-02-06 16:56:30,631 - INFO - Epoch 4: EPOCH_AVG_TRAIN_LOSS=0.9703
2025-02-06 16:56:30,633 - INFO - #################### Training epoch 5 ####################
2025-02-06 16:56:30,634 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 16:56:30,927 - INFO - Epoch 5: train_loss=0.9382
2025-02-06 16:56:31,169 - INFO - Epoch 5: train_loss=0.9541
2025-02-06 16:56:31,461 - INFO - Epoch 5: val_loss=2.7144, val_acc=0.00%
2025-02-06 16:56:31,465 - INFO - Epoch 5: EPOCH_AVG_TRAIN_LOSS=0.9461
2025-02-06 16:56:31,467 - INFO - #################### Training epoch 6 ####################
2025-02-06 16:56:31,467 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 16:56:31,759 - INFO - Epoch 6: train_loss=1.0134
2025-02-06 16:56:32,001 - INFO - Epoch 6: train_loss=0.5755
2025-02-06 16:56:32,293 - INFO - Epoch 6: val_loss=2.9957, val_acc=33.33%
2025-02-06 16:56:32,297 - INFO - Epoch 6: EPOCH_AVG_TRAIN_LOSS=0.7945
2025-02-06 16:56:32,299 - INFO - #################### Training epoch 7 ####################
2025-02-06 16:56:32,299 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 16:56:32,592 - INFO - Epoch 7: train_loss=0.7987
2025-02-06 16:56:32,836 - INFO - Epoch 7: train_loss=0.7892
2025-02-06 16:56:33,127 - INFO - Epoch 7: val_loss=3.3004, val_acc=33.33%
2025-02-06 16:56:33,130 - INFO - Epoch 7: EPOCH_AVG_TRAIN_LOSS=0.7940
2025-02-06 16:56:33,133 - INFO - #################### Training epoch 8 ####################
2025-02-06 16:56:33,133 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 16:56:33,427 - INFO - Epoch 8: train_loss=0.7765
2025-02-06 16:56:33,670 - INFO - Epoch 8: train_loss=0.6560
2025-02-06 16:56:33,962 - INFO - Epoch 8: val_loss=3.3470, val_acc=33.33%
2025-02-06 16:56:33,966 - INFO - Epoch 8: EPOCH_AVG_TRAIN_LOSS=0.7162
2025-02-06 16:56:33,968 - INFO - #################### Training epoch 9 ####################
2025-02-06 16:56:33,968 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 16:56:34,262 - INFO - Epoch 9: train_loss=0.7746
2025-02-06 16:56:34,506 - INFO - Epoch 9: train_loss=0.7435
2025-02-06 16:56:34,797 - INFO - Epoch 9: val_loss=3.4954, val_acc=33.33%
2025-02-06 16:56:34,800 - INFO - Epoch 9: EPOCH_AVG_TRAIN_LOSS=0.7591
2025-02-06 16:56:34,803 - INFO - #################### Training epoch 10 ####################
2025-02-06 16:56:34,803 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 16:56:35,100 - INFO - Epoch 10: train_loss=0.8440
2025-02-06 16:56:35,344 - INFO - Epoch 10: train_loss=0.4643
2025-02-06 16:56:35,637 - INFO - Epoch 10: val_loss=3.6665, val_acc=33.33%
2025-02-06 16:56:35,641 - INFO - Epoch 10: EPOCH_AVG_TRAIN_LOSS=0.6542
2025-02-06 16:56:35,643 - INFO - #################### Training epoch 11 ####################
2025-02-06 16:56:35,643 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 16:56:35,939 - INFO - Epoch 11: train_loss=0.7386
2025-02-06 16:56:36,183 - INFO - Epoch 11: train_loss=0.6392
2025-02-06 16:56:36,476 - INFO - Epoch 11: val_loss=3.7000, val_acc=33.33%
2025-02-06 16:56:36,480 - INFO - Epoch 11: EPOCH_AVG_TRAIN_LOSS=0.6889
2025-02-06 16:56:36,482 - INFO - #################### Training epoch 12 ####################
2025-02-06 16:56:36,482 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 16:56:36,778 - INFO - Epoch 12: train_loss=0.6039
2025-02-06 16:56:37,021 - INFO - Epoch 12: train_loss=0.8270
2025-02-06 16:56:37,312 - INFO - Epoch 12: val_loss=3.2805, val_acc=0.00%
2025-02-06 16:56:37,315 - INFO - Epoch 12: EPOCH_AVG_TRAIN_LOSS=0.7154
2025-02-06 16:56:37,318 - INFO - #################### Training epoch 13 ####################
2025-02-06 16:56:37,318 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 16:56:37,613 - INFO - Epoch 13: train_loss=0.7990
2025-02-06 16:56:37,857 - INFO - Epoch 13: train_loss=0.4893
2025-02-06 16:56:38,150 - INFO - Epoch 13: val_loss=3.6786, val_acc=33.33%
2025-02-06 16:56:38,154 - INFO - Epoch 13: EPOCH_AVG_TRAIN_LOSS=0.6442
2025-02-06 16:56:38,156 - INFO - #################### Training epoch 14 ####################
2025-02-06 16:56:38,156 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 16:56:38,451 - INFO - Epoch 14: train_loss=0.4886
2025-02-06 16:56:38,695 - INFO - Epoch 14: train_loss=1.0053
2025-02-06 16:56:38,985 - INFO - Epoch 14: val_loss=4.0079, val_acc=33.33%
2025-02-06 16:56:38,989 - INFO - Epoch 14: EPOCH_AVG_TRAIN_LOSS=0.7470
2025-02-06 16:56:38,991 - INFO - #################### Training epoch 15 ####################
2025-02-06 16:56:38,991 - INFO - Current Learning Rate: 5.000000e-04
2025-02-06 16:56:39,287 - INFO - Epoch 15: train_loss=0.6979
2025-02-06 16:56:39,531 - INFO - Epoch 15: train_loss=0.4554
2025-02-06 16:56:39,825 - INFO - Epoch 15: val_loss=3.8833, val_acc=0.00%
2025-02-06 16:56:39,828 - INFO - Epoch 15: EPOCH_AVG_TRAIN_LOSS=0.5767
2025-02-06 16:56:39,830 - INFO - #################### Training epoch 16 ####################
2025-02-06 16:56:39,830 - INFO - Current Learning Rate: 5.000000e-04
2025-02-06 16:56:40,127 - INFO - Epoch 16: train_loss=0.5633
2025-02-06 16:56:40,370 - INFO - Epoch 16: train_loss=0.7528
2025-02-06 16:56:40,663 - INFO - Epoch 16: val_loss=4.2818, val_acc=0.00%
2025-02-06 16:56:40,667 - INFO - Epoch 16: EPOCH_AVG_TRAIN_LOSS=0.6580
2025-02-06 16:56:40,669 - INFO - #################### Training epoch 17 ####################
2025-02-06 16:56:40,669 - INFO - Current Learning Rate: 5.000000e-04
2025-02-06 16:56:40,964 - INFO - Epoch 17: train_loss=0.6758
2025-02-06 16:56:41,207 - INFO - Epoch 17: train_loss=0.5008
2025-02-06 16:56:41,500 - INFO - Epoch 17: val_loss=4.3878, val_acc=33.33%
2025-02-06 16:56:41,503 - INFO - Epoch 17: EPOCH_AVG_TRAIN_LOSS=0.5883
2025-02-06 16:56:41,506 - INFO - #################### Training epoch 18 ####################
2025-02-06 16:56:41,506 - INFO - Current Learning Rate: 5.000000e-04
2025-02-06 16:56:41,799 - INFO - Epoch 18: train_loss=0.6760
2025-02-06 16:56:42,043 - INFO - Epoch 18: train_loss=0.6115
2025-02-06 16:56:42,332 - INFO - Epoch 18: val_loss=4.3894, val_acc=33.33%
2025-02-06 16:56:42,335 - INFO - Epoch 18: EPOCH_AVG_TRAIN_LOSS=0.6437
2025-02-06 16:56:42,338 - INFO - #################### Training epoch 19 ####################
2025-02-06 16:56:42,338 - INFO - Current Learning Rate: 5.000000e-04
2025-02-06 16:56:42,633 - INFO - Epoch 19: train_loss=0.6403
2025-02-06 16:56:42,877 - INFO - Epoch 19: train_loss=0.6523
2025-02-06 16:56:43,169 - INFO - Epoch 19: val_loss=4.3965, val_acc=33.33%
2025-02-06 16:56:43,172 - INFO - Epoch 19: EPOCH_AVG_TRAIN_LOSS=0.6463
2025-02-06 16:56:43,174 - INFO - #################### Training epoch 20 ####################
2025-02-06 16:56:43,175 - INFO - Current Learning Rate: 2.500000e-04
2025-02-06 16:56:43,470 - INFO - Epoch 20: train_loss=0.6327
2025-02-06 16:56:43,713 - INFO - Epoch 20: train_loss=0.6185
2025-02-06 16:56:44,006 - INFO - Epoch 20: val_loss=4.4318, val_acc=33.33%
2025-02-06 16:56:44,010 - INFO - Epoch 20: EPOCH_AVG_TRAIN_LOSS=0.6256
2025-02-06 16:56:44,012 - INFO - #################### Training epoch 21 ####################
2025-02-06 16:56:44,012 - INFO - Current Learning Rate: 2.500000e-04
2025-02-06 16:56:44,305 - INFO - Epoch 21: train_loss=0.6187
2025-02-06 16:56:44,549 - INFO - Epoch 21: train_loss=0.5243
2025-02-06 16:56:44,839 - INFO - Epoch 21: val_loss=4.2592, val_acc=0.00%
2025-02-06 16:56:44,843 - INFO - Epoch 21: EPOCH_AVG_TRAIN_LOSS=0.5715
2025-02-06 16:56:44,845 - INFO - #################### Training epoch 22 ####################
2025-02-06 16:56:44,845 - INFO - Current Learning Rate: 2.500000e-04
2025-02-06 16:56:45,138 - INFO - Epoch 22: train_loss=0.6154
2025-02-06 16:56:45,382 - INFO - Epoch 22: train_loss=0.5061
2025-02-06 16:56:45,673 - INFO - Epoch 22: val_loss=4.1213, val_acc=0.00%
2025-02-06 16:56:45,677 - INFO - Epoch 22: EPOCH_AVG_TRAIN_LOSS=0.5608
2025-02-06 16:56:45,679 - INFO - #################### Training epoch 23 ####################
2025-02-06 16:56:45,679 - INFO - Current Learning Rate: 2.500000e-04
2025-02-06 16:56:45,973 - INFO - Epoch 23: train_loss=0.4999
2025-02-06 16:56:46,217 - INFO - Epoch 23: train_loss=0.7830
2025-02-06 16:56:46,509 - INFO - Epoch 23: val_loss=4.2588, val_acc=0.00%
2025-02-06 16:56:46,513 - INFO - Epoch 23: EPOCH_AVG_TRAIN_LOSS=0.6415
2025-02-06 16:56:46,515 - INFO - #################### Training epoch 24 ####################
2025-02-06 16:56:46,515 - INFO - Current Learning Rate: 1.250000e-04
2025-02-06 16:56:46,808 - INFO - Epoch 24: train_loss=0.5480
2025-02-06 16:56:47,051 - INFO - Epoch 24: train_loss=0.6112
2025-02-06 16:56:47,342 - INFO - Epoch 24: val_loss=4.3451, val_acc=0.00%
2025-02-06 16:56:47,346 - INFO - Epoch 24: EPOCH_AVG_TRAIN_LOSS=0.5796
2025-02-06 16:56:47,348 - INFO - #################### Training epoch 25 ####################
2025-02-06 16:56:47,348 - INFO - Current Learning Rate: 1.250000e-04
2025-02-06 16:56:47,644 - INFO - Epoch 25: train_loss=0.5760
2025-02-06 16:56:47,887 - INFO - Epoch 25: train_loss=0.5375
2025-02-06 16:56:48,178 - INFO - Epoch 25: val_loss=4.4379, val_acc=33.33%
2025-02-06 16:56:48,182 - INFO - Epoch 25: EPOCH_AVG_TRAIN_LOSS=0.5568
2025-02-06 16:56:48,186 - INFO - #################### Training epoch 26 ####################
2025-02-06 16:56:48,186 - INFO - Current Learning Rate: 1.250000e-04
2025-02-06 16:56:48,478 - INFO - Epoch 26: train_loss=0.6090
2025-02-06 16:56:48,722 - INFO - Epoch 26: train_loss=0.4801
2025-02-06 16:56:49,013 - INFO - Epoch 26: val_loss=4.3905, val_acc=33.33%
2025-02-06 16:56:49,016 - INFO - Epoch 26: EPOCH_AVG_TRAIN_LOSS=0.5445
2025-02-06 16:56:49,018 - INFO - #################### Training epoch 27 ####################
2025-02-06 16:56:49,018 - INFO - Current Learning Rate: 1.250000e-04
2025-02-06 16:56:49,312 - INFO - Epoch 27: train_loss=0.6033
2025-02-06 16:56:49,555 - INFO - Epoch 27: train_loss=0.5524
2025-02-06 16:56:49,846 - INFO - Epoch 27: val_loss=4.3646, val_acc=33.33%
2025-02-06 16:56:49,850 - INFO - Epoch 27: EPOCH_AVG_TRAIN_LOSS=0.5779
2025-02-06 16:56:49,852 - INFO - #################### Training epoch 28 ####################
2025-02-06 16:56:49,852 - INFO - Current Learning Rate: 6.250000e-05
2025-02-06 16:56:50,146 - INFO - Epoch 28: train_loss=0.5454
2025-02-06 16:56:50,390 - INFO - Epoch 28: train_loss=0.6425
2025-02-06 16:56:50,681 - INFO - Epoch 28: val_loss=4.3776, val_acc=33.33%
2025-02-06 16:56:50,684 - INFO - Epoch 28: EPOCH_AVG_TRAIN_LOSS=0.5939
2025-02-06 16:56:50,687 - INFO - #################### Training epoch 29 ####################
2025-02-06 16:56:50,687 - INFO - Current Learning Rate: 6.250000e-05
2025-02-06 16:56:50,981 - INFO - Epoch 29: train_loss=0.5254
2025-02-06 16:56:51,225 - INFO - Epoch 29: train_loss=0.7486
2025-02-06 16:56:51,517 - INFO - Epoch 29: val_loss=4.3854, val_acc=33.33%
2025-02-06 16:56:51,521 - INFO - Epoch 29: EPOCH_AVG_TRAIN_LOSS=0.6370
2025-02-06 16:56:51,523 - INFO - #################### Training epoch 30 ####################
2025-02-06 16:56:51,523 - INFO - Current Learning Rate: 6.250000e-05
2025-02-06 16:56:51,816 - INFO - Epoch 30: train_loss=0.7197
2025-02-06 16:56:52,059 - INFO - Epoch 30: train_loss=0.3413
2025-02-06 16:56:52,351 - INFO - Epoch 30: val_loss=4.3702, val_acc=33.33%
2025-02-06 16:56:52,355 - INFO - Epoch 30: EPOCH_AVG_TRAIN_LOSS=0.5305
2025-02-06 16:56:52,357 - INFO - #################### Training epoch 31 ####################
2025-02-06 16:56:52,357 - INFO - Current Learning Rate: 6.250000e-05
2025-02-06 16:56:52,652 - INFO - Epoch 31: train_loss=0.5848
2025-02-06 16:56:52,895 - INFO - Epoch 31: train_loss=0.5873
2025-02-06 16:56:53,187 - INFO - Epoch 31: val_loss=4.3863, val_acc=33.33%
2025-02-06 16:56:53,191 - INFO - Epoch 31: EPOCH_AVG_TRAIN_LOSS=0.5860
2025-02-06 16:56:53,193 - INFO - #################### Training epoch 32 ####################
2025-02-06 16:56:53,193 - INFO - Current Learning Rate: 6.250000e-05
2025-02-06 16:56:53,486 - INFO - Epoch 32: train_loss=0.6579
2025-02-06 16:56:53,730 - INFO - Epoch 32: train_loss=0.4296
2025-02-06 16:56:54,024 - INFO - Epoch 32: val_loss=4.3446, val_acc=33.33%
2025-02-06 16:56:54,027 - INFO - Epoch 32: EPOCH_AVG_TRAIN_LOSS=0.5438
2025-02-06 16:56:54,029 - INFO - #################### Training epoch 33 ####################
2025-02-06 16:56:54,029 - INFO - Current Learning Rate: 6.250000e-05
2025-02-06 16:56:54,322 - INFO - Epoch 33: train_loss=0.5558
2025-02-06 16:56:54,565 - INFO - Epoch 33: train_loss=0.6141
2025-02-06 16:56:54,858 - INFO - Epoch 33: val_loss=4.3804, val_acc=33.33%
2025-02-06 16:56:54,861 - INFO - Epoch 33: EPOCH_AVG_TRAIN_LOSS=0.5849
2025-02-06 16:56:54,863 - INFO - #################### Training epoch 34 ####################
2025-02-06 16:56:54,863 - INFO - Current Learning Rate: 6.250000e-05
2025-02-06 16:56:55,156 - INFO - Epoch 34: train_loss=0.4773
2025-02-06 16:56:55,400 - INFO - Epoch 34: train_loss=0.7378
2025-02-06 16:56:55,688 - INFO - Epoch 34: val_loss=4.3743, val_acc=33.33%
2025-02-06 16:56:55,691 - INFO - Epoch 34: EPOCH_AVG_TRAIN_LOSS=0.6075
2025-02-06 16:56:55,693 - INFO - #################### Training epoch 35 ####################
2025-02-06 16:56:55,693 - INFO - Current Learning Rate: 3.125000e-05
2025-02-06 16:56:55,987 - INFO - Epoch 35: train_loss=0.6166
2025-02-06 16:56:56,231 - INFO - Epoch 35: train_loss=0.4297
2025-02-06 16:56:56,522 - INFO - Epoch 35: val_loss=4.3945, val_acc=33.33%
2025-02-06 16:56:56,526 - INFO - Epoch 35: EPOCH_AVG_TRAIN_LOSS=0.5231
2025-02-06 16:56:56,528 - INFO - #################### Training epoch 36 ####################
2025-02-06 16:56:56,528 - INFO - Current Learning Rate: 3.125000e-05
2025-02-06 16:56:56,821 - INFO - Epoch 36: train_loss=0.5212
2025-02-06 16:56:57,064 - INFO - Epoch 36: train_loss=0.6168
2025-02-06 16:56:57,354 - INFO - Epoch 36: val_loss=4.3536, val_acc=33.33%
2025-02-06 16:56:57,358 - INFO - Epoch 36: EPOCH_AVG_TRAIN_LOSS=0.5690
2025-02-06 16:56:57,360 - INFO - #################### Training epoch 37 ####################
2025-02-06 16:56:57,360 - INFO - Current Learning Rate: 3.125000e-05
2025-02-06 16:56:57,654 - INFO - Epoch 37: train_loss=0.6210
2025-02-06 16:56:57,898 - INFO - Epoch 37: train_loss=0.4009
2025-02-06 16:56:58,188 - INFO - Epoch 37: val_loss=4.3648, val_acc=33.33%
2025-02-06 16:56:58,192 - INFO - Epoch 37: EPOCH_AVG_TRAIN_LOSS=0.5110
2025-02-06 16:56:58,194 - INFO - #################### Training epoch 38 ####################
2025-02-06 16:56:58,194 - INFO - Current Learning Rate: 3.125000e-05
2025-02-06 16:56:58,488 - INFO - Epoch 38: train_loss=0.5268
2025-02-06 16:56:58,732 - INFO - Epoch 38: train_loss=0.5822
2025-02-06 16:56:59,019 - INFO - Epoch 38: val_loss=4.3778, val_acc=33.33%
2025-02-06 16:56:59,023 - INFO - Epoch 38: EPOCH_AVG_TRAIN_LOSS=0.5545
2025-02-06 16:56:59,025 - INFO - #################### Training epoch 39 ####################
2025-02-06 16:56:59,025 - INFO - Current Learning Rate: 1.562500e-05
2025-02-06 16:56:59,319 - INFO - Epoch 39: train_loss=0.6148
2025-02-06 16:56:59,563 - INFO - Epoch 39: train_loss=0.4053
2025-02-06 16:56:59,856 - INFO - Epoch 39: val_loss=4.3541, val_acc=33.33%
2025-02-06 16:56:59,860 - INFO - Epoch 39: EPOCH_AVG_TRAIN_LOSS=0.5100
2025-02-06 16:56:59,862 - INFO - #################### Training epoch 40 ####################
2025-02-06 16:56:59,862 - INFO - Current Learning Rate: 1.562500e-05
2025-02-06 16:57:00,157 - INFO - Epoch 40: train_loss=0.5917
2025-02-06 16:57:00,401 - INFO - Epoch 40: train_loss=0.4521
2025-02-06 16:57:00,692 - INFO - Epoch 40: val_loss=4.3952, val_acc=33.33%
2025-02-06 16:57:00,696 - INFO - Epoch 40: EPOCH_AVG_TRAIN_LOSS=0.5219
2025-02-06 16:57:00,698 - INFO - #################### Training epoch 41 ####################
2025-02-06 16:57:00,698 - INFO - Current Learning Rate: 1.562500e-05
2025-02-06 16:57:00,993 - INFO - Epoch 41: train_loss=0.5532
2025-02-06 16:57:01,236 - INFO - Epoch 41: train_loss=0.5349
2025-02-06 16:57:01,526 - INFO - Epoch 41: val_loss=4.3628, val_acc=33.33%
2025-02-06 16:57:01,530 - INFO - Epoch 41: EPOCH_AVG_TRAIN_LOSS=0.5441
2025-02-06 16:57:01,532 - INFO - #################### Training epoch 42 ####################
2025-02-06 16:57:01,532 - INFO - Current Learning Rate: 1.562500e-05
2025-02-06 16:57:01,827 - INFO - Epoch 42: train_loss=0.5795
2025-02-06 16:57:02,071 - INFO - Epoch 42: train_loss=0.4570
2025-02-06 16:57:02,363 - INFO - Epoch 42: val_loss=4.3918, val_acc=33.33%
2025-02-06 16:57:02,367 - INFO - Epoch 42: EPOCH_AVG_TRAIN_LOSS=0.5182
2025-02-06 16:57:02,369 - INFO - #################### Training epoch 43 ####################
2025-02-06 16:57:02,369 - INFO - Current Learning Rate: 7.812500e-06
2025-02-06 16:57:02,663 - INFO - Epoch 43: train_loss=0.6351
2025-02-06 16:57:02,906 - INFO - Epoch 43: train_loss=0.3480
2025-02-06 16:57:03,195 - INFO - Epoch 43: val_loss=4.3989, val_acc=0.00%
2025-02-06 16:57:03,199 - INFO - Epoch 43: EPOCH_AVG_TRAIN_LOSS=0.4916
2025-02-06 16:57:03,201 - INFO - #################### Training epoch 44 ####################
2025-02-06 16:57:03,201 - INFO - Current Learning Rate: 7.812500e-06
2025-02-06 16:57:03,496 - INFO - Epoch 44: train_loss=0.5333
2025-02-06 16:57:03,740 - INFO - Epoch 44: train_loss=0.5446
2025-02-06 16:57:04,028 - INFO - Epoch 44: val_loss=4.3985, val_acc=0.00%
2025-02-06 16:57:04,032 - INFO - Epoch 44: EPOCH_AVG_TRAIN_LOSS=0.5390
2025-02-06 16:57:04,034 - INFO - #################### Training epoch 45 ####################
2025-02-06 16:57:04,034 - INFO - Current Learning Rate: 7.812500e-06
2025-02-06 16:57:04,328 - INFO - Epoch 45: train_loss=0.5638
2025-02-06 16:57:04,572 - INFO - Epoch 45: train_loss=0.4966
2025-02-06 16:57:04,861 - INFO - Epoch 45: val_loss=4.3406, val_acc=33.33%
2025-02-06 16:57:04,865 - INFO - Epoch 45: EPOCH_AVG_TRAIN_LOSS=0.5302
2025-02-06 16:57:04,867 - INFO - #################### Training epoch 46 ####################
2025-02-06 16:57:04,867 - INFO - Current Learning Rate: 7.812500e-06
2025-02-06 16:57:05,159 - INFO - Epoch 46: train_loss=0.5507
2025-02-06 16:57:05,403 - INFO - Epoch 46: train_loss=0.5121
2025-02-06 16:57:05,695 - INFO - Epoch 46: val_loss=4.3737, val_acc=0.00%
2025-02-06 16:57:05,698 - INFO - Epoch 46: EPOCH_AVG_TRAIN_LOSS=0.5314
2025-02-06 16:57:05,700 - INFO - #################### Training epoch 47 ####################
2025-02-06 16:57:05,700 - INFO - Current Learning Rate: 3.906250e-06
2025-02-06 16:57:05,996 - INFO - Epoch 47: train_loss=0.5846
2025-02-06 16:57:06,239 - INFO - Epoch 47: train_loss=0.4401
2025-02-06 16:57:06,530 - INFO - Epoch 47: val_loss=4.3811, val_acc=0.00%
2025-02-06 16:57:06,533 - INFO - Epoch 47: EPOCH_AVG_TRAIN_LOSS=0.5124
2025-02-06 16:57:06,535 - INFO - #################### Training epoch 48 ####################
2025-02-06 16:57:06,536 - INFO - Current Learning Rate: 3.906250e-06
2025-02-06 16:57:06,828 - INFO - Epoch 48: train_loss=0.6482
2025-02-06 16:57:07,072 - INFO - Epoch 48: train_loss=0.3197
2025-02-06 16:57:07,363 - INFO - Epoch 48: val_loss=4.3956, val_acc=0.00%
2025-02-06 16:57:07,367 - INFO - Epoch 48: EPOCH_AVG_TRAIN_LOSS=0.4840
2025-02-06 16:57:07,369 - INFO - #################### Training epoch 49 ####################
2025-02-06 16:57:07,369 - INFO - Current Learning Rate: 3.906250e-06
2025-02-06 16:57:07,665 - INFO - Epoch 49: train_loss=0.5168
2025-02-06 16:57:07,909 - INFO - Epoch 49: train_loss=0.5754
2025-02-06 16:57:08,201 - INFO - Epoch 49: val_loss=4.3752, val_acc=0.00%
2025-02-06 16:57:08,204 - INFO - Epoch 49: EPOCH_AVG_TRAIN_LOSS=0.5461
2025-02-06 16:57:08,207 - INFO - #################### Training epoch 50 ####################
2025-02-06 16:57:08,207 - INFO - Current Learning Rate: 3.906250e-06
2025-02-06 16:57:08,500 - INFO - Epoch 50: train_loss=0.5055
2025-02-06 16:57:08,744 - INFO - Epoch 50: train_loss=0.6117
2025-02-06 16:57:09,034 - INFO - Epoch 50: val_loss=4.4011, val_acc=0.00%
2025-02-06 16:57:09,037 - INFO - Epoch 50: EPOCH_AVG_TRAIN_LOSS=0.5586
2025-02-06 16:57:09,040 - INFO - #################### Training epoch 51 ####################
2025-02-06 16:57:09,040 - INFO - Current Learning Rate: 3.906250e-06
2025-02-06 16:57:09,332 - INFO - Epoch 51: train_loss=0.4924
2025-02-06 16:57:09,577 - INFO - Epoch 51: train_loss=0.6471
2025-02-06 16:57:09,868 - INFO - Epoch 51: val_loss=4.3990, val_acc=0.00%
2025-02-06 16:57:09,871 - INFO - Epoch 51: EPOCH_AVG_TRAIN_LOSS=0.5697
2025-02-06 16:57:09,874 - INFO - #################### Training epoch 52 ####################
2025-02-06 16:57:09,874 - INFO - Current Learning Rate: 3.906250e-06
2025-02-06 16:57:10,169 - INFO - Epoch 52: train_loss=0.3324
2025-02-06 16:57:10,414 - INFO - Epoch 52: train_loss=0.9586
2025-02-06 16:57:10,704 - INFO - Epoch 52: val_loss=4.4025, val_acc=0.00%
2025-02-06 16:57:10,708 - INFO - Epoch 52: EPOCH_AVG_TRAIN_LOSS=0.6455
2025-02-06 16:57:10,710 - INFO - #################### Training epoch 53 ####################
2025-02-06 16:57:10,710 - INFO - Current Learning Rate: 1.953125e-06
2025-02-06 16:57:11,004 - INFO - Epoch 53: train_loss=0.6755
2025-02-06 16:57:11,248 - INFO - Epoch 53: train_loss=0.2709
2025-02-06 16:57:11,538 - INFO - Epoch 53: val_loss=4.3879, val_acc=0.00%
2025-02-06 16:57:11,542 - INFO - Epoch 53: EPOCH_AVG_TRAIN_LOSS=0.4732
2025-02-06 16:57:11,544 - INFO - #################### Training epoch 54 ####################
2025-02-06 16:57:11,544 - INFO - Current Learning Rate: 1.953125e-06
2025-02-06 16:57:11,838 - INFO - Epoch 54: train_loss=0.5528
2025-02-06 16:57:12,082 - INFO - Epoch 54: train_loss=0.5175
2025-02-06 16:57:12,374 - INFO - Epoch 54: val_loss=4.3541, val_acc=0.00%
2025-02-06 16:57:12,377 - INFO - Epoch 54: EPOCH_AVG_TRAIN_LOSS=0.5352
2025-02-06 16:57:12,379 - INFO - #################### Training epoch 55 ####################
2025-02-06 16:57:12,379 - INFO - Current Learning Rate: 1.953125e-06
2025-02-06 16:57:12,673 - INFO - Epoch 55: train_loss=0.4553
2025-02-06 16:57:12,918 - INFO - Epoch 55: train_loss=0.7083
2025-02-06 16:57:13,210 - INFO - Epoch 55: val_loss=4.4162, val_acc=0.00%
2025-02-06 16:57:13,214 - INFO - Epoch 55: EPOCH_AVG_TRAIN_LOSS=0.5818
2025-02-06 16:57:13,216 - INFO - #################### Training epoch 56 ####################
2025-02-06 16:57:13,216 - INFO - Current Learning Rate: 1.953125e-06
2025-02-06 16:57:13,509 - INFO - Epoch 56: train_loss=0.5808
2025-02-06 16:57:13,754 - INFO - Epoch 56: train_loss=0.4633
2025-02-06 16:57:14,046 - INFO - Epoch 56: val_loss=4.4268, val_acc=0.00%
2025-02-06 16:57:14,050 - INFO - Epoch 56: EPOCH_AVG_TRAIN_LOSS=0.5220
2025-02-06 16:57:14,052 - INFO - #################### Training epoch 57 ####################
2025-02-06 16:57:14,052 - INFO - Current Learning Rate: 1.953125e-06
2025-02-06 16:57:14,346 - INFO - Epoch 57: train_loss=0.4836
2025-02-06 16:57:14,591 - INFO - Epoch 57: train_loss=0.6441
2025-02-06 16:57:14,879 - INFO - Epoch 57: val_loss=4.4156, val_acc=0.00%
2025-02-06 16:57:14,883 - INFO - Epoch 57: EPOCH_AVG_TRAIN_LOSS=0.5638
2025-02-06 16:57:14,885 - INFO - #################### Training epoch 58 ####################
2025-02-06 16:57:14,885 - INFO - Current Learning Rate: 9.765625e-07
2025-02-06 16:57:15,182 - INFO - Epoch 58: train_loss=0.6044
2025-02-06 16:57:15,427 - INFO - Epoch 58: train_loss=0.4112
2025-02-06 16:57:15,719 - INFO - Epoch 58: val_loss=4.4186, val_acc=0.00%
2025-02-06 16:57:15,723 - INFO - Epoch 58: EPOCH_AVG_TRAIN_LOSS=0.5078
2025-02-06 16:57:15,725 - INFO - #################### Training epoch 59 ####################
2025-02-06 16:57:15,725 - INFO - Current Learning Rate: 9.765625e-07
2025-02-06 16:57:16,020 - INFO - Epoch 59: train_loss=0.6251
2025-02-06 16:57:16,265 - INFO - Epoch 59: train_loss=0.3632
2025-02-06 16:57:16,556 - INFO - Epoch 59: val_loss=4.3771, val_acc=0.00%
2025-02-06 16:57:16,560 - INFO - Epoch 59: EPOCH_AVG_TRAIN_LOSS=0.4941
2025-02-06 16:57:16,562 - INFO - #################### Training epoch 60 ####################
2025-02-06 16:57:16,562 - INFO - Current Learning Rate: 9.765625e-07
2025-02-06 16:57:16,857 - INFO - Epoch 60: train_loss=0.3836
2025-02-06 16:57:17,101 - INFO - Epoch 60: train_loss=0.8443
2025-02-06 16:57:17,392 - INFO - Epoch 60: val_loss=4.3831, val_acc=0.00%
2025-02-06 16:57:17,396 - INFO - Epoch 60: EPOCH_AVG_TRAIN_LOSS=0.6140
2025-02-06 16:57:17,398 - INFO - #################### Training epoch 61 ####################
2025-02-06 16:57:17,398 - INFO - Current Learning Rate: 9.765625e-07
2025-02-06 16:57:17,695 - INFO - Epoch 61: train_loss=0.3786
2025-02-06 16:57:17,940 - INFO - Epoch 61: train_loss=0.8734
2025-02-06 16:57:18,233 - INFO - Epoch 61: val_loss=4.3866, val_acc=0.00%
2025-02-06 16:57:18,236 - INFO - Epoch 61: EPOCH_AVG_TRAIN_LOSS=0.6260
2025-02-06 16:57:18,239 - INFO - #################### Training epoch 62 ####################
2025-02-06 16:57:18,239 - INFO - Current Learning Rate: 4.882813e-07
2025-02-06 16:57:18,535 - INFO - Epoch 62: train_loss=0.5493
2025-02-06 16:57:18,780 - INFO - Epoch 62: train_loss=0.5175
2025-02-06 16:57:19,073 - INFO - Epoch 62: val_loss=4.4171, val_acc=0.00%
2025-02-06 16:57:19,077 - INFO - Epoch 62: EPOCH_AVG_TRAIN_LOSS=0.5334
2025-02-06 16:57:19,079 - INFO - #################### Training epoch 63 ####################
2025-02-06 16:57:19,079 - INFO - Current Learning Rate: 4.882813e-07
2025-02-06 16:57:19,372 - INFO - Epoch 63: train_loss=0.6244
2025-02-06 16:57:19,617 - INFO - Epoch 63: train_loss=0.3589
2025-02-06 16:57:19,910 - INFO - Epoch 63: val_loss=4.3729, val_acc=0.00%
2025-02-06 16:57:19,913 - INFO - Epoch 63: EPOCH_AVG_TRAIN_LOSS=0.4917
2025-02-06 16:57:19,915 - INFO - #################### Training epoch 64 ####################
2025-02-06 16:57:19,915 - INFO - Current Learning Rate: 4.882813e-07
2025-02-06 16:57:20,209 - INFO - Epoch 64: train_loss=0.5759
2025-02-06 16:57:20,454 - INFO - Epoch 64: train_loss=0.4774
2025-02-06 16:57:20,748 - INFO - Epoch 64: val_loss=4.4136, val_acc=0.00%
2025-02-06 16:57:20,751 - INFO - Epoch 64: EPOCH_AVG_TRAIN_LOSS=0.5266
2025-02-06 16:57:20,753 - INFO - #################### Training epoch 65 ####################
2025-02-06 16:57:20,753 - INFO - Current Learning Rate: 4.882813e-07
2025-02-06 16:57:21,049 - INFO - Epoch 65: train_loss=0.6468
2025-02-06 16:57:21,295 - INFO - Epoch 65: train_loss=0.3280
2025-02-06 16:57:21,585 - INFO - Epoch 65: val_loss=4.3744, val_acc=33.33%
2025-02-06 16:57:21,589 - INFO - Epoch 65: EPOCH_AVG_TRAIN_LOSS=0.4874
2025-02-06 16:57:21,591 - INFO - #################### Training epoch 66 ####################
2025-02-06 16:57:21,591 - INFO - Current Learning Rate: 2.441406e-07
2025-02-06 16:57:21,887 - INFO - Epoch 66: train_loss=0.6062
2025-02-06 16:57:22,132 - INFO - Epoch 66: train_loss=0.4011
2025-02-06 16:57:22,422 - INFO - Epoch 66: val_loss=4.4292, val_acc=0.00%
2025-02-06 16:57:22,426 - INFO - Epoch 66: EPOCH_AVG_TRAIN_LOSS=0.5036
2025-02-06 16:57:22,428 - INFO - #################### Training epoch 67 ####################
2025-02-06 16:57:22,428 - INFO - Current Learning Rate: 2.441406e-07
2025-02-06 16:57:22,722 - INFO - Epoch 67: train_loss=0.4903
2025-02-06 16:57:22,966 - INFO - Epoch 67: train_loss=0.6449
2025-02-06 16:57:23,259 - INFO - Epoch 67: val_loss=4.3766, val_acc=0.00%
2025-02-06 16:57:23,263 - INFO - Epoch 67: EPOCH_AVG_TRAIN_LOSS=0.5676
2025-02-06 16:57:23,265 - INFO - #################### Training epoch 68 ####################
2025-02-06 16:57:23,265 - INFO - Current Learning Rate: 2.441406e-07
2025-02-06 16:57:23,560 - INFO - Epoch 68: train_loss=0.6075
2025-02-06 16:57:23,805 - INFO - Epoch 68: train_loss=0.3951
2025-02-06 16:57:24,097 - INFO - Epoch 68: val_loss=4.4182, val_acc=0.00%
2025-02-06 16:57:24,100 - INFO - Epoch 68: EPOCH_AVG_TRAIN_LOSS=0.5013
2025-02-06 16:57:24,103 - INFO - #################### Training epoch 69 ####################
2025-02-06 16:57:24,103 - INFO - Current Learning Rate: 2.441406e-07
2025-02-06 16:57:24,399 - INFO - Epoch 69: train_loss=0.6039
2025-02-06 16:57:24,644 - INFO - Epoch 69: train_loss=0.4122
2025-02-06 16:57:24,936 - INFO - Epoch 69: val_loss=4.3581, val_acc=33.33%
2025-02-06 16:57:24,939 - INFO - Epoch 69: EPOCH_AVG_TRAIN_LOSS=0.5080
2025-02-06 16:57:24,942 - INFO - #################### Training epoch 70 ####################
2025-02-06 16:57:24,942 - INFO - Current Learning Rate: 1.220703e-07
2025-02-06 16:57:25,236 - INFO - Epoch 70: train_loss=0.3732
2025-02-06 16:57:25,481 - INFO - Epoch 70: train_loss=0.8778
2025-02-06 16:57:25,771 - INFO - Epoch 70: val_loss=4.3865, val_acc=0.00%
2025-02-06 16:57:25,775 - INFO - Epoch 70: EPOCH_AVG_TRAIN_LOSS=0.6255
2025-02-06 16:57:25,777 - INFO - #################### Training epoch 71 ####################
2025-02-06 16:57:25,777 - INFO - Current Learning Rate: 1.220703e-07
2025-02-06 16:57:26,072 - INFO - Epoch 71: train_loss=0.4247
2025-02-06 16:57:26,318 - INFO - Epoch 71: train_loss=0.7725
2025-02-06 16:57:26,612 - INFO - Epoch 71: val_loss=4.4468, val_acc=0.00%
2025-02-06 16:57:26,615 - INFO - Epoch 71: EPOCH_AVG_TRAIN_LOSS=0.5986
2025-02-06 16:57:26,618 - INFO - #################### Training epoch 72 ####################
2025-02-06 16:57:26,618 - INFO - Current Learning Rate: 1.220703e-07
2025-02-06 16:57:26,912 - INFO - Epoch 72: train_loss=0.5319
2025-02-06 16:57:27,157 - INFO - Epoch 72: train_loss=0.5413
2025-02-06 16:57:27,450 - INFO - Epoch 72: val_loss=4.4410, val_acc=0.00%
2025-02-06 16:57:27,453 - INFO - Epoch 72: EPOCH_AVG_TRAIN_LOSS=0.5366
2025-02-06 16:57:27,455 - INFO - #################### Training epoch 73 ####################
2025-02-06 16:57:27,455 - INFO - Current Learning Rate: 1.220703e-07
2025-02-06 16:57:27,751 - INFO - Epoch 73: train_loss=0.6255
2025-02-06 16:57:27,996 - INFO - Epoch 73: train_loss=0.3550
2025-02-06 16:57:28,289 - INFO - Epoch 73: val_loss=4.4285, val_acc=0.00%
2025-02-06 16:57:28,293 - INFO - Epoch 73: EPOCH_AVG_TRAIN_LOSS=0.4902
2025-02-06 16:57:28,295 - INFO - #################### Training epoch 74 ####################
2025-02-06 16:57:28,295 - INFO - Current Learning Rate: 6.103516e-08
2025-02-06 16:57:28,589 - INFO - Epoch 74: train_loss=0.3566
2025-02-06 16:57:28,834 - INFO - Epoch 74: train_loss=0.9086
2025-02-06 16:57:29,127 - INFO - Epoch 74: val_loss=4.3738, val_acc=0.00%
2025-02-06 16:57:29,131 - INFO - Epoch 74: EPOCH_AVG_TRAIN_LOSS=0.6326
2025-02-06 16:57:29,133 - INFO - #################### Training epoch 75 ####################
2025-02-06 16:57:29,133 - INFO - Current Learning Rate: 6.103516e-08
2025-02-06 16:57:29,429 - INFO - Epoch 75: train_loss=0.4641
2025-02-06 16:57:29,674 - INFO - Epoch 75: train_loss=0.6762
2025-02-06 16:57:29,966 - INFO - Epoch 75: val_loss=4.3938, val_acc=0.00%
2025-02-06 16:57:29,970 - INFO - Epoch 75: EPOCH_AVG_TRAIN_LOSS=0.5702
2025-02-06 16:57:29,972 - INFO - #################### Training epoch 76 ####################
2025-02-06 16:57:29,972 - INFO - Current Learning Rate: 6.103516e-08
2025-02-06 16:57:30,266 - INFO - Epoch 76: train_loss=0.6124
2025-02-06 16:57:30,511 - INFO - Epoch 76: train_loss=0.3948
2025-02-06 16:57:30,802 - INFO - Epoch 76: val_loss=4.3975, val_acc=0.00%
2025-02-06 16:57:30,806 - INFO - Epoch 76: EPOCH_AVG_TRAIN_LOSS=0.5036
2025-02-06 16:57:30,808 - INFO - #################### Training epoch 77 ####################
2025-02-06 16:57:30,808 - INFO - Current Learning Rate: 6.103516e-08
2025-02-06 16:57:31,096 - INFO - Epoch 77: train_loss=0.5256
2025-02-06 16:57:31,341 - INFO - Epoch 77: train_loss=0.5748
2025-02-06 16:57:31,634 - INFO - Epoch 77: val_loss=4.3890, val_acc=0.00%
2025-02-06 16:57:31,638 - INFO - Epoch 77: EPOCH_AVG_TRAIN_LOSS=0.5502
2025-02-06 16:57:31,640 - INFO - #################### Training epoch 78 ####################
2025-02-06 16:57:31,640 - INFO - Current Learning Rate: 3.051758e-08
2025-02-06 16:57:31,935 - INFO - Epoch 78: train_loss=0.5810
2025-02-06 16:57:32,180 - INFO - Epoch 78: train_loss=0.4548
2025-02-06 16:57:32,470 - INFO - Epoch 78: val_loss=4.3604, val_acc=33.33%
2025-02-06 16:57:32,474 - INFO - Epoch 78: EPOCH_AVG_TRAIN_LOSS=0.5179
2025-02-06 16:57:32,476 - INFO - #################### Training epoch 79 ####################
2025-02-06 16:57:32,476 - INFO - Current Learning Rate: 3.051758e-08
2025-02-06 16:57:32,773 - INFO - Epoch 79: train_loss=0.5539
2025-02-06 16:57:33,018 - INFO - Epoch 79: train_loss=0.5102
2025-02-06 16:57:33,310 - INFO - Epoch 79: val_loss=4.3200, val_acc=0.00%
2025-02-06 16:57:33,313 - INFO - Epoch 79: EPOCH_AVG_TRAIN_LOSS=0.5321
2025-02-06 16:57:33,315 - INFO - #################### Training epoch 80 ####################
2025-02-06 16:57:33,315 - INFO - Current Learning Rate: 3.051758e-08
2025-02-06 16:57:33,609 - INFO - Epoch 80: train_loss=0.5476
2025-02-06 16:57:33,854 - INFO - Epoch 80: train_loss=0.5260
2025-02-06 16:57:34,149 - INFO - Epoch 80: val_loss=4.4105, val_acc=0.00%
2025-02-06 16:57:34,152 - INFO - Epoch 80: EPOCH_AVG_TRAIN_LOSS=0.5368
2025-02-06 16:57:34,154 - INFO - #################### Training epoch 81 ####################
2025-02-06 16:57:34,154 - INFO - Current Learning Rate: 3.051758e-08
2025-02-06 16:57:34,450 - INFO - Epoch 81: train_loss=0.6225
2025-02-06 16:57:34,695 - INFO - Epoch 81: train_loss=0.3785
2025-02-06 16:57:34,987 - INFO - Epoch 81: val_loss=4.3750, val_acc=0.00%
2025-02-06 16:57:34,990 - INFO - Epoch 81: EPOCH_AVG_TRAIN_LOSS=0.5005
2025-02-06 16:57:34,993 - INFO - #################### Training epoch 82 ####################
2025-02-06 16:57:34,993 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:35,288 - INFO - Epoch 82: train_loss=0.5805
2025-02-06 16:57:35,533 - INFO - Epoch 82: train_loss=0.4532
2025-02-06 16:57:35,825 - INFO - Epoch 82: val_loss=4.4098, val_acc=0.00%
2025-02-06 16:57:35,829 - INFO - Epoch 82: EPOCH_AVG_TRAIN_LOSS=0.5168
2025-02-06 16:57:35,831 - INFO - #################### Training epoch 83 ####################
2025-02-06 16:57:35,831 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:36,125 - INFO - Epoch 83: train_loss=0.5792
2025-02-06 16:57:36,371 - INFO - Epoch 83: train_loss=0.4464
2025-02-06 16:57:36,662 - INFO - Epoch 83: val_loss=4.3846, val_acc=0.00%
2025-02-06 16:57:36,666 - INFO - Epoch 83: EPOCH_AVG_TRAIN_LOSS=0.5128
2025-02-06 16:57:36,668 - INFO - #################### Training epoch 84 ####################
2025-02-06 16:57:36,668 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:36,963 - INFO - Epoch 84: train_loss=0.5324
2025-02-06 16:57:37,209 - INFO - Epoch 84: train_loss=0.5463
2025-02-06 16:57:37,499 - INFO - Epoch 84: val_loss=4.4189, val_acc=0.00%
2025-02-06 16:57:37,502 - INFO - Epoch 84: EPOCH_AVG_TRAIN_LOSS=0.5393
2025-02-06 16:57:37,504 - INFO - #################### Training epoch 85 ####################
2025-02-06 16:57:37,504 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:37,799 - INFO - Epoch 85: train_loss=0.4226
2025-02-06 16:57:38,043 - INFO - Epoch 85: train_loss=0.7592
2025-02-06 16:57:38,332 - INFO - Epoch 85: val_loss=4.4199, val_acc=0.00%
2025-02-06 16:57:38,335 - INFO - Epoch 85: EPOCH_AVG_TRAIN_LOSS=0.5909
2025-02-06 16:57:38,337 - INFO - #################### Training epoch 86 ####################
2025-02-06 16:57:38,337 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:38,633 - INFO - Epoch 86: train_loss=0.5008
2025-02-06 16:57:38,877 - INFO - Epoch 86: train_loss=0.6141
2025-02-06 16:57:39,168 - INFO - Epoch 86: val_loss=4.4272, val_acc=0.00%
2025-02-06 16:57:39,171 - INFO - Epoch 86: EPOCH_AVG_TRAIN_LOSS=0.5575
2025-02-06 16:57:39,174 - INFO - #################### Training epoch 87 ####################
2025-02-06 16:57:39,174 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:39,467 - INFO - Epoch 87: train_loss=0.5985
2025-02-06 16:57:39,712 - INFO - Epoch 87: train_loss=0.4138
2025-02-06 16:57:40,002 - INFO - Epoch 87: val_loss=4.4441, val_acc=0.00%
2025-02-06 16:57:40,006 - INFO - Epoch 87: EPOCH_AVG_TRAIN_LOSS=0.5062
2025-02-06 16:57:40,008 - INFO - #################### Training epoch 88 ####################
2025-02-06 16:57:40,008 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:40,302 - INFO - Epoch 88: train_loss=0.4134
2025-02-06 16:57:40,548 - INFO - Epoch 88: train_loss=0.7923
2025-02-06 16:57:40,836 - INFO - Epoch 88: val_loss=4.4018, val_acc=0.00%
2025-02-06 16:57:40,840 - INFO - Epoch 88: EPOCH_AVG_TRAIN_LOSS=0.6028
2025-02-06 16:57:40,842 - INFO - #################### Training epoch 89 ####################
2025-02-06 16:57:40,842 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:41,136 - INFO - Epoch 89: train_loss=0.5371
2025-02-06 16:57:41,380 - INFO - Epoch 89: train_loss=0.5486
2025-02-06 16:57:41,670 - INFO - Epoch 89: val_loss=4.4016, val_acc=33.33%
2025-02-06 16:57:41,674 - INFO - Epoch 89: EPOCH_AVG_TRAIN_LOSS=0.5429
2025-02-06 16:57:41,676 - INFO - #################### Training epoch 90 ####################
2025-02-06 16:57:41,676 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:41,971 - INFO - Epoch 90: train_loss=0.6465
2025-02-06 16:57:42,216 - INFO - Epoch 90: train_loss=0.3266
2025-02-06 16:57:42,509 - INFO - Epoch 90: val_loss=4.4156, val_acc=0.00%
2025-02-06 16:57:42,512 - INFO - Epoch 90: EPOCH_AVG_TRAIN_LOSS=0.4865
2025-02-06 16:57:42,515 - INFO - #################### Training epoch 91 ####################
2025-02-06 16:57:42,515 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:42,810 - INFO - Epoch 91: train_loss=0.6341
2025-02-06 16:57:43,055 - INFO - Epoch 91: train_loss=0.3561
2025-02-06 16:57:43,344 - INFO - Epoch 91: val_loss=4.4041, val_acc=0.00%
2025-02-06 16:57:43,348 - INFO - Epoch 91: EPOCH_AVG_TRAIN_LOSS=0.4951
2025-02-06 16:57:43,350 - INFO - #################### Training epoch 92 ####################
2025-02-06 16:57:43,350 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:43,645 - INFO - Epoch 92: train_loss=0.5645
2025-02-06 16:57:43,890 - INFO - Epoch 92: train_loss=0.4960
2025-02-06 16:57:44,183 - INFO - Epoch 92: val_loss=4.4667, val_acc=0.00%
2025-02-06 16:57:44,187 - INFO - Epoch 92: EPOCH_AVG_TRAIN_LOSS=0.5302
2025-02-06 16:57:44,189 - INFO - #################### Training epoch 93 ####################
2025-02-06 16:57:44,189 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:44,483 - INFO - Epoch 93: train_loss=0.5600
2025-02-06 16:57:44,729 - INFO - Epoch 93: train_loss=0.5093
2025-02-06 16:57:45,023 - INFO - Epoch 93: val_loss=4.3976, val_acc=0.00%
2025-02-06 16:57:45,027 - INFO - Epoch 93: EPOCH_AVG_TRAIN_LOSS=0.5347
2025-02-06 16:57:45,029 - INFO - #################### Training epoch 94 ####################
2025-02-06 16:57:45,029 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:45,325 - INFO - Epoch 94: train_loss=0.5451
2025-02-06 16:57:45,571 - INFO - Epoch 94: train_loss=0.5353
2025-02-06 16:57:45,862 - INFO - Epoch 94: val_loss=4.3920, val_acc=0.00%
2025-02-06 16:57:45,865 - INFO - Epoch 94: EPOCH_AVG_TRAIN_LOSS=0.5402
2025-02-06 16:57:45,867 - INFO - #################### Training epoch 95 ####################
2025-02-06 16:57:45,868 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:46,164 - INFO - Epoch 95: train_loss=0.5932
2025-02-06 16:57:46,409 - INFO - Epoch 95: train_loss=0.4220
2025-02-06 16:57:46,700 - INFO - Epoch 95: val_loss=4.3821, val_acc=0.00%
2025-02-06 16:57:46,704 - INFO - Epoch 95: EPOCH_AVG_TRAIN_LOSS=0.5076
2025-02-06 16:57:46,706 - INFO - #################### Training epoch 96 ####################
2025-02-06 16:57:46,706 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:46,999 - INFO - Epoch 96: train_loss=0.6393
2025-02-06 16:57:47,245 - INFO - Epoch 96: train_loss=0.3348
2025-02-06 16:57:47,539 - INFO - Epoch 96: val_loss=4.3811, val_acc=0.00%
2025-02-06 16:57:47,543 - INFO - Epoch 96: EPOCH_AVG_TRAIN_LOSS=0.4870
2025-02-06 16:57:47,545 - INFO - #################### Training epoch 97 ####################
2025-02-06 16:57:47,546 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:47,842 - INFO - Epoch 97: train_loss=0.6423
2025-02-06 16:57:48,087 - INFO - Epoch 97: train_loss=0.3306
2025-02-06 16:57:48,379 - INFO - Epoch 97: val_loss=4.3841, val_acc=0.00%
2025-02-06 16:57:48,382 - INFO - Epoch 97: EPOCH_AVG_TRAIN_LOSS=0.4864
2025-02-06 16:57:48,385 - INFO - #################### Training epoch 98 ####################
2025-02-06 16:57:48,385 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:48,680 - INFO - Epoch 98: train_loss=0.5905
2025-02-06 16:57:48,926 - INFO - Epoch 98: train_loss=0.4385
2025-02-06 16:57:49,216 - INFO - Epoch 98: val_loss=4.3610, val_acc=33.33%
2025-02-06 16:57:49,220 - INFO - Epoch 98: EPOCH_AVG_TRAIN_LOSS=0.5145
2025-02-06 16:57:49,222 - INFO - #################### Training epoch 99 ####################
2025-02-06 16:57:49,222 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:49,516 - INFO - Epoch 99: train_loss=0.5604
2025-02-06 16:57:49,761 - INFO - Epoch 99: train_loss=0.4934
2025-02-06 16:57:50,055 - INFO - Epoch 99: val_loss=4.3819, val_acc=33.33%
2025-02-06 16:57:50,059 - INFO - Epoch 99: EPOCH_AVG_TRAIN_LOSS=0.5269
2025-02-06 16:57:50,061 - INFO - #################### Training epoch 100 ####################
2025-02-06 16:57:50,061 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:50,354 - INFO - Epoch 100: train_loss=0.4068
2025-02-06 16:57:50,600 - INFO - Epoch 100: train_loss=0.7960
2025-02-06 16:57:50,890 - INFO - Epoch 100: val_loss=4.3952, val_acc=0.00%
2025-02-06 16:57:50,894 - INFO - Epoch 100: EPOCH_AVG_TRAIN_LOSS=0.6014
2025-02-06 16:57:50,896 - INFO - #################### Training epoch 101 ####################
2025-02-06 16:57:50,896 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:51,190 - INFO - Epoch 101: train_loss=0.5568
2025-02-06 16:57:51,435 - INFO - Epoch 101: train_loss=0.5042
2025-02-06 16:57:51,728 - INFO - Epoch 101: val_loss=4.4038, val_acc=0.00%
2025-02-06 16:57:51,732 - INFO - Epoch 101: EPOCH_AVG_TRAIN_LOSS=0.5305
2025-02-06 16:57:51,734 - INFO - #################### Training epoch 102 ####################
2025-02-06 16:57:51,734 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:52,028 - INFO - Epoch 102: train_loss=0.6596
2025-02-06 16:57:52,273 - INFO - Epoch 102: train_loss=0.3034
2025-02-06 16:57:52,566 - INFO - Epoch 102: val_loss=4.4528, val_acc=0.00%
2025-02-06 16:57:52,570 - INFO - Epoch 102: EPOCH_AVG_TRAIN_LOSS=0.4815
2025-02-06 16:57:52,572 - INFO - #################### Training epoch 103 ####################
2025-02-06 16:57:52,572 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:52,865 - INFO - Epoch 103: train_loss=0.5907
2025-02-06 16:57:53,110 - INFO - Epoch 103: train_loss=0.4491
2025-02-06 16:57:53,402 - INFO - Epoch 103: val_loss=4.3774, val_acc=0.00%
2025-02-06 16:57:53,405 - INFO - Epoch 103: EPOCH_AVG_TRAIN_LOSS=0.5199
2025-02-06 16:57:53,407 - INFO - #################### Training epoch 104 ####################
2025-02-06 16:57:53,407 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:53,703 - INFO - Epoch 104: train_loss=0.5288
2025-02-06 16:57:53,948 - INFO - Epoch 104: train_loss=0.5560
2025-02-06 16:57:54,241 - INFO - Epoch 104: val_loss=4.3085, val_acc=0.00%
2025-02-06 16:57:54,244 - INFO - Epoch 104: EPOCH_AVG_TRAIN_LOSS=0.5424
2025-02-06 16:57:54,246 - INFO - #################### Training epoch 105 ####################
2025-02-06 16:57:54,246 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:54,542 - INFO - Epoch 105: train_loss=0.4892
2025-02-06 16:57:54,787 - INFO - Epoch 105: train_loss=0.6309
2025-02-06 16:57:55,077 - INFO - Epoch 105: val_loss=4.3696, val_acc=0.00%
2025-02-06 16:57:55,081 - INFO - Epoch 105: EPOCH_AVG_TRAIN_LOSS=0.5601
2025-02-06 16:57:55,083 - INFO - #################### Training epoch 106 ####################
2025-02-06 16:57:55,083 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:55,380 - INFO - Epoch 106: train_loss=0.5264
2025-02-06 16:57:55,626 - INFO - Epoch 106: train_loss=0.5611
2025-02-06 16:57:55,914 - INFO - Epoch 106: val_loss=4.3825, val_acc=0.00%
2025-02-06 16:57:55,917 - INFO - Epoch 106: EPOCH_AVG_TRAIN_LOSS=0.5438
2025-02-06 16:57:55,920 - INFO - #################### Training epoch 107 ####################
2025-02-06 16:57:55,920 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:56,214 - INFO - Epoch 107: train_loss=0.6424
2025-02-06 16:57:56,459 - INFO - Epoch 107: train_loss=0.3402
2025-02-06 16:57:56,753 - INFO - Epoch 107: val_loss=4.3346, val_acc=33.33%
2025-02-06 16:57:56,757 - INFO - Epoch 107: EPOCH_AVG_TRAIN_LOSS=0.4913
2025-02-06 16:57:56,759 - INFO - #################### Training epoch 108 ####################
2025-02-06 16:57:56,759 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:57,051 - INFO - Epoch 108: train_loss=0.3970
2025-02-06 16:57:57,297 - INFO - Epoch 108: train_loss=0.8318
2025-02-06 16:57:57,590 - INFO - Epoch 108: val_loss=4.4288, val_acc=0.00%
2025-02-06 16:57:57,593 - INFO - Epoch 108: EPOCH_AVG_TRAIN_LOSS=0.6144
2025-02-06 16:57:57,596 - INFO - #################### Training epoch 109 ####################
2025-02-06 16:57:57,596 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:57,890 - INFO - Epoch 109: train_loss=0.6387
2025-02-06 16:57:58,135 - INFO - Epoch 109: train_loss=0.3333
2025-02-06 16:57:58,426 - INFO - Epoch 109: val_loss=4.3565, val_acc=0.00%
2025-02-06 16:57:58,429 - INFO - Epoch 109: EPOCH_AVG_TRAIN_LOSS=0.4860
2025-02-06 16:57:58,431 - INFO - #################### Training epoch 110 ####################
2025-02-06 16:57:58,431 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:58,723 - INFO - Epoch 110: train_loss=0.6544
2025-02-06 16:57:58,968 - INFO - Epoch 110: train_loss=0.3080
2025-02-06 16:57:59,260 - INFO - Epoch 110: val_loss=4.3974, val_acc=0.00%
2025-02-06 16:57:59,263 - INFO - Epoch 110: EPOCH_AVG_TRAIN_LOSS=0.4812
2025-02-06 16:57:59,266 - INFO - #################### Training epoch 111 ####################
2025-02-06 16:57:59,266 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:57:59,563 - INFO - Epoch 111: train_loss=0.5136
2025-02-06 16:57:59,808 - INFO - Epoch 111: train_loss=0.5947
2025-02-06 16:58:00,101 - INFO - Epoch 111: val_loss=4.3936, val_acc=0.00%
2025-02-06 16:58:00,104 - INFO - Epoch 111: EPOCH_AVG_TRAIN_LOSS=0.5541
2025-02-06 16:58:00,106 - INFO - #################### Training epoch 112 ####################
2025-02-06 16:58:00,106 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:00,402 - INFO - Epoch 112: train_loss=0.5195
2025-02-06 16:58:00,648 - INFO - Epoch 112: train_loss=0.5776
2025-02-06 16:58:00,939 - INFO - Epoch 112: val_loss=4.4311, val_acc=0.00%
2025-02-06 16:58:00,943 - INFO - Epoch 112: EPOCH_AVG_TRAIN_LOSS=0.5486
2025-02-06 16:58:00,945 - INFO - #################### Training epoch 113 ####################
2025-02-06 16:58:00,945 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:01,239 - INFO - Epoch 113: train_loss=0.6065
2025-02-06 16:58:01,485 - INFO - Epoch 113: train_loss=0.3945
2025-02-06 16:58:01,776 - INFO - Epoch 113: val_loss=4.3625, val_acc=33.33%
2025-02-06 16:58:01,779 - INFO - Epoch 113: EPOCH_AVG_TRAIN_LOSS=0.5005
2025-02-06 16:58:01,781 - INFO - #################### Training epoch 114 ####################
2025-02-06 16:58:01,782 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:02,079 - INFO - Epoch 114: train_loss=0.5027
2025-02-06 16:58:02,325 - INFO - Epoch 114: train_loss=0.6114
2025-02-06 16:58:02,617 - INFO - Epoch 114: val_loss=4.4231, val_acc=0.00%
2025-02-06 16:58:02,620 - INFO - Epoch 114: EPOCH_AVG_TRAIN_LOSS=0.5570
2025-02-06 16:58:02,622 - INFO - #################### Training epoch 115 ####################
2025-02-06 16:58:02,622 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:02,916 - INFO - Epoch 115: train_loss=0.6195
2025-02-06 16:58:03,162 - INFO - Epoch 115: train_loss=0.3843
2025-02-06 16:58:03,457 - INFO - Epoch 115: val_loss=4.4368, val_acc=0.00%
2025-02-06 16:58:03,460 - INFO - Epoch 115: EPOCH_AVG_TRAIN_LOSS=0.5019
2025-02-06 16:58:03,463 - INFO - #################### Training epoch 116 ####################
2025-02-06 16:58:03,463 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:03,752 - INFO - Epoch 116: train_loss=0.5440
2025-02-06 16:58:03,998 - INFO - Epoch 116: train_loss=0.5375
2025-02-06 16:58:04,287 - INFO - Epoch 116: val_loss=4.3811, val_acc=0.00%
2025-02-06 16:58:04,291 - INFO - Epoch 116: EPOCH_AVG_TRAIN_LOSS=0.5408
2025-02-06 16:58:04,293 - INFO - #################### Training epoch 117 ####################
2025-02-06 16:58:04,293 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:04,586 - INFO - Epoch 117: train_loss=0.6692
2025-02-06 16:58:04,832 - INFO - Epoch 117: train_loss=0.2794
2025-02-06 16:58:05,124 - INFO - Epoch 117: val_loss=4.3741, val_acc=0.00%
2025-02-06 16:58:05,128 - INFO - Epoch 117: EPOCH_AVG_TRAIN_LOSS=0.4743
2025-02-06 16:58:05,130 - INFO - #################### Training epoch 118 ####################
2025-02-06 16:58:05,130 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:05,422 - INFO - Epoch 118: train_loss=0.6675
2025-02-06 16:58:05,667 - INFO - Epoch 118: train_loss=0.2905
2025-02-06 16:58:05,954 - INFO - Epoch 118: val_loss=4.4069, val_acc=0.00%
2025-02-06 16:58:05,957 - INFO - Epoch 118: EPOCH_AVG_TRAIN_LOSS=0.4790
2025-02-06 16:58:05,959 - INFO - #################### Training epoch 119 ####################
2025-02-06 16:58:05,960 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:06,256 - INFO - Epoch 119: train_loss=0.5866
2025-02-06 16:58:06,501 - INFO - Epoch 119: train_loss=0.4375
2025-02-06 16:58:06,795 - INFO - Epoch 119: val_loss=4.4108, val_acc=0.00%
2025-02-06 16:58:06,798 - INFO - Epoch 119: EPOCH_AVG_TRAIN_LOSS=0.5120
2025-02-06 16:58:06,800 - INFO - #################### Training epoch 120 ####################
2025-02-06 16:58:06,800 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:07,097 - INFO - Epoch 120: train_loss=0.5947
2025-02-06 16:58:07,342 - INFO - Epoch 120: train_loss=0.4303
2025-02-06 16:58:07,631 - INFO - Epoch 120: val_loss=4.4470, val_acc=0.00%
2025-02-06 16:58:07,635 - INFO - Epoch 120: EPOCH_AVG_TRAIN_LOSS=0.5125
2025-02-06 16:58:07,637 - INFO - #################### Training epoch 121 ####################
2025-02-06 16:58:07,637 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:07,930 - INFO - Epoch 121: train_loss=0.5814
2025-02-06 16:58:08,176 - INFO - Epoch 121: train_loss=0.4454
2025-02-06 16:58:08,465 - INFO - Epoch 121: val_loss=4.3867, val_acc=0.00%
2025-02-06 16:58:08,469 - INFO - Epoch 121: EPOCH_AVG_TRAIN_LOSS=0.5134
2025-02-06 16:58:08,471 - INFO - #################### Training epoch 122 ####################
2025-02-06 16:58:08,471 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:08,765 - INFO - Epoch 122: train_loss=0.5961
2025-02-06 16:58:09,011 - INFO - Epoch 122: train_loss=0.4132
2025-02-06 16:58:09,305 - INFO - Epoch 122: val_loss=4.3938, val_acc=0.00%
2025-02-06 16:58:09,309 - INFO - Epoch 122: EPOCH_AVG_TRAIN_LOSS=0.5046
2025-02-06 16:58:09,311 - INFO - #################### Training epoch 123 ####################
2025-02-06 16:58:09,311 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:09,605 - INFO - Epoch 123: train_loss=0.6508
2025-02-06 16:58:09,850 - INFO - Epoch 123: train_loss=0.3081
2025-02-06 16:58:10,143 - INFO - Epoch 123: val_loss=4.3805, val_acc=0.00%
2025-02-06 16:58:10,147 - INFO - Epoch 123: EPOCH_AVG_TRAIN_LOSS=0.4795
2025-02-06 16:58:10,149 - INFO - #################### Training epoch 124 ####################
2025-02-06 16:58:10,149 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:10,441 - INFO - Epoch 124: train_loss=0.5841
2025-02-06 16:58:10,686 - INFO - Epoch 124: train_loss=0.4507
2025-02-06 16:58:10,976 - INFO - Epoch 124: val_loss=4.3561, val_acc=0.00%
2025-02-06 16:58:10,980 - INFO - Epoch 124: EPOCH_AVG_TRAIN_LOSS=0.5174
2025-02-06 16:58:10,982 - INFO - #################### Training epoch 125 ####################
2025-02-06 16:58:10,982 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:11,276 - INFO - Epoch 125: train_loss=0.6662
2025-02-06 16:58:11,521 - INFO - Epoch 125: train_loss=0.2893
2025-02-06 16:58:11,813 - INFO - Epoch 125: val_loss=4.3752, val_acc=0.00%
2025-02-06 16:58:11,817 - INFO - Epoch 125: EPOCH_AVG_TRAIN_LOSS=0.4777
2025-02-06 16:58:11,819 - INFO - #################### Training epoch 126 ####################
2025-02-06 16:58:11,819 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:12,116 - INFO - Epoch 126: train_loss=0.4874
2025-02-06 16:58:12,362 - INFO - Epoch 126: train_loss=0.6395
2025-02-06 16:58:12,652 - INFO - Epoch 126: val_loss=4.3504, val_acc=0.00%
2025-02-06 16:58:12,656 - INFO - Epoch 126: EPOCH_AVG_TRAIN_LOSS=0.5634
2025-02-06 16:58:12,658 - INFO - #################### Training epoch 127 ####################
2025-02-06 16:58:12,658 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:12,952 - INFO - Epoch 127: train_loss=0.4601
2025-02-06 16:58:13,198 - INFO - Epoch 127: train_loss=0.6979
2025-02-06 16:58:13,488 - INFO - Epoch 127: val_loss=4.4502, val_acc=0.00%
2025-02-06 16:58:13,492 - INFO - Epoch 127: EPOCH_AVG_TRAIN_LOSS=0.5790
2025-02-06 16:58:13,494 - INFO - #################### Training epoch 128 ####################
2025-02-06 16:58:13,494 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:13,787 - INFO - Epoch 128: train_loss=0.5205
2025-02-06 16:58:14,032 - INFO - Epoch 128: train_loss=0.5687
2025-02-06 16:58:14,322 - INFO - Epoch 128: val_loss=4.4318, val_acc=0.00%
2025-02-06 16:58:14,326 - INFO - Epoch 128: EPOCH_AVG_TRAIN_LOSS=0.5446
2025-02-06 16:58:14,328 - INFO - #################### Training epoch 129 ####################
2025-02-06 16:58:14,328 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:14,621 - INFO - Epoch 129: train_loss=0.5384
2025-02-06 16:58:14,866 - INFO - Epoch 129: train_loss=0.5331
2025-02-06 16:58:15,154 - INFO - Epoch 129: val_loss=4.3573, val_acc=0.00%
2025-02-06 16:58:15,157 - INFO - Epoch 129: EPOCH_AVG_TRAIN_LOSS=0.5357
2025-02-06 16:58:15,159 - INFO - #################### Training epoch 130 ####################
2025-02-06 16:58:15,159 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:15,455 - INFO - Epoch 130: train_loss=0.4617
2025-02-06 16:58:15,700 - INFO - Epoch 130: train_loss=0.6920
2025-02-06 16:58:15,990 - INFO - Epoch 130: val_loss=4.4057, val_acc=0.00%
2025-02-06 16:58:15,993 - INFO - Epoch 130: EPOCH_AVG_TRAIN_LOSS=0.5768
2025-02-06 16:58:15,995 - INFO - #################### Training epoch 131 ####################
2025-02-06 16:58:15,995 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:16,289 - INFO - Epoch 131: train_loss=0.5094
2025-02-06 16:58:16,535 - INFO - Epoch 131: train_loss=0.5958
2025-02-06 16:58:16,823 - INFO - Epoch 131: val_loss=4.3421, val_acc=0.00%
2025-02-06 16:58:16,826 - INFO - Epoch 131: EPOCH_AVG_TRAIN_LOSS=0.5526
2025-02-06 16:58:16,828 - INFO - #################### Training epoch 132 ####################
2025-02-06 16:58:16,828 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:17,121 - INFO - Epoch 132: train_loss=0.5897
2025-02-06 16:58:17,366 - INFO - Epoch 132: train_loss=0.4419
2025-02-06 16:58:17,651 - INFO - Epoch 132: val_loss=4.3541, val_acc=0.00%
2025-02-06 16:58:17,654 - INFO - Epoch 132: EPOCH_AVG_TRAIN_LOSS=0.5158
2025-02-06 16:58:17,656 - INFO - #################### Training epoch 133 ####################
2025-02-06 16:58:17,656 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:17,951 - INFO - Epoch 133: train_loss=0.4763
2025-02-06 16:58:18,196 - INFO - Epoch 133: train_loss=0.6692
2025-02-06 16:58:18,488 - INFO - Epoch 133: val_loss=4.3355, val_acc=0.00%
2025-02-06 16:58:18,491 - INFO - Epoch 133: EPOCH_AVG_TRAIN_LOSS=0.5727
2025-02-06 16:58:18,494 - INFO - #################### Training epoch 134 ####################
2025-02-06 16:58:18,494 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:18,788 - INFO - Epoch 134: train_loss=0.4365
2025-02-06 16:58:19,034 - INFO - Epoch 134: train_loss=0.7656
2025-02-06 16:58:19,324 - INFO - Epoch 134: val_loss=4.3973, val_acc=0.00%
2025-02-06 16:58:19,328 - INFO - Epoch 134: EPOCH_AVG_TRAIN_LOSS=0.6010
2025-02-06 16:58:19,330 - INFO - #################### Training epoch 135 ####################
2025-02-06 16:58:19,330 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:19,623 - INFO - Epoch 135: train_loss=0.4608
2025-02-06 16:58:19,870 - INFO - Epoch 135: train_loss=0.6894
2025-02-06 16:58:20,160 - INFO - Epoch 135: val_loss=4.4140, val_acc=0.00%
2025-02-06 16:58:20,163 - INFO - Epoch 135: EPOCH_AVG_TRAIN_LOSS=0.5751
2025-02-06 16:58:20,166 - INFO - #################### Training epoch 136 ####################
2025-02-06 16:58:20,166 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:20,463 - INFO - Epoch 136: train_loss=0.4065
2025-02-06 16:58:20,708 - INFO - Epoch 136: train_loss=0.8041
2025-02-06 16:58:20,995 - INFO - Epoch 136: val_loss=4.4355, val_acc=0.00%
2025-02-06 16:58:20,999 - INFO - Epoch 136: EPOCH_AVG_TRAIN_LOSS=0.6053
2025-02-06 16:58:21,001 - INFO - #################### Training epoch 137 ####################
2025-02-06 16:58:21,001 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:21,296 - INFO - Epoch 137: train_loss=0.5178
2025-02-06 16:58:21,542 - INFO - Epoch 137: train_loss=0.5893
2025-02-06 16:58:21,836 - INFO - Epoch 137: val_loss=4.4081, val_acc=0.00%
2025-02-06 16:58:21,839 - INFO - Epoch 137: EPOCH_AVG_TRAIN_LOSS=0.5535
2025-02-06 16:58:21,841 - INFO - #################### Training epoch 138 ####################
2025-02-06 16:58:21,841 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:22,138 - INFO - Epoch 138: train_loss=0.4763
2025-02-06 16:58:22,383 - INFO - Epoch 138: train_loss=0.6613
2025-02-06 16:58:22,673 - INFO - Epoch 138: val_loss=4.3525, val_acc=33.33%
2025-02-06 16:58:22,676 - INFO - Epoch 138: EPOCH_AVG_TRAIN_LOSS=0.5688
2025-02-06 16:58:22,678 - INFO - #################### Training epoch 139 ####################
2025-02-06 16:58:22,678 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:22,976 - INFO - Epoch 139: train_loss=0.4705
2025-02-06 16:58:23,222 - INFO - Epoch 139: train_loss=0.6819
2025-02-06 16:58:23,512 - INFO - Epoch 139: val_loss=4.3920, val_acc=0.00%
2025-02-06 16:58:23,516 - INFO - Epoch 139: EPOCH_AVG_TRAIN_LOSS=0.5762
2025-02-06 16:58:23,518 - INFO - #################### Training epoch 140 ####################
2025-02-06 16:58:23,518 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:23,811 - INFO - Epoch 140: train_loss=0.5177
2025-02-06 16:58:24,057 - INFO - Epoch 140: train_loss=0.5880
2025-02-06 16:58:24,351 - INFO - Epoch 140: val_loss=4.3533, val_acc=33.33%
2025-02-06 16:58:24,354 - INFO - Epoch 140: EPOCH_AVG_TRAIN_LOSS=0.5529
2025-02-06 16:58:24,357 - INFO - #################### Training epoch 141 ####################
2025-02-06 16:58:24,357 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:24,652 - INFO - Epoch 141: train_loss=0.5650
2025-02-06 16:58:24,898 - INFO - Epoch 141: train_loss=0.4833
2025-02-06 16:58:25,186 - INFO - Epoch 141: val_loss=4.4273, val_acc=0.00%
2025-02-06 16:58:25,190 - INFO - Epoch 141: EPOCH_AVG_TRAIN_LOSS=0.5242
2025-02-06 16:58:25,192 - INFO - #################### Training epoch 142 ####################
2025-02-06 16:58:25,192 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:25,487 - INFO - Epoch 142: train_loss=0.6246
2025-02-06 16:58:25,732 - INFO - Epoch 142: train_loss=0.3712
2025-02-06 16:58:26,024 - INFO - Epoch 142: val_loss=4.4103, val_acc=0.00%
2025-02-06 16:58:26,027 - INFO - Epoch 142: EPOCH_AVG_TRAIN_LOSS=0.4979
2025-02-06 16:58:26,029 - INFO - #################### Training epoch 143 ####################
2025-02-06 16:58:26,030 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:26,324 - INFO - Epoch 143: train_loss=0.6063
2025-02-06 16:58:26,570 - INFO - Epoch 143: train_loss=0.4078
2025-02-06 16:58:26,861 - INFO - Epoch 143: val_loss=4.4090, val_acc=0.00%
2025-02-06 16:58:26,865 - INFO - Epoch 143: EPOCH_AVG_TRAIN_LOSS=0.5070
2025-02-06 16:58:26,867 - INFO - #################### Training epoch 144 ####################
2025-02-06 16:58:26,867 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:27,161 - INFO - Epoch 144: train_loss=0.5580
2025-02-06 16:58:27,408 - INFO - Epoch 144: train_loss=0.5074
2025-02-06 16:58:27,698 - INFO - Epoch 144: val_loss=4.4045, val_acc=0.00%
2025-02-06 16:58:27,701 - INFO - Epoch 144: EPOCH_AVG_TRAIN_LOSS=0.5327
2025-02-06 16:58:27,704 - INFO - #################### Training epoch 145 ####################
2025-02-06 16:58:27,704 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:27,998 - INFO - Epoch 145: train_loss=0.4021
2025-02-06 16:58:28,242 - INFO - Epoch 145: train_loss=0.8091
2025-02-06 16:58:28,531 - INFO - Epoch 145: val_loss=4.3713, val_acc=0.00%
2025-02-06 16:58:28,534 - INFO - Epoch 145: EPOCH_AVG_TRAIN_LOSS=0.6056
2025-02-06 16:58:28,536 - INFO - #################### Training epoch 146 ####################
2025-02-06 16:58:28,536 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:28,831 - INFO - Epoch 146: train_loss=0.5330
2025-02-06 16:58:29,077 - INFO - Epoch 146: train_loss=0.5459
2025-02-06 16:58:29,369 - INFO - Epoch 146: val_loss=4.4053, val_acc=0.00%
2025-02-06 16:58:29,373 - INFO - Epoch 146: EPOCH_AVG_TRAIN_LOSS=0.5395
2025-02-06 16:58:29,375 - INFO - #################### Training epoch 147 ####################
2025-02-06 16:58:29,375 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:29,670 - INFO - Epoch 147: train_loss=0.4412
2025-02-06 16:58:29,916 - INFO - Epoch 147: train_loss=0.7496
2025-02-06 16:58:30,204 - INFO - Epoch 147: val_loss=4.4217, val_acc=0.00%
2025-02-06 16:58:30,208 - INFO - Epoch 147: EPOCH_AVG_TRAIN_LOSS=0.5954
2025-02-06 16:58:30,210 - INFO - #################### Training epoch 148 ####################
2025-02-06 16:58:30,210 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:30,505 - INFO - Epoch 148: train_loss=0.5686
2025-02-06 16:58:30,751 - INFO - Epoch 148: train_loss=0.4648
2025-02-06 16:58:31,041 - INFO - Epoch 148: val_loss=4.3674, val_acc=0.00%
2025-02-06 16:58:31,045 - INFO - Epoch 148: EPOCH_AVG_TRAIN_LOSS=0.5167
2025-02-06 16:58:31,047 - INFO - #################### Training epoch 149 ####################
2025-02-06 16:58:31,047 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:31,343 - INFO - Epoch 149: train_loss=0.5103
2025-02-06 16:58:31,588 - INFO - Epoch 149: train_loss=0.6009
2025-02-06 16:58:31,880 - INFO - Epoch 149: val_loss=4.4469, val_acc=0.00%
2025-02-06 16:58:31,884 - INFO - Epoch 149: EPOCH_AVG_TRAIN_LOSS=0.5556
2025-02-06 16:58:31,886 - INFO - #################### Training epoch 150 ####################
2025-02-06 16:58:31,886 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:32,184 - INFO - Epoch 150: train_loss=0.5644
2025-02-06 16:58:32,429 - INFO - Epoch 150: train_loss=0.4791
2025-02-06 16:58:32,719 - INFO - Epoch 150: val_loss=4.4610, val_acc=0.00%
2025-02-06 16:58:32,722 - INFO - Epoch 150: EPOCH_AVG_TRAIN_LOSS=0.5218
2025-02-06 16:58:32,725 - INFO - #################### Training epoch 151 ####################
2025-02-06 16:58:32,725 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:33,021 - INFO - Epoch 151: train_loss=0.4773
2025-02-06 16:58:33,267 - INFO - Epoch 151: train_loss=0.6564
2025-02-06 16:58:33,559 - INFO - Epoch 151: val_loss=4.3613, val_acc=0.00%
2025-02-06 16:58:33,562 - INFO - Epoch 151: EPOCH_AVG_TRAIN_LOSS=0.5668
2025-02-06 16:58:33,565 - INFO - #################### Training epoch 152 ####################
2025-02-06 16:58:33,565 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:33,860 - INFO - Epoch 152: train_loss=0.5088
2025-02-06 16:58:34,105 - INFO - Epoch 152: train_loss=0.6024
2025-02-06 16:58:34,396 - INFO - Epoch 152: val_loss=4.3873, val_acc=0.00%
2025-02-06 16:58:34,399 - INFO - Epoch 152: EPOCH_AVG_TRAIN_LOSS=0.5556
2025-02-06 16:58:34,402 - INFO - #################### Training epoch 153 ####################
2025-02-06 16:58:34,402 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:34,700 - INFO - Epoch 153: train_loss=0.5681
2025-02-06 16:58:34,945 - INFO - Epoch 153: train_loss=0.4839
2025-02-06 16:58:35,233 - INFO - Epoch 153: val_loss=4.3626, val_acc=33.33%
2025-02-06 16:58:35,237 - INFO - Epoch 153: EPOCH_AVG_TRAIN_LOSS=0.5260
2025-02-06 16:58:35,239 - INFO - #################### Training epoch 154 ####################
2025-02-06 16:58:35,239 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:35,532 - INFO - Epoch 154: train_loss=0.6133
2025-02-06 16:58:35,776 - INFO - Epoch 154: train_loss=0.3894
2025-02-06 16:58:36,069 - INFO - Epoch 154: val_loss=4.3881, val_acc=0.00%
2025-02-06 16:58:36,073 - INFO - Epoch 154: EPOCH_AVG_TRAIN_LOSS=0.5013
2025-02-06 16:58:36,075 - INFO - #################### Training epoch 155 ####################
2025-02-06 16:58:36,075 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:36,369 - INFO - Epoch 155: train_loss=0.3982
2025-02-06 16:58:36,614 - INFO - Epoch 155: train_loss=0.8221
2025-02-06 16:58:36,902 - INFO - Epoch 155: val_loss=4.4115, val_acc=0.00%
2025-02-06 16:58:36,905 - INFO - Epoch 155: EPOCH_AVG_TRAIN_LOSS=0.6101
2025-02-06 16:58:36,907 - INFO - #################### Training epoch 156 ####################
2025-02-06 16:58:36,907 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:37,200 - INFO - Epoch 156: train_loss=0.5464
2025-02-06 16:58:37,446 - INFO - Epoch 156: train_loss=0.5242
2025-02-06 16:58:37,735 - INFO - Epoch 156: val_loss=4.4284, val_acc=0.00%
2025-02-06 16:58:37,738 - INFO - Epoch 156: EPOCH_AVG_TRAIN_LOSS=0.5353
2025-02-06 16:58:37,741 - INFO - #################### Training epoch 157 ####################
2025-02-06 16:58:37,741 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:38,035 - INFO - Epoch 157: train_loss=0.6076
2025-02-06 16:58:38,281 - INFO - Epoch 157: train_loss=0.3945
2025-02-06 16:58:38,569 - INFO - Epoch 157: val_loss=4.4194, val_acc=0.00%
2025-02-06 16:58:38,572 - INFO - Epoch 157: EPOCH_AVG_TRAIN_LOSS=0.5010
2025-02-06 16:58:38,574 - INFO - #################### Training epoch 158 ####################
2025-02-06 16:58:38,574 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:38,868 - INFO - Epoch 158: train_loss=0.5038
2025-02-06 16:58:39,113 - INFO - Epoch 158: train_loss=0.6105
2025-02-06 16:58:39,403 - INFO - Epoch 158: val_loss=4.4555, val_acc=0.00%
2025-02-06 16:58:39,406 - INFO - Epoch 158: EPOCH_AVG_TRAIN_LOSS=0.5572
2025-02-06 16:58:39,409 - INFO - #################### Training epoch 159 ####################
2025-02-06 16:58:39,409 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:39,703 - INFO - Epoch 159: train_loss=0.5308
2025-02-06 16:58:39,949 - INFO - Epoch 159: train_loss=0.5573
2025-02-06 16:58:40,237 - INFO - Epoch 159: val_loss=4.4258, val_acc=0.00%
2025-02-06 16:58:40,241 - INFO - Epoch 159: EPOCH_AVG_TRAIN_LOSS=0.5440
2025-02-06 16:58:40,243 - INFO - #################### Training epoch 160 ####################
2025-02-06 16:58:40,243 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:40,537 - INFO - Epoch 160: train_loss=0.5465
2025-02-06 16:58:40,781 - INFO - Epoch 160: train_loss=0.5164
2025-02-06 16:58:41,073 - INFO - Epoch 160: val_loss=4.3507, val_acc=0.00%
2025-02-06 16:58:41,076 - INFO - Epoch 160: EPOCH_AVG_TRAIN_LOSS=0.5315
2025-02-06 16:58:41,079 - INFO - #################### Training epoch 161 ####################
2025-02-06 16:58:41,079 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:41,375 - INFO - Epoch 161: train_loss=0.5516
2025-02-06 16:58:41,620 - INFO - Epoch 161: train_loss=0.5145
2025-02-06 16:58:41,911 - INFO - Epoch 161: val_loss=4.4107, val_acc=0.00%
2025-02-06 16:58:41,915 - INFO - Epoch 161: EPOCH_AVG_TRAIN_LOSS=0.5331
2025-02-06 16:58:41,917 - INFO - #################### Training epoch 162 ####################
2025-02-06 16:58:41,917 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:42,210 - INFO - Epoch 162: train_loss=0.6160
2025-02-06 16:58:42,455 - INFO - Epoch 162: train_loss=0.3722
2025-02-06 16:58:42,746 - INFO - Epoch 162: val_loss=4.3519, val_acc=33.33%
2025-02-06 16:58:42,749 - INFO - Epoch 162: EPOCH_AVG_TRAIN_LOSS=0.4941
2025-02-06 16:58:42,752 - INFO - #################### Training epoch 163 ####################
2025-02-06 16:58:42,752 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:43,047 - INFO - Epoch 163: train_loss=0.4831
2025-02-06 16:58:43,292 - INFO - Epoch 163: train_loss=0.6519
2025-02-06 16:58:43,580 - INFO - Epoch 163: val_loss=4.3832, val_acc=0.00%
2025-02-06 16:58:43,583 - INFO - Epoch 163: EPOCH_AVG_TRAIN_LOSS=0.5675
2025-02-06 16:58:43,585 - INFO - #################### Training epoch 164 ####################
2025-02-06 16:58:43,585 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:43,879 - INFO - Epoch 164: train_loss=0.5525
2025-02-06 16:58:44,125 - INFO - Epoch 164: train_loss=0.5201
2025-02-06 16:58:44,418 - INFO - Epoch 164: val_loss=4.3490, val_acc=33.33%
2025-02-06 16:58:44,421 - INFO - Epoch 164: EPOCH_AVG_TRAIN_LOSS=0.5363
2025-02-06 16:58:44,424 - INFO - #################### Training epoch 165 ####################
2025-02-06 16:58:44,424 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:44,718 - INFO - Epoch 165: train_loss=0.4835
2025-02-06 16:58:44,964 - INFO - Epoch 165: train_loss=0.6604
2025-02-06 16:58:45,254 - INFO - Epoch 165: val_loss=4.4055, val_acc=0.00%
2025-02-06 16:58:45,257 - INFO - Epoch 165: EPOCH_AVG_TRAIN_LOSS=0.5719
2025-02-06 16:58:45,260 - INFO - #################### Training epoch 166 ####################
2025-02-06 16:58:45,260 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:45,555 - INFO - Epoch 166: train_loss=0.5494
2025-02-06 16:58:45,801 - INFO - Epoch 166: train_loss=0.5138
2025-02-06 16:58:46,096 - INFO - Epoch 166: val_loss=4.4138, val_acc=0.00%
2025-02-06 16:58:46,100 - INFO - Epoch 166: EPOCH_AVG_TRAIN_LOSS=0.5316
2025-02-06 16:58:46,102 - INFO - #################### Training epoch 167 ####################
2025-02-06 16:58:46,102 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:46,398 - INFO - Epoch 167: train_loss=0.5958
2025-02-06 16:58:46,643 - INFO - Epoch 167: train_loss=0.4291
2025-02-06 16:58:46,937 - INFO - Epoch 167: val_loss=4.3937, val_acc=0.00%
2025-02-06 16:58:46,941 - INFO - Epoch 167: EPOCH_AVG_TRAIN_LOSS=0.5124
2025-02-06 16:58:46,943 - INFO - #################### Training epoch 168 ####################
2025-02-06 16:58:46,943 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:47,235 - INFO - Epoch 168: train_loss=0.5849
2025-02-06 16:58:47,480 - INFO - Epoch 168: train_loss=0.4471
2025-02-06 16:58:47,772 - INFO - Epoch 168: val_loss=4.4017, val_acc=0.00%
2025-02-06 16:58:47,775 - INFO - Epoch 168: EPOCH_AVG_TRAIN_LOSS=0.5160
2025-02-06 16:58:47,777 - INFO - #################### Training epoch 169 ####################
2025-02-06 16:58:47,777 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:48,072 - INFO - Epoch 169: train_loss=0.5892
2025-02-06 16:58:48,317 - INFO - Epoch 169: train_loss=0.4386
2025-02-06 16:58:48,611 - INFO - Epoch 169: val_loss=4.4473, val_acc=0.00%
2025-02-06 16:58:48,615 - INFO - Epoch 169: EPOCH_AVG_TRAIN_LOSS=0.5139
2025-02-06 16:58:48,617 - INFO - #################### Training epoch 170 ####################
2025-02-06 16:58:48,617 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:48,911 - INFO - Epoch 170: train_loss=0.5568
2025-02-06 16:58:49,156 - INFO - Epoch 170: train_loss=0.4969
2025-02-06 16:58:49,450 - INFO - Epoch 170: val_loss=4.3681, val_acc=0.00%
2025-02-06 16:58:49,455 - INFO - Epoch 170: EPOCH_AVG_TRAIN_LOSS=0.5268
2025-02-06 16:58:49,458 - INFO - #################### Training epoch 171 ####################
2025-02-06 16:58:49,458 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:49,753 - INFO - Epoch 171: train_loss=0.6156
2025-02-06 16:58:50,001 - INFO - Epoch 171: train_loss=0.3995
2025-02-06 16:58:50,292 - INFO - Epoch 171: val_loss=4.4164, val_acc=0.00%
2025-02-06 16:58:50,296 - INFO - Epoch 171: EPOCH_AVG_TRAIN_LOSS=0.5076
2025-02-06 16:58:50,298 - INFO - #################### Training epoch 172 ####################
2025-02-06 16:58:50,298 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:50,590 - INFO - Epoch 172: train_loss=0.6228
2025-02-06 16:58:50,835 - INFO - Epoch 172: train_loss=0.3731
2025-02-06 16:58:51,127 - INFO - Epoch 172: val_loss=4.3952, val_acc=0.00%
2025-02-06 16:58:51,130 - INFO - Epoch 172: EPOCH_AVG_TRAIN_LOSS=0.4980
2025-02-06 16:58:51,133 - INFO - #################### Training epoch 173 ####################
2025-02-06 16:58:51,133 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:51,429 - INFO - Epoch 173: train_loss=0.6181
2025-02-06 16:58:51,676 - INFO - Epoch 173: train_loss=0.3858
2025-02-06 16:58:51,968 - INFO - Epoch 173: val_loss=4.4015, val_acc=0.00%
2025-02-06 16:58:51,971 - INFO - Epoch 173: EPOCH_AVG_TRAIN_LOSS=0.5020
2025-02-06 16:58:51,974 - INFO - #################### Training epoch 174 ####################
2025-02-06 16:58:51,974 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:52,271 - INFO - Epoch 174: train_loss=0.6054
2025-02-06 16:58:52,517 - INFO - Epoch 174: train_loss=0.4066
2025-02-06 16:58:52,808 - INFO - Epoch 174: val_loss=4.4167, val_acc=0.00%
2025-02-06 16:58:52,812 - INFO - Epoch 174: EPOCH_AVG_TRAIN_LOSS=0.5060
2025-02-06 16:58:52,814 - INFO - #################### Training epoch 175 ####################
2025-02-06 16:58:52,814 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:53,108 - INFO - Epoch 175: train_loss=0.4520
2025-02-06 16:58:53,353 - INFO - Epoch 175: train_loss=0.7022
2025-02-06 16:58:53,644 - INFO - Epoch 175: val_loss=4.4233, val_acc=0.00%
2025-02-06 16:58:53,647 - INFO - Epoch 175: EPOCH_AVG_TRAIN_LOSS=0.5771
2025-02-06 16:58:53,650 - INFO - #################### Training epoch 176 ####################
2025-02-06 16:58:53,650 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:53,944 - INFO - Epoch 176: train_loss=0.5773
2025-02-06 16:58:54,191 - INFO - Epoch 176: train_loss=0.4553
2025-02-06 16:58:54,481 - INFO - Epoch 176: val_loss=4.4101, val_acc=0.00%
2025-02-06 16:58:54,485 - INFO - Epoch 176: EPOCH_AVG_TRAIN_LOSS=0.5163
2025-02-06 16:58:54,487 - INFO - #################### Training epoch 177 ####################
2025-02-06 16:58:54,487 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:54,784 - INFO - Epoch 177: train_loss=0.4908
2025-02-06 16:58:55,030 - INFO - Epoch 177: train_loss=0.6506
2025-02-06 16:58:55,321 - INFO - Epoch 177: val_loss=4.3717, val_acc=0.00%
2025-02-06 16:58:55,324 - INFO - Epoch 177: EPOCH_AVG_TRAIN_LOSS=0.5707
2025-02-06 16:58:55,327 - INFO - #################### Training epoch 178 ####################
2025-02-06 16:58:55,327 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:55,621 - INFO - Epoch 178: train_loss=0.4220
2025-02-06 16:58:55,867 - INFO - Epoch 178: train_loss=0.7755
2025-02-06 16:58:56,158 - INFO - Epoch 178: val_loss=4.4201, val_acc=0.00%
2025-02-06 16:58:56,162 - INFO - Epoch 178: EPOCH_AVG_TRAIN_LOSS=0.5987
2025-02-06 16:58:56,164 - INFO - #################### Training epoch 179 ####################
2025-02-06 16:58:56,164 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:56,464 - INFO - Epoch 179: train_loss=0.4144
2025-02-06 16:58:56,711 - INFO - Epoch 179: train_loss=0.7908
2025-02-06 16:58:57,001 - INFO - Epoch 179: val_loss=4.3937, val_acc=0.00%
2025-02-06 16:58:57,005 - INFO - Epoch 179: EPOCH_AVG_TRAIN_LOSS=0.6026
2025-02-06 16:58:57,007 - INFO - #################### Training epoch 180 ####################
2025-02-06 16:58:57,007 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:57,303 - INFO - Epoch 180: train_loss=0.5508
2025-02-06 16:58:57,549 - INFO - Epoch 180: train_loss=0.5148
2025-02-06 16:58:57,841 - INFO - Epoch 180: val_loss=4.3812, val_acc=0.00%
2025-02-06 16:58:57,845 - INFO - Epoch 180: EPOCH_AVG_TRAIN_LOSS=0.5328
2025-02-06 16:58:57,847 - INFO - #################### Training epoch 181 ####################
2025-02-06 16:58:57,847 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:58,142 - INFO - Epoch 181: train_loss=0.4307
2025-02-06 16:58:58,387 - INFO - Epoch 181: train_loss=0.7561
2025-02-06 16:58:58,679 - INFO - Epoch 181: val_loss=4.3631, val_acc=0.00%
2025-02-06 16:58:58,683 - INFO - Epoch 181: EPOCH_AVG_TRAIN_LOSS=0.5934
2025-02-06 16:58:58,685 - INFO - #################### Training epoch 182 ####################
2025-02-06 16:58:58,685 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:58,977 - INFO - Epoch 182: train_loss=0.6200
2025-02-06 16:58:59,223 - INFO - Epoch 182: train_loss=0.3762
2025-02-06 16:58:59,512 - INFO - Epoch 182: val_loss=4.3970, val_acc=0.00%
2025-02-06 16:58:59,516 - INFO - Epoch 182: EPOCH_AVG_TRAIN_LOSS=0.4981
2025-02-06 16:58:59,518 - INFO - #################### Training epoch 183 ####################
2025-02-06 16:58:59,518 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:58:59,814 - INFO - Epoch 183: train_loss=0.5993
2025-02-06 16:59:00,060 - INFO - Epoch 183: train_loss=0.4206
2025-02-06 16:59:00,350 - INFO - Epoch 183: val_loss=4.4323, val_acc=0.00%
2025-02-06 16:59:00,354 - INFO - Epoch 183: EPOCH_AVG_TRAIN_LOSS=0.5099
2025-02-06 16:59:00,356 - INFO - #################### Training epoch 184 ####################
2025-02-06 16:59:00,356 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:00,651 - INFO - Epoch 184: train_loss=0.6350
2025-02-06 16:59:00,897 - INFO - Epoch 184: train_loss=0.3461
2025-02-06 16:59:01,188 - INFO - Epoch 184: val_loss=4.3847, val_acc=0.00%
2025-02-06 16:59:01,192 - INFO - Epoch 184: EPOCH_AVG_TRAIN_LOSS=0.4906
2025-02-06 16:59:01,194 - INFO - #################### Training epoch 185 ####################
2025-02-06 16:59:01,194 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:01,488 - INFO - Epoch 185: train_loss=0.4270
2025-02-06 16:59:01,733 - INFO - Epoch 185: train_loss=0.7804
2025-02-06 16:59:02,021 - INFO - Epoch 185: val_loss=4.3875, val_acc=0.00%
2025-02-06 16:59:02,024 - INFO - Epoch 185: EPOCH_AVG_TRAIN_LOSS=0.6037
2025-02-06 16:59:02,026 - INFO - #################### Training epoch 186 ####################
2025-02-06 16:59:02,026 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:02,321 - INFO - Epoch 186: train_loss=0.5077
2025-02-06 16:59:02,567 - INFO - Epoch 186: train_loss=0.6145
2025-02-06 16:59:02,859 - INFO - Epoch 186: val_loss=4.4620, val_acc=0.00%
2025-02-06 16:59:02,863 - INFO - Epoch 186: EPOCH_AVG_TRAIN_LOSS=0.5611
2025-02-06 16:59:02,865 - INFO - #################### Training epoch 187 ####################
2025-02-06 16:59:02,865 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:03,161 - INFO - Epoch 187: train_loss=0.4453
2025-02-06 16:59:03,407 - INFO - Epoch 187: train_loss=0.7332
2025-02-06 16:59:03,699 - INFO - Epoch 187: val_loss=4.4104, val_acc=0.00%
2025-02-06 16:59:03,702 - INFO - Epoch 187: EPOCH_AVG_TRAIN_LOSS=0.5892
2025-02-06 16:59:03,704 - INFO - #################### Training epoch 188 ####################
2025-02-06 16:59:03,705 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:03,999 - INFO - Epoch 188: train_loss=0.5419
2025-02-06 16:59:04,245 - INFO - Epoch 188: train_loss=0.5312
2025-02-06 16:59:04,534 - INFO - Epoch 188: val_loss=4.4028, val_acc=0.00%
2025-02-06 16:59:04,537 - INFO - Epoch 188: EPOCH_AVG_TRAIN_LOSS=0.5365
2025-02-06 16:59:04,539 - INFO - #################### Training epoch 189 ####################
2025-02-06 16:59:04,540 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:04,834 - INFO - Epoch 189: train_loss=0.6329
2025-02-06 16:59:05,079 - INFO - Epoch 189: train_loss=0.3629
2025-02-06 16:59:05,371 - INFO - Epoch 189: val_loss=4.3851, val_acc=0.00%
2025-02-06 16:59:05,375 - INFO - Epoch 189: EPOCH_AVG_TRAIN_LOSS=0.4979
2025-02-06 16:59:05,377 - INFO - #################### Training epoch 190 ####################
2025-02-06 16:59:05,377 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:05,670 - INFO - Epoch 190: train_loss=0.4130
2025-02-06 16:59:05,915 - INFO - Epoch 190: train_loss=0.7870
2025-02-06 16:59:06,207 - INFO - Epoch 190: val_loss=4.3919, val_acc=0.00%
2025-02-06 16:59:06,211 - INFO - Epoch 190: EPOCH_AVG_TRAIN_LOSS=0.6000
2025-02-06 16:59:06,213 - INFO - #################### Training epoch 191 ####################
2025-02-06 16:59:06,213 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:06,505 - INFO - Epoch 191: train_loss=0.4705
2025-02-06 16:59:06,750 - INFO - Epoch 191: train_loss=0.6653
2025-02-06 16:59:07,038 - INFO - Epoch 191: val_loss=4.3470, val_acc=33.33%
2025-02-06 16:59:07,041 - INFO - Epoch 191: EPOCH_AVG_TRAIN_LOSS=0.5679
2025-02-06 16:59:07,043 - INFO - #################### Training epoch 192 ####################
2025-02-06 16:59:07,043 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:07,342 - INFO - Epoch 192: train_loss=0.4504
2025-02-06 16:59:07,588 - INFO - Epoch 192: train_loss=0.7246
2025-02-06 16:59:07,881 - INFO - Epoch 192: val_loss=4.3406, val_acc=33.33%
2025-02-06 16:59:07,885 - INFO - Epoch 192: EPOCH_AVG_TRAIN_LOSS=0.5875
2025-02-06 16:59:07,887 - INFO - #################### Training epoch 193 ####################
2025-02-06 16:59:07,887 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:08,182 - INFO - Epoch 193: train_loss=0.5020
2025-02-06 16:59:08,428 - INFO - Epoch 193: train_loss=0.6124
2025-02-06 16:59:08,718 - INFO - Epoch 193: val_loss=4.3556, val_acc=0.00%
2025-02-06 16:59:08,722 - INFO - Epoch 193: EPOCH_AVG_TRAIN_LOSS=0.5572
2025-02-06 16:59:08,724 - INFO - #################### Training epoch 194 ####################
2025-02-06 16:59:08,724 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:09,018 - INFO - Epoch 194: train_loss=0.5832
2025-02-06 16:59:09,263 - INFO - Epoch 194: train_loss=0.4371
2025-02-06 16:59:09,551 - INFO - Epoch 194: val_loss=4.4543, val_acc=0.00%
2025-02-06 16:59:09,555 - INFO - Epoch 194: EPOCH_AVG_TRAIN_LOSS=0.5102
2025-02-06 16:59:09,557 - INFO - #################### Training epoch 195 ####################
2025-02-06 16:59:09,557 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:09,850 - INFO - Epoch 195: train_loss=0.6521
2025-02-06 16:59:10,096 - INFO - Epoch 195: train_loss=0.3095
2025-02-06 16:59:10,387 - INFO - Epoch 195: val_loss=4.3848, val_acc=0.00%
2025-02-06 16:59:10,391 - INFO - Epoch 195: EPOCH_AVG_TRAIN_LOSS=0.4808
2025-02-06 16:59:10,393 - INFO - #################### Training epoch 196 ####################
2025-02-06 16:59:10,393 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:10,687 - INFO - Epoch 196: train_loss=0.5570
2025-02-06 16:59:10,932 - INFO - Epoch 196: train_loss=0.4986
2025-02-06 16:59:11,223 - INFO - Epoch 196: val_loss=4.4190, val_acc=0.00%
2025-02-06 16:59:11,227 - INFO - Epoch 196: EPOCH_AVG_TRAIN_LOSS=0.5278
2025-02-06 16:59:11,229 - INFO - #################### Training epoch 197 ####################
2025-02-06 16:59:11,229 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:11,522 - INFO - Epoch 197: train_loss=0.5485
2025-02-06 16:59:11,767 - INFO - Epoch 197: train_loss=0.5171
2025-02-06 16:59:12,059 - INFO - Epoch 197: val_loss=4.3921, val_acc=0.00%
2025-02-06 16:59:12,063 - INFO - Epoch 197: EPOCH_AVG_TRAIN_LOSS=0.5328
2025-02-06 16:59:12,065 - INFO - #################### Training epoch 198 ####################
2025-02-06 16:59:12,065 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:12,360 - INFO - Epoch 198: train_loss=0.5811
2025-02-06 16:59:12,605 - INFO - Epoch 198: train_loss=0.4489
2025-02-06 16:59:12,898 - INFO - Epoch 198: val_loss=4.4486, val_acc=0.00%
2025-02-06 16:59:12,902 - INFO - Epoch 198: EPOCH_AVG_TRAIN_LOSS=0.5150
2025-02-06 16:59:12,904 - INFO - #################### Training epoch 199 ####################
2025-02-06 16:59:12,904 - INFO - Current Learning Rate: 1.525879e-08
2025-02-06 16:59:13,198 - INFO - Epoch 199: train_loss=0.4285
2025-02-06 16:59:13,443 - INFO - Epoch 199: train_loss=0.7660
2025-02-06 16:59:13,735 - INFO - Epoch 199: val_loss=4.4209, val_acc=0.00%
2025-02-06 16:59:13,738 - INFO - Epoch 199: EPOCH_AVG_TRAIN_LOSS=0.5972
2025-02-06 16:59:13,895 - INFO - Model saved.
