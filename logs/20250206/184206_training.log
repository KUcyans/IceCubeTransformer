2025-02-06 18:42:16,059 - INFO - Starting training with the following parameters:
2025-02-06 18:42:16,059 - INFO - | Parameter       | Value               |
|-----------------|---------------------|
| attention       | Scaled Dot-Product|
| d_model         | 128            |
| n_heads         | 8              |
| d_f             | 128            |
| num_layers      | 3              |
| d_input         | 32             |
| num_classes     | 3              |
| dropout         | 0.1            |
| learning_rate   | 0.001          |
| epochs          | 200            |
| batch_size      | 16             |

2025-02-06 18:42:16,684 - INFO - Epoch 0: val_loss=1.1117, val_acc=0.00%
2025-02-06 18:42:16,836 - INFO - #################### Training epoch 0 ####################
2025-02-06 18:42:16,836 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:17,081 - INFO - Epoch 0: train_loss=1.1501
2025-02-06 18:42:17,440 - INFO - Epoch 0: train_loss=1.2649
2025-02-06 18:42:17,748 - INFO - Epoch 0: val_loss=1.1342, val_acc=0.00%
2025-02-06 18:42:17,765 - INFO - Epoch 0: EPOCH_AVG_TRAIN_LOSS=1.1501
2025-02-06 18:42:17,817 - INFO - #################### Training epoch 1 ####################
2025-02-06 18:42:17,817 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:18,231 - INFO - Epoch 1: train_loss=1.0783
2025-02-06 18:42:18,526 - INFO - Epoch 1: train_loss=1.0629
2025-02-06 18:42:18,862 - INFO - Epoch 1: val_loss=1.0987, val_acc=33.33%
2025-02-06 18:42:18,865 - INFO - Epoch 1: EPOCH_AVG_TRAIN_LOSS=1.0629
2025-02-06 18:42:18,981 - INFO - #################### Training epoch 2 ####################
2025-02-06 18:42:18,981 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:19,396 - INFO - Epoch 2: train_loss=1.0437
2025-02-06 18:42:19,688 - INFO - Epoch 2: train_loss=1.1804
2025-02-06 18:42:20,018 - INFO - Epoch 2: val_loss=1.1666, val_acc=0.00%
2025-02-06 18:42:20,022 - INFO - Epoch 2: EPOCH_AVG_TRAIN_LOSS=1.0437
2025-02-06 18:42:20,049 - INFO - #################### Training epoch 3 ####################
2025-02-06 18:42:20,049 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:20,460 - INFO - Epoch 3: train_loss=1.0832
2025-02-06 18:42:20,751 - INFO - Epoch 3: train_loss=1.0244
2025-02-06 18:42:21,079 - INFO - Epoch 3: val_loss=1.0924, val_acc=33.33%
2025-02-06 18:42:21,082 - INFO - Epoch 3: EPOCH_AVG_TRAIN_LOSS=1.0244
2025-02-06 18:42:21,129 - INFO - #################### Training epoch 4 ####################
2025-02-06 18:42:21,129 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:21,540 - INFO - Epoch 4: train_loss=1.0578
2025-02-06 18:42:21,831 - INFO - Epoch 4: train_loss=0.9089
2025-02-06 18:42:22,159 - INFO - Epoch 4: val_loss=1.0495, val_acc=33.33%
2025-02-06 18:42:22,163 - INFO - Epoch 4: EPOCH_AVG_TRAIN_LOSS=0.9089
2025-02-06 18:42:22,190 - INFO - #################### Training epoch 5 ####################
2025-02-06 18:42:22,190 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:22,605 - INFO - Epoch 5: train_loss=0.9933
2025-02-06 18:42:22,897 - INFO - Epoch 5: train_loss=0.9616
2025-02-06 18:42:23,225 - INFO - Epoch 5: val_loss=1.1246, val_acc=33.33%
2025-02-06 18:42:23,229 - INFO - Epoch 5: EPOCH_AVG_TRAIN_LOSS=0.9616
2025-02-06 18:42:23,231 - INFO - #################### Training epoch 6 ####################
2025-02-06 18:42:23,231 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:23,647 - INFO - Epoch 6: train_loss=1.0889
2025-02-06 18:42:23,938 - INFO - Epoch 6: train_loss=0.8439
2025-02-06 18:42:24,268 - INFO - Epoch 6: val_loss=1.1051, val_acc=33.33%
2025-02-06 18:42:24,271 - INFO - Epoch 6: EPOCH_AVG_TRAIN_LOSS=0.8439
2025-02-06 18:42:24,274 - INFO - #################### Training epoch 7 ####################
2025-02-06 18:42:24,274 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:24,682 - INFO - Epoch 7: train_loss=0.9290
2025-02-06 18:42:24,973 - INFO - Epoch 7: train_loss=0.9743
2025-02-06 18:42:25,307 - INFO - Epoch 7: val_loss=1.1188, val_acc=33.33%
2025-02-06 18:42:25,310 - INFO - Epoch 7: EPOCH_AVG_TRAIN_LOSS=0.9290
2025-02-06 18:42:25,313 - INFO - #################### Training epoch 8 ####################
2025-02-06 18:42:25,313 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:25,725 - INFO - Epoch 8: train_loss=0.9090
2025-02-06 18:42:26,015 - INFO - Epoch 8: train_loss=0.8720
2025-02-06 18:42:26,346 - INFO - Epoch 8: val_loss=1.4294, val_acc=0.00%
2025-02-06 18:42:26,350 - INFO - Epoch 8: EPOCH_AVG_TRAIN_LOSS=0.8720
2025-02-06 18:42:26,352 - INFO - #################### Training epoch 9 ####################
2025-02-06 18:42:26,352 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:26,768 - INFO - Epoch 9: train_loss=1.0272
2025-02-06 18:42:27,058 - INFO - Epoch 9: train_loss=0.6637
2025-02-06 18:42:27,388 - INFO - Epoch 9: val_loss=1.1218, val_acc=33.33%
2025-02-06 18:42:27,392 - INFO - Epoch 9: EPOCH_AVG_TRAIN_LOSS=0.6637
2025-02-06 18:42:27,394 - INFO - #################### Training epoch 10 ####################
2025-02-06 18:42:27,394 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:27,808 - INFO - Epoch 10: train_loss=0.8859
2025-02-06 18:42:28,098 - INFO - Epoch 10: train_loss=0.9594
2025-02-06 18:42:28,431 - INFO - Epoch 10: val_loss=1.0814, val_acc=33.33%
2025-02-06 18:42:28,435 - INFO - Epoch 10: EPOCH_AVG_TRAIN_LOSS=0.8859
2025-02-06 18:42:28,462 - INFO - #################### Training epoch 11 ####################
2025-02-06 18:42:28,462 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:28,876 - INFO - Epoch 11: train_loss=0.8784
2025-02-06 18:42:29,166 - INFO - Epoch 11: train_loss=0.8810
2025-02-06 18:42:29,492 - INFO - Epoch 11: val_loss=1.2539, val_acc=33.33%
2025-02-06 18:42:29,496 - INFO - Epoch 11: EPOCH_AVG_TRAIN_LOSS=0.8784
2025-02-06 18:42:29,499 - INFO - #################### Training epoch 12 ####################
2025-02-06 18:42:29,499 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:29,910 - INFO - Epoch 12: train_loss=0.9689
2025-02-06 18:42:30,201 - INFO - Epoch 12: train_loss=0.9475
2025-02-06 18:42:30,532 - INFO - Epoch 12: val_loss=1.1313, val_acc=0.00%
2025-02-06 18:42:30,536 - INFO - Epoch 12: EPOCH_AVG_TRAIN_LOSS=0.9475
2025-02-06 18:42:30,538 - INFO - #################### Training epoch 13 ####################
2025-02-06 18:42:30,538 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:30,951 - INFO - Epoch 13: train_loss=0.8980
2025-02-06 18:42:31,241 - INFO - Epoch 13: train_loss=0.6292
2025-02-06 18:42:31,572 - INFO - Epoch 13: val_loss=1.0982, val_acc=33.33%
2025-02-06 18:42:31,575 - INFO - Epoch 13: EPOCH_AVG_TRAIN_LOSS=0.6292
2025-02-06 18:42:31,578 - INFO - #################### Training epoch 14 ####################
2025-02-06 18:42:31,578 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:31,991 - INFO - Epoch 14: train_loss=0.7290
2025-02-06 18:42:32,282 - INFO - Epoch 14: train_loss=1.1333
2025-02-06 18:42:32,619 - INFO - Epoch 14: val_loss=1.1393, val_acc=33.33%
2025-02-06 18:42:32,623 - INFO - Epoch 14: EPOCH_AVG_TRAIN_LOSS=0.7290
2025-02-06 18:42:32,625 - INFO - #################### Training epoch 15 ####################
2025-02-06 18:42:32,625 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:33,041 - INFO - Epoch 15: train_loss=1.0346
2025-02-06 18:42:33,332 - INFO - Epoch 15: train_loss=0.5027
2025-02-06 18:42:33,664 - INFO - Epoch 15: val_loss=1.1016, val_acc=33.33%
2025-02-06 18:42:33,668 - INFO - Epoch 15: EPOCH_AVG_TRAIN_LOSS=0.5027
2025-02-06 18:42:33,670 - INFO - #################### Training epoch 16 ####################
2025-02-06 18:42:33,670 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:34,085 - INFO - Epoch 16: train_loss=0.6818
2025-02-06 18:42:34,376 - INFO - Epoch 16: train_loss=1.1347
2025-02-06 18:42:34,701 - INFO - Epoch 16: val_loss=1.0327, val_acc=33.33%
2025-02-06 18:42:34,705 - INFO - Epoch 16: EPOCH_AVG_TRAIN_LOSS=0.6818
2025-02-06 18:42:34,732 - INFO - #################### Training epoch 17 ####################
2025-02-06 18:42:34,732 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:35,148 - INFO - Epoch 17: train_loss=0.8560
2025-02-06 18:42:35,438 - INFO - Epoch 17: train_loss=0.5752
2025-02-06 18:42:35,772 - INFO - Epoch 17: val_loss=0.9637, val_acc=33.33%
2025-02-06 18:42:35,776 - INFO - Epoch 17: EPOCH_AVG_TRAIN_LOSS=0.5752
2025-02-06 18:42:35,818 - INFO - #################### Training epoch 18 ####################
2025-02-06 18:42:35,819 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:36,235 - INFO - Epoch 18: train_loss=0.7420
2025-02-06 18:42:36,526 - INFO - Epoch 18: train_loss=0.7318
2025-02-06 18:42:36,855 - INFO - Epoch 18: val_loss=1.0065, val_acc=33.33%
2025-02-06 18:42:36,859 - INFO - Epoch 18: EPOCH_AVG_TRAIN_LOSS=0.7318
2025-02-06 18:42:36,887 - INFO - #################### Training epoch 19 ####################
2025-02-06 18:42:36,887 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:37,303 - INFO - Epoch 19: train_loss=0.7025
2025-02-06 18:42:37,594 - INFO - Epoch 19: train_loss=0.7962
2025-02-06 18:42:37,924 - INFO - Epoch 19: val_loss=1.0836, val_acc=33.33%
2025-02-06 18:42:37,928 - INFO - Epoch 19: EPOCH_AVG_TRAIN_LOSS=0.7025
2025-02-06 18:42:37,930 - INFO - #################### Training epoch 20 ####################
2025-02-06 18:42:37,930 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:38,348 - INFO - Epoch 20: train_loss=0.6593
2025-02-06 18:42:38,638 - INFO - Epoch 20: train_loss=0.7369
2025-02-06 18:42:38,969 - INFO - Epoch 20: val_loss=1.1753, val_acc=33.33%
2025-02-06 18:42:38,973 - INFO - Epoch 20: EPOCH_AVG_TRAIN_LOSS=0.6593
2025-02-06 18:42:38,975 - INFO - #################### Training epoch 21 ####################
2025-02-06 18:42:38,975 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:39,392 - INFO - Epoch 21: train_loss=0.8446
2025-02-06 18:42:39,682 - INFO - Epoch 21: train_loss=0.4400
2025-02-06 18:42:40,013 - INFO - Epoch 21: val_loss=1.2633, val_acc=33.33%
2025-02-06 18:42:40,016 - INFO - Epoch 21: EPOCH_AVG_TRAIN_LOSS=0.4400
2025-02-06 18:42:40,019 - INFO - #################### Training epoch 22 ####################
2025-02-06 18:42:40,019 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:40,437 - INFO - Epoch 22: train_loss=0.7220
2025-02-06 18:42:40,727 - INFO - Epoch 22: train_loss=0.6719
2025-02-06 18:42:41,061 - INFO - Epoch 22: val_loss=1.3448, val_acc=0.00%
2025-02-06 18:42:41,065 - INFO - Epoch 22: EPOCH_AVG_TRAIN_LOSS=0.6719
2025-02-06 18:42:41,067 - INFO - #################### Training epoch 23 ####################
2025-02-06 18:42:41,067 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:41,484 - INFO - Epoch 23: train_loss=0.8211
2025-02-06 18:42:41,774 - INFO - Epoch 23: train_loss=0.5534
2025-02-06 18:42:42,105 - INFO - Epoch 23: val_loss=1.4310, val_acc=0.00%
2025-02-06 18:42:42,109 - INFO - Epoch 23: EPOCH_AVG_TRAIN_LOSS=0.5534
2025-02-06 18:42:42,112 - INFO - #################### Training epoch 24 ####################
2025-02-06 18:42:42,112 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:42,528 - INFO - Epoch 24: train_loss=0.6357
2025-02-06 18:42:42,819 - INFO - Epoch 24: train_loss=0.9583
2025-02-06 18:42:43,153 - INFO - Epoch 24: val_loss=1.4900, val_acc=0.00%
2025-02-06 18:42:43,157 - INFO - Epoch 24: EPOCH_AVG_TRAIN_LOSS=0.6357
2025-02-06 18:42:43,159 - INFO - #################### Training epoch 25 ####################
2025-02-06 18:42:43,159 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:43,575 - INFO - Epoch 25: train_loss=0.6799
2025-02-06 18:42:43,865 - INFO - Epoch 25: train_loss=1.0430
2025-02-06 18:42:44,200 - INFO - Epoch 25: val_loss=1.5667, val_acc=0.00%
2025-02-06 18:42:44,205 - INFO - Epoch 25: EPOCH_AVG_TRAIN_LOSS=0.6799
2025-02-06 18:42:44,208 - INFO - #################### Training epoch 26 ####################
2025-02-06 18:42:44,208 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:44,622 - INFO - Epoch 26: train_loss=0.5878
2025-02-06 18:42:44,912 - INFO - Epoch 26: train_loss=1.2318
2025-02-06 18:42:45,239 - INFO - Epoch 26: val_loss=1.5911, val_acc=0.00%
2025-02-06 18:42:45,243 - INFO - Epoch 26: EPOCH_AVG_TRAIN_LOSS=0.5878
2025-02-06 18:42:45,245 - INFO - #################### Training epoch 27 ####################
2025-02-06 18:42:45,245 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:45,660 - INFO - Epoch 27: train_loss=0.8653
2025-02-06 18:42:45,951 - INFO - Epoch 27: train_loss=0.8187
2025-02-06 18:42:46,284 - INFO - Epoch 27: val_loss=1.6381, val_acc=0.00%
2025-02-06 18:42:46,288 - INFO - Epoch 27: EPOCH_AVG_TRAIN_LOSS=0.8187
2025-02-06 18:42:46,290 - INFO - #################### Training epoch 28 ####################
2025-02-06 18:42:46,290 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:46,706 - INFO - Epoch 28: train_loss=0.8708
2025-02-06 18:42:46,997 - INFO - Epoch 28: train_loss=0.8688
2025-02-06 18:42:47,321 - INFO - Epoch 28: val_loss=1.6760, val_acc=0.00%
2025-02-06 18:42:47,325 - INFO - Epoch 28: EPOCH_AVG_TRAIN_LOSS=0.8688
2025-02-06 18:42:47,327 - INFO - #################### Training epoch 29 ####################
2025-02-06 18:42:47,327 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:47,742 - INFO - Epoch 29: train_loss=0.9413
2025-02-06 18:42:48,032 - INFO - Epoch 29: train_loss=0.7669
2025-02-06 18:42:48,364 - INFO - Epoch 29: val_loss=1.7114, val_acc=0.00%
2025-02-06 18:42:48,367 - INFO - Epoch 29: EPOCH_AVG_TRAIN_LOSS=0.7669
2025-02-06 18:42:48,369 - INFO - #################### Training epoch 30 ####################
2025-02-06 18:42:48,369 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:48,785 - INFO - Epoch 30: train_loss=0.9305
2025-02-06 18:42:49,074 - INFO - Epoch 30: train_loss=nan
2025-02-06 18:42:49,407 - INFO - Epoch 30: val_loss=nan, val_acc=66.67%
2025-02-06 18:42:49,411 - INFO - Epoch 30: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:42:49,413 - INFO - #################### Training epoch 31 ####################
2025-02-06 18:42:49,413 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:49,824 - INFO - Epoch 31: train_loss=nan
2025-02-06 18:42:50,113 - INFO - Epoch 31: train_loss=nan
2025-02-06 18:42:50,444 - INFO - Epoch 31: val_loss=nan, val_acc=66.67%
2025-02-06 18:42:50,448 - INFO - Epoch 31: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:42:50,450 - INFO - #################### Training epoch 32 ####################
2025-02-06 18:42:50,450 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:50,860 - INFO - Epoch 32: train_loss=nan
2025-02-06 18:42:51,150 - INFO - Epoch 32: train_loss=nan
2025-02-06 18:42:51,478 - INFO - Epoch 32: val_loss=nan, val_acc=66.67%
2025-02-06 18:42:51,482 - INFO - Epoch 32: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:42:51,484 - INFO - #################### Training epoch 33 ####################
2025-02-06 18:42:51,484 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:51,897 - INFO - Epoch 33: train_loss=nan
2025-02-06 18:42:52,186 - INFO - Epoch 33: train_loss=nan
2025-02-06 18:42:52,520 - INFO - Epoch 33: val_loss=nan, val_acc=66.67%
2025-02-06 18:42:52,524 - INFO - Epoch 33: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:42:52,526 - INFO - #################### Training epoch 34 ####################
2025-02-06 18:42:52,526 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:52,940 - INFO - Epoch 34: train_loss=nan
2025-02-06 18:42:53,230 - INFO - Epoch 34: train_loss=nan
2025-02-06 18:42:53,563 - INFO - Epoch 34: val_loss=nan, val_acc=66.67%
2025-02-06 18:42:53,567 - INFO - Epoch 34: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:42:53,569 - INFO - #################### Training epoch 35 ####################
2025-02-06 18:42:53,569 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:53,983 - INFO - Epoch 35: train_loss=nan
2025-02-06 18:42:54,272 - INFO - Epoch 35: train_loss=nan
2025-02-06 18:42:54,598 - INFO - Epoch 35: val_loss=nan, val_acc=66.67%
2025-02-06 18:42:54,602 - INFO - Epoch 35: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:42:54,604 - INFO - #################### Training epoch 36 ####################
2025-02-06 18:42:54,604 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:55,018 - INFO - Epoch 36: train_loss=nan
2025-02-06 18:42:55,308 - INFO - Epoch 36: train_loss=nan
2025-02-06 18:42:55,640 - INFO - Epoch 36: val_loss=nan, val_acc=66.67%
2025-02-06 18:42:55,644 - INFO - Epoch 36: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:42:55,647 - INFO - #################### Training epoch 37 ####################
2025-02-06 18:42:55,647 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:56,054 - INFO - Epoch 37: train_loss=nan
2025-02-06 18:42:56,344 - INFO - Epoch 37: train_loss=nan
2025-02-06 18:42:56,674 - INFO - Epoch 37: val_loss=nan, val_acc=66.67%
2025-02-06 18:42:56,678 - INFO - Epoch 37: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:42:56,680 - INFO - #################### Training epoch 38 ####################
2025-02-06 18:42:56,680 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:57,088 - INFO - Epoch 38: train_loss=nan
2025-02-06 18:42:57,377 - INFO - Epoch 38: train_loss=nan
2025-02-06 18:42:57,702 - INFO - Epoch 38: val_loss=nan, val_acc=66.67%
2025-02-06 18:42:57,705 - INFO - Epoch 38: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:42:57,707 - INFO - #################### Training epoch 39 ####################
2025-02-06 18:42:57,708 - INFO - Current Learning Rate: 1.000000e-03
2025-02-06 18:42:58,119 - INFO - Epoch 39: train_loss=nan
2025-02-06 18:42:58,409 - INFO - Epoch 39: train_loss=nan
2025-02-06 18:42:58,739 - INFO - Epoch 39: val_loss=nan, val_acc=66.67%
2025-02-06 18:42:58,742 - INFO - Epoch 39: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:42:58,745 - INFO - #################### Training epoch 40 ####################
2025-02-06 18:42:58,745 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:42:59,153 - INFO - Epoch 40: train_loss=nan
2025-02-06 18:42:59,443 - INFO - Epoch 40: train_loss=nan
2025-02-06 18:42:59,773 - INFO - Epoch 40: val_loss=nan, val_acc=66.67%
2025-02-06 18:42:59,776 - INFO - Epoch 40: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:42:59,779 - INFO - #################### Training epoch 41 ####################
2025-02-06 18:42:59,779 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:00,191 - INFO - Epoch 41: train_loss=nan
2025-02-06 18:43:00,481 - INFO - Epoch 41: train_loss=nan
2025-02-06 18:43:00,811 - INFO - Epoch 41: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:00,815 - INFO - Epoch 41: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:00,817 - INFO - #################### Training epoch 42 ####################
2025-02-06 18:43:00,817 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:01,231 - INFO - Epoch 42: train_loss=nan
2025-02-06 18:43:01,521 - INFO - Epoch 42: train_loss=nan
2025-02-06 18:43:01,847 - INFO - Epoch 42: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:01,851 - INFO - Epoch 42: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:01,853 - INFO - #################### Training epoch 43 ####################
2025-02-06 18:43:01,853 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:02,267 - INFO - Epoch 43: train_loss=nan
2025-02-06 18:43:02,556 - INFO - Epoch 43: train_loss=nan
2025-02-06 18:43:02,889 - INFO - Epoch 43: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:02,893 - INFO - Epoch 43: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:02,895 - INFO - #################### Training epoch 44 ####################
2025-02-06 18:43:02,895 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:03,307 - INFO - Epoch 44: train_loss=nan
2025-02-06 18:43:03,597 - INFO - Epoch 44: train_loss=nan
2025-02-06 18:43:03,929 - INFO - Epoch 44: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:03,933 - INFO - Epoch 44: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:03,935 - INFO - #################### Training epoch 45 ####################
2025-02-06 18:43:03,935 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:04,345 - INFO - Epoch 45: train_loss=nan
2025-02-06 18:43:04,633 - INFO - Epoch 45: train_loss=nan
2025-02-06 18:43:04,965 - INFO - Epoch 45: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:04,968 - INFO - Epoch 45: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:04,971 - INFO - #################### Training epoch 46 ####################
2025-02-06 18:43:04,971 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:05,384 - INFO - Epoch 46: train_loss=nan
2025-02-06 18:43:05,673 - INFO - Epoch 46: train_loss=nan
2025-02-06 18:43:06,003 - INFO - Epoch 46: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:06,006 - INFO - Epoch 46: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:06,008 - INFO - #################### Training epoch 47 ####################
2025-02-06 18:43:06,009 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:06,420 - INFO - Epoch 47: train_loss=nan
2025-02-06 18:43:06,710 - INFO - Epoch 47: train_loss=nan
2025-02-06 18:43:07,040 - INFO - Epoch 47: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:07,043 - INFO - Epoch 47: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:07,045 - INFO - #################### Training epoch 48 ####################
2025-02-06 18:43:07,045 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:07,460 - INFO - Epoch 48: train_loss=nan
2025-02-06 18:43:07,750 - INFO - Epoch 48: train_loss=nan
2025-02-06 18:43:08,082 - INFO - Epoch 48: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:08,085 - INFO - Epoch 48: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:08,088 - INFO - #################### Training epoch 49 ####################
2025-02-06 18:43:08,088 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:08,503 - INFO - Epoch 49: train_loss=nan
2025-02-06 18:43:08,792 - INFO - Epoch 49: train_loss=nan
2025-02-06 18:43:09,123 - INFO - Epoch 49: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:09,127 - INFO - Epoch 49: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:09,129 - INFO - #################### Training epoch 50 ####################
2025-02-06 18:43:09,129 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:09,544 - INFO - Epoch 50: train_loss=nan
2025-02-06 18:43:09,833 - INFO - Epoch 50: train_loss=nan
2025-02-06 18:43:10,165 - INFO - Epoch 50: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:10,169 - INFO - Epoch 50: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:10,171 - INFO - #################### Training epoch 51 ####################
2025-02-06 18:43:10,171 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:10,585 - INFO - Epoch 51: train_loss=nan
2025-02-06 18:43:10,874 - INFO - Epoch 51: train_loss=nan
2025-02-06 18:43:11,206 - INFO - Epoch 51: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:11,209 - INFO - Epoch 51: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:11,211 - INFO - #################### Training epoch 52 ####################
2025-02-06 18:43:11,211 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:11,624 - INFO - Epoch 52: train_loss=nan
2025-02-06 18:43:11,913 - INFO - Epoch 52: train_loss=nan
2025-02-06 18:43:12,239 - INFO - Epoch 52: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:12,243 - INFO - Epoch 52: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:12,245 - INFO - #################### Training epoch 53 ####################
2025-02-06 18:43:12,245 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:12,658 - INFO - Epoch 53: train_loss=nan
2025-02-06 18:43:12,949 - INFO - Epoch 53: train_loss=nan
2025-02-06 18:43:13,282 - INFO - Epoch 53: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:13,286 - INFO - Epoch 53: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:13,288 - INFO - #################### Training epoch 54 ####################
2025-02-06 18:43:13,288 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:13,703 - INFO - Epoch 54: train_loss=nan
2025-02-06 18:43:13,993 - INFO - Epoch 54: train_loss=nan
2025-02-06 18:43:14,320 - INFO - Epoch 54: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:14,323 - INFO - Epoch 54: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:14,326 - INFO - #################### Training epoch 55 ####################
2025-02-06 18:43:14,326 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:14,736 - INFO - Epoch 55: train_loss=nan
2025-02-06 18:43:15,026 - INFO - Epoch 55: train_loss=nan
2025-02-06 18:43:15,356 - INFO - Epoch 55: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:15,359 - INFO - Epoch 55: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:15,361 - INFO - #################### Training epoch 56 ####################
2025-02-06 18:43:15,361 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:15,774 - INFO - Epoch 56: train_loss=nan
2025-02-06 18:43:16,064 - INFO - Epoch 56: train_loss=nan
2025-02-06 18:43:16,394 - INFO - Epoch 56: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:16,398 - INFO - Epoch 56: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:16,400 - INFO - #################### Training epoch 57 ####################
2025-02-06 18:43:16,400 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:16,814 - INFO - Epoch 57: train_loss=nan
2025-02-06 18:43:17,104 - INFO - Epoch 57: train_loss=nan
2025-02-06 18:43:17,430 - INFO - Epoch 57: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:17,434 - INFO - Epoch 57: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:17,436 - INFO - #################### Training epoch 58 ####################
2025-02-06 18:43:17,436 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:17,849 - INFO - Epoch 58: train_loss=nan
2025-02-06 18:43:18,140 - INFO - Epoch 58: train_loss=nan
2025-02-06 18:43:18,473 - INFO - Epoch 58: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:18,476 - INFO - Epoch 58: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:18,478 - INFO - #################### Training epoch 59 ####################
2025-02-06 18:43:18,479 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:18,892 - INFO - Epoch 59: train_loss=nan
2025-02-06 18:43:19,183 - INFO - Epoch 59: train_loss=nan
2025-02-06 18:43:19,509 - INFO - Epoch 59: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:19,513 - INFO - Epoch 59: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:19,515 - INFO - #################### Training epoch 60 ####################
2025-02-06 18:43:19,515 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:19,929 - INFO - Epoch 60: train_loss=nan
2025-02-06 18:43:20,221 - INFO - Epoch 60: train_loss=nan
2025-02-06 18:43:20,545 - INFO - Epoch 60: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:20,549 - INFO - Epoch 60: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:20,551 - INFO - #################### Training epoch 61 ####################
2025-02-06 18:43:20,551 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:20,964 - INFO - Epoch 61: train_loss=nan
2025-02-06 18:43:21,255 - INFO - Epoch 61: train_loss=nan
2025-02-06 18:43:21,584 - INFO - Epoch 61: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:21,587 - INFO - Epoch 61: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:21,590 - INFO - #################### Training epoch 62 ####################
2025-02-06 18:43:21,590 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:22,003 - INFO - Epoch 62: train_loss=nan
2025-02-06 18:43:22,294 - INFO - Epoch 62: train_loss=nan
2025-02-06 18:43:22,623 - INFO - Epoch 62: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:22,627 - INFO - Epoch 62: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:22,629 - INFO - #################### Training epoch 63 ####################
2025-02-06 18:43:22,629 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:23,039 - INFO - Epoch 63: train_loss=nan
2025-02-06 18:43:23,330 - INFO - Epoch 63: train_loss=nan
2025-02-06 18:43:23,658 - INFO - Epoch 63: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:23,662 - INFO - Epoch 63: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:23,664 - INFO - #################### Training epoch 64 ####################
2025-02-06 18:43:23,664 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:24,076 - INFO - Epoch 64: train_loss=nan
2025-02-06 18:43:24,366 - INFO - Epoch 64: train_loss=nan
2025-02-06 18:43:24,698 - INFO - Epoch 64: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:24,702 - INFO - Epoch 64: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:24,704 - INFO - #################### Training epoch 65 ####################
2025-02-06 18:43:24,704 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:25,118 - INFO - Epoch 65: train_loss=nan
2025-02-06 18:43:25,409 - INFO - Epoch 65: train_loss=nan
2025-02-06 18:43:25,738 - INFO - Epoch 65: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:25,742 - INFO - Epoch 65: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:25,744 - INFO - #################### Training epoch 66 ####################
2025-02-06 18:43:25,744 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:26,156 - INFO - Epoch 66: train_loss=nan
2025-02-06 18:43:26,446 - INFO - Epoch 66: train_loss=nan
2025-02-06 18:43:26,777 - INFO - Epoch 66: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:26,780 - INFO - Epoch 66: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:26,783 - INFO - #################### Training epoch 67 ####################
2025-02-06 18:43:26,783 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:27,200 - INFO - Epoch 67: train_loss=nan
2025-02-06 18:43:27,490 - INFO - Epoch 67: train_loss=nan
2025-02-06 18:43:27,822 - INFO - Epoch 67: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:27,825 - INFO - Epoch 67: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:27,827 - INFO - #################### Training epoch 68 ####################
2025-02-06 18:43:27,827 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:28,242 - INFO - Epoch 68: train_loss=nan
2025-02-06 18:43:28,533 - INFO - Epoch 68: train_loss=nan
2025-02-06 18:43:28,863 - INFO - Epoch 68: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:28,867 - INFO - Epoch 68: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:28,869 - INFO - #################### Training epoch 69 ####################
2025-02-06 18:43:28,869 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:29,283 - INFO - Epoch 69: train_loss=nan
2025-02-06 18:43:29,575 - INFO - Epoch 69: train_loss=nan
2025-02-06 18:43:29,906 - INFO - Epoch 69: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:29,910 - INFO - Epoch 69: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:29,912 - INFO - #################### Training epoch 70 ####################
2025-02-06 18:43:29,912 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:30,326 - INFO - Epoch 70: train_loss=nan
2025-02-06 18:43:30,617 - INFO - Epoch 70: train_loss=nan
2025-02-06 18:43:30,948 - INFO - Epoch 70: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:30,951 - INFO - Epoch 70: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:30,954 - INFO - #################### Training epoch 71 ####################
2025-02-06 18:43:30,954 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:31,368 - INFO - Epoch 71: train_loss=nan
2025-02-06 18:43:31,658 - INFO - Epoch 71: train_loss=nan
2025-02-06 18:43:31,987 - INFO - Epoch 71: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:31,991 - INFO - Epoch 71: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:31,993 - INFO - #################### Training epoch 72 ####################
2025-02-06 18:43:31,993 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:32,405 - INFO - Epoch 72: train_loss=nan
2025-02-06 18:43:32,695 - INFO - Epoch 72: train_loss=nan
2025-02-06 18:43:33,028 - INFO - Epoch 72: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:33,032 - INFO - Epoch 72: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:33,034 - INFO - #################### Training epoch 73 ####################
2025-02-06 18:43:33,034 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:33,448 - INFO - Epoch 73: train_loss=nan
2025-02-06 18:43:33,738 - INFO - Epoch 73: train_loss=nan
2025-02-06 18:43:34,066 - INFO - Epoch 73: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:34,070 - INFO - Epoch 73: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:34,073 - INFO - #################### Training epoch 74 ####################
2025-02-06 18:43:34,073 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:34,486 - INFO - Epoch 74: train_loss=nan
2025-02-06 18:43:34,776 - INFO - Epoch 74: train_loss=nan
2025-02-06 18:43:35,101 - INFO - Epoch 74: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:35,105 - INFO - Epoch 74: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:35,107 - INFO - #################### Training epoch 75 ####################
2025-02-06 18:43:35,107 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:35,519 - INFO - Epoch 75: train_loss=nan
2025-02-06 18:43:35,810 - INFO - Epoch 75: train_loss=nan
2025-02-06 18:43:36,138 - INFO - Epoch 75: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:36,142 - INFO - Epoch 75: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:36,144 - INFO - #################### Training epoch 76 ####################
2025-02-06 18:43:36,144 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:36,555 - INFO - Epoch 76: train_loss=nan
2025-02-06 18:43:36,845 - INFO - Epoch 76: train_loss=nan
2025-02-06 18:43:37,183 - INFO - Epoch 76: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:37,186 - INFO - Epoch 76: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:37,189 - INFO - #################### Training epoch 77 ####################
2025-02-06 18:43:37,189 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:37,597 - INFO - Epoch 77: train_loss=nan
2025-02-06 18:43:37,887 - INFO - Epoch 77: train_loss=nan
2025-02-06 18:43:38,214 - INFO - Epoch 77: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:38,217 - INFO - Epoch 77: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:38,220 - INFO - #################### Training epoch 78 ####################
2025-02-06 18:43:38,220 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:38,628 - INFO - Epoch 78: train_loss=nan
2025-02-06 18:43:38,918 - INFO - Epoch 78: train_loss=nan
2025-02-06 18:43:39,247 - INFO - Epoch 78: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:39,250 - INFO - Epoch 78: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:39,253 - INFO - #################### Training epoch 79 ####################
2025-02-06 18:43:39,253 - INFO - Current Learning Rate: 7.000000e-04
2025-02-06 18:43:39,665 - INFO - Epoch 79: train_loss=nan
2025-02-06 18:43:39,956 - INFO - Epoch 79: train_loss=nan
2025-02-06 18:43:40,286 - INFO - Epoch 79: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:40,289 - INFO - Epoch 79: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:40,292 - INFO - #################### Training epoch 80 ####################
2025-02-06 18:43:40,292 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:40,704 - INFO - Epoch 80: train_loss=nan
2025-02-06 18:43:40,994 - INFO - Epoch 80: train_loss=nan
2025-02-06 18:43:41,324 - INFO - Epoch 80: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:41,327 - INFO - Epoch 80: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:41,330 - INFO - #################### Training epoch 81 ####################
2025-02-06 18:43:41,330 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:41,743 - INFO - Epoch 81: train_loss=nan
2025-02-06 18:43:42,034 - INFO - Epoch 81: train_loss=nan
2025-02-06 18:43:42,359 - INFO - Epoch 81: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:42,362 - INFO - Epoch 81: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:42,365 - INFO - #################### Training epoch 82 ####################
2025-02-06 18:43:42,365 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:42,777 - INFO - Epoch 82: train_loss=nan
2025-02-06 18:43:43,068 - INFO - Epoch 82: train_loss=nan
2025-02-06 18:43:43,397 - INFO - Epoch 82: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:43,401 - INFO - Epoch 82: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:43,403 - INFO - #################### Training epoch 83 ####################
2025-02-06 18:43:43,403 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:43,816 - INFO - Epoch 83: train_loss=nan
2025-02-06 18:43:44,106 - INFO - Epoch 83: train_loss=nan
2025-02-06 18:43:44,432 - INFO - Epoch 83: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:44,436 - INFO - Epoch 83: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:44,438 - INFO - #################### Training epoch 84 ####################
2025-02-06 18:43:44,438 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:44,851 - INFO - Epoch 84: train_loss=nan
2025-02-06 18:43:45,142 - INFO - Epoch 84: train_loss=nan
2025-02-06 18:43:45,474 - INFO - Epoch 84: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:45,478 - INFO - Epoch 84: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:45,480 - INFO - #################### Training epoch 85 ####################
2025-02-06 18:43:45,480 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:45,893 - INFO - Epoch 85: train_loss=nan
2025-02-06 18:43:46,183 - INFO - Epoch 85: train_loss=nan
2025-02-06 18:43:46,513 - INFO - Epoch 85: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:46,517 - INFO - Epoch 85: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:46,519 - INFO - #################### Training epoch 86 ####################
2025-02-06 18:43:46,519 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:46,930 - INFO - Epoch 86: train_loss=nan
2025-02-06 18:43:47,220 - INFO - Epoch 86: train_loss=nan
2025-02-06 18:43:47,550 - INFO - Epoch 86: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:47,554 - INFO - Epoch 86: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:47,556 - INFO - #################### Training epoch 87 ####################
2025-02-06 18:43:47,556 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:47,970 - INFO - Epoch 87: train_loss=nan
2025-02-06 18:43:48,260 - INFO - Epoch 87: train_loss=nan
2025-02-06 18:43:48,590 - INFO - Epoch 87: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:48,593 - INFO - Epoch 87: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:48,596 - INFO - #################### Training epoch 88 ####################
2025-02-06 18:43:48,596 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:49,009 - INFO - Epoch 88: train_loss=nan
2025-02-06 18:43:49,298 - INFO - Epoch 88: train_loss=nan
2025-02-06 18:43:49,627 - INFO - Epoch 88: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:49,631 - INFO - Epoch 88: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:49,633 - INFO - #################### Training epoch 89 ####################
2025-02-06 18:43:49,633 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:50,051 - INFO - Epoch 89: train_loss=nan
2025-02-06 18:43:50,341 - INFO - Epoch 89: train_loss=nan
2025-02-06 18:43:50,669 - INFO - Epoch 89: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:50,673 - INFO - Epoch 89: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:50,675 - INFO - #################### Training epoch 90 ####################
2025-02-06 18:43:50,675 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:51,085 - INFO - Epoch 90: train_loss=nan
2025-02-06 18:43:51,375 - INFO - Epoch 90: train_loss=nan
2025-02-06 18:43:51,703 - INFO - Epoch 90: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:51,706 - INFO - Epoch 90: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:51,708 - INFO - #################### Training epoch 91 ####################
2025-02-06 18:43:51,708 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:52,113 - INFO - Epoch 91: train_loss=nan
2025-02-06 18:43:52,403 - INFO - Epoch 91: train_loss=nan
2025-02-06 18:43:52,729 - INFO - Epoch 91: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:52,732 - INFO - Epoch 91: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:52,734 - INFO - #################### Training epoch 92 ####################
2025-02-06 18:43:52,734 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:53,142 - INFO - Epoch 92: train_loss=nan
2025-02-06 18:43:53,433 - INFO - Epoch 92: train_loss=nan
2025-02-06 18:43:53,760 - INFO - Epoch 92: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:53,763 - INFO - Epoch 92: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:53,765 - INFO - #################### Training epoch 93 ####################
2025-02-06 18:43:53,765 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:54,175 - INFO - Epoch 93: train_loss=nan
2025-02-06 18:43:54,465 - INFO - Epoch 93: train_loss=nan
2025-02-06 18:43:54,799 - INFO - Epoch 93: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:54,802 - INFO - Epoch 93: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:54,805 - INFO - #################### Training epoch 94 ####################
2025-02-06 18:43:54,805 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:55,216 - INFO - Epoch 94: train_loss=nan
2025-02-06 18:43:55,506 - INFO - Epoch 94: train_loss=nan
2025-02-06 18:43:55,832 - INFO - Epoch 94: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:55,836 - INFO - Epoch 94: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:55,838 - INFO - #################### Training epoch 95 ####################
2025-02-06 18:43:55,838 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:56,252 - INFO - Epoch 95: train_loss=nan
2025-02-06 18:43:56,543 - INFO - Epoch 95: train_loss=nan
2025-02-06 18:43:56,868 - INFO - Epoch 95: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:56,871 - INFO - Epoch 95: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:56,873 - INFO - #################### Training epoch 96 ####################
2025-02-06 18:43:56,873 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:57,289 - INFO - Epoch 96: train_loss=nan
2025-02-06 18:43:57,580 - INFO - Epoch 96: train_loss=nan
2025-02-06 18:43:57,917 - INFO - Epoch 96: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:57,920 - INFO - Epoch 96: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:57,923 - INFO - #################### Training epoch 97 ####################
2025-02-06 18:43:57,923 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:58,332 - INFO - Epoch 97: train_loss=nan
2025-02-06 18:43:58,622 - INFO - Epoch 97: train_loss=nan
2025-02-06 18:43:58,951 - INFO - Epoch 97: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:58,955 - INFO - Epoch 97: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:43:58,957 - INFO - #################### Training epoch 98 ####################
2025-02-06 18:43:58,957 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:43:59,369 - INFO - Epoch 98: train_loss=nan
2025-02-06 18:43:59,659 - INFO - Epoch 98: train_loss=nan
2025-02-06 18:43:59,994 - INFO - Epoch 98: val_loss=nan, val_acc=66.67%
2025-02-06 18:43:59,998 - INFO - Epoch 98: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:00,001 - INFO - #################### Training epoch 99 ####################
2025-02-06 18:44:00,001 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:00,414 - INFO - Epoch 99: train_loss=nan
2025-02-06 18:44:00,705 - INFO - Epoch 99: train_loss=nan
2025-02-06 18:44:01,037 - INFO - Epoch 99: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:01,040 - INFO - Epoch 99: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:01,043 - INFO - #################### Training epoch 100 ####################
2025-02-06 18:44:01,043 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:01,457 - INFO - Epoch 100: train_loss=nan
2025-02-06 18:44:01,749 - INFO - Epoch 100: train_loss=nan
2025-02-06 18:44:02,076 - INFO - Epoch 100: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:02,079 - INFO - Epoch 100: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:02,082 - INFO - #################### Training epoch 101 ####################
2025-02-06 18:44:02,082 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:02,491 - INFO - Epoch 101: train_loss=nan
2025-02-06 18:44:02,782 - INFO - Epoch 101: train_loss=nan
2025-02-06 18:44:03,106 - INFO - Epoch 101: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:03,109 - INFO - Epoch 101: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:03,112 - INFO - #################### Training epoch 102 ####################
2025-02-06 18:44:03,112 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:03,524 - INFO - Epoch 102: train_loss=nan
2025-02-06 18:44:03,814 - INFO - Epoch 102: train_loss=nan
2025-02-06 18:44:04,142 - INFO - Epoch 102: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:04,146 - INFO - Epoch 102: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:04,148 - INFO - #################### Training epoch 103 ####################
2025-02-06 18:44:04,148 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:04,557 - INFO - Epoch 103: train_loss=nan
2025-02-06 18:44:04,848 - INFO - Epoch 103: train_loss=nan
2025-02-06 18:44:05,175 - INFO - Epoch 103: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:05,179 - INFO - Epoch 103: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:05,181 - INFO - #################### Training epoch 104 ####################
2025-02-06 18:44:05,181 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:05,595 - INFO - Epoch 104: train_loss=nan
2025-02-06 18:44:05,886 - INFO - Epoch 104: train_loss=nan
2025-02-06 18:44:06,215 - INFO - Epoch 104: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:06,219 - INFO - Epoch 104: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:06,221 - INFO - #################### Training epoch 105 ####################
2025-02-06 18:44:06,221 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:06,632 - INFO - Epoch 105: train_loss=nan
2025-02-06 18:44:06,923 - INFO - Epoch 105: train_loss=nan
2025-02-06 18:44:07,250 - INFO - Epoch 105: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:07,254 - INFO - Epoch 105: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:07,256 - INFO - #################### Training epoch 106 ####################
2025-02-06 18:44:07,256 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:07,669 - INFO - Epoch 106: train_loss=nan
2025-02-06 18:44:07,959 - INFO - Epoch 106: train_loss=nan
2025-02-06 18:44:08,283 - INFO - Epoch 106: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:08,287 - INFO - Epoch 106: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:08,289 - INFO - #################### Training epoch 107 ####################
2025-02-06 18:44:08,289 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:08,696 - INFO - Epoch 107: train_loss=nan
2025-02-06 18:44:08,987 - INFO - Epoch 107: train_loss=nan
2025-02-06 18:44:09,314 - INFO - Epoch 107: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:09,318 - INFO - Epoch 107: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:09,320 - INFO - #################### Training epoch 108 ####################
2025-02-06 18:44:09,320 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:09,730 - INFO - Epoch 108: train_loss=nan
2025-02-06 18:44:10,021 - INFO - Epoch 108: train_loss=nan
2025-02-06 18:44:10,344 - INFO - Epoch 108: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:10,348 - INFO - Epoch 108: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:10,350 - INFO - #################### Training epoch 109 ####################
2025-02-06 18:44:10,350 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:10,758 - INFO - Epoch 109: train_loss=nan
2025-02-06 18:44:11,049 - INFO - Epoch 109: train_loss=nan
2025-02-06 18:44:11,379 - INFO - Epoch 109: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:11,382 - INFO - Epoch 109: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:11,384 - INFO - #################### Training epoch 110 ####################
2025-02-06 18:44:11,384 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:11,793 - INFO - Epoch 110: train_loss=nan
2025-02-06 18:44:12,084 - INFO - Epoch 110: train_loss=nan
2025-02-06 18:44:12,411 - INFO - Epoch 110: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:12,415 - INFO - Epoch 110: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:12,417 - INFO - #################### Training epoch 111 ####################
2025-02-06 18:44:12,417 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:12,825 - INFO - Epoch 111: train_loss=nan
2025-02-06 18:44:13,115 - INFO - Epoch 111: train_loss=nan
2025-02-06 18:44:13,439 - INFO - Epoch 111: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:13,443 - INFO - Epoch 111: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:13,445 - INFO - #################### Training epoch 112 ####################
2025-02-06 18:44:13,445 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:13,861 - INFO - Epoch 112: train_loss=nan
2025-02-06 18:44:14,151 - INFO - Epoch 112: train_loss=nan
2025-02-06 18:44:14,481 - INFO - Epoch 112: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:14,484 - INFO - Epoch 112: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:14,486 - INFO - #################### Training epoch 113 ####################
2025-02-06 18:44:14,486 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:14,901 - INFO - Epoch 113: train_loss=nan
2025-02-06 18:44:15,193 - INFO - Epoch 113: train_loss=nan
2025-02-06 18:44:15,519 - INFO - Epoch 113: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:15,523 - INFO - Epoch 113: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:15,525 - INFO - #################### Training epoch 114 ####################
2025-02-06 18:44:15,525 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:15,936 - INFO - Epoch 114: train_loss=nan
2025-02-06 18:44:16,227 - INFO - Epoch 114: train_loss=nan
2025-02-06 18:44:16,554 - INFO - Epoch 114: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:16,558 - INFO - Epoch 114: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:16,560 - INFO - #################### Training epoch 115 ####################
2025-02-06 18:44:16,560 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:16,972 - INFO - Epoch 115: train_loss=nan
2025-02-06 18:44:17,263 - INFO - Epoch 115: train_loss=nan
2025-02-06 18:44:17,592 - INFO - Epoch 115: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:17,595 - INFO - Epoch 115: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:17,598 - INFO - #################### Training epoch 116 ####################
2025-02-06 18:44:17,598 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:18,010 - INFO - Epoch 116: train_loss=nan
2025-02-06 18:44:18,301 - INFO - Epoch 116: train_loss=nan
2025-02-06 18:44:18,629 - INFO - Epoch 116: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:18,633 - INFO - Epoch 116: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:18,635 - INFO - #################### Training epoch 117 ####################
2025-02-06 18:44:18,635 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:19,046 - INFO - Epoch 117: train_loss=nan
2025-02-06 18:44:19,336 - INFO - Epoch 117: train_loss=nan
2025-02-06 18:44:19,665 - INFO - Epoch 117: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:19,668 - INFO - Epoch 117: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:19,670 - INFO - #################### Training epoch 118 ####################
2025-02-06 18:44:19,670 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:20,079 - INFO - Epoch 118: train_loss=nan
2025-02-06 18:44:20,370 - INFO - Epoch 118: train_loss=nan
2025-02-06 18:44:20,699 - INFO - Epoch 118: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:20,703 - INFO - Epoch 118: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:20,705 - INFO - #################### Training epoch 119 ####################
2025-02-06 18:44:20,705 - INFO - Current Learning Rate: 4.900000e-04
2025-02-06 18:44:21,115 - INFO - Epoch 119: train_loss=nan
2025-02-06 18:44:21,405 - INFO - Epoch 119: train_loss=nan
2025-02-06 18:44:21,734 - INFO - Epoch 119: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:21,738 - INFO - Epoch 119: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:21,740 - INFO - #################### Training epoch 120 ####################
2025-02-06 18:44:21,740 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:22,150 - INFO - Epoch 120: train_loss=nan
2025-02-06 18:44:22,441 - INFO - Epoch 120: train_loss=nan
2025-02-06 18:44:22,771 - INFO - Epoch 120: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:22,774 - INFO - Epoch 120: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:22,777 - INFO - #################### Training epoch 121 ####################
2025-02-06 18:44:22,777 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:23,189 - INFO - Epoch 121: train_loss=nan
2025-02-06 18:44:23,480 - INFO - Epoch 121: train_loss=nan
2025-02-06 18:44:23,804 - INFO - Epoch 121: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:23,808 - INFO - Epoch 121: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:23,810 - INFO - #################### Training epoch 122 ####################
2025-02-06 18:44:23,810 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:24,221 - INFO - Epoch 122: train_loss=nan
2025-02-06 18:44:24,511 - INFO - Epoch 122: train_loss=nan
2025-02-06 18:44:24,838 - INFO - Epoch 122: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:24,842 - INFO - Epoch 122: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:24,844 - INFO - #################### Training epoch 123 ####################
2025-02-06 18:44:24,844 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:25,256 - INFO - Epoch 123: train_loss=nan
2025-02-06 18:44:25,546 - INFO - Epoch 123: train_loss=nan
2025-02-06 18:44:25,871 - INFO - Epoch 123: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:25,875 - INFO - Epoch 123: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:25,877 - INFO - #################### Training epoch 124 ####################
2025-02-06 18:44:25,877 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:26,292 - INFO - Epoch 124: train_loss=nan
2025-02-06 18:44:26,584 - INFO - Epoch 124: train_loss=nan
2025-02-06 18:44:26,911 - INFO - Epoch 124: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:26,915 - INFO - Epoch 124: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:26,917 - INFO - #################### Training epoch 125 ####################
2025-02-06 18:44:26,917 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:27,330 - INFO - Epoch 125: train_loss=nan
2025-02-06 18:44:27,621 - INFO - Epoch 125: train_loss=nan
2025-02-06 18:44:27,950 - INFO - Epoch 125: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:27,954 - INFO - Epoch 125: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:27,956 - INFO - #################### Training epoch 126 ####################
2025-02-06 18:44:27,956 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:28,367 - INFO - Epoch 126: train_loss=nan
2025-02-06 18:44:28,658 - INFO - Epoch 126: train_loss=nan
2025-02-06 18:44:28,984 - INFO - Epoch 126: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:28,988 - INFO - Epoch 126: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:28,990 - INFO - #################### Training epoch 127 ####################
2025-02-06 18:44:28,990 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:29,400 - INFO - Epoch 127: train_loss=nan
2025-02-06 18:44:29,690 - INFO - Epoch 127: train_loss=nan
2025-02-06 18:44:30,017 - INFO - Epoch 127: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:30,021 - INFO - Epoch 127: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:30,023 - INFO - #################### Training epoch 128 ####################
2025-02-06 18:44:30,023 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:30,436 - INFO - Epoch 128: train_loss=nan
2025-02-06 18:44:30,726 - INFO - Epoch 128: train_loss=nan
2025-02-06 18:44:31,057 - INFO - Epoch 128: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:31,060 - INFO - Epoch 128: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:31,062 - INFO - #################### Training epoch 129 ####################
2025-02-06 18:44:31,062 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:31,474 - INFO - Epoch 129: train_loss=nan
2025-02-06 18:44:31,765 - INFO - Epoch 129: train_loss=nan
2025-02-06 18:44:32,094 - INFO - Epoch 129: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:32,098 - INFO - Epoch 129: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:32,100 - INFO - #################### Training epoch 130 ####################
2025-02-06 18:44:32,100 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:32,510 - INFO - Epoch 130: train_loss=nan
2025-02-06 18:44:32,801 - INFO - Epoch 130: train_loss=nan
2025-02-06 18:44:33,129 - INFO - Epoch 130: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:33,132 - INFO - Epoch 130: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:33,135 - INFO - #################### Training epoch 131 ####################
2025-02-06 18:44:33,135 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:33,546 - INFO - Epoch 131: train_loss=nan
2025-02-06 18:44:33,838 - INFO - Epoch 131: train_loss=nan
2025-02-06 18:44:34,161 - INFO - Epoch 131: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:34,165 - INFO - Epoch 131: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:34,167 - INFO - #################### Training epoch 132 ####################
2025-02-06 18:44:34,167 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:34,580 - INFO - Epoch 132: train_loss=nan
2025-02-06 18:44:34,870 - INFO - Epoch 132: train_loss=nan
2025-02-06 18:44:35,199 - INFO - Epoch 132: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:35,202 - INFO - Epoch 132: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:35,204 - INFO - #################### Training epoch 133 ####################
2025-02-06 18:44:35,204 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:35,614 - INFO - Epoch 133: train_loss=nan
2025-02-06 18:44:35,905 - INFO - Epoch 133: train_loss=nan
2025-02-06 18:44:36,236 - INFO - Epoch 133: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:36,239 - INFO - Epoch 133: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:36,241 - INFO - #################### Training epoch 134 ####################
2025-02-06 18:44:36,241 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:36,653 - INFO - Epoch 134: train_loss=nan
2025-02-06 18:44:36,943 - INFO - Epoch 134: train_loss=nan
2025-02-06 18:44:37,272 - INFO - Epoch 134: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:37,276 - INFO - Epoch 134: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:37,278 - INFO - #################### Training epoch 135 ####################
2025-02-06 18:44:37,278 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:37,690 - INFO - Epoch 135: train_loss=nan
2025-02-06 18:44:37,981 - INFO - Epoch 135: train_loss=nan
2025-02-06 18:44:38,308 - INFO - Epoch 135: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:38,311 - INFO - Epoch 135: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:38,314 - INFO - #################### Training epoch 136 ####################
2025-02-06 18:44:38,314 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:38,725 - INFO - Epoch 136: train_loss=nan
2025-02-06 18:44:39,016 - INFO - Epoch 136: train_loss=nan
2025-02-06 18:44:39,344 - INFO - Epoch 136: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:39,348 - INFO - Epoch 136: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:39,350 - INFO - #################### Training epoch 137 ####################
2025-02-06 18:44:39,350 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:39,760 - INFO - Epoch 137: train_loss=nan
2025-02-06 18:44:40,051 - INFO - Epoch 137: train_loss=nan
2025-02-06 18:44:40,379 - INFO - Epoch 137: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:40,383 - INFO - Epoch 137: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:40,385 - INFO - #################### Training epoch 138 ####################
2025-02-06 18:44:40,385 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:40,797 - INFO - Epoch 138: train_loss=nan
2025-02-06 18:44:41,088 - INFO - Epoch 138: train_loss=nan
2025-02-06 18:44:41,415 - INFO - Epoch 138: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:41,419 - INFO - Epoch 138: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:41,421 - INFO - #################### Training epoch 139 ####################
2025-02-06 18:44:41,421 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:41,834 - INFO - Epoch 139: train_loss=nan
2025-02-06 18:44:42,124 - INFO - Epoch 139: train_loss=nan
2025-02-06 18:44:42,454 - INFO - Epoch 139: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:42,458 - INFO - Epoch 139: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:42,460 - INFO - #################### Training epoch 140 ####################
2025-02-06 18:44:42,460 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:42,874 - INFO - Epoch 140: train_loss=nan
2025-02-06 18:44:43,164 - INFO - Epoch 140: train_loss=nan
2025-02-06 18:44:43,492 - INFO - Epoch 140: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:43,496 - INFO - Epoch 140: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:43,498 - INFO - #################### Training epoch 141 ####################
2025-02-06 18:44:43,498 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:43,907 - INFO - Epoch 141: train_loss=nan
2025-02-06 18:44:44,197 - INFO - Epoch 141: train_loss=nan
2025-02-06 18:44:44,525 - INFO - Epoch 141: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:44,529 - INFO - Epoch 141: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:44,531 - INFO - #################### Training epoch 142 ####################
2025-02-06 18:44:44,531 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:44,941 - INFO - Epoch 142: train_loss=nan
2025-02-06 18:44:45,232 - INFO - Epoch 142: train_loss=nan
2025-02-06 18:44:45,558 - INFO - Epoch 142: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:45,561 - INFO - Epoch 142: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:45,564 - INFO - #################### Training epoch 143 ####################
2025-02-06 18:44:45,564 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:45,972 - INFO - Epoch 143: train_loss=nan
2025-02-06 18:44:46,262 - INFO - Epoch 143: train_loss=nan
2025-02-06 18:44:46,591 - INFO - Epoch 143: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:46,595 - INFO - Epoch 143: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:46,597 - INFO - #################### Training epoch 144 ####################
2025-02-06 18:44:46,597 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:47,012 - INFO - Epoch 144: train_loss=nan
2025-02-06 18:44:47,303 - INFO - Epoch 144: train_loss=nan
2025-02-06 18:44:47,633 - INFO - Epoch 144: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:47,637 - INFO - Epoch 144: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:47,639 - INFO - #################### Training epoch 145 ####################
2025-02-06 18:44:47,639 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:48,053 - INFO - Epoch 145: train_loss=nan
2025-02-06 18:44:48,343 - INFO - Epoch 145: train_loss=nan
2025-02-06 18:44:48,673 - INFO - Epoch 145: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:48,676 - INFO - Epoch 145: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:48,678 - INFO - #################### Training epoch 146 ####################
2025-02-06 18:44:48,678 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:49,091 - INFO - Epoch 146: train_loss=nan
2025-02-06 18:44:49,381 - INFO - Epoch 146: train_loss=nan
2025-02-06 18:44:49,707 - INFO - Epoch 146: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:49,711 - INFO - Epoch 146: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:49,713 - INFO - #################### Training epoch 147 ####################
2025-02-06 18:44:49,713 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:50,122 - INFO - Epoch 147: train_loss=nan
2025-02-06 18:44:50,413 - INFO - Epoch 147: train_loss=nan
2025-02-06 18:44:50,742 - INFO - Epoch 147: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:50,746 - INFO - Epoch 147: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:50,748 - INFO - #################### Training epoch 148 ####################
2025-02-06 18:44:50,748 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:51,157 - INFO - Epoch 148: train_loss=nan
2025-02-06 18:44:51,448 - INFO - Epoch 148: train_loss=nan
2025-02-06 18:44:51,774 - INFO - Epoch 148: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:51,777 - INFO - Epoch 148: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:51,780 - INFO - #################### Training epoch 149 ####################
2025-02-06 18:44:51,780 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:52,193 - INFO - Epoch 149: train_loss=nan
2025-02-06 18:44:52,483 - INFO - Epoch 149: train_loss=nan
2025-02-06 18:44:52,810 - INFO - Epoch 149: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:52,814 - INFO - Epoch 149: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:52,816 - INFO - #################### Training epoch 150 ####################
2025-02-06 18:44:52,816 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:53,229 - INFO - Epoch 150: train_loss=nan
2025-02-06 18:44:53,520 - INFO - Epoch 150: train_loss=nan
2025-02-06 18:44:53,846 - INFO - Epoch 150: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:53,850 - INFO - Epoch 150: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:53,852 - INFO - #################### Training epoch 151 ####################
2025-02-06 18:44:53,852 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:54,264 - INFO - Epoch 151: train_loss=nan
2025-02-06 18:44:54,555 - INFO - Epoch 151: train_loss=nan
2025-02-06 18:44:54,883 - INFO - Epoch 151: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:54,886 - INFO - Epoch 151: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:54,889 - INFO - #################### Training epoch 152 ####################
2025-02-06 18:44:54,889 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:55,300 - INFO - Epoch 152: train_loss=nan
2025-02-06 18:44:55,591 - INFO - Epoch 152: train_loss=nan
2025-02-06 18:44:55,917 - INFO - Epoch 152: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:55,921 - INFO - Epoch 152: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:55,923 - INFO - #################### Training epoch 153 ####################
2025-02-06 18:44:55,923 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:56,335 - INFO - Epoch 153: train_loss=nan
2025-02-06 18:44:56,625 - INFO - Epoch 153: train_loss=nan
2025-02-06 18:44:56,956 - INFO - Epoch 153: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:56,960 - INFO - Epoch 153: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:56,962 - INFO - #################### Training epoch 154 ####################
2025-02-06 18:44:56,962 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:57,370 - INFO - Epoch 154: train_loss=nan
2025-02-06 18:44:57,660 - INFO - Epoch 154: train_loss=nan
2025-02-06 18:44:57,987 - INFO - Epoch 154: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:57,991 - INFO - Epoch 154: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:57,993 - INFO - #################### Training epoch 155 ####################
2025-02-06 18:44:57,993 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:58,405 - INFO - Epoch 155: train_loss=nan
2025-02-06 18:44:58,696 - INFO - Epoch 155: train_loss=nan
2025-02-06 18:44:59,022 - INFO - Epoch 155: val_loss=nan, val_acc=66.67%
2025-02-06 18:44:59,025 - INFO - Epoch 155: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:44:59,027 - INFO - #################### Training epoch 156 ####################
2025-02-06 18:44:59,028 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:44:59,440 - INFO - Epoch 156: train_loss=nan
2025-02-06 18:44:59,731 - INFO - Epoch 156: train_loss=nan
2025-02-06 18:45:00,056 - INFO - Epoch 156: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:00,060 - INFO - Epoch 156: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:00,062 - INFO - #################### Training epoch 157 ####################
2025-02-06 18:45:00,062 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:45:00,471 - INFO - Epoch 157: train_loss=nan
2025-02-06 18:45:00,760 - INFO - Epoch 157: train_loss=nan
2025-02-06 18:45:01,088 - INFO - Epoch 157: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:01,092 - INFO - Epoch 157: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:01,094 - INFO - #################### Training epoch 158 ####################
2025-02-06 18:45:01,094 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:45:01,505 - INFO - Epoch 158: train_loss=nan
2025-02-06 18:45:01,795 - INFO - Epoch 158: train_loss=nan
2025-02-06 18:45:02,125 - INFO - Epoch 158: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:02,128 - INFO - Epoch 158: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:02,130 - INFO - #################### Training epoch 159 ####################
2025-02-06 18:45:02,131 - INFO - Current Learning Rate: 3.430000e-04
2025-02-06 18:45:02,540 - INFO - Epoch 159: train_loss=nan
2025-02-06 18:45:02,831 - INFO - Epoch 159: train_loss=nan
2025-02-06 18:45:03,162 - INFO - Epoch 159: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:03,165 - INFO - Epoch 159: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:03,167 - INFO - #################### Training epoch 160 ####################
2025-02-06 18:45:03,167 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:03,578 - INFO - Epoch 160: train_loss=nan
2025-02-06 18:45:03,869 - INFO - Epoch 160: train_loss=nan
2025-02-06 18:45:04,197 - INFO - Epoch 160: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:04,201 - INFO - Epoch 160: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:04,203 - INFO - #################### Training epoch 161 ####################
2025-02-06 18:45:04,203 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:04,614 - INFO - Epoch 161: train_loss=nan
2025-02-06 18:45:04,905 - INFO - Epoch 161: train_loss=nan
2025-02-06 18:45:05,232 - INFO - Epoch 161: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:05,236 - INFO - Epoch 161: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:05,238 - INFO - #################### Training epoch 162 ####################
2025-02-06 18:45:05,238 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:05,649 - INFO - Epoch 162: train_loss=nan
2025-02-06 18:45:05,939 - INFO - Epoch 162: train_loss=nan
2025-02-06 18:45:06,270 - INFO - Epoch 162: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:06,274 - INFO - Epoch 162: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:06,276 - INFO - #################### Training epoch 163 ####################
2025-02-06 18:45:06,276 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:06,690 - INFO - Epoch 163: train_loss=nan
2025-02-06 18:45:06,980 - INFO - Epoch 163: train_loss=nan
2025-02-06 18:45:07,315 - INFO - Epoch 163: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:07,318 - INFO - Epoch 163: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:07,321 - INFO - #################### Training epoch 164 ####################
2025-02-06 18:45:07,321 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:07,733 - INFO - Epoch 164: train_loss=nan
2025-02-06 18:45:08,025 - INFO - Epoch 164: train_loss=nan
2025-02-06 18:45:08,359 - INFO - Epoch 164: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:08,363 - INFO - Epoch 164: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:08,365 - INFO - #################### Training epoch 165 ####################
2025-02-06 18:45:08,365 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:08,778 - INFO - Epoch 165: train_loss=nan
2025-02-06 18:45:09,069 - INFO - Epoch 165: train_loss=nan
2025-02-06 18:45:09,400 - INFO - Epoch 165: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:09,404 - INFO - Epoch 165: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:09,406 - INFO - #################### Training epoch 166 ####################
2025-02-06 18:45:09,406 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:09,819 - INFO - Epoch 166: train_loss=nan
2025-02-06 18:45:10,110 - INFO - Epoch 166: train_loss=nan
2025-02-06 18:45:10,442 - INFO - Epoch 166: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:10,446 - INFO - Epoch 166: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:10,448 - INFO - #################### Training epoch 167 ####################
2025-02-06 18:45:10,448 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:10,861 - INFO - Epoch 167: train_loss=nan
2025-02-06 18:45:11,152 - INFO - Epoch 167: train_loss=nan
2025-02-06 18:45:11,478 - INFO - Epoch 167: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:11,482 - INFO - Epoch 167: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:11,484 - INFO - #################### Training epoch 168 ####################
2025-02-06 18:45:11,484 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:11,899 - INFO - Epoch 168: train_loss=nan
2025-02-06 18:45:12,190 - INFO - Epoch 168: train_loss=nan
2025-02-06 18:45:12,522 - INFO - Epoch 168: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:12,526 - INFO - Epoch 168: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:12,528 - INFO - #################### Training epoch 169 ####################
2025-02-06 18:45:12,528 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:12,940 - INFO - Epoch 169: train_loss=nan
2025-02-06 18:45:13,231 - INFO - Epoch 169: train_loss=nan
2025-02-06 18:45:13,561 - INFO - Epoch 169: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:13,565 - INFO - Epoch 169: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:13,567 - INFO - #################### Training epoch 170 ####################
2025-02-06 18:45:13,567 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:13,978 - INFO - Epoch 170: train_loss=nan
2025-02-06 18:45:14,268 - INFO - Epoch 170: train_loss=nan
2025-02-06 18:45:14,596 - INFO - Epoch 170: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:14,600 - INFO - Epoch 170: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:14,602 - INFO - #################### Training epoch 171 ####################
2025-02-06 18:45:14,602 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:15,015 - INFO - Epoch 171: train_loss=nan
2025-02-06 18:45:15,306 - INFO - Epoch 171: train_loss=nan
2025-02-06 18:45:15,638 - INFO - Epoch 171: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:15,642 - INFO - Epoch 171: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:15,644 - INFO - #################### Training epoch 172 ####################
2025-02-06 18:45:15,644 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:16,056 - INFO - Epoch 172: train_loss=nan
2025-02-06 18:45:16,347 - INFO - Epoch 172: train_loss=nan
2025-02-06 18:45:16,675 - INFO - Epoch 172: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:16,678 - INFO - Epoch 172: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:16,680 - INFO - #################### Training epoch 173 ####################
2025-02-06 18:45:16,680 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:17,093 - INFO - Epoch 173: train_loss=nan
2025-02-06 18:45:17,385 - INFO - Epoch 173: train_loss=nan
2025-02-06 18:45:17,718 - INFO - Epoch 173: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:17,722 - INFO - Epoch 173: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:17,724 - INFO - #################### Training epoch 174 ####################
2025-02-06 18:45:17,724 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:18,136 - INFO - Epoch 174: train_loss=nan
2025-02-06 18:45:18,427 - INFO - Epoch 174: train_loss=nan
2025-02-06 18:45:18,756 - INFO - Epoch 174: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:18,760 - INFO - Epoch 174: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:18,762 - INFO - #################### Training epoch 175 ####################
2025-02-06 18:45:18,762 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:19,176 - INFO - Epoch 175: train_loss=nan
2025-02-06 18:45:19,467 - INFO - Epoch 175: train_loss=nan
2025-02-06 18:45:19,799 - INFO - Epoch 175: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:19,803 - INFO - Epoch 175: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:19,805 - INFO - #################### Training epoch 176 ####################
2025-02-06 18:45:19,805 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:20,219 - INFO - Epoch 176: train_loss=nan
2025-02-06 18:45:20,511 - INFO - Epoch 176: train_loss=nan
2025-02-06 18:45:20,840 - INFO - Epoch 176: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:20,844 - INFO - Epoch 176: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:20,846 - INFO - #################### Training epoch 177 ####################
2025-02-06 18:45:20,846 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:21,258 - INFO - Epoch 177: train_loss=nan
2025-02-06 18:45:21,548 - INFO - Epoch 177: train_loss=nan
2025-02-06 18:45:21,880 - INFO - Epoch 177: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:21,883 - INFO - Epoch 177: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:21,885 - INFO - #################### Training epoch 178 ####################
2025-02-06 18:45:21,886 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:22,294 - INFO - Epoch 178: train_loss=nan
2025-02-06 18:45:22,585 - INFO - Epoch 178: train_loss=nan
2025-02-06 18:45:22,914 - INFO - Epoch 178: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:22,918 - INFO - Epoch 178: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:22,920 - INFO - #################### Training epoch 179 ####################
2025-02-06 18:45:22,920 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:23,334 - INFO - Epoch 179: train_loss=nan
2025-02-06 18:45:23,624 - INFO - Epoch 179: train_loss=nan
2025-02-06 18:45:23,947 - INFO - Epoch 179: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:23,951 - INFO - Epoch 179: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:23,953 - INFO - #################### Training epoch 180 ####################
2025-02-06 18:45:23,953 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:24,368 - INFO - Epoch 180: train_loss=nan
2025-02-06 18:45:24,661 - INFO - Epoch 180: train_loss=nan
2025-02-06 18:45:24,990 - INFO - Epoch 180: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:24,994 - INFO - Epoch 180: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:24,996 - INFO - #################### Training epoch 181 ####################
2025-02-06 18:45:24,996 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:25,412 - INFO - Epoch 181: train_loss=nan
2025-02-06 18:45:25,705 - INFO - Epoch 181: train_loss=nan
2025-02-06 18:45:26,037 - INFO - Epoch 181: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:26,040 - INFO - Epoch 181: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:26,042 - INFO - #################### Training epoch 182 ####################
2025-02-06 18:45:26,043 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:26,454 - INFO - Epoch 182: train_loss=nan
2025-02-06 18:45:26,746 - INFO - Epoch 182: train_loss=nan
2025-02-06 18:45:27,077 - INFO - Epoch 182: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:27,081 - INFO - Epoch 182: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:27,083 - INFO - #################### Training epoch 183 ####################
2025-02-06 18:45:27,083 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:27,498 - INFO - Epoch 183: train_loss=nan
2025-02-06 18:45:27,788 - INFO - Epoch 183: train_loss=nan
2025-02-06 18:45:28,117 - INFO - Epoch 183: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:28,121 - INFO - Epoch 183: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:28,123 - INFO - #################### Training epoch 184 ####################
2025-02-06 18:45:28,123 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:28,537 - INFO - Epoch 184: train_loss=nan
2025-02-06 18:45:28,827 - INFO - Epoch 184: train_loss=nan
2025-02-06 18:45:29,153 - INFO - Epoch 184: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:29,156 - INFO - Epoch 184: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:29,158 - INFO - #################### Training epoch 185 ####################
2025-02-06 18:45:29,158 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:29,568 - INFO - Epoch 185: train_loss=nan
2025-02-06 18:45:29,859 - INFO - Epoch 185: train_loss=nan
2025-02-06 18:45:30,188 - INFO - Epoch 185: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:30,192 - INFO - Epoch 185: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:30,194 - INFO - #################### Training epoch 186 ####################
2025-02-06 18:45:30,194 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:30,603 - INFO - Epoch 186: train_loss=nan
2025-02-06 18:45:30,894 - INFO - Epoch 186: train_loss=nan
2025-02-06 18:45:31,221 - INFO - Epoch 186: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:31,225 - INFO - Epoch 186: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:31,227 - INFO - #################### Training epoch 187 ####################
2025-02-06 18:45:31,227 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:31,639 - INFO - Epoch 187: train_loss=nan
2025-02-06 18:45:31,929 - INFO - Epoch 187: train_loss=nan
2025-02-06 18:45:32,262 - INFO - Epoch 187: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:32,266 - INFO - Epoch 187: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:32,269 - INFO - #################### Training epoch 188 ####################
2025-02-06 18:45:32,269 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:32,683 - INFO - Epoch 188: train_loss=nan
2025-02-06 18:45:32,974 - INFO - Epoch 188: train_loss=nan
2025-02-06 18:45:33,302 - INFO - Epoch 188: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:33,305 - INFO - Epoch 188: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:33,308 - INFO - #################### Training epoch 189 ####################
2025-02-06 18:45:33,308 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:33,719 - INFO - Epoch 189: train_loss=nan
2025-02-06 18:45:34,009 - INFO - Epoch 189: train_loss=nan
2025-02-06 18:45:34,339 - INFO - Epoch 189: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:34,343 - INFO - Epoch 189: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:34,345 - INFO - #################### Training epoch 190 ####################
2025-02-06 18:45:34,345 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:34,758 - INFO - Epoch 190: train_loss=nan
2025-02-06 18:45:35,048 - INFO - Epoch 190: train_loss=nan
2025-02-06 18:45:35,377 - INFO - Epoch 190: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:35,380 - INFO - Epoch 190: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:35,382 - INFO - #################### Training epoch 191 ####################
2025-02-06 18:45:35,383 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:35,796 - INFO - Epoch 191: train_loss=nan
2025-02-06 18:45:36,086 - INFO - Epoch 191: train_loss=nan
2025-02-06 18:45:36,412 - INFO - Epoch 191: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:36,416 - INFO - Epoch 191: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:36,418 - INFO - #################### Training epoch 192 ####################
2025-02-06 18:45:36,418 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:36,829 - INFO - Epoch 192: train_loss=nan
2025-02-06 18:45:37,119 - INFO - Epoch 192: train_loss=nan
2025-02-06 18:45:37,447 - INFO - Epoch 192: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:37,450 - INFO - Epoch 192: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:37,452 - INFO - #################### Training epoch 193 ####################
2025-02-06 18:45:37,452 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:37,860 - INFO - Epoch 193: train_loss=nan
2025-02-06 18:45:38,150 - INFO - Epoch 193: train_loss=nan
2025-02-06 18:45:38,479 - INFO - Epoch 193: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:38,483 - INFO - Epoch 193: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:38,485 - INFO - #################### Training epoch 194 ####################
2025-02-06 18:45:38,485 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:38,890 - INFO - Epoch 194: train_loss=nan
2025-02-06 18:45:39,180 - INFO - Epoch 194: train_loss=nan
2025-02-06 18:45:39,508 - INFO - Epoch 194: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:39,512 - INFO - Epoch 194: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:39,514 - INFO - #################### Training epoch 195 ####################
2025-02-06 18:45:39,514 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:39,929 - INFO - Epoch 195: train_loss=nan
2025-02-06 18:45:40,220 - INFO - Epoch 195: train_loss=nan
2025-02-06 18:45:40,543 - INFO - Epoch 195: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:40,547 - INFO - Epoch 195: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:40,549 - INFO - #################### Training epoch 196 ####################
2025-02-06 18:45:40,549 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:40,960 - INFO - Epoch 196: train_loss=nan
2025-02-06 18:45:41,251 - INFO - Epoch 196: train_loss=nan
2025-02-06 18:45:41,576 - INFO - Epoch 196: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:41,580 - INFO - Epoch 196: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:41,582 - INFO - #################### Training epoch 197 ####################
2025-02-06 18:45:41,582 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:41,991 - INFO - Epoch 197: train_loss=nan
2025-02-06 18:45:42,281 - INFO - Epoch 197: train_loss=nan
2025-02-06 18:45:42,608 - INFO - Epoch 197: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:42,612 - INFO - Epoch 197: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:42,614 - INFO - #################### Training epoch 198 ####################
2025-02-06 18:45:42,614 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:43,026 - INFO - Epoch 198: train_loss=nan
2025-02-06 18:45:43,317 - INFO - Epoch 198: train_loss=nan
2025-02-06 18:45:43,647 - INFO - Epoch 198: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:43,651 - INFO - Epoch 198: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:43,653 - INFO - #################### Training epoch 199 ####################
2025-02-06 18:45:43,653 - INFO - Current Learning Rate: 2.401000e-04
2025-02-06 18:45:44,063 - INFO - Epoch 199: train_loss=nan
2025-02-06 18:45:44,354 - INFO - Epoch 199: train_loss=nan
2025-02-06 18:45:44,687 - INFO - Epoch 199: val_loss=nan, val_acc=66.67%
2025-02-06 18:45:44,691 - INFO - Epoch 199: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-06 18:45:44,915 - INFO - Model saved.
