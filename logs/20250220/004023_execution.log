Starting job at Thu Feb 20 12:40:23 AM CET 2025
Training script
Checking allocated GPU...
Thu Feb 20 00:40:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.06              Driver Version: 555.42.06      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3070        Off |   00000000:21:00.0 Off |                  N/A |
| 30%   49C    P0             50W /  220W |       1MiB /   8192MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
CUDA_VISIBLE_DEVICES: 0
wandb: Currently logged in as: cyans (cyans-k-benhavns-universitet). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /lustre/hpc/icecube/cyan/factory/IceCubeTransformer/wandb/run-20250220_004118-b5ratc97
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-butterfly-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/cyans-k-benhavns-universitet/%5B2025-02-20%5D%20Flavour%20Classification
wandb: üöÄ View run at https://wandb.ai/cyans-k-benhavns-universitet/%5B2025-02-20%5D%20Flavour%20Classification/runs/b5ratc97
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                        | Type       | Params | Mode 
-------------------------------------------------------------------
0 | input_projection            | Linear     | 4.2 K  | train
1 | position_embedding          | Embedding  | 32.8 K | train
2 | encoder_blocks              | ModuleList | 597 K  | train
3 | pooling                     | Pooling    | 0      | train
4 | classification_output_layer | Linear     | 387    | train
-------------------------------------------------------------------
634 K     Trainable params
0         Non-trainable params
634 K     Total params
2.540     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Mask shape before expansion: torch.Size([64, 256])Mask shape before expansion: torch.Size([64, 256])Mask shape before expansion: torch.Size([64, 256])Mask shape before expansion: torch.Size([64, 256])



Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Mask shape before expansion: torch.Size([64, 256])
Mask shape before expansion: torch.Size([64, 256])
Mask shape before expansion: torch.Size([64, 256])
Mask shape before expansion: torch.Size([64, 256])
Mask shape before expansion: torch.Size([64, 256])
Sanity Checking DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:01<00:01,  0.56it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.10it/s]                                                                           Mask shape before expansion: torch.Size([64, 256])
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/11015 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/11015 [00:00<?, ?it/s] Mask shape before expansion: torch.Size([64, 256])
Mask shape before expansion: torch.Size([64, 256])
Batch 0: train_loss=0.8168

prob Mask shape before expansion: torch.Size([64, 256])
tensor([[-0.0208, -0.9779, -0.0429],
        [-0.1136, -0.7049, -0.2050],
        [-0.1784, -0.6237, -0.2958],
        [-0.1443, -0.7238, -0.2200],
        [-0.0168, -0.9255, -0.0569],
        [ 0.4671,  0.1341, -0.2449],
        [-0.1839, -0.5592, -0.4751],
        [-0.1550, -0.6683, -0.2697],
        [-0.1930, -0.5927, -0.4278],
        [-0.1514, -0.6823, -0.2584],
        [-0.1594, -0.7632, -0.3015],
        [-0.2996, -0.4726, -0.5712],
        [-0.1019, -0.8365, -0.1085],
        [-0.1219, -0.7668, -0.2152],
        [-0.1519, -0.7364, -0.2261],
        [ 0.0644,  0.0147, -0.2951],
        [ 0.0165, -0.8602, -0.1280],
        [ 0.0600, -0.8758, -0.0984],
        [-0.1625, -0.7267, -0.2858],
        [ 0.0134, -0.8804, -0.1076],
        [ 0.1040, -0.8926, -0.0518],
        [-0.0914, -0.7317, -0.2415],
        [ 0.0940,  0.0824, -0.2327],
        [-0.1401, -0.6510, -0.3646],
        [-0.2315, -0.6250, -0.4561],
        [-0.1769, -0.9006, -0.0907],
        [-0.2093, -0.5452, -0.5039],
        [-0.1507, -0.6767, -0.3419],
        [-0.1108, -0.6759, -0.4353],
        [-0.1579, -0.6948, -0.2867],
        [-0.1521, -0.6817, -0.2510],
        [-0.4866, -0.2374, -0.3028],
        [-0.1099, -0.6823, -0.3229],
        [-0.1696, -0.7532, -0.2896],
        [-0.1486, -0.7588, -0.1596],
        [-0.0896, -0.8634, -0.1254],
        [-0.0964, -0.7296, -0.2584],
        [-0.0624, -0.9294, -0.1618],
        [-0.1269, -0.7218, -0.2478],
        [-0.1647, -0.8665, -0.1904],
        [-0.0932, -0.8505, -0.1249],
        [-0.0155, -0.9222, -0.0476],
        [-0.1856, -0.6595, -0.4101],
        [-0.0959, -0.9484, -0.1471],
        [-0.0973, -0.7260, -0.2051],
        [ 0.3492,  0.1421, -0.2084],
        [-0.0691, -0.8406, -0.0910],
        [-0.1691, -0.7212, -0.3123],
        [-0.1167, -0.7774, -0.3121],
        [-0.1327, -0.8380, -0.1861],
        [-0.1841, -0.6395, -0.4138],
        [-0.1319, -0.7985, -0.2559],
        [-0.0152, -0.9026, -0.0958],
        [-0.1442, -0.6451, -0.3949],
        [-0.0561, -0.7525, -0.2411],
        [-0.0852, -0.7413, -0.1991],
        [-0.1646, -0.7324, -0.2973],
        [-0.1920, -0.5789, -0.4478],
        [-0.1129, -0.7193, -0.1761],
        [-0.1825, -0.6261, -0.2853],
        [-0.0430, -0.8523, -0.1282],
        [-0.1077, -0.7873, -0.1668],
        [-0.2105, -0.5145, -0.5137],
        [-0.0147, -0.9100, -0.0783]], device='cuda:0',
       grad_fn=<AddmmBackward0>)
target tensor([[0., 1., 0.],
        [0., 0., 1.],
        [0., 0., 1.],
        [1., 0., 0.],
        [0., 0., 1.],
        [0., 1., 0.],
        [1., 0., 0.],
        [0., 1., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [0., 1., 0.],
        [0., 1., 0.],
        [0., 0., 1.],
        [0., 1., 0.],
        [0., 1., 0.],
        [1., 0., 0.],
        [0., 1., 0.],
        [0., 1., 0.],
        [1., 0., 0.],
        [0., 0., 1.],
        [0., 0., 1.],
        [0., 0., 1.],
        [0., 1., 0.],
        [0., 0., 1.],
        [0., 0., 1.],
        [0., 1., 0.],
        [0., 0., 1.],
        [0., 0., 1.],
        [0., 0., 1.],
        [0., 0., 1.],
        [0., 1., 0.],
        [0., 1., 0.],
        [0., 0., 1.],
        [0., 1., 0.],
        [0., 0., 1.],
        [0., 1., 0.],
        [1., 0., 0.],
        [0., 0., 1.],
        [0., 0., 1.],
        [0., 0., 1.],
        [0., 1., 0.],
        [0., 0., 1.],
        [1., 0., 0.],
        [0., 1., 0.],
        [0., 1., 0.],
        [0., 1., 0.],
        [0., 1., 0.],
        [0., 1., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [0., 1., 0.],
        [0., 1., 0.],
        [0., 1., 0.],
        [0., 1., 0.],
        [0., 1., 0.],
        [0., 1., 0.],
        [0., 0., 1.],
        [0., 0., 1.],
        [1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.],
        [1., 0., 0.],
        [0., 0., 1.]], device='cuda:0')
Epoch 0:   0%|          | 1/11015 [00:43<133:44:17,  0.02it/s]Epoch 0:   0%|          | 1/11015 [00:43<133:44:28,  0.02it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Epoch 0:   0%|          | 2/11015 [00:43<67:08:30,  0.05it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5] Epoch 0:   0%|          | 2/11015 [00:43<67:12:04,  0.05it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Epoch 0:   0%|          | 3/11015 [00:44<44:57:35,  0.07it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Epoch 0:   0%|          | 3/11015 [00:44<45:00:55,  0.07it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Mask shape before expansion: torch.Size([64, 256])
Epoch 0:   0%|          | 4/11015 [00:44<34:22:45,  0.09it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Epoch 0:   0%|          | 4/11015 [00:44<34:22:46,  0.09it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Mask shape before expansion: torch.Size([64, 256])
Mask shape before expansion: torch.Size([64, 256])
Epoch 0:   0%|          | 5/11015 [00:53<32:29:31,  0.09it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Mask shape before expansion: torch.Size([64, 256])
Epoch 0:   0%|          | 5/11015 [00:53<32:34:33,  0.09it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Epoch 0:   0%|          | 6/11015 [00:53<27:15:49,  0.11it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Epoch 0:   0%|          | 6/11015 [00:53<27:17:07,  0.11it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Epoch 0:   0%|          | 7/11015 [00:53<23:27:28,  0.13it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Epoch 0:   0%|          | 7/11015 [00:53<23:28:05,  0.13it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Mask shape before expansion: torch.Size([64, 256])
Epoch 0:   0%|          | 8/11015 [01:00<22:56:41,  0.13it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Epoch 0:   0%|          | 8/11015 [01:00<22:56:42,  0.13it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Mask shape before expansion: torch.Size([64, 256])
Mask shape before expansion: torch.Size([64, 256])
Epoch 0:   0%|          | 9/11015 [01:06<22:45:18,  0.13it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Epoch 0:   0%|          | 9/11015 [01:06<22:45:19,  0.13it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Mask shape before expansion: torch.Size([64, 256])
Epoch 0:   0%|          | 10/11015 [01:08<21:00:11,  0.15it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Epoch 0:   0%|          | 10/11015 [01:08<21:00:19,  0.15it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Epoch 0:   0%|          | 11/11015 [01:09<19:10:29,  0.16it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Epoch 0:   0%|          | 11/11015 [01:09<19:10:29,  0.16it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Mask shape before expansion: torch.Size([64, 256])
Epoch 0:   0%|          | 12/11015 [01:16<19:28:15,  0.16it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]Epoch 0:   0%|          | 12/11015 [01:16<19:28:39,  0.16it/s, v_num=tc97, train_loss=0.817, learning rate=2e-5]