Starting job at Wed Feb 19 11:16:56 PM CET 2025
Training script
Checking allocated GPU...
Wed Feb 19 23:16:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.06              Driver Version: 555.42.06      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3070        Off |   00000000:21:00.0 Off |                  N/A |
| 30%   49C    P0             47W /  220W |       1MiB /   8192MiB |      5%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
CUDA_VISIBLE_DEVICES: 0
wandb: Currently logged in as: cyans (cyans-k-benhavns-universitet). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /lustre/hpc/icecube/cyan/factory/IceCubeTransformer/wandb/run-20250219_231711-2k6f4sls
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-eon-24
wandb: ‚≠êÔ∏è View project at https://wandb.ai/cyans-k-benhavns-universitet/%5B2025-02-19%5D%20Flavour%20Classification
wandb: üöÄ View run at https://wandb.ai/cyans-k-benhavns-universitet/%5B2025-02-19%5D%20Flavour%20Classification/runs/2k6f4sls
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                        | Type       | Params | Mode 
-------------------------------------------------------------------
0 | input_projection            | Linear     | 4.2 K  | train
1 | position_embedding          | Embedding  | 32.8 K | train
2 | encoder_blocks              | ModuleList | 597 K  | train
3 | pooling                     | Pooling    | 0      | train
4 | classification_output_layer | Linear     | 387    | train
-------------------------------------------------------------------
634 K     Trainable params
0         Non-trainable params
634 K     Total params
2.540     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Mask shape before expansion: torch.Size([64, 256])
Mask shape before expansion: torch.Size([64, 256])
Mask shape before expansion: torch.Size([64, 256])
Mask shape before expansion: torch.Size([64, 256])
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Mask shape before expansion: torch.Size([64, 256])
Mask shape before expansion: torch.Size([64, 256])
Mask shape before expansion: torch.Size([64, 256])
Mask shape before expansion: torch.Size([64, 256])
Mask shape before expansion: torch.Size([64, 256])
Sanity Checking DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  2.13it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  4.00it/s]                                                                           Mask shape before expansion: torch.Size([64, 256])
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/11015 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/11015 [00:00<?, ?it/s] Traceback (most recent call last):
  File "/lustre/hpc/icecube/cyan/factory/IceCubeTransformer/run_training.py", line 237, in <module>
    run_training(config_file=os.path.join(config_dir, config_file),
  File "/lustre/hpc/icecube/cyan/factory/IceCubeTransformer/run_training.py", line 228, in run_training
    trainer.fit(model, datamodule=datamodule)
  File "/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1030, in _run_stage
    self.fit_loop.run()
  File "/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
                   ^^^^^^^^^^^^^^^^^^
  File "/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
            ^^^^^^^^^^^^^^^^^^
  File "/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
            ^^^^^^^^^^^^^^^^^^^
  File "/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
          ^^^^^^^^^^^^^^^^^^^^
  File "/groups/icecube/cyan/.local/lib/python3.12/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/groups/icecube/cyan/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 708, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/groups/icecube/cyan/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1480, in _next_data
    return self._process_data(data)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/groups/icecube/cyan/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1505, in _process_data
    data.reraise()
  File "/groups/icecube/cyan/miniconda3/lib/python3.12/site-packages/torch/_utils.py", line 733, in reraise
    raise exception
IndexError: Caught IndexError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/groups/icecube/cyan/miniconda3/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
           ^^^^^^^^^^^^^^^^^^^^
  File "/groups/icecube/cyan/miniconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "/lustre/hpc/icecube/cyan/factory/IceCubeTransformer/SnowyDataSocket/MultiPartDataset.py", line 53, in __getitem__
    return self.datasets[dataset_idx][local_idx]
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^
  File "/lustre/hpc/icecube/cyan/factory/IceCubeTransformer/SnowyDataSocket/PartDataset.py", line 81, in __getitem__
    event_meta = self.metadata[idx]
                 ~~~~~~~~~~~~~^^^^^
IndexError: list index out of range

wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.006 MB uploadedwandb: | 0.006 MB of 0.022 MB uploadedwandb: üöÄ View run dark-eon-24 at: https://wandb.ai/cyans-k-benhavns-universitet/%5B2025-02-19%5D%20Flavour%20Classification/runs/2k6f4sls
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/cyans-k-benhavns-universitet/%5B2025-02-19%5D%20Flavour%20Classification
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250219_231711-2k6f4sls/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Job completed at Wed Feb 19 11:17:26 PM CET 2025
