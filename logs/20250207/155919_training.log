2025-02-07 15:59:30,086 - INFO - Starting training with the following parameters:
2025-02-07 15:59:30,087 - INFO - | Parameter       | Value               |
|-----------------|---------------------|
| attention       | Scaled Dot-Product|
| d_model         | 128            |
| n_heads         | 8              |
| d_f             | 128            |
| num_layers      | 3              |
| d_input         | 32             |
| num_classes     | 3              |
| dropout         | 0.1            |
| learning_rate   | 0.001          |
| epochs          | 50             |
| batch_size      | 16             |

2025-02-07 15:59:30,859 - INFO - Epoch 0: val_loss=1.0964, val_acc=66.67%
2025-02-07 15:59:30,875 - INFO - #################### Training epoch 0 ####################
2025-02-07 15:59:30,876 - INFO - Current Learning Rate: 4.000000e-04
2025-02-07 15:59:31,050 - INFO - Epoch 0: train_loss=1.1072
2025-02-07 15:59:31,259 - INFO - Epoch 0: train_loss=1.1128
2025-02-07 15:59:31,478 - INFO - Epoch 0: val_loss=1.0664, val_acc=66.67%
2025-02-07 15:59:31,495 - INFO - Epoch 0: EPOCH_AVG_TRAIN_LOSS=1.1072
2025-02-07 15:59:31,524 - INFO - #################### Training epoch 1 ####################
2025-02-07 15:59:31,524 - INFO - Current Learning Rate: 5.122213e-04
2025-02-07 15:59:31,856 - INFO - Epoch 1: train_loss=1.0364
2025-02-07 15:59:31,999 - INFO - Epoch 1: train_loss=1.1074
2025-02-07 15:59:32,238 - INFO - Epoch 1: val_loss=1.0260, val_acc=33.33%
2025-02-07 15:59:32,242 - INFO - Epoch 1: EPOCH_AVG_TRAIN_LOSS=1.0364
2025-02-07 15:59:32,268 - INFO - #################### Training epoch 2 ####################
2025-02-07 15:59:32,268 - INFO - Current Learning Rate: 8.436380e-04
2025-02-07 15:59:32,613 - INFO - Epoch 2: train_loss=1.0337
2025-02-07 15:59:32,756 - INFO - Epoch 2: train_loss=1.0508
2025-02-07 15:59:32,996 - INFO - Epoch 2: val_loss=0.9818, val_acc=66.67%
2025-02-07 15:59:33,000 - INFO - Epoch 2: EPOCH_AVG_TRAIN_LOSS=1.0337
2025-02-07 15:59:33,027 - INFO - #################### Training epoch 3 ####################
2025-02-07 15:59:33,027 - INFO - Current Learning Rate: 1.378753e-03
2025-02-07 15:59:33,374 - INFO - Epoch 3: train_loss=0.9714
2025-02-07 15:59:33,517 - INFO - Epoch 3: train_loss=1.0654
2025-02-07 15:59:33,761 - INFO - Epoch 3: val_loss=1.1367, val_acc=33.33%
2025-02-07 15:59:33,765 - INFO - Epoch 3: EPOCH_AVG_TRAIN_LOSS=0.9714
2025-02-07 15:59:33,767 - INFO - #################### Training epoch 4 ####################
2025-02-07 15:59:33,767 - INFO - Current Learning Rate: 2.092546e-03
2025-02-07 15:59:34,114 - INFO - Epoch 4: train_loss=0.9978
2025-02-07 15:59:34,258 - INFO - Epoch 4: train_loss=0.9519
2025-02-07 15:59:34,501 - INFO - Epoch 4: val_loss=1.1562, val_acc=0.00%
2025-02-07 15:59:34,504 - INFO - Epoch 4: EPOCH_AVG_TRAIN_LOSS=0.9519
2025-02-07 15:59:34,507 - INFO - #################### Training epoch 5 ####################
2025-02-07 15:59:34,507 - INFO - Current Learning Rate: 2.951639e-03
2025-02-07 15:59:34,853 - INFO - Epoch 5: train_loss=0.9884
2025-02-07 15:59:34,996 - INFO - Epoch 5: train_loss=0.9233
2025-02-07 15:59:35,238 - INFO - Epoch 5: val_loss=1.1458, val_acc=0.00%
2025-02-07 15:59:35,242 - INFO - Epoch 5: EPOCH_AVG_TRAIN_LOSS=0.9233
2025-02-07 15:59:35,244 - INFO - #################### Training epoch 6 ####################
2025-02-07 15:59:35,244 - INFO - Current Learning Rate: 3.915864e-03
2025-02-07 15:59:35,593 - INFO - Epoch 6: train_loss=0.8905
2025-02-07 15:59:35,736 - INFO - Epoch 6: train_loss=0.9326
2025-02-07 15:59:35,978 - INFO - Epoch 6: val_loss=1.1341, val_acc=33.33%
2025-02-07 15:59:35,981 - INFO - Epoch 6: EPOCH_AVG_TRAIN_LOSS=0.8905
2025-02-07 15:59:35,983 - INFO - #################### Training epoch 7 ####################
2025-02-07 15:59:35,983 - INFO - Current Learning Rate: 4.940133e-03
2025-02-07 15:59:36,330 - INFO - Epoch 7: train_loss=1.0155
2025-02-07 15:59:36,472 - INFO - Epoch 7: train_loss=0.7312
2025-02-07 15:59:36,717 - INFO - Epoch 7: val_loss=1.1654, val_acc=33.33%
2025-02-07 15:59:36,721 - INFO - Epoch 7: EPOCH_AVG_TRAIN_LOSS=0.7312
2025-02-07 15:59:36,723 - INFO - #################### Training epoch 8 ####################
2025-02-07 15:59:36,723 - INFO - Current Learning Rate: 5.976554e-03
2025-02-07 15:59:37,069 - INFO - Epoch 8: train_loss=0.9218
2025-02-07 15:59:37,212 - INFO - Epoch 8: train_loss=0.9502
2025-02-07 15:59:37,454 - INFO - Epoch 8: val_loss=1.2760, val_acc=33.33%
2025-02-07 15:59:37,458 - INFO - Epoch 8: EPOCH_AVG_TRAIN_LOSS=0.9218
2025-02-07 15:59:37,460 - INFO - #################### Training epoch 9 ####################
2025-02-07 15:59:37,460 - INFO - Current Learning Rate: 6.976663e-03
2025-02-07 15:59:37,803 - INFO - Epoch 9: train_loss=1.0688
2025-02-07 15:59:37,946 - INFO - Epoch 9: train_loss=0.6006
2025-02-07 15:59:38,190 - INFO - Epoch 9: val_loss=1.2904, val_acc=33.33%
2025-02-07 15:59:38,194 - INFO - Epoch 9: EPOCH_AVG_TRAIN_LOSS=0.6006
2025-02-07 15:59:38,196 - INFO - #################### Training epoch 10 ####################
2025-02-07 15:59:38,196 - INFO - Current Learning Rate: 7.893698e-03
2025-02-07 15:59:38,542 - INFO - Epoch 10: train_loss=0.9488
2025-02-07 15:59:38,685 - INFO - Epoch 10: train_loss=1.0441
2025-02-07 15:59:38,928 - INFO - Epoch 10: val_loss=1.3398, val_acc=33.33%
2025-02-07 15:59:38,932 - INFO - Epoch 10: EPOCH_AVG_TRAIN_LOSS=0.9488
2025-02-07 15:59:38,934 - INFO - #################### Training epoch 11 ####################
2025-02-07 15:59:38,934 - INFO - Current Learning Rate: 8.684778e-03
2025-02-07 15:59:39,282 - INFO - Epoch 11: train_loss=0.8317
2025-02-07 15:59:39,424 - INFO - Epoch 11: train_loss=nan
2025-02-07 15:59:39,669 - INFO - Epoch 11: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:39,673 - INFO - Epoch 11: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:39,675 - INFO - #################### Training epoch 12 ####################
2025-02-07 15:59:39,675 - INFO - Current Learning Rate: 9.312914e-03
2025-02-07 15:59:40,024 - INFO - Epoch 12: train_loss=nan
2025-02-07 15:59:40,166 - INFO - Epoch 12: train_loss=nan
2025-02-07 15:59:40,410 - INFO - Epoch 12: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:40,414 - INFO - Epoch 12: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:40,416 - INFO - #################### Training epoch 13 ####################
2025-02-07 15:59:40,416 - INFO - Current Learning Rate: 9.748735e-03
2025-02-07 15:59:40,761 - INFO - Epoch 13: train_loss=nan
2025-02-07 15:59:40,903 - INFO - Epoch 13: train_loss=nan
2025-02-07 15:59:41,149 - INFO - Epoch 13: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:41,152 - INFO - Epoch 13: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:41,154 - INFO - #################### Training epoch 14 ####################
2025-02-07 15:59:41,155 - INFO - Current Learning Rate: 9.971862e-03
2025-02-07 15:59:41,505 - INFO - Epoch 14: train_loss=nan
2025-02-07 15:59:41,647 - INFO - Epoch 14: train_loss=nan
2025-02-07 15:59:41,893 - INFO - Epoch 14: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:41,897 - INFO - Epoch 14: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:41,899 - INFO - #################### Training epoch 15 ####################
2025-02-07 15:59:41,899 - INFO - Current Learning Rate: 9.994965e-03
2025-02-07 15:59:42,246 - INFO - Epoch 15: train_loss=nan
2025-02-07 15:59:42,388 - INFO - Epoch 15: train_loss=nan
2025-02-07 15:59:42,630 - INFO - Epoch 15: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:42,634 - INFO - Epoch 15: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:42,636 - INFO - #################### Training epoch 16 ####################
2025-02-07 15:59:42,636 - INFO - Current Learning Rate: 9.954749e-03
2025-02-07 15:59:42,981 - INFO - Epoch 16: train_loss=nan
2025-02-07 15:59:43,123 - INFO - Epoch 16: train_loss=nan
2025-02-07 15:59:43,365 - INFO - Epoch 16: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:43,369 - INFO - Epoch 16: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:43,371 - INFO - #################### Training epoch 17 ####################
2025-02-07 15:59:43,372 - INFO - Current Learning Rate: 9.874640e-03
2025-02-07 15:59:43,721 - INFO - Epoch 17: train_loss=nan
2025-02-07 15:59:43,864 - INFO - Epoch 17: train_loss=nan
2025-02-07 15:59:44,108 - INFO - Epoch 17: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:44,112 - INFO - Epoch 17: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:44,114 - INFO - #################### Training epoch 18 ####################
2025-02-07 15:59:44,114 - INFO - Current Learning Rate: 9.755284e-03
2025-02-07 15:59:44,460 - INFO - Epoch 18: train_loss=nan
2025-02-07 15:59:44,602 - INFO - Epoch 18: train_loss=nan
2025-02-07 15:59:44,844 - INFO - Epoch 18: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:44,848 - INFO - Epoch 18: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:44,850 - INFO - #################### Training epoch 19 ####################
2025-02-07 15:59:44,850 - INFO - Current Learning Rate: 9.597640e-03
2025-02-07 15:59:45,196 - INFO - Epoch 19: train_loss=nan
2025-02-07 15:59:45,338 - INFO - Epoch 19: train_loss=nan
2025-02-07 15:59:45,583 - INFO - Epoch 19: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:45,586 - INFO - Epoch 19: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:45,589 - INFO - #################### Training epoch 20 ####################
2025-02-07 15:59:45,589 - INFO - Current Learning Rate: 9.402980e-03
2025-02-07 15:59:45,937 - INFO - Epoch 20: train_loss=nan
2025-02-07 15:59:46,080 - INFO - Epoch 20: train_loss=nan
2025-02-07 15:59:46,325 - INFO - Epoch 20: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:46,329 - INFO - Epoch 20: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:46,331 - INFO - #################### Training epoch 21 ####################
2025-02-07 15:59:46,331 - INFO - Current Learning Rate: 9.172870e-03
2025-02-07 15:59:46,681 - INFO - Epoch 21: train_loss=nan
2025-02-07 15:59:46,826 - INFO - Epoch 21: train_loss=nan
2025-02-07 15:59:47,071 - INFO - Epoch 21: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:47,075 - INFO - Epoch 21: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:47,078 - INFO - #################### Training epoch 22 ####################
2025-02-07 15:59:47,078 - INFO - Current Learning Rate: 8.909162e-03
2025-02-07 15:59:47,427 - INFO - Epoch 22: train_loss=nan
2025-02-07 15:59:47,570 - INFO - Epoch 22: train_loss=nan
2025-02-07 15:59:47,816 - INFO - Epoch 22: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:47,820 - INFO - Epoch 22: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:47,822 - INFO - #################### Training epoch 23 ####################
2025-02-07 15:59:47,822 - INFO - Current Learning Rate: 8.613980e-03
2025-02-07 15:59:48,169 - INFO - Epoch 23: train_loss=nan
2025-02-07 15:59:48,311 - INFO - Epoch 23: train_loss=nan
2025-02-07 15:59:48,551 - INFO - Epoch 23: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:48,555 - INFO - Epoch 23: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:48,557 - INFO - #################### Training epoch 24 ####################
2025-02-07 15:59:48,557 - INFO - Current Learning Rate: 8.289700e-03
2025-02-07 15:59:48,905 - INFO - Epoch 24: train_loss=nan
2025-02-07 15:59:49,047 - INFO - Epoch 24: train_loss=nan
2025-02-07 15:59:49,285 - INFO - Epoch 24: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:49,289 - INFO - Epoch 24: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:49,291 - INFO - #################### Training epoch 25 ####################
2025-02-07 15:59:49,291 - INFO - Current Learning Rate: 7.938935e-03
2025-02-07 15:59:49,636 - INFO - Epoch 25: train_loss=nan
2025-02-07 15:59:49,777 - INFO - Epoch 25: train_loss=nan
2025-02-07 15:59:50,021 - INFO - Epoch 25: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:50,024 - INFO - Epoch 25: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:50,026 - INFO - #################### Training epoch 26 ####################
2025-02-07 15:59:50,026 - INFO - Current Learning Rate: 7.564506e-03
2025-02-07 15:59:50,373 - INFO - Epoch 26: train_loss=nan
2025-02-07 15:59:50,515 - INFO - Epoch 26: train_loss=nan
2025-02-07 15:59:50,758 - INFO - Epoch 26: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:50,761 - INFO - Epoch 26: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:50,764 - INFO - #################### Training epoch 27 ####################
2025-02-07 15:59:50,764 - INFO - Current Learning Rate: 7.169430e-03
2025-02-07 15:59:51,110 - INFO - Epoch 27: train_loss=nan
2025-02-07 15:59:51,252 - INFO - Epoch 27: train_loss=nan
2025-02-07 15:59:51,495 - INFO - Epoch 27: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:51,499 - INFO - Epoch 27: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:51,501 - INFO - #################### Training epoch 28 ####################
2025-02-07 15:59:51,501 - INFO - Current Learning Rate: 6.756887e-03
2025-02-07 15:59:51,846 - INFO - Epoch 28: train_loss=nan
2025-02-07 15:59:51,988 - INFO - Epoch 28: train_loss=nan
2025-02-07 15:59:52,231 - INFO - Epoch 28: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:52,235 - INFO - Epoch 28: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:52,237 - INFO - #################### Training epoch 29 ####################
2025-02-07 15:59:52,237 - INFO - Current Learning Rate: 6.330199e-03
2025-02-07 15:59:52,582 - INFO - Epoch 29: train_loss=nan
2025-02-07 15:59:52,724 - INFO - Epoch 29: train_loss=nan
2025-02-07 15:59:52,967 - INFO - Epoch 29: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:52,971 - INFO - Epoch 29: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:52,973 - INFO - #################### Training epoch 30 ####################
2025-02-07 15:59:52,973 - INFO - Current Learning Rate: 5.892801e-03
2025-02-07 15:59:53,322 - INFO - Epoch 30: train_loss=nan
2025-02-07 15:59:53,464 - INFO - Epoch 30: train_loss=nan
2025-02-07 15:59:53,708 - INFO - Epoch 30: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:53,712 - INFO - Epoch 30: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:53,714 - INFO - #################### Training epoch 31 ####################
2025-02-07 15:59:53,714 - INFO - Current Learning Rate: 5.448215e-03
2025-02-07 15:59:54,062 - INFO - Epoch 31: train_loss=nan
2025-02-07 15:59:54,204 - INFO - Epoch 31: train_loss=nan
2025-02-07 15:59:54,450 - INFO - Epoch 31: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:54,453 - INFO - Epoch 31: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:54,455 - INFO - #################### Training epoch 32 ####################
2025-02-07 15:59:54,455 - INFO - Current Learning Rate: 5.000020e-03
2025-02-07 15:59:54,803 - INFO - Epoch 32: train_loss=nan
2025-02-07 15:59:54,945 - INFO - Epoch 32: train_loss=nan
2025-02-07 15:59:55,191 - INFO - Epoch 32: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:55,195 - INFO - Epoch 32: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:55,197 - INFO - #################### Training epoch 33 ####################
2025-02-07 15:59:55,197 - INFO - Current Learning Rate: 4.551825e-03
2025-02-07 15:59:55,544 - INFO - Epoch 33: train_loss=nan
2025-02-07 15:59:55,686 - INFO - Epoch 33: train_loss=nan
2025-02-07 15:59:55,929 - INFO - Epoch 33: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:55,932 - INFO - Epoch 33: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:55,935 - INFO - #################### Training epoch 34 ####################
2025-02-07 15:59:55,935 - INFO - Current Learning Rate: 4.107239e-03
2025-02-07 15:59:56,282 - INFO - Epoch 34: train_loss=nan
2025-02-07 15:59:56,424 - INFO - Epoch 34: train_loss=nan
2025-02-07 15:59:56,663 - INFO - Epoch 34: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:56,666 - INFO - Epoch 34: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:56,668 - INFO - #################### Training epoch 35 ####################
2025-02-07 15:59:56,668 - INFO - Current Learning Rate: 3.669841e-03
2025-02-07 15:59:57,016 - INFO - Epoch 35: train_loss=nan
2025-02-07 15:59:57,158 - INFO - Epoch 35: train_loss=nan
2025-02-07 15:59:57,401 - INFO - Epoch 35: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:57,405 - INFO - Epoch 35: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:57,407 - INFO - #################### Training epoch 36 ####################
2025-02-07 15:59:57,407 - INFO - Current Learning Rate: 3.243153e-03
2025-02-07 15:59:57,753 - INFO - Epoch 36: train_loss=nan
2025-02-07 15:59:57,895 - INFO - Epoch 36: train_loss=nan
2025-02-07 15:59:58,140 - INFO - Epoch 36: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:58,143 - INFO - Epoch 36: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:58,145 - INFO - #################### Training epoch 37 ####################
2025-02-07 15:59:58,145 - INFO - Current Learning Rate: 2.830610e-03
2025-02-07 15:59:58,492 - INFO - Epoch 37: train_loss=nan
2025-02-07 15:59:58,635 - INFO - Epoch 37: train_loss=nan
2025-02-07 15:59:58,874 - INFO - Epoch 37: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:58,877 - INFO - Epoch 37: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:58,880 - INFO - #################### Training epoch 38 ####################
2025-02-07 15:59:58,880 - INFO - Current Learning Rate: 2.435534e-03
2025-02-07 15:59:59,227 - INFO - Epoch 38: train_loss=nan
2025-02-07 15:59:59,370 - INFO - Epoch 38: train_loss=nan
2025-02-07 15:59:59,614 - INFO - Epoch 38: val_loss=nan, val_acc=66.67%
2025-02-07 15:59:59,618 - INFO - Epoch 38: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 15:59:59,620 - INFO - #################### Training epoch 39 ####################
2025-02-07 15:59:59,620 - INFO - Current Learning Rate: 2.061105e-03
2025-02-07 15:59:59,968 - INFO - Epoch 39: train_loss=nan
2025-02-07 16:00:00,111 - INFO - Epoch 39: train_loss=nan
2025-02-07 16:00:00,354 - INFO - Epoch 39: val_loss=nan, val_acc=66.67%
2025-02-07 16:00:00,358 - INFO - Epoch 39: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 16:00:00,360 - INFO - #################### Training epoch 40 ####################
2025-02-07 16:00:00,360 - INFO - Current Learning Rate: 1.710340e-03
2025-02-07 16:00:00,707 - INFO - Epoch 40: train_loss=nan
2025-02-07 16:00:00,849 - INFO - Epoch 40: train_loss=nan
2025-02-07 16:00:01,090 - INFO - Epoch 40: val_loss=nan, val_acc=66.67%
2025-02-07 16:00:01,094 - INFO - Epoch 40: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 16:00:01,096 - INFO - #################### Training epoch 41 ####################
2025-02-07 16:00:01,096 - INFO - Current Learning Rate: 1.386060e-03
2025-02-07 16:00:01,444 - INFO - Epoch 41: train_loss=nan
2025-02-07 16:00:01,586 - INFO - Epoch 41: train_loss=nan
2025-02-07 16:00:01,832 - INFO - Epoch 41: val_loss=nan, val_acc=66.67%
2025-02-07 16:00:01,836 - INFO - Epoch 41: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 16:00:01,838 - INFO - #################### Training epoch 42 ####################
2025-02-07 16:00:01,838 - INFO - Current Learning Rate: 1.090878e-03
2025-02-07 16:00:02,186 - INFO - Epoch 42: train_loss=nan
2025-02-07 16:00:02,328 - INFO - Epoch 42: train_loss=nan
2025-02-07 16:00:02,575 - INFO - Epoch 42: val_loss=nan, val_acc=66.67%
2025-02-07 16:00:02,578 - INFO - Epoch 42: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 16:00:02,580 - INFO - #################### Training epoch 43 ####################
2025-02-07 16:00:02,580 - INFO - Current Learning Rate: 8.271704e-04
2025-02-07 16:00:02,929 - INFO - Epoch 43: train_loss=nan
2025-02-07 16:00:03,072 - INFO - Epoch 43: train_loss=nan
2025-02-07 16:00:03,315 - INFO - Epoch 43: val_loss=nan, val_acc=66.67%
2025-02-07 16:00:03,318 - INFO - Epoch 43: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 16:00:03,320 - INFO - #################### Training epoch 44 ####################
2025-02-07 16:00:03,321 - INFO - Current Learning Rate: 5.970600e-04
2025-02-07 16:00:03,669 - INFO - Epoch 44: train_loss=nan
2025-02-07 16:00:03,812 - INFO - Epoch 44: train_loss=nan
2025-02-07 16:00:04,056 - INFO - Epoch 44: val_loss=nan, val_acc=66.67%
2025-02-07 16:00:04,060 - INFO - Epoch 44: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 16:00:04,062 - INFO - #################### Training epoch 45 ####################
2025-02-07 16:00:04,062 - INFO - Current Learning Rate: 4.023995e-04
2025-02-07 16:00:04,409 - INFO - Epoch 45: train_loss=nan
2025-02-07 16:00:04,551 - INFO - Epoch 45: train_loss=nan
2025-02-07 16:00:04,792 - INFO - Epoch 45: val_loss=nan, val_acc=66.67%
2025-02-07 16:00:04,796 - INFO - Epoch 45: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 16:00:04,798 - INFO - #################### Training epoch 46 ####################
2025-02-07 16:00:04,798 - INFO - Current Learning Rate: 2.447564e-04
2025-02-07 16:00:05,143 - INFO - Epoch 46: train_loss=nan
2025-02-07 16:00:05,285 - INFO - Epoch 46: train_loss=nan
2025-02-07 16:00:05,528 - INFO - Epoch 46: val_loss=nan, val_acc=66.67%
2025-02-07 16:00:05,532 - INFO - Epoch 46: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 16:00:05,534 - INFO - #################### Training epoch 47 ####################
2025-02-07 16:00:05,534 - INFO - Current Learning Rate: 1.253999e-04
2025-02-07 16:00:05,885 - INFO - Epoch 47: train_loss=nan
2025-02-07 16:00:06,027 - INFO - Epoch 47: train_loss=nan
2025-02-07 16:00:06,266 - INFO - Epoch 47: val_loss=nan, val_acc=66.67%
2025-02-07 16:00:06,270 - INFO - Epoch 47: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 16:00:06,272 - INFO - #################### Training epoch 48 ####################
2025-02-07 16:00:06,272 - INFO - Current Learning Rate: 4.529101e-05
2025-02-07 16:00:06,621 - INFO - Epoch 48: train_loss=nan
2025-02-07 16:00:06,763 - INFO - Epoch 48: train_loss=nan
2025-02-07 16:00:07,008 - INFO - Epoch 48: val_loss=nan, val_acc=66.67%
2025-02-07 16:00:07,012 - INFO - Epoch 48: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 16:00:07,014 - INFO - #################### Training epoch 49 ####################
2025-02-07 16:00:07,014 - INFO - Current Learning Rate: 5.074647e-06
2025-02-07 16:00:07,362 - INFO - Epoch 49: train_loss=nan
2025-02-07 16:00:07,505 - INFO - Epoch 49: train_loss=nan
2025-02-07 16:00:07,749 - INFO - Epoch 49: val_loss=nan, val_acc=66.67%
2025-02-07 16:00:07,753 - INFO - Epoch 49: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-07 16:00:07,933 - INFO - Model saved.
