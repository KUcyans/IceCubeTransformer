2025-02-01 16:53:30,296 - INFO - Starting training with the following parameters:
2025-02-01 16:53:30,297 - INFO - | Parameter       | Value               |
|-----------------|---------------------|
| attention       | Scaled Dot-Product |
| d_model         | 128            |
| n_heads         | 8              |
| d_f             | 128            |
| num_layers      | 3              |
| d_input         | 32             |
| num_classes     | 3              |
| dropout         | 0.1            |
| learning_rate   | 0.005          |
| epochs          | 200            |

2025-02-01 16:53:31,270 - INFO - Epoch 0: val_loss=1.5533, val_acc=33.33%
2025-02-01 16:53:31,404 - INFO - ####################Training epoch 0####################
2025-02-01 16:53:31,561 - INFO - Epoch 0: train_loss=1.4113
2025-02-01 16:53:32,240 - INFO - Epoch 0: val_loss=2.0009, val_acc=33.33%
2025-02-01 16:53:32,275 - INFO - ####################Training epoch 1####################
2025-02-01 16:53:32,556 - INFO - Epoch 1: train_loss=2.7345
2025-02-01 16:53:33,024 - INFO - Epoch 1: val_loss=3.5367, val_acc=33.33%
2025-02-01 16:53:33,051 - INFO - ####################Training epoch 2####################
2025-02-01 16:53:33,336 - INFO - Epoch 2: train_loss=3.4982
2025-02-01 16:53:33,807 - INFO - Epoch 2: val_loss=2.4958, val_acc=33.33%
2025-02-01 16:53:33,833 - INFO - ####################Training epoch 3####################
2025-02-01 16:53:34,121 - INFO - Epoch 3: train_loss=1.7932
2025-02-01 16:53:34,591 - INFO - Epoch 3: val_loss=2.2020, val_acc=0.00%
2025-02-01 16:53:34,632 - INFO - ####################Training epoch 4####################
2025-02-01 16:53:34,918 - INFO - Epoch 4: train_loss=1.6679
2025-02-01 16:53:35,391 - INFO - Epoch 4: val_loss=1.4065, val_acc=33.33%
2025-02-01 16:53:35,395 - INFO - ####################Training epoch 5####################
2025-02-01 16:53:35,681 - INFO - Epoch 5: train_loss=1.3164
2025-02-01 16:53:36,150 - INFO - Epoch 5: val_loss=1.4724, val_acc=33.33%
2025-02-01 16:53:36,154 - INFO - ####################Training epoch 6####################
2025-02-01 16:53:36,442 - INFO - Epoch 6: train_loss=1.1197
2025-02-01 16:53:36,915 - INFO - Epoch 6: val_loss=1.8110, val_acc=33.33%
2025-02-01 16:53:36,919 - INFO - ####################Training epoch 7####################
2025-02-01 16:53:37,206 - INFO - Epoch 7: train_loss=1.0853
2025-02-01 16:53:37,676 - INFO - Epoch 7: val_loss=1.9415, val_acc=33.33%
2025-02-01 16:53:37,680 - INFO - ####################Training epoch 8####################
2025-02-01 16:53:37,967 - INFO - Epoch 8: train_loss=1.0886
2025-02-01 16:53:38,439 - INFO - Epoch 8: val_loss=1.6283, val_acc=0.00%
2025-02-01 16:53:38,467 - INFO - ####################Training epoch 9####################
2025-02-01 16:53:38,750 - INFO - Epoch 9: train_loss=1.1202
2025-02-01 16:53:39,220 - INFO - Epoch 9: val_loss=1.3709, val_acc=0.00%
2025-02-01 16:53:39,249 - INFO - ####################Training epoch 10####################
2025-02-01 16:53:39,538 - INFO - Epoch 10: train_loss=1.1656
2025-02-01 16:53:40,008 - INFO - Epoch 10: val_loss=1.4231, val_acc=0.00%
2025-02-01 16:53:40,012 - INFO - ####################Training epoch 11####################
2025-02-01 16:53:40,300 - INFO - Epoch 11: train_loss=1.1956
2025-02-01 16:53:40,772 - INFO - Epoch 11: val_loss=1.4999, val_acc=0.00%
2025-02-01 16:53:40,776 - INFO - ####################Training epoch 12####################
2025-02-01 16:53:41,062 - INFO - Epoch 12: train_loss=1.2263
2025-02-01 16:53:41,533 - INFO - Epoch 12: val_loss=1.5756, val_acc=0.00%
2025-02-01 16:53:41,537 - INFO - ####################Training epoch 13####################
2025-02-01 16:53:41,826 - INFO - Epoch 13: train_loss=1.2554
2025-02-01 16:53:42,295 - INFO - Epoch 13: val_loss=1.6461, val_acc=0.00%
2025-02-01 16:53:42,298 - INFO - ####################Training epoch 14####################
2025-02-01 16:53:42,586 - INFO - Epoch 14: train_loss=1.2837
2025-02-01 16:53:43,055 - INFO - Epoch 14: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:43,059 - INFO - ####################Training epoch 15####################
2025-02-01 16:53:43,344 - INFO - Epoch 15: train_loss=nan
2025-02-01 16:53:43,815 - INFO - Epoch 15: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:43,819 - INFO - ####################Training epoch 16####################
2025-02-01 16:53:44,106 - INFO - Epoch 16: train_loss=nan
2025-02-01 16:53:44,577 - INFO - Epoch 16: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:44,580 - INFO - ####################Training epoch 17####################
2025-02-01 16:53:44,868 - INFO - Epoch 17: train_loss=nan
2025-02-01 16:53:45,339 - INFO - Epoch 17: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:45,342 - INFO - ####################Training epoch 18####################
2025-02-01 16:53:45,628 - INFO - Epoch 18: train_loss=nan
2025-02-01 16:53:46,099 - INFO - Epoch 18: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:46,103 - INFO - ####################Training epoch 19####################
2025-02-01 16:53:46,390 - INFO - Epoch 19: train_loss=nan
2025-02-01 16:53:46,863 - INFO - Epoch 19: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:46,867 - INFO - ####################Training epoch 20####################
2025-02-01 16:53:47,152 - INFO - Epoch 20: train_loss=nan
2025-02-01 16:53:47,623 - INFO - Epoch 20: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:47,627 - INFO - ####################Training epoch 21####################
2025-02-01 16:53:47,913 - INFO - Epoch 21: train_loss=nan
2025-02-01 16:53:48,386 - INFO - Epoch 21: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:48,390 - INFO - ####################Training epoch 22####################
2025-02-01 16:53:48,677 - INFO - Epoch 22: train_loss=nan
2025-02-01 16:53:49,146 - INFO - Epoch 22: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:49,150 - INFO - ####################Training epoch 23####################
2025-02-01 16:53:49,436 - INFO - Epoch 23: train_loss=nan
2025-02-01 16:53:49,907 - INFO - Epoch 23: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:49,911 - INFO - ####################Training epoch 24####################
2025-02-01 16:53:50,198 - INFO - Epoch 24: train_loss=nan
2025-02-01 16:53:50,669 - INFO - Epoch 24: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:50,673 - INFO - ####################Training epoch 25####################
2025-02-01 16:53:50,960 - INFO - Epoch 25: train_loss=nan
2025-02-01 16:53:51,432 - INFO - Epoch 25: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:51,436 - INFO - ####################Training epoch 26####################
2025-02-01 16:53:51,720 - INFO - Epoch 26: train_loss=nan
2025-02-01 16:53:52,190 - INFO - Epoch 26: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:52,194 - INFO - ####################Training epoch 27####################
2025-02-01 16:53:52,483 - INFO - Epoch 27: train_loss=nan
2025-02-01 16:53:52,956 - INFO - Epoch 27: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:52,960 - INFO - ####################Training epoch 28####################
2025-02-01 16:53:53,245 - INFO - Epoch 28: train_loss=nan
2025-02-01 16:53:53,716 - INFO - Epoch 28: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:53,720 - INFO - ####################Training epoch 29####################
2025-02-01 16:53:54,008 - INFO - Epoch 29: train_loss=nan
2025-02-01 16:53:54,481 - INFO - Epoch 29: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:54,485 - INFO - ####################Training epoch 30####################
2025-02-01 16:53:54,773 - INFO - Epoch 30: train_loss=nan
2025-02-01 16:53:55,243 - INFO - Epoch 30: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:55,247 - INFO - ####################Training epoch 31####################
2025-02-01 16:53:55,536 - INFO - Epoch 31: train_loss=nan
2025-02-01 16:53:56,009 - INFO - Epoch 31: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:56,013 - INFO - ####################Training epoch 32####################
2025-02-01 16:53:56,299 - INFO - Epoch 32: train_loss=nan
2025-02-01 16:53:56,771 - INFO - Epoch 32: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:56,776 - INFO - ####################Training epoch 33####################
2025-02-01 16:53:57,061 - INFO - Epoch 33: train_loss=nan
2025-02-01 16:53:57,530 - INFO - Epoch 33: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:57,534 - INFO - ####################Training epoch 34####################
2025-02-01 16:53:57,818 - INFO - Epoch 34: train_loss=nan
2025-02-01 16:53:58,288 - INFO - Epoch 34: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:58,292 - INFO - ####################Training epoch 35####################
2025-02-01 16:53:58,579 - INFO - Epoch 35: train_loss=nan
2025-02-01 16:53:59,053 - INFO - Epoch 35: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:59,056 - INFO - ####################Training epoch 36####################
2025-02-01 16:53:59,341 - INFO - Epoch 36: train_loss=nan
2025-02-01 16:53:59,812 - INFO - Epoch 36: val_loss=nan, val_acc=66.67%
2025-02-01 16:53:59,816 - INFO - ####################Training epoch 37####################
2025-02-01 16:54:00,103 - INFO - Epoch 37: train_loss=nan
2025-02-01 16:54:00,576 - INFO - Epoch 37: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:00,579 - INFO - ####################Training epoch 38####################
2025-02-01 16:54:00,867 - INFO - Epoch 38: train_loss=nan
2025-02-01 16:54:01,337 - INFO - Epoch 38: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:01,341 - INFO - ####################Training epoch 39####################
2025-02-01 16:54:01,628 - INFO - Epoch 39: train_loss=nan
2025-02-01 16:54:02,100 - INFO - Epoch 39: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:02,104 - INFO - ####################Training epoch 40####################
2025-02-01 16:54:02,390 - INFO - Epoch 40: train_loss=nan
2025-02-01 16:54:02,861 - INFO - Epoch 40: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:02,864 - INFO - ####################Training epoch 41####################
2025-02-01 16:54:03,150 - INFO - Epoch 41: train_loss=nan
2025-02-01 16:54:03,622 - INFO - Epoch 41: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:03,626 - INFO - ####################Training epoch 42####################
2025-02-01 16:54:03,911 - INFO - Epoch 42: train_loss=nan
2025-02-01 16:54:04,500 - INFO - Epoch 42: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:04,503 - INFO - ####################Training epoch 43####################
2025-02-01 16:54:04,790 - INFO - Epoch 43: train_loss=nan
2025-02-01 16:54:05,263 - INFO - Epoch 43: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:05,267 - INFO - ####################Training epoch 44####################
2025-02-01 16:54:05,556 - INFO - Epoch 44: train_loss=nan
2025-02-01 16:54:06,029 - INFO - Epoch 44: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:06,033 - INFO - ####################Training epoch 45####################
2025-02-01 16:54:06,320 - INFO - Epoch 45: train_loss=nan
2025-02-01 16:54:06,791 - INFO - Epoch 45: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:06,795 - INFO - ####################Training epoch 46####################
2025-02-01 16:54:07,080 - INFO - Epoch 46: train_loss=nan
2025-02-01 16:54:07,553 - INFO - Epoch 46: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:07,556 - INFO - ####################Training epoch 47####################
2025-02-01 16:54:07,842 - INFO - Epoch 47: train_loss=nan
2025-02-01 16:54:08,315 - INFO - Epoch 47: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:08,319 - INFO - ####################Training epoch 48####################
2025-02-01 16:54:08,609 - INFO - Epoch 48: train_loss=nan
2025-02-01 16:54:09,081 - INFO - Epoch 48: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:09,085 - INFO - ####################Training epoch 49####################
2025-02-01 16:54:09,372 - INFO - Epoch 49: train_loss=nan
2025-02-01 16:54:09,844 - INFO - Epoch 49: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:09,848 - INFO - ####################Training epoch 50####################
2025-02-01 16:54:10,136 - INFO - Epoch 50: train_loss=nan
2025-02-01 16:54:10,607 - INFO - Epoch 50: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:10,611 - INFO - ####################Training epoch 51####################
2025-02-01 16:54:10,897 - INFO - Epoch 51: train_loss=nan
2025-02-01 16:54:11,369 - INFO - Epoch 51: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:11,373 - INFO - ####################Training epoch 52####################
2025-02-01 16:54:11,660 - INFO - Epoch 52: train_loss=nan
2025-02-01 16:54:12,132 - INFO - Epoch 52: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:12,136 - INFO - ####################Training epoch 53####################
2025-02-01 16:54:12,421 - INFO - Epoch 53: train_loss=nan
2025-02-01 16:54:12,891 - INFO - Epoch 53: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:12,895 - INFO - ####################Training epoch 54####################
2025-02-01 16:54:13,183 - INFO - Epoch 54: train_loss=nan
2025-02-01 16:54:13,654 - INFO - Epoch 54: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:13,658 - INFO - ####################Training epoch 55####################
2025-02-01 16:54:13,946 - INFO - Epoch 55: train_loss=nan
2025-02-01 16:54:14,415 - INFO - Epoch 55: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:14,419 - INFO - ####################Training epoch 56####################
2025-02-01 16:54:14,706 - INFO - Epoch 56: train_loss=nan
2025-02-01 16:54:15,177 - INFO - Epoch 56: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:15,181 - INFO - ####################Training epoch 57####################
2025-02-01 16:54:15,467 - INFO - Epoch 57: train_loss=nan
2025-02-01 16:54:15,938 - INFO - Epoch 57: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:15,942 - INFO - ####################Training epoch 58####################
2025-02-01 16:54:16,227 - INFO - Epoch 58: train_loss=nan
2025-02-01 16:54:16,700 - INFO - Epoch 58: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:16,704 - INFO - ####################Training epoch 59####################
2025-02-01 16:54:16,991 - INFO - Epoch 59: train_loss=nan
2025-02-01 16:54:17,466 - INFO - Epoch 59: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:17,470 - INFO - ####################Training epoch 60####################
2025-02-01 16:54:17,755 - INFO - Epoch 60: train_loss=nan
2025-02-01 16:54:18,227 - INFO - Epoch 60: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:18,230 - INFO - ####################Training epoch 61####################
2025-02-01 16:54:18,517 - INFO - Epoch 61: train_loss=nan
2025-02-01 16:54:18,989 - INFO - Epoch 61: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:18,993 - INFO - ####################Training epoch 62####################
2025-02-01 16:54:19,279 - INFO - Epoch 62: train_loss=nan
2025-02-01 16:54:19,754 - INFO - Epoch 62: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:19,758 - INFO - ####################Training epoch 63####################
2025-02-01 16:54:20,044 - INFO - Epoch 63: train_loss=nan
2025-02-01 16:54:20,519 - INFO - Epoch 63: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:20,522 - INFO - ####################Training epoch 64####################
2025-02-01 16:54:20,810 - INFO - Epoch 64: train_loss=nan
2025-02-01 16:54:21,284 - INFO - Epoch 64: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:21,288 - INFO - ####################Training epoch 65####################
2025-02-01 16:54:21,576 - INFO - Epoch 65: train_loss=nan
2025-02-01 16:54:22,049 - INFO - Epoch 65: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:22,053 - INFO - ####################Training epoch 66####################
2025-02-01 16:54:22,341 - INFO - Epoch 66: train_loss=nan
2025-02-01 16:54:22,815 - INFO - Epoch 66: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:22,819 - INFO - ####################Training epoch 67####################
2025-02-01 16:54:23,106 - INFO - Epoch 67: train_loss=nan
2025-02-01 16:54:23,580 - INFO - Epoch 67: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:23,583 - INFO - ####################Training epoch 68####################
2025-02-01 16:54:23,869 - INFO - Epoch 68: train_loss=nan
2025-02-01 16:54:24,342 - INFO - Epoch 68: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:24,345 - INFO - ####################Training epoch 69####################
2025-02-01 16:54:24,633 - INFO - Epoch 69: train_loss=nan
2025-02-01 16:54:25,108 - INFO - Epoch 69: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:25,111 - INFO - ####################Training epoch 70####################
2025-02-01 16:54:25,398 - INFO - Epoch 70: train_loss=nan
2025-02-01 16:54:25,869 - INFO - Epoch 70: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:25,873 - INFO - ####################Training epoch 71####################
2025-02-01 16:54:26,160 - INFO - Epoch 71: train_loss=nan
2025-02-01 16:54:26,634 - INFO - Epoch 71: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:26,638 - INFO - ####################Training epoch 72####################
2025-02-01 16:54:26,926 - INFO - Epoch 72: train_loss=nan
2025-02-01 16:54:27,399 - INFO - Epoch 72: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:27,403 - INFO - ####################Training epoch 73####################
2025-02-01 16:54:27,691 - INFO - Epoch 73: train_loss=nan
2025-02-01 16:54:28,163 - INFO - Epoch 73: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:28,167 - INFO - ####################Training epoch 74####################
2025-02-01 16:54:28,453 - INFO - Epoch 74: train_loss=nan
2025-02-01 16:54:28,927 - INFO - Epoch 74: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:28,931 - INFO - ####################Training epoch 75####################
2025-02-01 16:54:29,217 - INFO - Epoch 75: train_loss=nan
2025-02-01 16:54:29,692 - INFO - Epoch 75: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:29,696 - INFO - ####################Training epoch 76####################
2025-02-01 16:54:29,983 - INFO - Epoch 76: train_loss=nan
2025-02-01 16:54:30,455 - INFO - Epoch 76: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:30,459 - INFO - ####################Training epoch 77####################
2025-02-01 16:54:30,746 - INFO - Epoch 77: train_loss=nan
2025-02-01 16:54:31,219 - INFO - Epoch 77: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:31,222 - INFO - ####################Training epoch 78####################
2025-02-01 16:54:31,512 - INFO - Epoch 78: train_loss=nan
2025-02-01 16:54:31,985 - INFO - Epoch 78: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:31,989 - INFO - ####################Training epoch 79####################
2025-02-01 16:54:32,279 - INFO - Epoch 79: train_loss=nan
2025-02-01 16:54:32,753 - INFO - Epoch 79: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:32,757 - INFO - ####################Training epoch 80####################
2025-02-01 16:54:33,046 - INFO - Epoch 80: train_loss=nan
2025-02-01 16:54:33,523 - INFO - Epoch 80: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:33,526 - INFO - ####################Training epoch 81####################
2025-02-01 16:54:33,814 - INFO - Epoch 81: train_loss=nan
2025-02-01 16:54:34,287 - INFO - Epoch 81: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:34,291 - INFO - ####################Training epoch 82####################
2025-02-01 16:54:34,581 - INFO - Epoch 82: train_loss=nan
2025-02-01 16:54:35,054 - INFO - Epoch 82: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:35,058 - INFO - ####################Training epoch 83####################
2025-02-01 16:54:35,349 - INFO - Epoch 83: train_loss=nan
2025-02-01 16:54:35,823 - INFO - Epoch 83: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:35,827 - INFO - ####################Training epoch 84####################
2025-02-01 16:54:36,119 - INFO - Epoch 84: train_loss=nan
2025-02-01 16:54:36,593 - INFO - Epoch 84: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:36,597 - INFO - ####################Training epoch 85####################
2025-02-01 16:54:36,883 - INFO - Epoch 85: train_loss=nan
2025-02-01 16:54:37,358 - INFO - Epoch 85: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:37,363 - INFO - ####################Training epoch 86####################
2025-02-01 16:54:37,652 - INFO - Epoch 86: train_loss=nan
2025-02-01 16:54:38,124 - INFO - Epoch 86: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:38,128 - INFO - ####################Training epoch 87####################
2025-02-01 16:54:38,415 - INFO - Epoch 87: train_loss=nan
2025-02-01 16:54:38,890 - INFO - Epoch 87: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:38,894 - INFO - ####################Training epoch 88####################
2025-02-01 16:54:39,181 - INFO - Epoch 88: train_loss=nan
2025-02-01 16:54:39,656 - INFO - Epoch 88: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:39,660 - INFO - ####################Training epoch 89####################
2025-02-01 16:54:39,948 - INFO - Epoch 89: train_loss=nan
2025-02-01 16:54:40,421 - INFO - Epoch 89: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:40,425 - INFO - ####################Training epoch 90####################
2025-02-01 16:54:40,716 - INFO - Epoch 90: train_loss=nan
2025-02-01 16:54:41,191 - INFO - Epoch 90: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:41,195 - INFO - ####################Training epoch 91####################
2025-02-01 16:54:41,483 - INFO - Epoch 91: train_loss=nan
2025-02-01 16:54:41,955 - INFO - Epoch 91: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:41,959 - INFO - ####################Training epoch 92####################
2025-02-01 16:54:42,247 - INFO - Epoch 92: train_loss=nan
2025-02-01 16:54:42,719 - INFO - Epoch 92: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:42,723 - INFO - ####################Training epoch 93####################
2025-02-01 16:54:43,008 - INFO - Epoch 93: train_loss=nan
2025-02-01 16:54:43,479 - INFO - Epoch 93: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:43,484 - INFO - ####################Training epoch 94####################
2025-02-01 16:54:43,771 - INFO - Epoch 94: train_loss=nan
2025-02-01 16:54:44,243 - INFO - Epoch 94: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:44,247 - INFO - ####################Training epoch 95####################
2025-02-01 16:54:44,531 - INFO - Epoch 95: train_loss=nan
2025-02-01 16:54:45,001 - INFO - Epoch 95: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:45,005 - INFO - ####################Training epoch 96####################
2025-02-01 16:54:45,290 - INFO - Epoch 96: train_loss=nan
2025-02-01 16:54:45,762 - INFO - Epoch 96: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:45,766 - INFO - ####################Training epoch 97####################
2025-02-01 16:54:46,053 - INFO - Epoch 97: train_loss=nan
2025-02-01 16:54:46,522 - INFO - Epoch 97: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:46,525 - INFO - ####################Training epoch 98####################
2025-02-01 16:54:46,813 - INFO - Epoch 98: train_loss=nan
2025-02-01 16:54:47,283 - INFO - Epoch 98: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:47,287 - INFO - ####################Training epoch 99####################
2025-02-01 16:54:47,575 - INFO - Epoch 99: train_loss=nan
2025-02-01 16:54:48,046 - INFO - Epoch 99: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:48,050 - INFO - ####################Training epoch 100####################
2025-02-01 16:54:48,337 - INFO - Epoch 100: train_loss=nan
2025-02-01 16:54:48,808 - INFO - Epoch 100: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:48,812 - INFO - ####################Training epoch 101####################
2025-02-01 16:54:49,097 - INFO - Epoch 101: train_loss=nan
2025-02-01 16:54:49,570 - INFO - Epoch 101: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:49,573 - INFO - ####################Training epoch 102####################
2025-02-01 16:54:49,859 - INFO - Epoch 102: train_loss=nan
2025-02-01 16:54:50,331 - INFO - Epoch 102: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:50,335 - INFO - ####################Training epoch 103####################
2025-02-01 16:54:50,621 - INFO - Epoch 103: train_loss=nan
2025-02-01 16:54:51,093 - INFO - Epoch 103: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:51,097 - INFO - ####################Training epoch 104####################
2025-02-01 16:54:51,384 - INFO - Epoch 104: train_loss=nan
2025-02-01 16:54:51,855 - INFO - Epoch 104: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:51,859 - INFO - ####################Training epoch 105####################
2025-02-01 16:54:52,146 - INFO - Epoch 105: train_loss=nan
2025-02-01 16:54:52,619 - INFO - Epoch 105: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:52,623 - INFO - ####################Training epoch 106####################
2025-02-01 16:54:52,909 - INFO - Epoch 106: train_loss=nan
2025-02-01 16:54:53,381 - INFO - Epoch 106: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:53,385 - INFO - ####################Training epoch 107####################
2025-02-01 16:54:53,672 - INFO - Epoch 107: train_loss=nan
2025-02-01 16:54:54,143 - INFO - Epoch 107: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:54,147 - INFO - ####################Training epoch 108####################
2025-02-01 16:54:54,434 - INFO - Epoch 108: train_loss=nan
2025-02-01 16:54:54,907 - INFO - Epoch 108: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:54,910 - INFO - ####################Training epoch 109####################
2025-02-01 16:54:55,197 - INFO - Epoch 109: train_loss=nan
2025-02-01 16:54:55,668 - INFO - Epoch 109: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:55,672 - INFO - ####################Training epoch 110####################
2025-02-01 16:54:55,959 - INFO - Epoch 110: train_loss=nan
2025-02-01 16:54:56,430 - INFO - Epoch 110: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:56,434 - INFO - ####################Training epoch 111####################
2025-02-01 16:54:56,721 - INFO - Epoch 111: train_loss=nan
2025-02-01 16:54:57,193 - INFO - Epoch 111: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:57,197 - INFO - ####################Training epoch 112####################
2025-02-01 16:54:57,484 - INFO - Epoch 112: train_loss=nan
2025-02-01 16:54:57,954 - INFO - Epoch 112: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:57,958 - INFO - ####################Training epoch 113####################
2025-02-01 16:54:58,244 - INFO - Epoch 113: train_loss=nan
2025-02-01 16:54:58,719 - INFO - Epoch 113: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:58,723 - INFO - ####################Training epoch 114####################
2025-02-01 16:54:59,011 - INFO - Epoch 114: train_loss=nan
2025-02-01 16:54:59,479 - INFO - Epoch 114: val_loss=nan, val_acc=66.67%
2025-02-01 16:54:59,483 - INFO - ####################Training epoch 115####################
2025-02-01 16:54:59,771 - INFO - Epoch 115: train_loss=nan
2025-02-01 16:55:00,240 - INFO - Epoch 115: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:00,244 - INFO - ####################Training epoch 116####################
2025-02-01 16:55:00,532 - INFO - Epoch 116: train_loss=nan
2025-02-01 16:55:01,004 - INFO - Epoch 116: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:01,007 - INFO - ####################Training epoch 117####################
2025-02-01 16:55:01,291 - INFO - Epoch 117: train_loss=nan
2025-02-01 16:55:01,762 - INFO - Epoch 117: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:01,765 - INFO - ####################Training epoch 118####################
2025-02-01 16:55:02,051 - INFO - Epoch 118: train_loss=nan
2025-02-01 16:55:02,522 - INFO - Epoch 118: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:02,525 - INFO - ####################Training epoch 119####################
2025-02-01 16:55:02,812 - INFO - Epoch 119: train_loss=nan
2025-02-01 16:55:03,281 - INFO - Epoch 119: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:03,285 - INFO - ####################Training epoch 120####################
2025-02-01 16:55:03,569 - INFO - Epoch 120: train_loss=nan
2025-02-01 16:55:04,041 - INFO - Epoch 120: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:04,045 - INFO - ####################Training epoch 121####################
2025-02-01 16:55:04,331 - INFO - Epoch 121: train_loss=nan
2025-02-01 16:55:04,801 - INFO - Epoch 121: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:04,805 - INFO - ####################Training epoch 122####################
2025-02-01 16:55:05,090 - INFO - Epoch 122: train_loss=nan
2025-02-01 16:55:05,560 - INFO - Epoch 122: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:05,564 - INFO - ####################Training epoch 123####################
2025-02-01 16:55:05,849 - INFO - Epoch 123: train_loss=nan
2025-02-01 16:55:06,316 - INFO - Epoch 123: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:06,320 - INFO - ####################Training epoch 124####################
2025-02-01 16:55:06,608 - INFO - Epoch 124: train_loss=nan
2025-02-01 16:55:07,077 - INFO - Epoch 124: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:07,081 - INFO - ####################Training epoch 125####################
2025-02-01 16:55:07,368 - INFO - Epoch 125: train_loss=nan
2025-02-01 16:55:07,840 - INFO - Epoch 125: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:07,844 - INFO - ####################Training epoch 126####################
2025-02-01 16:55:08,130 - INFO - Epoch 126: train_loss=nan
2025-02-01 16:55:08,600 - INFO - Epoch 126: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:08,604 - INFO - ####################Training epoch 127####################
2025-02-01 16:55:08,891 - INFO - Epoch 127: train_loss=nan
2025-02-01 16:55:09,360 - INFO - Epoch 127: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:09,364 - INFO - ####################Training epoch 128####################
2025-02-01 16:55:09,651 - INFO - Epoch 128: train_loss=nan
2025-02-01 16:55:10,121 - INFO - Epoch 128: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:10,125 - INFO - ####################Training epoch 129####################
2025-02-01 16:55:10,411 - INFO - Epoch 129: train_loss=nan
2025-02-01 16:55:10,882 - INFO - Epoch 129: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:10,886 - INFO - ####################Training epoch 130####################
2025-02-01 16:55:11,170 - INFO - Epoch 130: train_loss=nan
2025-02-01 16:55:11,641 - INFO - Epoch 130: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:11,645 - INFO - ####################Training epoch 131####################
2025-02-01 16:55:11,931 - INFO - Epoch 131: train_loss=nan
2025-02-01 16:55:12,402 - INFO - Epoch 131: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:12,405 - INFO - ####################Training epoch 132####################
2025-02-01 16:55:12,693 - INFO - Epoch 132: train_loss=nan
2025-02-01 16:55:13,165 - INFO - Epoch 132: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:13,169 - INFO - ####################Training epoch 133####################
2025-02-01 16:55:13,455 - INFO - Epoch 133: train_loss=nan
2025-02-01 16:55:13,924 - INFO - Epoch 133: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:13,928 - INFO - ####################Training epoch 134####################
2025-02-01 16:55:14,213 - INFO - Epoch 134: train_loss=nan
2025-02-01 16:55:14,684 - INFO - Epoch 134: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:14,688 - INFO - ####################Training epoch 135####################
2025-02-01 16:55:14,974 - INFO - Epoch 135: train_loss=nan
2025-02-01 16:55:15,445 - INFO - Epoch 135: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:15,449 - INFO - ####################Training epoch 136####################
2025-02-01 16:55:15,735 - INFO - Epoch 136: train_loss=nan
2025-02-01 16:55:16,205 - INFO - Epoch 136: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:16,209 - INFO - ####################Training epoch 137####################
2025-02-01 16:55:16,496 - INFO - Epoch 137: train_loss=nan
2025-02-01 16:55:16,967 - INFO - Epoch 137: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:16,971 - INFO - ####################Training epoch 138####################
2025-02-01 16:55:17,258 - INFO - Epoch 138: train_loss=nan
2025-02-01 16:55:17,729 - INFO - Epoch 138: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:17,733 - INFO - ####################Training epoch 139####################
2025-02-01 16:55:18,019 - INFO - Epoch 139: train_loss=nan
2025-02-01 16:55:18,491 - INFO - Epoch 139: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:18,494 - INFO - ####################Training epoch 140####################
2025-02-01 16:55:18,780 - INFO - Epoch 140: train_loss=nan
2025-02-01 16:55:19,253 - INFO - Epoch 140: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:19,257 - INFO - ####################Training epoch 141####################
2025-02-01 16:55:19,545 - INFO - Epoch 141: train_loss=nan
2025-02-01 16:55:20,013 - INFO - Epoch 141: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:20,017 - INFO - ####################Training epoch 142####################
2025-02-01 16:55:20,303 - INFO - Epoch 142: train_loss=nan
2025-02-01 16:55:20,771 - INFO - Epoch 142: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:20,774 - INFO - ####################Training epoch 143####################
2025-02-01 16:55:21,060 - INFO - Epoch 143: train_loss=nan
2025-02-01 16:55:21,530 - INFO - Epoch 143: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:21,534 - INFO - ####################Training epoch 144####################
2025-02-01 16:55:21,820 - INFO - Epoch 144: train_loss=nan
2025-02-01 16:55:22,290 - INFO - Epoch 144: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:22,294 - INFO - ####################Training epoch 145####################
2025-02-01 16:55:22,582 - INFO - Epoch 145: train_loss=nan
2025-02-01 16:55:23,054 - INFO - Epoch 145: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:23,058 - INFO - ####################Training epoch 146####################
2025-02-01 16:55:23,346 - INFO - Epoch 146: train_loss=nan
2025-02-01 16:55:23,817 - INFO - Epoch 146: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:23,821 - INFO - ####################Training epoch 147####################
2025-02-01 16:55:24,109 - INFO - Epoch 147: train_loss=nan
2025-02-01 16:55:24,582 - INFO - Epoch 147: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:24,585 - INFO - ####################Training epoch 148####################
2025-02-01 16:55:24,873 - INFO - Epoch 148: train_loss=nan
2025-02-01 16:55:25,344 - INFO - Epoch 148: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:25,347 - INFO - ####################Training epoch 149####################
2025-02-01 16:55:25,633 - INFO - Epoch 149: train_loss=nan
2025-02-01 16:55:26,107 - INFO - Epoch 149: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:26,111 - INFO - ####################Training epoch 150####################
2025-02-01 16:55:26,398 - INFO - Epoch 150: train_loss=nan
2025-02-01 16:55:26,869 - INFO - Epoch 150: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:26,873 - INFO - ####################Training epoch 151####################
2025-02-01 16:55:27,159 - INFO - Epoch 151: train_loss=nan
2025-02-01 16:55:27,631 - INFO - Epoch 151: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:27,634 - INFO - ####################Training epoch 152####################
2025-02-01 16:55:27,924 - INFO - Epoch 152: train_loss=nan
2025-02-01 16:55:28,395 - INFO - Epoch 152: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:28,399 - INFO - ####################Training epoch 153####################
2025-02-01 16:55:28,685 - INFO - Epoch 153: train_loss=nan
2025-02-01 16:55:29,158 - INFO - Epoch 153: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:29,162 - INFO - ####################Training epoch 154####################
2025-02-01 16:55:29,448 - INFO - Epoch 154: train_loss=nan
2025-02-01 16:55:29,921 - INFO - Epoch 154: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:29,925 - INFO - ####################Training epoch 155####################
2025-02-01 16:55:30,212 - INFO - Epoch 155: train_loss=nan
2025-02-01 16:55:30,687 - INFO - Epoch 155: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:30,691 - INFO - ####################Training epoch 156####################
2025-02-01 16:55:30,978 - INFO - Epoch 156: train_loss=nan
2025-02-01 16:55:31,445 - INFO - Epoch 156: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:31,449 - INFO - ####################Training epoch 157####################
2025-02-01 16:55:31,734 - INFO - Epoch 157: train_loss=nan
2025-02-01 16:55:32,204 - INFO - Epoch 157: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:32,207 - INFO - ####################Training epoch 158####################
2025-02-01 16:55:32,493 - INFO - Epoch 158: train_loss=nan
2025-02-01 16:55:32,964 - INFO - Epoch 158: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:32,968 - INFO - ####################Training epoch 159####################
2025-02-01 16:55:33,254 - INFO - Epoch 159: train_loss=nan
2025-02-01 16:55:33,727 - INFO - Epoch 159: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:33,731 - INFO - ####################Training epoch 160####################
2025-02-01 16:55:34,019 - INFO - Epoch 160: train_loss=nan
2025-02-01 16:55:34,489 - INFO - Epoch 160: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:34,493 - INFO - ####################Training epoch 161####################
2025-02-01 16:55:34,780 - INFO - Epoch 161: train_loss=nan
2025-02-01 16:55:35,244 - INFO - Epoch 161: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:35,248 - INFO - ####################Training epoch 162####################
2025-02-01 16:55:35,536 - INFO - Epoch 162: train_loss=nan
2025-02-01 16:55:36,007 - INFO - Epoch 162: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:36,011 - INFO - ####################Training epoch 163####################
2025-02-01 16:55:36,295 - INFO - Epoch 163: train_loss=nan
2025-02-01 16:55:36,768 - INFO - Epoch 163: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:36,772 - INFO - ####################Training epoch 164####################
2025-02-01 16:55:37,059 - INFO - Epoch 164: train_loss=nan
2025-02-01 16:55:37,531 - INFO - Epoch 164: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:37,535 - INFO - ####################Training epoch 165####################
2025-02-01 16:55:37,824 - INFO - Epoch 165: train_loss=nan
2025-02-01 16:55:38,296 - INFO - Epoch 165: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:38,300 - INFO - ####################Training epoch 166####################
2025-02-01 16:55:38,585 - INFO - Epoch 166: train_loss=nan
2025-02-01 16:55:39,058 - INFO - Epoch 166: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:39,061 - INFO - ####################Training epoch 167####################
2025-02-01 16:55:39,348 - INFO - Epoch 167: train_loss=nan
2025-02-01 16:55:39,821 - INFO - Epoch 167: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:39,825 - INFO - ####################Training epoch 168####################
2025-02-01 16:55:40,113 - INFO - Epoch 168: train_loss=nan
2025-02-01 16:55:40,585 - INFO - Epoch 168: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:40,588 - INFO - ####################Training epoch 169####################
2025-02-01 16:55:40,880 - INFO - Epoch 169: train_loss=nan
2025-02-01 16:55:41,351 - INFO - Epoch 169: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:41,355 - INFO - ####################Training epoch 170####################
2025-02-01 16:55:41,641 - INFO - Epoch 170: train_loss=nan
2025-02-01 16:55:42,112 - INFO - Epoch 170: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:42,116 - INFO - ####################Training epoch 171####################
2025-02-01 16:55:42,399 - INFO - Epoch 171: train_loss=nan
2025-02-01 16:55:42,870 - INFO - Epoch 171: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:42,873 - INFO - ####################Training epoch 172####################
2025-02-01 16:55:43,158 - INFO - Epoch 172: train_loss=nan
2025-02-01 16:55:43,630 - INFO - Epoch 172: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:43,634 - INFO - ####################Training epoch 173####################
2025-02-01 16:55:43,919 - INFO - Epoch 173: train_loss=nan
2025-02-01 16:55:44,389 - INFO - Epoch 173: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:44,393 - INFO - ####################Training epoch 174####################
2025-02-01 16:55:44,678 - INFO - Epoch 174: train_loss=nan
2025-02-01 16:55:45,148 - INFO - Epoch 174: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:45,152 - INFO - ####################Training epoch 175####################
2025-02-01 16:55:45,437 - INFO - Epoch 175: train_loss=nan
2025-02-01 16:55:45,906 - INFO - Epoch 175: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:45,910 - INFO - ####################Training epoch 176####################
2025-02-01 16:55:46,194 - INFO - Epoch 176: train_loss=nan
2025-02-01 16:55:46,667 - INFO - Epoch 176: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:46,670 - INFO - ####################Training epoch 177####################
2025-02-01 16:55:46,957 - INFO - Epoch 177: train_loss=nan
2025-02-01 16:55:47,424 - INFO - Epoch 177: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:47,427 - INFO - ####################Training epoch 178####################
2025-02-01 16:55:47,714 - INFO - Epoch 178: train_loss=nan
2025-02-01 16:55:48,184 - INFO - Epoch 178: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:48,187 - INFO - ####################Training epoch 179####################
2025-02-01 16:55:48,472 - INFO - Epoch 179: train_loss=nan
2025-02-01 16:55:48,941 - INFO - Epoch 179: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:48,944 - INFO - ####################Training epoch 180####################
2025-02-01 16:55:49,226 - INFO - Epoch 180: train_loss=nan
2025-02-01 16:55:49,696 - INFO - Epoch 180: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:49,700 - INFO - ####################Training epoch 181####################
2025-02-01 16:55:49,985 - INFO - Epoch 181: train_loss=nan
2025-02-01 16:55:50,456 - INFO - Epoch 181: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:50,460 - INFO - ####################Training epoch 182####################
2025-02-01 16:55:50,746 - INFO - Epoch 182: train_loss=nan
2025-02-01 16:55:51,215 - INFO - Epoch 182: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:51,219 - INFO - ####################Training epoch 183####################
2025-02-01 16:55:51,504 - INFO - Epoch 183: train_loss=nan
2025-02-01 16:55:51,975 - INFO - Epoch 183: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:51,979 - INFO - ####################Training epoch 184####################
2025-02-01 16:55:52,263 - INFO - Epoch 184: train_loss=nan
2025-02-01 16:55:52,732 - INFO - Epoch 184: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:52,736 - INFO - ####################Training epoch 185####################
2025-02-01 16:55:53,023 - INFO - Epoch 185: train_loss=nan
2025-02-01 16:55:53,491 - INFO - Epoch 185: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:53,494 - INFO - ####################Training epoch 186####################
2025-02-01 16:55:53,781 - INFO - Epoch 186: train_loss=nan
2025-02-01 16:55:54,251 - INFO - Epoch 186: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:54,255 - INFO - ####################Training epoch 187####################
2025-02-01 16:55:54,541 - INFO - Epoch 187: train_loss=nan
2025-02-01 16:55:55,006 - INFO - Epoch 187: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:55,010 - INFO - ####################Training epoch 188####################
2025-02-01 16:55:55,287 - INFO - Epoch 188: train_loss=nan
2025-02-01 16:55:55,754 - INFO - Epoch 188: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:55,758 - INFO - ####################Training epoch 189####################
2025-02-01 16:55:56,041 - INFO - Epoch 189: train_loss=nan
2025-02-01 16:55:56,507 - INFO - Epoch 189: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:56,511 - INFO - ####################Training epoch 190####################
2025-02-01 16:55:56,793 - INFO - Epoch 190: train_loss=nan
2025-02-01 16:55:57,259 - INFO - Epoch 190: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:57,263 - INFO - ####################Training epoch 191####################
2025-02-01 16:55:57,548 - INFO - Epoch 191: train_loss=nan
2025-02-01 16:55:58,012 - INFO - Epoch 191: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:58,016 - INFO - ####################Training epoch 192####################
2025-02-01 16:55:58,302 - INFO - Epoch 192: train_loss=nan
2025-02-01 16:55:58,773 - INFO - Epoch 192: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:58,777 - INFO - ####################Training epoch 193####################
2025-02-01 16:55:59,062 - INFO - Epoch 193: train_loss=nan
2025-02-01 16:55:59,525 - INFO - Epoch 193: val_loss=nan, val_acc=66.67%
2025-02-01 16:55:59,528 - INFO - ####################Training epoch 194####################
2025-02-01 16:55:59,817 - INFO - Epoch 194: train_loss=nan
2025-02-01 16:56:00,287 - INFO - Epoch 194: val_loss=nan, val_acc=66.67%
2025-02-01 16:56:00,291 - INFO - ####################Training epoch 195####################
2025-02-01 16:56:00,577 - INFO - Epoch 195: train_loss=nan
2025-02-01 16:56:01,042 - INFO - Epoch 195: val_loss=nan, val_acc=66.67%
2025-02-01 16:56:01,046 - INFO - ####################Training epoch 196####################
2025-02-01 16:56:01,331 - INFO - Epoch 196: train_loss=nan
2025-02-01 16:56:01,792 - INFO - Epoch 196: val_loss=nan, val_acc=66.67%
2025-02-01 16:56:01,796 - INFO - ####################Training epoch 197####################
2025-02-01 16:56:02,080 - INFO - Epoch 197: train_loss=nan
2025-02-01 16:56:02,546 - INFO - Epoch 197: val_loss=nan, val_acc=66.67%
2025-02-01 16:56:02,550 - INFO - ####################Training epoch 198####################
2025-02-01 16:56:02,836 - INFO - Epoch 198: train_loss=nan
2025-02-01 16:56:03,302 - INFO - Epoch 198: val_loss=nan, val_acc=66.67%
2025-02-01 16:56:03,306 - INFO - ####################Training epoch 199####################
2025-02-01 16:56:03,592 - INFO - Epoch 199: train_loss=nan
2025-02-01 16:56:04,058 - INFO - Epoch 199: val_loss=nan, val_acc=66.67%
2025-02-01 16:56:04,190 - INFO - Model saved.
