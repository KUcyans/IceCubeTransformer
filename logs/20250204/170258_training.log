2025-02-04 17:03:08,683 - INFO - Starting training with the following parameters:
2025-02-04 17:03:08,683 - INFO - | Parameter       | Value               |
|-----------------|---------------------|
| attention       | Scaled Dot-Product|
| d_model         | 128            |
| n_heads         | 8              |
| d_f             | 128            |
| num_layers      | 3              |
| d_input         | 32             |
| num_classes     | 3              |
| dropout         | 0.1            |
| learning_rate   | 0.001          |
| epochs          | 200            |
| batch_size      | 16             |

2025-02-04 17:03:09,255 - INFO - Epoch 0: val_loss=1.3364, val_acc=0.00%
2025-02-04 17:03:09,396 - INFO - #################### Training epoch 0 ####################
2025-02-04 17:03:09,396 - INFO - Current Learning Rate: 1.000000e-03
2025-02-04 17:03:09,553 - INFO - Epoch 0: train_loss=1.2112
2025-02-04 17:03:10,711 - INFO - Epoch 0: train_loss=1.4592
2025-02-04 17:03:10,998 - INFO - Epoch 0: val_loss=1.3823, val_acc=33.33%
2025-02-04 17:03:11,003 - INFO - Epoch 0: EPOCH_AVG_TRAIN_LOSS=1.3352
2025-02-04 17:03:11,048 - INFO - #################### Training epoch 1 ####################
2025-02-04 17:03:11,048 - INFO - Current Learning Rate: 1.000000e-03
2025-02-04 17:03:11,358 - INFO - Epoch 1: train_loss=1.2329
2025-02-04 17:03:11,600 - INFO - Epoch 1: train_loss=1.4758
2025-02-04 17:03:11,909 - INFO - Epoch 1: val_loss=2.1774, val_acc=0.00%
2025-02-04 17:03:11,912 - INFO - Epoch 1: EPOCH_AVG_TRAIN_LOSS=1.3543
2025-02-04 17:03:11,949 - INFO - #################### Training epoch 2 ####################
2025-02-04 17:03:11,949 - INFO - Current Learning Rate: 1.000000e-03
2025-02-04 17:03:12,265 - INFO - Epoch 2: train_loss=1.1748
2025-02-04 17:03:12,507 - INFO - Epoch 2: train_loss=1.1809
2025-02-04 17:03:12,816 - INFO - Epoch 2: val_loss=1.7881, val_acc=33.33%
2025-02-04 17:03:12,819 - INFO - Epoch 2: EPOCH_AVG_TRAIN_LOSS=1.1778
2025-02-04 17:03:12,860 - INFO - #################### Training epoch 3 ####################
2025-02-04 17:03:12,861 - INFO - Current Learning Rate: 1.000000e-03
2025-02-04 17:03:13,179 - INFO - Epoch 3: train_loss=0.9443
2025-02-04 17:03:13,422 - INFO - Epoch 3: train_loss=1.0246
2025-02-04 17:03:13,737 - INFO - Epoch 3: val_loss=1.6854, val_acc=33.33%
2025-02-04 17:03:13,741 - INFO - Epoch 3: EPOCH_AVG_TRAIN_LOSS=0.9844
2025-02-04 17:03:13,804 - INFO - #################### Training epoch 4 ####################
2025-02-04 17:03:13,804 - INFO - Current Learning Rate: 1.000000e-03
2025-02-04 17:03:14,116 - INFO - Epoch 4: train_loss=0.8916
2025-02-04 17:03:14,359 - INFO - Epoch 4: train_loss=1.1275
2025-02-04 17:03:14,670 - INFO - Epoch 4: val_loss=2.0065, val_acc=33.33%
2025-02-04 17:03:14,673 - INFO - Epoch 4: EPOCH_AVG_TRAIN_LOSS=1.0095
2025-02-04 17:03:14,676 - INFO - #################### Training epoch 5 ####################
2025-02-04 17:03:14,676 - INFO - Current Learning Rate: 1.000000e-03
2025-02-04 17:03:14,992 - INFO - Epoch 5: train_loss=0.9523
2025-02-04 17:03:15,235 - INFO - Epoch 5: train_loss=0.6846
2025-02-04 17:03:15,546 - INFO - Epoch 5: val_loss=2.3939, val_acc=33.33%
2025-02-04 17:03:15,550 - INFO - Epoch 5: EPOCH_AVG_TRAIN_LOSS=0.8184
2025-02-04 17:03:15,552 - INFO - #################### Training epoch 6 ####################
2025-02-04 17:03:15,552 - INFO - Current Learning Rate: 1.000000e-03
2025-02-04 17:03:15,864 - INFO - Epoch 6: train_loss=0.8269
2025-02-04 17:03:16,106 - INFO - Epoch 6: train_loss=0.9774
2025-02-04 17:03:16,410 - INFO - Epoch 6: val_loss=2.9615, val_acc=0.00%
2025-02-04 17:03:16,414 - INFO - Epoch 6: EPOCH_AVG_TRAIN_LOSS=0.9021
2025-02-04 17:03:16,417 - INFO - #################### Training epoch 7 ####################
2025-02-04 17:03:16,417 - INFO - Current Learning Rate: 1.000000e-03
2025-02-04 17:03:16,729 - INFO - Epoch 7: train_loss=0.7984
2025-02-04 17:03:16,972 - INFO - Epoch 7: train_loss=0.7895
2025-02-04 17:03:17,273 - INFO - Epoch 7: val_loss=3.0822, val_acc=0.00%
2025-02-04 17:03:17,277 - INFO - Epoch 7: EPOCH_AVG_TRAIN_LOSS=0.7940
2025-02-04 17:03:17,279 - INFO - #################### Training epoch 8 ####################
2025-02-04 17:03:17,279 - INFO - Current Learning Rate: 1.000000e-03
2025-02-04 17:03:17,595 - INFO - Epoch 8: train_loss=0.8456
2025-02-04 17:03:17,838 - INFO - Epoch 8: train_loss=0.7892
2025-02-04 17:03:18,148 - INFO - Epoch 8: val_loss=3.1931, val_acc=33.33%
2025-02-04 17:03:18,152 - INFO - Epoch 8: EPOCH_AVG_TRAIN_LOSS=0.8174
2025-02-04 17:03:18,154 - INFO - #################### Training epoch 9 ####################
2025-02-04 17:03:18,154 - INFO - Current Learning Rate: 1.000000e-03
2025-02-04 17:03:18,473 - INFO - Epoch 9: train_loss=0.8108
2025-02-04 17:03:18,716 - INFO - Epoch 9: train_loss=0.4966
2025-02-04 17:03:19,028 - INFO - Epoch 9: val_loss=3.4812, val_acc=33.33%
2025-02-04 17:03:19,031 - INFO - Epoch 9: EPOCH_AVG_TRAIN_LOSS=0.6537
2025-02-04 17:03:19,034 - INFO - #################### Training epoch 10 ####################
2025-02-04 17:03:19,034 - INFO - Current Learning Rate: 1.000000e-03
2025-02-04 17:03:19,350 - INFO - Epoch 10: train_loss=0.8909
2025-02-04 17:03:19,594 - INFO - Epoch 10: train_loss=0.8237
2025-02-04 17:03:19,907 - INFO - Epoch 10: val_loss=3.3564, val_acc=33.33%
2025-02-04 17:03:19,911 - INFO - Epoch 10: EPOCH_AVG_TRAIN_LOSS=0.8573
2025-02-04 17:03:19,914 - INFO - #################### Training epoch 11 ####################
2025-02-04 17:03:19,914 - INFO - Current Learning Rate: 1.000000e-03
2025-02-04 17:03:20,231 - INFO - Epoch 11: train_loss=0.7738
2025-02-04 17:03:20,474 - INFO - Epoch 11: train_loss=0.5422
2025-02-04 17:03:20,787 - INFO - Epoch 11: val_loss=3.0566, val_acc=0.00%
2025-02-04 17:03:20,791 - INFO - Epoch 11: EPOCH_AVG_TRAIN_LOSS=0.6580
2025-02-04 17:03:20,794 - INFO - #################### Training epoch 12 ####################
2025-02-04 17:03:20,794 - INFO - Current Learning Rate: 1.000000e-03
2025-02-04 17:03:21,110 - INFO - Epoch 12: train_loss=0.7977
2025-02-04 17:03:21,354 - INFO - Epoch 12: train_loss=0.6645
2025-02-04 17:03:21,666 - INFO - Epoch 12: val_loss=3.1608, val_acc=33.33%
2025-02-04 17:03:21,670 - INFO - Epoch 12: EPOCH_AVG_TRAIN_LOSS=0.7311
2025-02-04 17:03:21,672 - INFO - #################### Training epoch 13 ####################
2025-02-04 17:03:21,673 - INFO - Current Learning Rate: 1.000000e-03
2025-02-04 17:03:21,992 - INFO - Epoch 13: train_loss=0.7718
2025-02-04 17:03:22,236 - INFO - Epoch 13: train_loss=0.5976
2025-02-04 17:03:22,548 - INFO - Epoch 13: val_loss=3.5455, val_acc=33.33%
2025-02-04 17:03:22,551 - INFO - Epoch 13: EPOCH_AVG_TRAIN_LOSS=0.6847
2025-02-04 17:03:22,554 - INFO - #################### Training epoch 14 ####################
2025-02-04 17:03:22,554 - INFO - Current Learning Rate: 5.000000e-04
2025-02-04 17:03:22,872 - INFO - Epoch 14: train_loss=0.6162
2025-02-04 17:03:23,116 - INFO - Epoch 14: train_loss=0.8860
2025-02-04 17:03:23,429 - INFO - Epoch 14: val_loss=3.6034, val_acc=33.33%
2025-02-04 17:03:23,433 - INFO - Epoch 14: EPOCH_AVG_TRAIN_LOSS=0.7511
2025-02-04 17:03:23,435 - INFO - #################### Training epoch 15 ####################
2025-02-04 17:03:23,435 - INFO - Current Learning Rate: 5.000000e-04
2025-02-04 17:03:23,756 - INFO - Epoch 15: train_loss=0.6839
2025-02-04 17:03:24,000 - INFO - Epoch 15: train_loss=0.5606
2025-02-04 17:03:24,310 - INFO - Epoch 15: val_loss=3.4264, val_acc=0.00%
2025-02-04 17:03:24,314 - INFO - Epoch 15: EPOCH_AVG_TRAIN_LOSS=0.6222
2025-02-04 17:03:24,316 - INFO - #################### Training epoch 16 ####################
2025-02-04 17:03:24,316 - INFO - Current Learning Rate: 5.000000e-04
2025-02-04 17:03:24,635 - INFO - Epoch 16: train_loss=0.6730
2025-02-04 17:03:24,879 - INFO - Epoch 16: train_loss=0.6029
2025-02-04 17:03:25,191 - INFO - Epoch 16: val_loss=3.3353, val_acc=0.00%
2025-02-04 17:03:25,195 - INFO - Epoch 16: EPOCH_AVG_TRAIN_LOSS=0.6379
2025-02-04 17:03:25,197 - INFO - #################### Training epoch 17 ####################
2025-02-04 17:03:25,197 - INFO - Current Learning Rate: 5.000000e-04
2025-02-04 17:03:25,516 - INFO - Epoch 17: train_loss=0.7338
2025-02-04 17:03:25,759 - INFO - Epoch 17: train_loss=0.5661
2025-02-04 17:03:26,074 - INFO - Epoch 17: val_loss=3.5684, val_acc=0.00%
2025-02-04 17:03:26,077 - INFO - Epoch 17: EPOCH_AVG_TRAIN_LOSS=0.6499
2025-02-04 17:03:26,080 - INFO - #################### Training epoch 18 ####################
2025-02-04 17:03:26,080 - INFO - Current Learning Rate: 2.500000e-04
2025-02-04 17:03:26,397 - INFO - Epoch 18: train_loss=0.7572
2025-02-04 17:03:26,641 - INFO - Epoch 18: train_loss=0.3206
2025-02-04 17:03:26,951 - INFO - Epoch 18: val_loss=3.6900, val_acc=33.33%
2025-02-04 17:03:26,955 - INFO - Epoch 18: EPOCH_AVG_TRAIN_LOSS=0.5389
2025-02-04 17:03:26,957 - INFO - #################### Training epoch 19 ####################
2025-02-04 17:03:26,957 - INFO - Current Learning Rate: 2.500000e-04
2025-02-04 17:03:27,274 - INFO - Epoch 19: train_loss=0.4925
2025-02-04 17:03:27,518 - INFO - Epoch 19: train_loss=0.8355
2025-02-04 17:03:27,831 - INFO - Epoch 19: val_loss=3.7658, val_acc=33.33%
2025-02-04 17:03:27,834 - INFO - Epoch 19: EPOCH_AVG_TRAIN_LOSS=0.6640
2025-02-04 17:03:27,837 - INFO - #################### Training epoch 20 ####################
2025-02-04 17:03:27,837 - INFO - Current Learning Rate: 2.500000e-04
2025-02-04 17:03:28,156 - INFO - Epoch 20: train_loss=0.4786
2025-02-04 17:03:28,399 - INFO - Epoch 20: train_loss=0.9121
2025-02-04 17:03:28,715 - INFO - Epoch 20: val_loss=3.7565, val_acc=33.33%
2025-02-04 17:03:28,719 - INFO - Epoch 20: EPOCH_AVG_TRAIN_LOSS=0.6953
2025-02-04 17:03:28,721 - INFO - #################### Training epoch 21 ####################
2025-02-04 17:03:28,721 - INFO - Current Learning Rate: 2.500000e-04
2025-02-04 17:03:29,039 - INFO - Epoch 21: train_loss=0.6075
2025-02-04 17:03:29,282 - INFO - Epoch 21: train_loss=0.6790
2025-02-04 17:03:29,593 - INFO - Epoch 21: val_loss=3.7442, val_acc=33.33%
2025-02-04 17:03:29,597 - INFO - Epoch 21: EPOCH_AVG_TRAIN_LOSS=0.6432
2025-02-04 17:03:29,599 - INFO - #################### Training epoch 22 ####################
2025-02-04 17:03:29,599 - INFO - Current Learning Rate: 2.500000e-04
2025-02-04 17:03:29,918 - INFO - Epoch 22: train_loss=0.4899
2025-02-04 17:03:30,161 - INFO - Epoch 22: train_loss=0.8941
2025-02-04 17:03:30,470 - INFO - Epoch 22: val_loss=3.7219, val_acc=33.33%
2025-02-04 17:03:30,474 - INFO - Epoch 22: EPOCH_AVG_TRAIN_LOSS=0.6920
2025-02-04 17:03:30,476 - INFO - #################### Training epoch 23 ####################
2025-02-04 17:03:30,476 - INFO - Current Learning Rate: 1.250000e-04
2025-02-04 17:03:30,806 - INFO - Epoch 23: train_loss=0.6906
2025-02-04 17:03:31,050 - INFO - Epoch 23: train_loss=0.4088
2025-02-04 17:03:31,362 - INFO - Epoch 23: val_loss=3.7013, val_acc=33.33%
2025-02-04 17:03:31,365 - INFO - Epoch 23: EPOCH_AVG_TRAIN_LOSS=0.5497
2025-02-04 17:03:31,368 - INFO - #################### Training epoch 24 ####################
2025-02-04 17:03:31,368 - INFO - Current Learning Rate: 1.250000e-04
2025-02-04 17:03:31,686 - INFO - Epoch 24: train_loss=0.4324
2025-02-04 17:03:31,930 - INFO - Epoch 24: train_loss=0.8799
2025-02-04 17:03:32,236 - INFO - Epoch 24: val_loss=3.6996, val_acc=33.33%
2025-02-04 17:03:32,240 - INFO - Epoch 24: EPOCH_AVG_TRAIN_LOSS=0.6561
2025-02-04 17:03:32,242 - INFO - #################### Training epoch 25 ####################
2025-02-04 17:03:32,242 - INFO - Current Learning Rate: 1.250000e-04
2025-02-04 17:03:32,553 - INFO - Epoch 25: train_loss=0.6036
2025-02-04 17:03:32,797 - INFO - Epoch 25: train_loss=0.5260
2025-02-04 17:03:33,110 - INFO - Epoch 25: val_loss=3.6749, val_acc=0.00%
2025-02-04 17:03:33,113 - INFO - Epoch 25: EPOCH_AVG_TRAIN_LOSS=0.5648
2025-02-04 17:03:33,118 - INFO - #################### Training epoch 26 ####################
2025-02-04 17:03:33,118 - INFO - Current Learning Rate: 1.250000e-04
2025-02-04 17:03:33,434 - INFO - Epoch 26: train_loss=0.6470
2025-02-04 17:03:33,677 - INFO - Epoch 26: train_loss=0.4483
2025-02-04 17:03:33,991 - INFO - Epoch 26: val_loss=3.6819, val_acc=0.00%
2025-02-04 17:03:33,995 - INFO - Epoch 26: EPOCH_AVG_TRAIN_LOSS=0.5476
2025-02-04 17:03:33,997 - INFO - #################### Training epoch 27 ####################
2025-02-04 17:03:33,997 - INFO - Current Learning Rate: 6.250000e-05
2025-02-04 17:03:34,314 - INFO - Epoch 27: train_loss=0.6117
2025-02-04 17:03:34,557 - INFO - Epoch 27: train_loss=0.4948
2025-02-04 17:03:34,862 - INFO - Epoch 27: val_loss=3.7004, val_acc=0.00%
2025-02-04 17:03:34,865 - INFO - Epoch 27: EPOCH_AVG_TRAIN_LOSS=0.5532
2025-02-04 17:03:34,868 - INFO - #################### Training epoch 28 ####################
2025-02-04 17:03:34,868 - INFO - Current Learning Rate: 6.250000e-05
2025-02-04 17:03:35,183 - INFO - Epoch 28: train_loss=0.5419
2025-02-04 17:03:35,427 - INFO - Epoch 28: train_loss=0.6381
2025-02-04 17:03:35,740 - INFO - Epoch 28: val_loss=3.7504, val_acc=0.00%
2025-02-04 17:03:35,744 - INFO - Epoch 28: EPOCH_AVG_TRAIN_LOSS=0.5900
2025-02-04 17:03:35,746 - INFO - #################### Training epoch 29 ####################
2025-02-04 17:03:35,746 - INFO - Current Learning Rate: 6.250000e-05
2025-02-04 17:03:36,063 - INFO - Epoch 29: train_loss=0.6281
2025-02-04 17:03:36,306 - INFO - Epoch 29: train_loss=0.4361
2025-02-04 17:03:36,616 - INFO - Epoch 29: val_loss=3.7816, val_acc=0.00%
2025-02-04 17:03:36,620 - INFO - Epoch 29: EPOCH_AVG_TRAIN_LOSS=0.5321
2025-02-04 17:03:36,622 - INFO - #################### Training epoch 30 ####################
2025-02-04 17:03:36,622 - INFO - Current Learning Rate: 6.250000e-05
2025-02-04 17:03:36,937 - INFO - Epoch 30: train_loss=0.3682
2025-02-04 17:03:37,181 - INFO - Epoch 30: train_loss=0.9616
2025-02-04 17:03:37,492 - INFO - Epoch 30: val_loss=3.8131, val_acc=0.00%
2025-02-04 17:03:37,495 - INFO - Epoch 30: EPOCH_AVG_TRAIN_LOSS=0.6649
2025-02-04 17:03:37,498 - INFO - #################### Training epoch 31 ####################
2025-02-04 17:03:37,498 - INFO - Current Learning Rate: 3.125000e-05
2025-02-04 17:03:37,815 - INFO - Epoch 31: train_loss=0.5011
2025-02-04 17:03:38,059 - INFO - Epoch 31: train_loss=0.6837
2025-02-04 17:03:38,373 - INFO - Epoch 31: val_loss=3.8415, val_acc=0.00%
2025-02-04 17:03:38,376 - INFO - Epoch 31: EPOCH_AVG_TRAIN_LOSS=0.5924
2025-02-04 17:03:38,379 - INFO - #################### Training epoch 32 ####################
2025-02-04 17:03:38,379 - INFO - Current Learning Rate: 3.125000e-05
2025-02-04 17:03:38,694 - INFO - Epoch 32: train_loss=0.5740
2025-02-04 17:03:38,938 - INFO - Epoch 32: train_loss=0.5322
2025-02-04 17:03:39,250 - INFO - Epoch 32: val_loss=3.8644, val_acc=0.00%
2025-02-04 17:03:39,253 - INFO - Epoch 32: EPOCH_AVG_TRAIN_LOSS=0.5531
2025-02-04 17:03:39,256 - INFO - #################### Training epoch 33 ####################
2025-02-04 17:03:39,256 - INFO - Current Learning Rate: 3.125000e-05
2025-02-04 17:03:39,570 - INFO - Epoch 33: train_loss=0.5991
2025-02-04 17:03:39,814 - INFO - Epoch 33: train_loss=0.4776
2025-02-04 17:03:40,126 - INFO - Epoch 33: val_loss=3.8742, val_acc=0.00%
2025-02-04 17:03:40,129 - INFO - Epoch 33: EPOCH_AVG_TRAIN_LOSS=0.5384
2025-02-04 17:03:40,132 - INFO - #################### Training epoch 34 ####################
2025-02-04 17:03:40,132 - INFO - Current Learning Rate: 3.125000e-05
2025-02-04 17:03:40,449 - INFO - Epoch 34: train_loss=0.4341
2025-02-04 17:03:40,692 - INFO - Epoch 34: train_loss=0.7928
2025-02-04 17:03:41,004 - INFO - Epoch 34: val_loss=3.8911, val_acc=0.00%
2025-02-04 17:03:41,008 - INFO - Epoch 34: EPOCH_AVG_TRAIN_LOSS=0.6135
2025-02-04 17:03:41,010 - INFO - #################### Training epoch 35 ####################
2025-02-04 17:03:41,010 - INFO - Current Learning Rate: 1.562500e-05
2025-02-04 17:03:41,329 - INFO - Epoch 35: train_loss=0.6549
2025-02-04 17:03:41,573 - INFO - Epoch 35: train_loss=0.3546
2025-02-04 17:03:41,884 - INFO - Epoch 35: val_loss=3.8768, val_acc=0.00%
2025-02-04 17:03:41,888 - INFO - Epoch 35: EPOCH_AVG_TRAIN_LOSS=0.5048
2025-02-04 17:03:41,890 - INFO - #################### Training epoch 36 ####################
2025-02-04 17:03:41,890 - INFO - Current Learning Rate: 1.562500e-05
2025-02-04 17:03:42,205 - INFO - Epoch 36: train_loss=0.4521
2025-02-04 17:03:42,449 - INFO - Epoch 36: train_loss=0.7641
2025-02-04 17:03:42,762 - INFO - Epoch 36: val_loss=3.8775, val_acc=0.00%
2025-02-04 17:03:42,765 - INFO - Epoch 36: EPOCH_AVG_TRAIN_LOSS=0.6081
2025-02-04 17:03:42,768 - INFO - #################### Training epoch 37 ####################
2025-02-04 17:03:42,768 - INFO - Current Learning Rate: 1.562500e-05
2025-02-04 17:03:43,087 - INFO - Epoch 37: train_loss=0.6076
2025-02-04 17:03:43,331 - INFO - Epoch 37: train_loss=0.4488
2025-02-04 17:03:43,641 - INFO - Epoch 37: val_loss=3.9186, val_acc=0.00%
2025-02-04 17:03:43,645 - INFO - Epoch 37: EPOCH_AVG_TRAIN_LOSS=0.5282
2025-02-04 17:03:43,647 - INFO - #################### Training epoch 38 ####################
2025-02-04 17:03:43,647 - INFO - Current Learning Rate: 1.562500e-05
2025-02-04 17:03:43,964 - INFO - Epoch 38: train_loss=0.4626
2025-02-04 17:03:44,207 - INFO - Epoch 38: train_loss=0.7323
2025-02-04 17:03:44,520 - INFO - Epoch 38: val_loss=3.9027, val_acc=0.00%
2025-02-04 17:03:44,523 - INFO - Epoch 38: EPOCH_AVG_TRAIN_LOSS=0.5974
2025-02-04 17:03:44,526 - INFO - #################### Training epoch 39 ####################
2025-02-04 17:03:44,526 - INFO - Current Learning Rate: 7.812500e-06
2025-02-04 17:03:44,843 - INFO - Epoch 39: train_loss=0.4499
2025-02-04 17:03:45,086 - INFO - Epoch 39: train_loss=0.7479
2025-02-04 17:03:45,398 - INFO - Epoch 39: val_loss=3.9011, val_acc=0.00%
2025-02-04 17:03:45,402 - INFO - Epoch 39: EPOCH_AVG_TRAIN_LOSS=0.5989
2025-02-04 17:03:45,404 - INFO - #################### Training epoch 40 ####################
2025-02-04 17:03:45,404 - INFO - Current Learning Rate: 7.812500e-06
2025-02-04 17:03:45,721 - INFO - Epoch 40: train_loss=0.6456
2025-02-04 17:03:45,964 - INFO - Epoch 40: train_loss=0.3676
2025-02-04 17:03:46,279 - INFO - Epoch 40: val_loss=3.8810, val_acc=33.33%
2025-02-04 17:03:46,282 - INFO - Epoch 40: EPOCH_AVG_TRAIN_LOSS=0.5066
2025-02-04 17:03:46,285 - INFO - #################### Training epoch 41 ####################
2025-02-04 17:03:46,285 - INFO - Current Learning Rate: 7.812500e-06
2025-02-04 17:03:46,603 - INFO - Epoch 41: train_loss=0.6651
2025-02-04 17:03:46,846 - INFO - Epoch 41: train_loss=0.3143
2025-02-04 17:03:47,159 - INFO - Epoch 41: val_loss=3.8706, val_acc=33.33%
2025-02-04 17:03:47,162 - INFO - Epoch 41: EPOCH_AVG_TRAIN_LOSS=0.4897
2025-02-04 17:03:47,165 - INFO - #################### Training epoch 42 ####################
2025-02-04 17:03:47,165 - INFO - Current Learning Rate: 7.812500e-06
2025-02-04 17:03:47,483 - INFO - Epoch 42: train_loss=0.6160
2025-02-04 17:03:47,727 - INFO - Epoch 42: train_loss=0.4223
2025-02-04 17:03:48,040 - INFO - Epoch 42: val_loss=3.9388, val_acc=0.00%
2025-02-04 17:03:48,043 - INFO - Epoch 42: EPOCH_AVG_TRAIN_LOSS=0.5191
2025-02-04 17:03:48,046 - INFO - #################### Training epoch 43 ####################
2025-02-04 17:03:48,046 - INFO - Current Learning Rate: 7.812500e-06
2025-02-04 17:03:48,364 - INFO - Epoch 43: train_loss=0.6277
2025-02-04 17:03:48,608 - INFO - Epoch 43: train_loss=0.3920
2025-02-04 17:03:48,920 - INFO - Epoch 43: val_loss=3.9050, val_acc=0.00%
2025-02-04 17:03:48,923 - INFO - Epoch 43: EPOCH_AVG_TRAIN_LOSS=0.5098
2025-02-04 17:03:48,926 - INFO - #################### Training epoch 44 ####################
2025-02-04 17:03:48,926 - INFO - Current Learning Rate: 7.812500e-06
2025-02-04 17:03:49,243 - INFO - Epoch 44: train_loss=0.4947
2025-02-04 17:03:49,487 - INFO - Epoch 44: train_loss=0.6617
2025-02-04 17:03:49,796 - INFO - Epoch 44: val_loss=3.9329, val_acc=0.00%
2025-02-04 17:03:49,800 - INFO - Epoch 44: EPOCH_AVG_TRAIN_LOSS=0.5782
2025-02-04 17:03:49,802 - INFO - #################### Training epoch 45 ####################
2025-02-04 17:03:49,802 - INFO - Current Learning Rate: 7.812500e-06
2025-02-04 17:03:50,120 - INFO - Epoch 45: train_loss=0.5618
2025-02-04 17:03:50,363 - INFO - Epoch 45: train_loss=0.5276
2025-02-04 17:03:50,681 - INFO - Epoch 45: val_loss=3.9233, val_acc=0.00%
2025-02-04 17:03:50,684 - INFO - Epoch 45: EPOCH_AVG_TRAIN_LOSS=0.5447
2025-02-04 17:03:50,687 - INFO - #################### Training epoch 46 ####################
2025-02-04 17:03:50,687 - INFO - Current Learning Rate: 3.906250e-06
2025-02-04 17:03:51,004 - INFO - Epoch 46: train_loss=0.6066
2025-02-04 17:03:51,247 - INFO - Epoch 46: train_loss=0.4252
2025-02-04 17:03:51,560 - INFO - Epoch 46: val_loss=3.8952, val_acc=0.00%
2025-02-04 17:03:51,564 - INFO - Epoch 46: EPOCH_AVG_TRAIN_LOSS=0.5159
2025-02-04 17:03:51,567 - INFO - #################### Training epoch 47 ####################
2025-02-04 17:03:51,567 - INFO - Current Learning Rate: 3.906250e-06
2025-02-04 17:03:51,884 - INFO - Epoch 47: train_loss=0.6053
2025-02-04 17:03:52,128 - INFO - Epoch 47: train_loss=0.4460
2025-02-04 17:03:52,437 - INFO - Epoch 47: val_loss=3.9100, val_acc=0.00%
2025-02-04 17:03:52,440 - INFO - Epoch 47: EPOCH_AVG_TRAIN_LOSS=0.5257
2025-02-04 17:03:52,443 - INFO - #################### Training epoch 48 ####################
2025-02-04 17:03:52,443 - INFO - Current Learning Rate: 3.906250e-06
2025-02-04 17:03:52,760 - INFO - Epoch 48: train_loss=0.6215
2025-02-04 17:03:53,004 - INFO - Epoch 48: train_loss=0.4031
2025-02-04 17:03:53,316 - INFO - Epoch 48: val_loss=3.9229, val_acc=0.00%
2025-02-04 17:03:53,319 - INFO - Epoch 48: EPOCH_AVG_TRAIN_LOSS=0.5123
2025-02-04 17:03:53,322 - INFO - #################### Training epoch 49 ####################
2025-02-04 17:03:53,322 - INFO - Current Learning Rate: 3.906250e-06
2025-02-04 17:03:53,637 - INFO - Epoch 49: train_loss=0.4459
2025-02-04 17:03:53,880 - INFO - Epoch 49: train_loss=0.7546
2025-02-04 17:03:54,195 - INFO - Epoch 49: val_loss=3.8986, val_acc=0.00%
2025-02-04 17:03:54,198 - INFO - Epoch 49: EPOCH_AVG_TRAIN_LOSS=0.6002
2025-02-04 17:03:54,201 - INFO - #################### Training epoch 50 ####################
2025-02-04 17:03:54,201 - INFO - Current Learning Rate: 1.953125e-06
2025-02-04 17:03:54,519 - INFO - Epoch 50: train_loss=0.4682
2025-02-04 17:03:54,763 - INFO - Epoch 50: train_loss=0.7123
2025-02-04 17:03:55,073 - INFO - Epoch 50: val_loss=3.9114, val_acc=0.00%
2025-02-04 17:03:55,077 - INFO - Epoch 50: EPOCH_AVG_TRAIN_LOSS=0.5902
2025-02-04 17:03:55,079 - INFO - #################### Training epoch 51 ####################
2025-02-04 17:03:55,079 - INFO - Current Learning Rate: 1.953125e-06
2025-02-04 17:03:55,396 - INFO - Epoch 51: train_loss=0.5308
2025-02-04 17:03:55,640 - INFO - Epoch 51: train_loss=0.5890
2025-02-04 17:03:55,952 - INFO - Epoch 51: val_loss=3.9244, val_acc=0.00%
2025-02-04 17:03:55,956 - INFO - Epoch 51: EPOCH_AVG_TRAIN_LOSS=0.5599
2025-02-04 17:03:55,958 - INFO - #################### Training epoch 52 ####################
2025-02-04 17:03:55,959 - INFO - Current Learning Rate: 1.953125e-06
2025-02-04 17:03:56,274 - INFO - Epoch 52: train_loss=0.5777
2025-02-04 17:03:56,517 - INFO - Epoch 52: train_loss=0.4967
2025-02-04 17:03:56,830 - INFO - Epoch 52: val_loss=3.8954, val_acc=0.00%
2025-02-04 17:03:56,834 - INFO - Epoch 52: EPOCH_AVG_TRAIN_LOSS=0.5372
2025-02-04 17:03:56,837 - INFO - #################### Training epoch 53 ####################
2025-02-04 17:03:56,837 - INFO - Current Learning Rate: 1.953125e-06
2025-02-04 17:03:57,153 - INFO - Epoch 53: train_loss=0.6730
2025-02-04 17:03:57,397 - INFO - Epoch 53: train_loss=0.3038
2025-02-04 17:03:57,706 - INFO - Epoch 53: val_loss=3.9178, val_acc=0.00%
2025-02-04 17:03:57,710 - INFO - Epoch 53: EPOCH_AVG_TRAIN_LOSS=0.4884
2025-02-04 17:03:57,712 - INFO - #################### Training epoch 54 ####################
2025-02-04 17:03:57,713 - INFO - Current Learning Rate: 1.953125e-06
2025-02-04 17:03:58,029 - INFO - Epoch 54: train_loss=0.5736
2025-02-04 17:03:58,273 - INFO - Epoch 54: train_loss=0.5017
2025-02-04 17:03:58,585 - INFO - Epoch 54: val_loss=3.9234, val_acc=0.00%
2025-02-04 17:03:58,589 - INFO - Epoch 54: EPOCH_AVG_TRAIN_LOSS=0.5377
2025-02-04 17:03:58,591 - INFO - #################### Training epoch 55 ####################
2025-02-04 17:03:58,591 - INFO - Current Learning Rate: 1.953125e-06
2025-02-04 17:03:58,911 - INFO - Epoch 55: train_loss=0.4140
2025-02-04 17:03:59,156 - INFO - Epoch 55: train_loss=0.8052
2025-02-04 17:03:59,469 - INFO - Epoch 55: val_loss=3.9280, val_acc=0.00%
2025-02-04 17:03:59,473 - INFO - Epoch 55: EPOCH_AVG_TRAIN_LOSS=0.6096
2025-02-04 17:03:59,475 - INFO - #################### Training epoch 56 ####################
2025-02-04 17:03:59,475 - INFO - Current Learning Rate: 1.953125e-06
2025-02-04 17:03:59,793 - INFO - Epoch 56: train_loss=0.4238
2025-02-04 17:04:00,037 - INFO - Epoch 56: train_loss=0.8007
2025-02-04 17:04:00,354 - INFO - Epoch 56: val_loss=3.9390, val_acc=0.00%
2025-02-04 17:04:00,358 - INFO - Epoch 56: EPOCH_AVG_TRAIN_LOSS=0.6123
2025-02-04 17:04:00,360 - INFO - #################### Training epoch 57 ####################
2025-02-04 17:04:00,360 - INFO - Current Learning Rate: 1.953125e-06
2025-02-04 17:04:00,676 - INFO - Epoch 57: train_loss=0.6108
2025-02-04 17:04:00,920 - INFO - Epoch 57: train_loss=0.4221
2025-02-04 17:04:01,233 - INFO - Epoch 57: val_loss=3.9199, val_acc=0.00%
2025-02-04 17:04:01,236 - INFO - Epoch 57: EPOCH_AVG_TRAIN_LOSS=0.5164
2025-02-04 17:04:01,239 - INFO - #################### Training epoch 58 ####################
2025-02-04 17:04:01,239 - INFO - Current Learning Rate: 9.765625e-07
2025-02-04 17:04:01,557 - INFO - Epoch 58: train_loss=0.4677
2025-02-04 17:04:01,801 - INFO - Epoch 58: train_loss=0.7165
2025-02-04 17:04:02,117 - INFO - Epoch 58: val_loss=3.9030, val_acc=0.00%
2025-02-04 17:04:02,120 - INFO - Epoch 58: EPOCH_AVG_TRAIN_LOSS=0.5921
2025-02-04 17:04:02,123 - INFO - #################### Training epoch 59 ####################
2025-02-04 17:04:02,123 - INFO - Current Learning Rate: 9.765625e-07
2025-02-04 17:04:02,437 - INFO - Epoch 59: train_loss=0.6602
2025-02-04 17:04:02,681 - INFO - Epoch 59: train_loss=0.3253
2025-02-04 17:04:02,993 - INFO - Epoch 59: val_loss=3.9171, val_acc=0.00%
2025-02-04 17:04:02,997 - INFO - Epoch 59: EPOCH_AVG_TRAIN_LOSS=0.4928
2025-02-04 17:04:02,999 - INFO - #################### Training epoch 60 ####################
2025-02-04 17:04:02,999 - INFO - Current Learning Rate: 9.765625e-07
2025-02-04 17:04:03,317 - INFO - Epoch 60: train_loss=0.5987
2025-02-04 17:04:03,561 - INFO - Epoch 60: train_loss=0.4451
2025-02-04 17:04:03,874 - INFO - Epoch 60: val_loss=3.9353, val_acc=0.00%
2025-02-04 17:04:03,878 - INFO - Epoch 60: EPOCH_AVG_TRAIN_LOSS=0.5219
2025-02-04 17:04:03,881 - INFO - #################### Training epoch 61 ####################
2025-02-04 17:04:03,881 - INFO - Current Learning Rate: 9.765625e-07
2025-02-04 17:04:04,198 - INFO - Epoch 61: train_loss=0.6202
2025-02-04 17:04:04,442 - INFO - Epoch 61: train_loss=0.4073
2025-02-04 17:04:04,757 - INFO - Epoch 61: val_loss=3.9390, val_acc=0.00%
2025-02-04 17:04:04,761 - INFO - Epoch 61: EPOCH_AVG_TRAIN_LOSS=0.5137
2025-02-04 17:04:04,763 - INFO - #################### Training epoch 62 ####################
2025-02-04 17:04:04,763 - INFO - Current Learning Rate: 4.882813e-07
2025-02-04 17:04:05,080 - INFO - Epoch 62: train_loss=0.5908
2025-02-04 17:04:05,324 - INFO - Epoch 62: train_loss=0.4718
2025-02-04 17:04:05,639 - INFO - Epoch 62: val_loss=3.9120, val_acc=0.00%
2025-02-04 17:04:05,643 - INFO - Epoch 62: EPOCH_AVG_TRAIN_LOSS=0.5313
2025-02-04 17:04:05,645 - INFO - #################### Training epoch 63 ####################
2025-02-04 17:04:05,645 - INFO - Current Learning Rate: 4.882813e-07
2025-02-04 17:04:05,962 - INFO - Epoch 63: train_loss=0.6023
2025-02-04 17:04:06,206 - INFO - Epoch 63: train_loss=0.4408
2025-02-04 17:04:06,521 - INFO - Epoch 63: val_loss=3.9109, val_acc=0.00%
2025-02-04 17:04:06,525 - INFO - Epoch 63: EPOCH_AVG_TRAIN_LOSS=0.5215
2025-02-04 17:04:06,527 - INFO - #################### Training epoch 64 ####################
2025-02-04 17:04:06,527 - INFO - Current Learning Rate: 4.882813e-07
2025-02-04 17:04:06,846 - INFO - Epoch 64: train_loss=0.4059
2025-02-04 17:04:07,090 - INFO - Epoch 64: train_loss=0.8394
2025-02-04 17:04:07,400 - INFO - Epoch 64: val_loss=3.9035, val_acc=0.00%
2025-02-04 17:04:07,403 - INFO - Epoch 64: EPOCH_AVG_TRAIN_LOSS=0.6226
2025-02-04 17:04:07,406 - INFO - #################### Training epoch 65 ####################
2025-02-04 17:04:07,406 - INFO - Current Learning Rate: 4.882813e-07
2025-02-04 17:04:07,725 - INFO - Epoch 65: train_loss=0.5623
2025-02-04 17:04:07,969 - INFO - Epoch 65: train_loss=0.5218
2025-02-04 17:04:08,284 - INFO - Epoch 65: val_loss=3.9178, val_acc=0.00%
2025-02-04 17:04:08,288 - INFO - Epoch 65: EPOCH_AVG_TRAIN_LOSS=0.5421
2025-02-04 17:04:08,290 - INFO - #################### Training epoch 66 ####################
2025-02-04 17:04:08,290 - INFO - Current Learning Rate: 2.441406e-07
2025-02-04 17:04:08,607 - INFO - Epoch 66: train_loss=0.4874
2025-02-04 17:04:08,850 - INFO - Epoch 66: train_loss=0.6640
2025-02-04 17:04:09,163 - INFO - Epoch 66: val_loss=3.9152, val_acc=0.00%
2025-02-04 17:04:09,167 - INFO - Epoch 66: EPOCH_AVG_TRAIN_LOSS=0.5757
2025-02-04 17:04:09,169 - INFO - #################### Training epoch 67 ####################
2025-02-04 17:04:09,169 - INFO - Current Learning Rate: 2.441406e-07
2025-02-04 17:04:09,486 - INFO - Epoch 67: train_loss=0.6092
2025-02-04 17:04:09,729 - INFO - Epoch 67: train_loss=0.4234
2025-02-04 17:04:10,038 - INFO - Epoch 67: val_loss=3.9213, val_acc=0.00%
2025-02-04 17:04:10,042 - INFO - Epoch 67: EPOCH_AVG_TRAIN_LOSS=0.5163
2025-02-04 17:04:10,044 - INFO - #################### Training epoch 68 ####################
2025-02-04 17:04:10,044 - INFO - Current Learning Rate: 2.441406e-07
2025-02-04 17:04:10,361 - INFO - Epoch 68: train_loss=0.5375
2025-02-04 17:04:10,605 - INFO - Epoch 68: train_loss=0.5701
2025-02-04 17:04:10,921 - INFO - Epoch 68: val_loss=3.9281, val_acc=0.00%
2025-02-04 17:04:10,924 - INFO - Epoch 68: EPOCH_AVG_TRAIN_LOSS=0.5538
2025-02-04 17:04:10,927 - INFO - #################### Training epoch 69 ####################
2025-02-04 17:04:10,927 - INFO - Current Learning Rate: 2.441406e-07
2025-02-04 17:04:11,243 - INFO - Epoch 69: train_loss=0.4191
2025-02-04 17:04:11,487 - INFO - Epoch 69: train_loss=0.8101
2025-02-04 17:04:11,800 - INFO - Epoch 69: val_loss=3.9349, val_acc=0.00%
2025-02-04 17:04:11,804 - INFO - Epoch 69: EPOCH_AVG_TRAIN_LOSS=0.6146
2025-02-04 17:04:11,807 - INFO - #################### Training epoch 70 ####################
2025-02-04 17:04:11,807 - INFO - Current Learning Rate: 1.220703e-07
2025-02-04 17:04:12,122 - INFO - Epoch 70: train_loss=0.6165
2025-02-04 17:04:12,366 - INFO - Epoch 70: train_loss=0.4071
2025-02-04 17:04:12,675 - INFO - Epoch 70: val_loss=3.9105, val_acc=0.00%
2025-02-04 17:04:12,679 - INFO - Epoch 70: EPOCH_AVG_TRAIN_LOSS=0.5118
2025-02-04 17:04:12,681 - INFO - #################### Training epoch 71 ####################
2025-02-04 17:04:12,681 - INFO - Current Learning Rate: 1.220703e-07
2025-02-04 17:04:13,000 - INFO - Epoch 71: train_loss=0.4199
2025-02-04 17:04:13,246 - INFO - Epoch 71: train_loss=0.8101
2025-02-04 17:04:13,565 - INFO - Epoch 71: val_loss=3.9339, val_acc=0.00%
2025-02-04 17:04:13,569 - INFO - Epoch 71: EPOCH_AVG_TRAIN_LOSS=0.6150
2025-02-04 17:04:13,571 - INFO - #################### Training epoch 72 ####################
2025-02-04 17:04:13,571 - INFO - Current Learning Rate: 1.220703e-07
2025-02-04 17:04:13,889 - INFO - Epoch 72: train_loss=0.5849
2025-02-04 17:04:14,132 - INFO - Epoch 72: train_loss=0.4708
2025-02-04 17:04:14,446 - INFO - Epoch 72: val_loss=3.9344, val_acc=0.00%
2025-02-04 17:04:14,450 - INFO - Epoch 72: EPOCH_AVG_TRAIN_LOSS=0.5278
2025-02-04 17:04:14,452 - INFO - #################### Training epoch 73 ####################
2025-02-04 17:04:14,452 - INFO - Current Learning Rate: 1.220703e-07
2025-02-04 17:04:14,769 - INFO - Epoch 73: train_loss=0.4267
2025-02-04 17:04:15,012 - INFO - Epoch 73: train_loss=0.7887
2025-02-04 17:04:15,323 - INFO - Epoch 73: val_loss=3.9391, val_acc=0.00%
2025-02-04 17:04:15,327 - INFO - Epoch 73: EPOCH_AVG_TRAIN_LOSS=0.6077
2025-02-04 17:04:15,329 - INFO - #################### Training epoch 74 ####################
2025-02-04 17:04:15,329 - INFO - Current Learning Rate: 6.103516e-08
2025-02-04 17:04:15,647 - INFO - Epoch 74: train_loss=0.4211
2025-02-04 17:04:15,891 - INFO - Epoch 74: train_loss=0.7875
2025-02-04 17:04:16,204 - INFO - Epoch 74: val_loss=3.9454, val_acc=0.00%
2025-02-04 17:04:16,207 - INFO - Epoch 74: EPOCH_AVG_TRAIN_LOSS=0.6043
2025-02-04 17:04:16,210 - INFO - #################### Training epoch 75 ####################
2025-02-04 17:04:16,210 - INFO - Current Learning Rate: 6.103516e-08
2025-02-04 17:04:16,526 - INFO - Epoch 75: train_loss=0.6149
2025-02-04 17:04:16,770 - INFO - Epoch 75: train_loss=0.4198
2025-02-04 17:04:17,085 - INFO - Epoch 75: val_loss=3.9274, val_acc=0.00%
2025-02-04 17:04:17,089 - INFO - Epoch 75: EPOCH_AVG_TRAIN_LOSS=0.5173
2025-02-04 17:04:17,091 - INFO - #################### Training epoch 76 ####################
2025-02-04 17:04:17,091 - INFO - Current Learning Rate: 6.103516e-08
2025-02-04 17:04:17,408 - INFO - Epoch 76: train_loss=0.6312
2025-02-04 17:04:17,651 - INFO - Epoch 76: train_loss=0.3797
2025-02-04 17:04:17,955 - INFO - Epoch 76: val_loss=3.9082, val_acc=0.00%
2025-02-04 17:04:17,958 - INFO - Epoch 76: EPOCH_AVG_TRAIN_LOSS=0.5054
2025-02-04 17:04:17,961 - INFO - #################### Training epoch 77 ####################
2025-02-04 17:04:17,961 - INFO - Current Learning Rate: 6.103516e-08
2025-02-04 17:04:18,277 - INFO - Epoch 77: train_loss=0.6377
2025-02-04 17:04:18,521 - INFO - Epoch 77: train_loss=0.3703
2025-02-04 17:04:18,834 - INFO - Epoch 77: val_loss=3.9090, val_acc=0.00%
2025-02-04 17:04:18,837 - INFO - Epoch 77: EPOCH_AVG_TRAIN_LOSS=0.5040
2025-02-04 17:04:18,840 - INFO - #################### Training epoch 78 ####################
2025-02-04 17:04:18,840 - INFO - Current Learning Rate: 3.051758e-08
2025-02-04 17:04:19,158 - INFO - Epoch 78: train_loss=0.4015
2025-02-04 17:04:19,402 - INFO - Epoch 78: train_loss=0.8390
2025-02-04 17:04:19,716 - INFO - Epoch 78: val_loss=3.9280, val_acc=0.00%
2025-02-04 17:04:19,720 - INFO - Epoch 78: EPOCH_AVG_TRAIN_LOSS=0.6203
2025-02-04 17:04:19,723 - INFO - #################### Training epoch 79 ####################
2025-02-04 17:04:19,723 - INFO - Current Learning Rate: 3.051758e-08
2025-02-04 17:04:20,041 - INFO - Epoch 79: train_loss=0.3881
2025-02-04 17:04:20,284 - INFO - Epoch 79: train_loss=0.8641
2025-02-04 17:04:20,595 - INFO - Epoch 79: val_loss=3.9242, val_acc=0.00%
2025-02-04 17:04:20,599 - INFO - Epoch 79: EPOCH_AVG_TRAIN_LOSS=0.6261
2025-02-04 17:04:20,601 - INFO - #################### Training epoch 80 ####################
2025-02-04 17:04:20,601 - INFO - Current Learning Rate: 3.051758e-08
2025-02-04 17:04:20,918 - INFO - Epoch 80: train_loss=0.6259
2025-02-04 17:04:21,162 - INFO - Epoch 80: train_loss=0.3912
2025-02-04 17:04:21,476 - INFO - Epoch 80: val_loss=3.9200, val_acc=0.00%
2025-02-04 17:04:21,480 - INFO - Epoch 80: EPOCH_AVG_TRAIN_LOSS=0.5085
2025-02-04 17:04:21,482 - INFO - #################### Training epoch 81 ####################
2025-02-04 17:04:21,482 - INFO - Current Learning Rate: 3.051758e-08
2025-02-04 17:04:21,801 - INFO - Epoch 81: train_loss=0.6179
2025-02-04 17:04:22,044 - INFO - Epoch 81: train_loss=0.4132
2025-02-04 17:04:22,358 - INFO - Epoch 81: val_loss=3.9563, val_acc=0.00%
2025-02-04 17:04:22,362 - INFO - Epoch 81: EPOCH_AVG_TRAIN_LOSS=0.5156
2025-02-04 17:04:22,364 - INFO - #################### Training epoch 82 ####################
2025-02-04 17:04:22,364 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:22,680 - INFO - Epoch 82: train_loss=0.6314
2025-02-04 17:04:22,923 - INFO - Epoch 82: train_loss=0.3701
2025-02-04 17:04:23,234 - INFO - Epoch 82: val_loss=3.9200, val_acc=0.00%
2025-02-04 17:04:23,238 - INFO - Epoch 82: EPOCH_AVG_TRAIN_LOSS=0.5007
2025-02-04 17:04:23,240 - INFO - #################### Training epoch 83 ####################
2025-02-04 17:04:23,240 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:23,558 - INFO - Epoch 83: train_loss=0.4619
2025-02-04 17:04:23,802 - INFO - Epoch 83: train_loss=0.7278
2025-02-04 17:04:24,115 - INFO - Epoch 83: val_loss=3.9180, val_acc=0.00%
2025-02-04 17:04:24,119 - INFO - Epoch 83: EPOCH_AVG_TRAIN_LOSS=0.5948
2025-02-04 17:04:24,121 - INFO - #################### Training epoch 84 ####################
2025-02-04 17:04:24,121 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:24,439 - INFO - Epoch 84: train_loss=0.3679
2025-02-04 17:04:24,683 - INFO - Epoch 84: train_loss=0.8999
2025-02-04 17:04:24,996 - INFO - Epoch 84: val_loss=3.9189, val_acc=0.00%
2025-02-04 17:04:25,000 - INFO - Epoch 84: EPOCH_AVG_TRAIN_LOSS=0.6339
2025-02-04 17:04:25,002 - INFO - #################### Training epoch 85 ####################
2025-02-04 17:04:25,002 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:25,317 - INFO - Epoch 85: train_loss=0.5822
2025-02-04 17:04:25,560 - INFO - Epoch 85: train_loss=0.4765
2025-02-04 17:04:25,872 - INFO - Epoch 85: val_loss=3.9484, val_acc=0.00%
2025-02-04 17:04:25,876 - INFO - Epoch 85: EPOCH_AVG_TRAIN_LOSS=0.5294
2025-02-04 17:04:25,878 - INFO - #################### Training epoch 86 ####################
2025-02-04 17:04:25,878 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:26,195 - INFO - Epoch 86: train_loss=0.6902
2025-02-04 17:04:26,439 - INFO - Epoch 86: train_loss=0.2673
2025-02-04 17:04:26,749 - INFO - Epoch 86: val_loss=3.9165, val_acc=0.00%
2025-02-04 17:04:26,752 - INFO - Epoch 86: EPOCH_AVG_TRAIN_LOSS=0.4787
2025-02-04 17:04:26,755 - INFO - #################### Training epoch 87 ####################
2025-02-04 17:04:26,755 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:27,070 - INFO - Epoch 87: train_loss=0.4808
2025-02-04 17:04:27,314 - INFO - Epoch 87: train_loss=0.6953
2025-02-04 17:04:27,629 - INFO - Epoch 87: val_loss=3.9357, val_acc=0.00%
2025-02-04 17:04:27,632 - INFO - Epoch 87: EPOCH_AVG_TRAIN_LOSS=0.5880
2025-02-04 17:04:27,635 - INFO - #################### Training epoch 88 ####################
2025-02-04 17:04:27,635 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:27,946 - INFO - Epoch 88: train_loss=0.5864
2025-02-04 17:04:28,189 - INFO - Epoch 88: train_loss=0.4717
2025-02-04 17:04:28,504 - INFO - Epoch 88: val_loss=3.9156, val_acc=0.00%
2025-02-04 17:04:28,507 - INFO - Epoch 88: EPOCH_AVG_TRAIN_LOSS=0.5290
2025-02-04 17:04:28,510 - INFO - #################### Training epoch 89 ####################
2025-02-04 17:04:28,510 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:28,828 - INFO - Epoch 89: train_loss=0.6889
2025-02-04 17:04:29,072 - INFO - Epoch 89: train_loss=0.2726
2025-02-04 17:04:29,385 - INFO - Epoch 89: val_loss=3.9524, val_acc=0.00%
2025-02-04 17:04:29,389 - INFO - Epoch 89: EPOCH_AVG_TRAIN_LOSS=0.4808
2025-02-04 17:04:29,392 - INFO - #################### Training epoch 90 ####################
2025-02-04 17:04:29,392 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:29,705 - INFO - Epoch 90: train_loss=0.6204
2025-02-04 17:04:29,948 - INFO - Epoch 90: train_loss=0.4068
2025-02-04 17:04:30,262 - INFO - Epoch 90: val_loss=3.9207, val_acc=0.00%
2025-02-04 17:04:30,266 - INFO - Epoch 90: EPOCH_AVG_TRAIN_LOSS=0.5136
2025-02-04 17:04:30,268 - INFO - #################### Training epoch 91 ####################
2025-02-04 17:04:30,268 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:30,584 - INFO - Epoch 91: train_loss=0.6542
2025-02-04 17:04:30,828 - INFO - Epoch 91: train_loss=0.3308
2025-02-04 17:04:31,141 - INFO - Epoch 91: val_loss=3.9431, val_acc=0.00%
2025-02-04 17:04:31,145 - INFO - Epoch 91: EPOCH_AVG_TRAIN_LOSS=0.4925
2025-02-04 17:04:31,147 - INFO - #################### Training epoch 92 ####################
2025-02-04 17:04:31,147 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:31,465 - INFO - Epoch 92: train_loss=0.5910
2025-02-04 17:04:31,709 - INFO - Epoch 92: train_loss=0.4612
2025-02-04 17:04:32,022 - INFO - Epoch 92: val_loss=3.9254, val_acc=0.00%
2025-02-04 17:04:32,026 - INFO - Epoch 92: EPOCH_AVG_TRAIN_LOSS=0.5261
2025-02-04 17:04:32,028 - INFO - #################### Training epoch 93 ####################
2025-02-04 17:04:32,028 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:32,345 - INFO - Epoch 93: train_loss=0.3998
2025-02-04 17:04:32,588 - INFO - Epoch 93: train_loss=0.8371
2025-02-04 17:04:32,899 - INFO - Epoch 93: val_loss=3.9261, val_acc=0.00%
2025-02-04 17:04:32,902 - INFO - Epoch 93: EPOCH_AVG_TRAIN_LOSS=0.6185
2025-02-04 17:04:32,905 - INFO - #################### Training epoch 94 ####################
2025-02-04 17:04:32,905 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:33,223 - INFO - Epoch 94: train_loss=0.5181
2025-02-04 17:04:33,467 - INFO - Epoch 94: train_loss=0.6015
2025-02-04 17:04:33,782 - INFO - Epoch 94: val_loss=3.9110, val_acc=0.00%
2025-02-04 17:04:33,786 - INFO - Epoch 94: EPOCH_AVG_TRAIN_LOSS=0.5598
2025-02-04 17:04:33,788 - INFO - #################### Training epoch 95 ####################
2025-02-04 17:04:33,788 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:34,106 - INFO - Epoch 95: train_loss=0.4806
2025-02-04 17:04:34,350 - INFO - Epoch 95: train_loss=0.6837
2025-02-04 17:04:34,665 - INFO - Epoch 95: val_loss=3.8993, val_acc=0.00%
2025-02-04 17:04:34,669 - INFO - Epoch 95: EPOCH_AVG_TRAIN_LOSS=0.5821
2025-02-04 17:04:34,671 - INFO - #################### Training epoch 96 ####################
2025-02-04 17:04:34,671 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:34,989 - INFO - Epoch 96: train_loss=0.4231
2025-02-04 17:04:35,232 - INFO - Epoch 96: train_loss=0.8042
2025-02-04 17:04:35,544 - INFO - Epoch 96: val_loss=3.9316, val_acc=0.00%
2025-02-04 17:04:35,548 - INFO - Epoch 96: EPOCH_AVG_TRAIN_LOSS=0.6136
2025-02-04 17:04:35,550 - INFO - #################### Training epoch 97 ####################
2025-02-04 17:04:35,550 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:35,867 - INFO - Epoch 97: train_loss=0.4467
2025-02-04 17:04:36,111 - INFO - Epoch 97: train_loss=0.7481
2025-02-04 17:04:36,424 - INFO - Epoch 97: val_loss=3.9106, val_acc=0.00%
2025-02-04 17:04:36,427 - INFO - Epoch 97: EPOCH_AVG_TRAIN_LOSS=0.5974
2025-02-04 17:04:36,430 - INFO - #################### Training epoch 98 ####################
2025-02-04 17:04:36,430 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:36,746 - INFO - Epoch 98: train_loss=0.4652
2025-02-04 17:04:36,989 - INFO - Epoch 98: train_loss=0.7207
2025-02-04 17:04:37,303 - INFO - Epoch 98: val_loss=3.9242, val_acc=0.00%
2025-02-04 17:04:37,307 - INFO - Epoch 98: EPOCH_AVG_TRAIN_LOSS=0.5930
2025-02-04 17:04:37,309 - INFO - #################### Training epoch 99 ####################
2025-02-04 17:04:37,309 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:37,628 - INFO - Epoch 99: train_loss=0.4894
2025-02-04 17:04:37,872 - INFO - Epoch 99: train_loss=0.6661
2025-02-04 17:04:38,182 - INFO - Epoch 99: val_loss=3.9066, val_acc=0.00%
2025-02-04 17:04:38,185 - INFO - Epoch 99: EPOCH_AVG_TRAIN_LOSS=0.5777
2025-02-04 17:04:38,188 - INFO - #################### Training epoch 100 ####################
2025-02-04 17:04:38,188 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:38,503 - INFO - Epoch 100: train_loss=0.5471
2025-02-04 17:04:38,747 - INFO - Epoch 100: train_loss=0.5513
2025-02-04 17:04:39,063 - INFO - Epoch 100: val_loss=3.9656, val_acc=0.00%
2025-02-04 17:04:39,067 - INFO - Epoch 100: EPOCH_AVG_TRAIN_LOSS=0.5492
2025-02-04 17:04:39,069 - INFO - #################### Training epoch 101 ####################
2025-02-04 17:04:39,069 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:39,384 - INFO - Epoch 101: train_loss=0.6703
2025-02-04 17:04:39,628 - INFO - Epoch 101: train_loss=0.3025
2025-02-04 17:04:39,941 - INFO - Epoch 101: val_loss=3.9418, val_acc=0.00%
2025-02-04 17:04:39,945 - INFO - Epoch 101: EPOCH_AVG_TRAIN_LOSS=0.4864
2025-02-04 17:04:39,947 - INFO - #################### Training epoch 102 ####################
2025-02-04 17:04:39,947 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:40,262 - INFO - Epoch 102: train_loss=0.4206
2025-02-04 17:04:40,506 - INFO - Epoch 102: train_loss=0.8077
2025-02-04 17:04:40,813 - INFO - Epoch 102: val_loss=3.9102, val_acc=0.00%
2025-02-04 17:04:40,817 - INFO - Epoch 102: EPOCH_AVG_TRAIN_LOSS=0.6142
2025-02-04 17:04:40,819 - INFO - #################### Training epoch 103 ####################
2025-02-04 17:04:40,819 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:41,139 - INFO - Epoch 103: train_loss=0.4714
2025-02-04 17:04:41,382 - INFO - Epoch 103: train_loss=0.7035
2025-02-04 17:04:41,696 - INFO - Epoch 103: val_loss=3.9482, val_acc=0.00%
2025-02-04 17:04:41,699 - INFO - Epoch 103: EPOCH_AVG_TRAIN_LOSS=0.5874
2025-02-04 17:04:41,702 - INFO - #################### Training epoch 104 ####################
2025-02-04 17:04:41,702 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:42,020 - INFO - Epoch 104: train_loss=0.3367
2025-02-04 17:04:42,264 - INFO - Epoch 104: train_loss=0.9569
2025-02-04 17:04:42,578 - INFO - Epoch 104: val_loss=3.9144, val_acc=0.00%
2025-02-04 17:04:42,581 - INFO - Epoch 104: EPOCH_AVG_TRAIN_LOSS=0.6468
2025-02-04 17:04:42,584 - INFO - #################### Training epoch 105 ####################
2025-02-04 17:04:42,584 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:42,906 - INFO - Epoch 105: train_loss=0.4308
2025-02-04 17:04:43,150 - INFO - Epoch 105: train_loss=0.7852
2025-02-04 17:04:43,452 - INFO - Epoch 105: val_loss=3.9309, val_acc=0.00%
2025-02-04 17:04:43,456 - INFO - Epoch 105: EPOCH_AVG_TRAIN_LOSS=0.6080
2025-02-04 17:04:43,459 - INFO - #################### Training epoch 106 ####################
2025-02-04 17:04:43,459 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:43,775 - INFO - Epoch 106: train_loss=0.5645
2025-02-04 17:04:44,019 - INFO - Epoch 106: train_loss=0.5069
2025-02-04 17:04:44,330 - INFO - Epoch 106: val_loss=3.9143, val_acc=0.00%
2025-02-04 17:04:44,333 - INFO - Epoch 106: EPOCH_AVG_TRAIN_LOSS=0.5357
2025-02-04 17:04:44,336 - INFO - #################### Training epoch 107 ####################
2025-02-04 17:04:44,336 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:44,652 - INFO - Epoch 107: train_loss=0.5953
2025-02-04 17:04:44,896 - INFO - Epoch 107: train_loss=0.4605
2025-02-04 17:04:45,210 - INFO - Epoch 107: val_loss=3.9177, val_acc=0.00%
2025-02-04 17:04:45,213 - INFO - Epoch 107: EPOCH_AVG_TRAIN_LOSS=0.5279
2025-02-04 17:04:45,216 - INFO - #################### Training epoch 108 ####################
2025-02-04 17:04:45,216 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:45,532 - INFO - Epoch 108: train_loss=0.4454
2025-02-04 17:04:45,776 - INFO - Epoch 108: train_loss=0.7469
2025-02-04 17:04:46,088 - INFO - Epoch 108: val_loss=3.9390, val_acc=0.00%
2025-02-04 17:04:46,092 - INFO - Epoch 108: EPOCH_AVG_TRAIN_LOSS=0.5961
2025-02-04 17:04:46,094 - INFO - #################### Training epoch 109 ####################
2025-02-04 17:04:46,094 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:46,410 - INFO - Epoch 109: train_loss=0.6107
2025-02-04 17:04:46,654 - INFO - Epoch 109: train_loss=0.4192
2025-02-04 17:04:46,967 - INFO - Epoch 109: val_loss=3.9227, val_acc=0.00%
2025-02-04 17:04:46,971 - INFO - Epoch 109: EPOCH_AVG_TRAIN_LOSS=0.5149
2025-02-04 17:04:46,974 - INFO - #################### Training epoch 110 ####################
2025-02-04 17:04:46,974 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:47,290 - INFO - Epoch 110: train_loss=0.6014
2025-02-04 17:04:47,534 - INFO - Epoch 110: train_loss=0.4333
2025-02-04 17:04:47,846 - INFO - Epoch 110: val_loss=3.9296, val_acc=0.00%
2025-02-04 17:04:47,850 - INFO - Epoch 110: EPOCH_AVG_TRAIN_LOSS=0.5173
2025-02-04 17:04:47,853 - INFO - #################### Training epoch 111 ####################
2025-02-04 17:04:47,853 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:48,169 - INFO - Epoch 111: train_loss=0.6113
2025-02-04 17:04:48,413 - INFO - Epoch 111: train_loss=0.4239
2025-02-04 17:04:48,726 - INFO - Epoch 111: val_loss=3.9194, val_acc=0.00%
2025-02-04 17:04:48,729 - INFO - Epoch 111: EPOCH_AVG_TRAIN_LOSS=0.5176
2025-02-04 17:04:48,732 - INFO - #################### Training epoch 112 ####################
2025-02-04 17:04:48,732 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:49,048 - INFO - Epoch 112: train_loss=0.6251
2025-02-04 17:04:49,292 - INFO - Epoch 112: train_loss=0.3931
2025-02-04 17:04:49,606 - INFO - Epoch 112: val_loss=3.9135, val_acc=0.00%
2025-02-04 17:04:49,610 - INFO - Epoch 112: EPOCH_AVG_TRAIN_LOSS=0.5091
2025-02-04 17:04:49,613 - INFO - #################### Training epoch 113 ####################
2025-02-04 17:04:49,613 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:49,930 - INFO - Epoch 113: train_loss=0.6300
2025-02-04 17:04:50,174 - INFO - Epoch 113: train_loss=0.3751
2025-02-04 17:04:50,487 - INFO - Epoch 113: val_loss=3.9547, val_acc=0.00%
2025-02-04 17:04:50,491 - INFO - Epoch 113: EPOCH_AVG_TRAIN_LOSS=0.5025
2025-02-04 17:04:50,493 - INFO - #################### Training epoch 114 ####################
2025-02-04 17:04:50,493 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:50,808 - INFO - Epoch 114: train_loss=0.5414
2025-02-04 17:04:51,052 - INFO - Epoch 114: train_loss=0.5694
2025-02-04 17:04:51,363 - INFO - Epoch 114: val_loss=3.9321, val_acc=0.00%
2025-02-04 17:04:51,367 - INFO - Epoch 114: EPOCH_AVG_TRAIN_LOSS=0.5554
2025-02-04 17:04:51,370 - INFO - #################### Training epoch 115 ####################
2025-02-04 17:04:51,370 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:51,687 - INFO - Epoch 115: train_loss=0.6432
2025-02-04 17:04:51,932 - INFO - Epoch 115: train_loss=0.3559
2025-02-04 17:04:52,246 - INFO - Epoch 115: val_loss=3.9120, val_acc=0.00%
2025-02-04 17:04:52,249 - INFO - Epoch 115: EPOCH_AVG_TRAIN_LOSS=0.4996
2025-02-04 17:04:52,252 - INFO - #################### Training epoch 116 ####################
2025-02-04 17:04:52,252 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:52,568 - INFO - Epoch 116: train_loss=0.5702
2025-02-04 17:04:52,813 - INFO - Epoch 116: train_loss=0.4965
2025-02-04 17:04:53,126 - INFO - Epoch 116: val_loss=3.9109, val_acc=0.00%
2025-02-04 17:04:53,130 - INFO - Epoch 116: EPOCH_AVG_TRAIN_LOSS=0.5334
2025-02-04 17:04:53,133 - INFO - #################### Training epoch 117 ####################
2025-02-04 17:04:53,133 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:53,451 - INFO - Epoch 117: train_loss=0.5338
2025-02-04 17:04:53,694 - INFO - Epoch 117: train_loss=0.5796
2025-02-04 17:04:54,008 - INFO - Epoch 117: val_loss=3.9251, val_acc=0.00%
2025-02-04 17:04:54,012 - INFO - Epoch 117: EPOCH_AVG_TRAIN_LOSS=0.5567
2025-02-04 17:04:54,014 - INFO - #################### Training epoch 118 ####################
2025-02-04 17:04:54,014 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:54,331 - INFO - Epoch 118: train_loss=0.5585
2025-02-04 17:04:54,577 - INFO - Epoch 118: train_loss=0.5232
2025-02-04 17:04:54,889 - INFO - Epoch 118: val_loss=3.9277, val_acc=0.00%
2025-02-04 17:04:54,892 - INFO - Epoch 118: EPOCH_AVG_TRAIN_LOSS=0.5408
2025-02-04 17:04:54,895 - INFO - #################### Training epoch 119 ####################
2025-02-04 17:04:54,895 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:55,212 - INFO - Epoch 119: train_loss=0.6107
2025-02-04 17:04:55,456 - INFO - Epoch 119: train_loss=0.4151
2025-02-04 17:04:55,764 - INFO - Epoch 119: val_loss=3.9001, val_acc=0.00%
2025-02-04 17:04:55,768 - INFO - Epoch 119: EPOCH_AVG_TRAIN_LOSS=0.5129
2025-02-04 17:04:55,770 - INFO - #################### Training epoch 120 ####################
2025-02-04 17:04:55,770 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:56,087 - INFO - Epoch 120: train_loss=0.6228
2025-02-04 17:04:56,332 - INFO - Epoch 120: train_loss=0.4015
2025-02-04 17:04:56,646 - INFO - Epoch 120: val_loss=3.9258, val_acc=0.00%
2025-02-04 17:04:56,649 - INFO - Epoch 120: EPOCH_AVG_TRAIN_LOSS=0.5122
2025-02-04 17:04:56,652 - INFO - #################### Training epoch 121 ####################
2025-02-04 17:04:56,652 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:56,969 - INFO - Epoch 121: train_loss=0.6426
2025-02-04 17:04:57,212 - INFO - Epoch 121: train_loss=0.3617
2025-02-04 17:04:57,522 - INFO - Epoch 121: val_loss=3.9292, val_acc=0.00%
2025-02-04 17:04:57,525 - INFO - Epoch 121: EPOCH_AVG_TRAIN_LOSS=0.5021
2025-02-04 17:04:57,528 - INFO - #################### Training epoch 122 ####################
2025-02-04 17:04:57,528 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:57,844 - INFO - Epoch 122: train_loss=0.3930
2025-02-04 17:04:58,088 - INFO - Epoch 122: train_loss=0.8696
2025-02-04 17:04:58,397 - INFO - Epoch 122: val_loss=3.9212, val_acc=0.00%
2025-02-04 17:04:58,401 - INFO - Epoch 122: EPOCH_AVG_TRAIN_LOSS=0.6313
2025-02-04 17:04:58,404 - INFO - #################### Training epoch 123 ####################
2025-02-04 17:04:58,404 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:58,719 - INFO - Epoch 123: train_loss=0.4577
2025-02-04 17:04:58,962 - INFO - Epoch 123: train_loss=0.7297
2025-02-04 17:04:59,277 - INFO - Epoch 123: val_loss=3.9402, val_acc=0.00%
2025-02-04 17:04:59,280 - INFO - Epoch 123: EPOCH_AVG_TRAIN_LOSS=0.5937
2025-02-04 17:04:59,283 - INFO - #################### Training epoch 124 ####################
2025-02-04 17:04:59,283 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:04:59,598 - INFO - Epoch 124: train_loss=0.5718
2025-02-04 17:04:59,842 - INFO - Epoch 124: train_loss=0.4951
2025-02-04 17:05:00,155 - INFO - Epoch 124: val_loss=3.9502, val_acc=0.00%
2025-02-04 17:05:00,158 - INFO - Epoch 124: EPOCH_AVG_TRAIN_LOSS=0.5334
2025-02-04 17:05:00,160 - INFO - #################### Training epoch 125 ####################
2025-02-04 17:05:00,161 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:00,477 - INFO - Epoch 125: train_loss=0.5010
2025-02-04 17:05:00,720 - INFO - Epoch 125: train_loss=0.6460
2025-02-04 17:05:01,029 - INFO - Epoch 125: val_loss=3.9069, val_acc=0.00%
2025-02-04 17:05:01,032 - INFO - Epoch 125: EPOCH_AVG_TRAIN_LOSS=0.5735
2025-02-04 17:05:01,035 - INFO - #################### Training epoch 126 ####################
2025-02-04 17:05:01,035 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:01,351 - INFO - Epoch 126: train_loss=0.5873
2025-02-04 17:05:01,594 - INFO - Epoch 126: train_loss=0.4683
2025-02-04 17:05:01,908 - INFO - Epoch 126: val_loss=3.9349, val_acc=0.00%
2025-02-04 17:05:01,912 - INFO - Epoch 126: EPOCH_AVG_TRAIN_LOSS=0.5278
2025-02-04 17:05:01,914 - INFO - #################### Training epoch 127 ####################
2025-02-04 17:05:01,914 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:02,230 - INFO - Epoch 127: train_loss=0.6467
2025-02-04 17:05:02,473 - INFO - Epoch 127: train_loss=0.3462
2025-02-04 17:05:02,786 - INFO - Epoch 127: val_loss=3.9068, val_acc=0.00%
2025-02-04 17:05:02,790 - INFO - Epoch 127: EPOCH_AVG_TRAIN_LOSS=0.4964
2025-02-04 17:05:02,792 - INFO - #################### Training epoch 128 ####################
2025-02-04 17:05:02,793 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:03,108 - INFO - Epoch 128: train_loss=0.5116
2025-02-04 17:05:03,351 - INFO - Epoch 128: train_loss=0.6209
2025-02-04 17:05:03,658 - INFO - Epoch 128: val_loss=3.9496, val_acc=0.00%
2025-02-04 17:05:03,661 - INFO - Epoch 128: EPOCH_AVG_TRAIN_LOSS=0.5663
2025-02-04 17:05:03,664 - INFO - #################### Training epoch 129 ####################
2025-02-04 17:05:03,664 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:03,980 - INFO - Epoch 129: train_loss=0.5742
2025-02-04 17:05:04,224 - INFO - Epoch 129: train_loss=0.4901
2025-02-04 17:05:04,537 - INFO - Epoch 129: val_loss=3.9187, val_acc=0.00%
2025-02-04 17:05:04,541 - INFO - Epoch 129: EPOCH_AVG_TRAIN_LOSS=0.5321
2025-02-04 17:05:04,543 - INFO - #################### Training epoch 130 ####################
2025-02-04 17:05:04,543 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:04,859 - INFO - Epoch 130: train_loss=0.4537
2025-02-04 17:05:05,105 - INFO - Epoch 130: train_loss=0.7402
2025-02-04 17:05:05,414 - INFO - Epoch 130: val_loss=3.8976, val_acc=0.00%
2025-02-04 17:05:05,418 - INFO - Epoch 130: EPOCH_AVG_TRAIN_LOSS=0.5970
2025-02-04 17:05:05,420 - INFO - #################### Training epoch 131 ####################
2025-02-04 17:05:05,420 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:05,737 - INFO - Epoch 131: train_loss=0.6354
2025-02-04 17:05:05,981 - INFO - Epoch 131: train_loss=0.3625
2025-02-04 17:05:06,290 - INFO - Epoch 131: val_loss=3.9354, val_acc=0.00%
2025-02-04 17:05:06,294 - INFO - Epoch 131: EPOCH_AVG_TRAIN_LOSS=0.4989
2025-02-04 17:05:06,296 - INFO - #################### Training epoch 132 ####################
2025-02-04 17:05:06,296 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:06,614 - INFO - Epoch 132: train_loss=0.4507
2025-02-04 17:05:06,858 - INFO - Epoch 132: train_loss=0.7337
2025-02-04 17:05:07,169 - INFO - Epoch 132: val_loss=3.9177, val_acc=0.00%
2025-02-04 17:05:07,173 - INFO - Epoch 132: EPOCH_AVG_TRAIN_LOSS=0.5922
2025-02-04 17:05:07,175 - INFO - #################### Training epoch 133 ####################
2025-02-04 17:05:07,175 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:07,491 - INFO - Epoch 133: train_loss=0.6206
2025-02-04 17:05:07,735 - INFO - Epoch 133: train_loss=0.4028
2025-02-04 17:05:08,048 - INFO - Epoch 133: val_loss=3.9197, val_acc=0.00%
2025-02-04 17:05:08,052 - INFO - Epoch 133: EPOCH_AVG_TRAIN_LOSS=0.5117
2025-02-04 17:05:08,054 - INFO - #################### Training epoch 134 ####################
2025-02-04 17:05:08,054 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:08,372 - INFO - Epoch 134: train_loss=0.6376
2025-02-04 17:05:08,616 - INFO - Epoch 134: train_loss=0.3702
2025-02-04 17:05:08,923 - INFO - Epoch 134: val_loss=3.9040, val_acc=0.00%
2025-02-04 17:05:08,927 - INFO - Epoch 134: EPOCH_AVG_TRAIN_LOSS=0.5039
2025-02-04 17:05:08,929 - INFO - #################### Training epoch 135 ####################
2025-02-04 17:05:08,930 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:09,247 - INFO - Epoch 135: train_loss=0.6257
2025-02-04 17:05:09,491 - INFO - Epoch 135: train_loss=0.3889
2025-02-04 17:05:09,806 - INFO - Epoch 135: val_loss=3.9197, val_acc=0.00%
2025-02-04 17:05:09,810 - INFO - Epoch 135: EPOCH_AVG_TRAIN_LOSS=0.5073
2025-02-04 17:05:09,812 - INFO - #################### Training epoch 136 ####################
2025-02-04 17:05:09,813 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:10,131 - INFO - Epoch 136: train_loss=0.5682
2025-02-04 17:05:10,376 - INFO - Epoch 136: train_loss=0.5111
2025-02-04 17:05:10,692 - INFO - Epoch 136: val_loss=3.9125, val_acc=0.00%
2025-02-04 17:05:10,696 - INFO - Epoch 136: EPOCH_AVG_TRAIN_LOSS=0.5397
2025-02-04 17:05:10,698 - INFO - #################### Training epoch 137 ####################
2025-02-04 17:05:10,699 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:11,017 - INFO - Epoch 137: train_loss=0.7030
2025-02-04 17:05:11,262 - INFO - Epoch 137: train_loss=0.2340
2025-02-04 17:05:11,573 - INFO - Epoch 137: val_loss=3.9338, val_acc=0.00%
2025-02-04 17:05:11,576 - INFO - Epoch 137: EPOCH_AVG_TRAIN_LOSS=0.4685
2025-02-04 17:05:11,579 - INFO - #################### Training epoch 138 ####################
2025-02-04 17:05:11,579 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:11,896 - INFO - Epoch 138: train_loss=0.5892
2025-02-04 17:05:12,140 - INFO - Epoch 138: train_loss=0.4722
2025-02-04 17:05:12,454 - INFO - Epoch 138: val_loss=3.9281, val_acc=0.00%
2025-02-04 17:05:12,458 - INFO - Epoch 138: EPOCH_AVG_TRAIN_LOSS=0.5307
2025-02-04 17:05:12,460 - INFO - #################### Training epoch 139 ####################
2025-02-04 17:05:12,460 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:12,775 - INFO - Epoch 139: train_loss=0.6762
2025-02-04 17:05:13,018 - INFO - Epoch 139: train_loss=0.2811
2025-02-04 17:05:13,332 - INFO - Epoch 139: val_loss=3.9482, val_acc=0.00%
2025-02-04 17:05:13,335 - INFO - Epoch 139: EPOCH_AVG_TRAIN_LOSS=0.4786
2025-02-04 17:05:13,338 - INFO - #################### Training epoch 140 ####################
2025-02-04 17:05:13,338 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:13,651 - INFO - Epoch 140: train_loss=0.4970
2025-02-04 17:05:13,895 - INFO - Epoch 140: train_loss=0.6490
2025-02-04 17:05:14,208 - INFO - Epoch 140: val_loss=3.9223, val_acc=0.00%
2025-02-04 17:05:14,211 - INFO - Epoch 140: EPOCH_AVG_TRAIN_LOSS=0.5730
2025-02-04 17:05:14,214 - INFO - #################### Training epoch 141 ####################
2025-02-04 17:05:14,214 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:14,531 - INFO - Epoch 141: train_loss=0.6396
2025-02-04 17:05:14,775 - INFO - Epoch 141: train_loss=0.3632
2025-02-04 17:05:15,088 - INFO - Epoch 141: val_loss=3.9327, val_acc=0.00%
2025-02-04 17:05:15,092 - INFO - Epoch 141: EPOCH_AVG_TRAIN_LOSS=0.5014
2025-02-04 17:05:15,094 - INFO - #################### Training epoch 142 ####################
2025-02-04 17:05:15,094 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:15,410 - INFO - Epoch 142: train_loss=0.5321
2025-02-04 17:05:15,654 - INFO - Epoch 142: train_loss=0.5892
2025-02-04 17:05:15,968 - INFO - Epoch 142: val_loss=3.8806, val_acc=0.00%
2025-02-04 17:05:15,972 - INFO - Epoch 142: EPOCH_AVG_TRAIN_LOSS=0.5606
2025-02-04 17:05:15,974 - INFO - #################### Training epoch 143 ####################
2025-02-04 17:05:15,974 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:16,290 - INFO - Epoch 143: train_loss=0.6434
2025-02-04 17:05:16,534 - INFO - Epoch 143: train_loss=0.3549
2025-02-04 17:05:16,847 - INFO - Epoch 143: val_loss=3.9469, val_acc=0.00%
2025-02-04 17:05:16,850 - INFO - Epoch 143: EPOCH_AVG_TRAIN_LOSS=0.4992
2025-02-04 17:05:16,853 - INFO - #################### Training epoch 144 ####################
2025-02-04 17:05:16,853 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:17,171 - INFO - Epoch 144: train_loss=0.5520
2025-02-04 17:05:17,415 - INFO - Epoch 144: train_loss=0.5372
2025-02-04 17:05:17,731 - INFO - Epoch 144: val_loss=3.9321, val_acc=0.00%
2025-02-04 17:05:17,734 - INFO - Epoch 144: EPOCH_AVG_TRAIN_LOSS=0.5446
2025-02-04 17:05:17,737 - INFO - #################### Training epoch 145 ####################
2025-02-04 17:05:17,737 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:18,053 - INFO - Epoch 145: train_loss=0.4739
2025-02-04 17:05:18,296 - INFO - Epoch 145: train_loss=0.7023
2025-02-04 17:05:18,607 - INFO - Epoch 145: val_loss=3.8946, val_acc=0.00%
2025-02-04 17:05:18,610 - INFO - Epoch 145: EPOCH_AVG_TRAIN_LOSS=0.5881
2025-02-04 17:05:18,613 - INFO - #################### Training epoch 146 ####################
2025-02-04 17:05:18,613 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:18,927 - INFO - Epoch 146: train_loss=0.4325
2025-02-04 17:05:19,171 - INFO - Epoch 146: train_loss=0.7780
2025-02-04 17:05:19,484 - INFO - Epoch 146: val_loss=3.9276, val_acc=0.00%
2025-02-04 17:05:19,488 - INFO - Epoch 146: EPOCH_AVG_TRAIN_LOSS=0.6053
2025-02-04 17:05:19,490 - INFO - #################### Training epoch 147 ####################
2025-02-04 17:05:19,491 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:19,810 - INFO - Epoch 147: train_loss=0.5547
2025-02-04 17:05:20,054 - INFO - Epoch 147: train_loss=0.5377
2025-02-04 17:05:20,368 - INFO - Epoch 147: val_loss=3.9481, val_acc=0.00%
2025-02-04 17:05:20,372 - INFO - Epoch 147: EPOCH_AVG_TRAIN_LOSS=0.5462
2025-02-04 17:05:20,375 - INFO - #################### Training epoch 148 ####################
2025-02-04 17:05:20,375 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:20,693 - INFO - Epoch 148: train_loss=0.3042
2025-02-04 17:05:20,936 - INFO - Epoch 148: train_loss=1.0350
2025-02-04 17:05:21,248 - INFO - Epoch 148: val_loss=3.9031, val_acc=0.00%
2025-02-04 17:05:21,252 - INFO - Epoch 148: EPOCH_AVG_TRAIN_LOSS=0.6696
2025-02-04 17:05:21,254 - INFO - #################### Training epoch 149 ####################
2025-02-04 17:05:21,254 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:21,574 - INFO - Epoch 149: train_loss=0.5344
2025-02-04 17:05:21,819 - INFO - Epoch 149: train_loss=0.5688
2025-02-04 17:05:22,136 - INFO - Epoch 149: val_loss=3.9017, val_acc=0.00%
2025-02-04 17:05:22,140 - INFO - Epoch 149: EPOCH_AVG_TRAIN_LOSS=0.5516
2025-02-04 17:05:22,142 - INFO - #################### Training epoch 150 ####################
2025-02-04 17:05:22,142 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:22,459 - INFO - Epoch 150: train_loss=0.4439
2025-02-04 17:05:22,704 - INFO - Epoch 150: train_loss=0.7533
2025-02-04 17:05:23,005 - INFO - Epoch 150: val_loss=3.9204, val_acc=0.00%
2025-02-04 17:05:23,009 - INFO - Epoch 150: EPOCH_AVG_TRAIN_LOSS=0.5986
2025-02-04 17:05:23,011 - INFO - #################### Training epoch 151 ####################
2025-02-04 17:05:23,011 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:23,326 - INFO - Epoch 151: train_loss=0.5221
2025-02-04 17:05:23,570 - INFO - Epoch 151: train_loss=0.5897
2025-02-04 17:05:23,879 - INFO - Epoch 151: val_loss=3.9075, val_acc=0.00%
2025-02-04 17:05:23,883 - INFO - Epoch 151: EPOCH_AVG_TRAIN_LOSS=0.5559
2025-02-04 17:05:23,885 - INFO - #################### Training epoch 152 ####################
2025-02-04 17:05:23,885 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:24,199 - INFO - Epoch 152: train_loss=0.6173
2025-02-04 17:05:24,443 - INFO - Epoch 152: train_loss=0.4101
2025-02-04 17:05:24,754 - INFO - Epoch 152: val_loss=3.8931, val_acc=0.00%
2025-02-04 17:05:24,757 - INFO - Epoch 152: EPOCH_AVG_TRAIN_LOSS=0.5137
2025-02-04 17:05:24,760 - INFO - #################### Training epoch 153 ####################
2025-02-04 17:05:24,760 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:25,078 - INFO - Epoch 153: train_loss=0.7383
2025-02-04 17:05:25,322 - INFO - Epoch 153: train_loss=0.1710
2025-02-04 17:05:25,634 - INFO - Epoch 153: val_loss=3.9302, val_acc=0.00%
2025-02-04 17:05:25,637 - INFO - Epoch 153: EPOCH_AVG_TRAIN_LOSS=0.4547
2025-02-04 17:05:25,640 - INFO - #################### Training epoch 154 ####################
2025-02-04 17:05:25,640 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:25,960 - INFO - Epoch 154: train_loss=0.6956
2025-02-04 17:05:26,204 - INFO - Epoch 154: train_loss=0.2567
2025-02-04 17:05:26,510 - INFO - Epoch 154: val_loss=3.9220, val_acc=0.00%
2025-02-04 17:05:26,513 - INFO - Epoch 154: EPOCH_AVG_TRAIN_LOSS=0.4761
2025-02-04 17:05:26,516 - INFO - #################### Training epoch 155 ####################
2025-02-04 17:05:26,516 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:26,833 - INFO - Epoch 155: train_loss=0.6706
2025-02-04 17:05:27,078 - INFO - Epoch 155: train_loss=0.3052
2025-02-04 17:05:27,389 - INFO - Epoch 155: val_loss=3.9347, val_acc=0.00%
2025-02-04 17:05:27,393 - INFO - Epoch 155: EPOCH_AVG_TRAIN_LOSS=0.4879
2025-02-04 17:05:27,395 - INFO - #################### Training epoch 156 ####################
2025-02-04 17:05:27,395 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:27,712 - INFO - Epoch 156: train_loss=0.5041
2025-02-04 17:05:27,956 - INFO - Epoch 156: train_loss=0.6403
2025-02-04 17:05:28,268 - INFO - Epoch 156: val_loss=3.9534, val_acc=0.00%
2025-02-04 17:05:28,272 - INFO - Epoch 156: EPOCH_AVG_TRAIN_LOSS=0.5722
2025-02-04 17:05:28,274 - INFO - #################### Training epoch 157 ####################
2025-02-04 17:05:28,274 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:28,586 - INFO - Epoch 157: train_loss=0.5388
2025-02-04 17:05:28,830 - INFO - Epoch 157: train_loss=0.5665
2025-02-04 17:05:29,138 - INFO - Epoch 157: val_loss=3.9436, val_acc=0.00%
2025-02-04 17:05:29,141 - INFO - Epoch 157: EPOCH_AVG_TRAIN_LOSS=0.5526
2025-02-04 17:05:29,144 - INFO - #################### Training epoch 158 ####################
2025-02-04 17:05:29,144 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:29,461 - INFO - Epoch 158: train_loss=0.5199
2025-02-04 17:05:29,705 - INFO - Epoch 158: train_loss=0.5970
2025-02-04 17:05:30,019 - INFO - Epoch 158: val_loss=3.9252, val_acc=0.00%
2025-02-04 17:05:30,023 - INFO - Epoch 158: EPOCH_AVG_TRAIN_LOSS=0.5585
2025-02-04 17:05:30,025 - INFO - #################### Training epoch 159 ####################
2025-02-04 17:05:30,025 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:30,344 - INFO - Epoch 159: train_loss=0.4902
2025-02-04 17:05:30,588 - INFO - Epoch 159: train_loss=0.6603
2025-02-04 17:05:30,897 - INFO - Epoch 159: val_loss=3.9182, val_acc=0.00%
2025-02-04 17:05:30,900 - INFO - Epoch 159: EPOCH_AVG_TRAIN_LOSS=0.5752
2025-02-04 17:05:30,903 - INFO - #################### Training epoch 160 ####################
2025-02-04 17:05:30,903 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:31,218 - INFO - Epoch 160: train_loss=0.4224
2025-02-04 17:05:31,462 - INFO - Epoch 160: train_loss=0.8044
2025-02-04 17:05:31,769 - INFO - Epoch 160: val_loss=3.9361, val_acc=0.00%
2025-02-04 17:05:31,773 - INFO - Epoch 160: EPOCH_AVG_TRAIN_LOSS=0.6134
2025-02-04 17:05:31,775 - INFO - #################### Training epoch 161 ####################
2025-02-04 17:05:31,776 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:32,091 - INFO - Epoch 161: train_loss=0.5592
2025-02-04 17:05:32,335 - INFO - Epoch 161: train_loss=0.5192
2025-02-04 17:05:32,647 - INFO - Epoch 161: val_loss=3.9199, val_acc=0.00%
2025-02-04 17:05:32,651 - INFO - Epoch 161: EPOCH_AVG_TRAIN_LOSS=0.5392
2025-02-04 17:05:32,653 - INFO - #################### Training epoch 162 ####################
2025-02-04 17:05:32,653 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:32,970 - INFO - Epoch 162: train_loss=0.6245
2025-02-04 17:05:33,213 - INFO - Epoch 162: train_loss=0.3967
2025-02-04 17:05:33,524 - INFO - Epoch 162: val_loss=3.9257, val_acc=0.00%
2025-02-04 17:05:33,527 - INFO - Epoch 162: EPOCH_AVG_TRAIN_LOSS=0.5106
2025-02-04 17:05:33,530 - INFO - #################### Training epoch 163 ####################
2025-02-04 17:05:33,530 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:33,844 - INFO - Epoch 163: train_loss=0.6630
2025-02-04 17:05:34,088 - INFO - Epoch 163: train_loss=0.3136
2025-02-04 17:05:34,398 - INFO - Epoch 163: val_loss=3.9220, val_acc=0.00%
2025-02-04 17:05:34,401 - INFO - Epoch 163: EPOCH_AVG_TRAIN_LOSS=0.4883
2025-02-04 17:05:34,404 - INFO - #################### Training epoch 164 ####################
2025-02-04 17:05:34,404 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:34,720 - INFO - Epoch 164: train_loss=0.6568
2025-02-04 17:05:34,963 - INFO - Epoch 164: train_loss=0.3259
2025-02-04 17:05:35,273 - INFO - Epoch 164: val_loss=3.9088, val_acc=0.00%
2025-02-04 17:05:35,277 - INFO - Epoch 164: EPOCH_AVG_TRAIN_LOSS=0.4913
2025-02-04 17:05:35,280 - INFO - #################### Training epoch 165 ####################
2025-02-04 17:05:35,280 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:35,598 - INFO - Epoch 165: train_loss=0.6783
2025-02-04 17:05:35,842 - INFO - Epoch 165: train_loss=0.2901
2025-02-04 17:05:36,150 - INFO - Epoch 165: val_loss=3.9151, val_acc=0.00%
2025-02-04 17:05:36,154 - INFO - Epoch 165: EPOCH_AVG_TRAIN_LOSS=0.4842
2025-02-04 17:05:36,156 - INFO - #################### Training epoch 166 ####################
2025-02-04 17:05:36,157 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:36,472 - INFO - Epoch 166: train_loss=0.5752
2025-02-04 17:05:36,716 - INFO - Epoch 166: train_loss=0.4860
2025-02-04 17:05:37,028 - INFO - Epoch 166: val_loss=3.9003, val_acc=0.00%
2025-02-04 17:05:37,032 - INFO - Epoch 166: EPOCH_AVG_TRAIN_LOSS=0.5306
2025-02-04 17:05:37,034 - INFO - #################### Training epoch 167 ####################
2025-02-04 17:05:37,035 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:37,357 - INFO - Epoch 167: train_loss=0.4413
2025-02-04 17:05:37,600 - INFO - Epoch 167: train_loss=0.7526
2025-02-04 17:05:37,908 - INFO - Epoch 167: val_loss=3.9183, val_acc=0.00%
2025-02-04 17:05:37,912 - INFO - Epoch 167: EPOCH_AVG_TRAIN_LOSS=0.5969
2025-02-04 17:05:37,914 - INFO - #################### Training epoch 168 ####################
2025-02-04 17:05:37,915 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:38,231 - INFO - Epoch 168: train_loss=0.5888
2025-02-04 17:05:38,475 - INFO - Epoch 168: train_loss=0.4672
2025-02-04 17:05:38,786 - INFO - Epoch 168: val_loss=3.9336, val_acc=0.00%
2025-02-04 17:05:38,789 - INFO - Epoch 168: EPOCH_AVG_TRAIN_LOSS=0.5280
2025-02-04 17:05:38,792 - INFO - #################### Training epoch 169 ####################
2025-02-04 17:05:38,792 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:39,110 - INFO - Epoch 169: train_loss=0.4394
2025-02-04 17:05:39,354 - INFO - Epoch 169: train_loss=0.7688
2025-02-04 17:05:39,667 - INFO - Epoch 169: val_loss=3.9228, val_acc=0.00%
2025-02-04 17:05:39,670 - INFO - Epoch 169: EPOCH_AVG_TRAIN_LOSS=0.6041
2025-02-04 17:05:39,673 - INFO - #################### Training epoch 170 ####################
2025-02-04 17:05:39,673 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:39,989 - INFO - Epoch 170: train_loss=0.5877
2025-02-04 17:05:40,233 - INFO - Epoch 170: train_loss=0.4617
2025-02-04 17:05:40,544 - INFO - Epoch 170: val_loss=3.9499, val_acc=0.00%
2025-02-04 17:05:40,547 - INFO - Epoch 170: EPOCH_AVG_TRAIN_LOSS=0.5247
2025-02-04 17:05:40,550 - INFO - #################### Training epoch 171 ####################
2025-02-04 17:05:40,550 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:40,866 - INFO - Epoch 171: train_loss=0.5737
2025-02-04 17:05:41,110 - INFO - Epoch 171: train_loss=0.4973
2025-02-04 17:05:41,419 - INFO - Epoch 171: val_loss=3.9087, val_acc=0.00%
2025-02-04 17:05:41,423 - INFO - Epoch 171: EPOCH_AVG_TRAIN_LOSS=0.5355
2025-02-04 17:05:41,425 - INFO - #################### Training epoch 172 ####################
2025-02-04 17:05:41,425 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:41,737 - INFO - Epoch 172: train_loss=0.6119
2025-02-04 17:05:41,981 - INFO - Epoch 172: train_loss=0.4172
2025-02-04 17:05:42,294 - INFO - Epoch 172: val_loss=3.9199, val_acc=0.00%
2025-02-04 17:05:42,298 - INFO - Epoch 172: EPOCH_AVG_TRAIN_LOSS=0.5145
2025-02-04 17:05:42,300 - INFO - #################### Training epoch 173 ####################
2025-02-04 17:05:42,300 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:42,615 - INFO - Epoch 173: train_loss=0.6438
2025-02-04 17:05:42,858 - INFO - Epoch 173: train_loss=0.3508
2025-02-04 17:05:43,169 - INFO - Epoch 173: val_loss=3.9193, val_acc=0.00%
2025-02-04 17:05:43,173 - INFO - Epoch 173: EPOCH_AVG_TRAIN_LOSS=0.4973
2025-02-04 17:05:43,175 - INFO - #################### Training epoch 174 ####################
2025-02-04 17:05:43,176 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:43,493 - INFO - Epoch 174: train_loss=0.5888
2025-02-04 17:05:43,737 - INFO - Epoch 174: train_loss=0.4581
2025-02-04 17:05:44,050 - INFO - Epoch 174: val_loss=3.9201, val_acc=0.00%
2025-02-04 17:05:44,053 - INFO - Epoch 174: EPOCH_AVG_TRAIN_LOSS=0.5235
2025-02-04 17:05:44,056 - INFO - #################### Training epoch 175 ####################
2025-02-04 17:05:44,056 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:44,372 - INFO - Epoch 175: train_loss=0.6699
2025-02-04 17:05:44,616 - INFO - Epoch 175: train_loss=0.3067
2025-02-04 17:05:44,927 - INFO - Epoch 175: val_loss=3.9174, val_acc=0.00%
2025-02-04 17:05:44,930 - INFO - Epoch 175: EPOCH_AVG_TRAIN_LOSS=0.4883
2025-02-04 17:05:44,933 - INFO - #################### Training epoch 176 ####################
2025-02-04 17:05:44,933 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:45,247 - INFO - Epoch 176: train_loss=0.3962
2025-02-04 17:05:45,491 - INFO - Epoch 176: train_loss=0.8590
2025-02-04 17:05:45,802 - INFO - Epoch 176: val_loss=3.9380, val_acc=0.00%
2025-02-04 17:05:45,806 - INFO - Epoch 176: EPOCH_AVG_TRAIN_LOSS=0.6276
2025-02-04 17:05:45,808 - INFO - #################### Training epoch 177 ####################
2025-02-04 17:05:45,808 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:46,126 - INFO - Epoch 177: train_loss=0.4319
2025-02-04 17:05:46,370 - INFO - Epoch 177: train_loss=0.7744
2025-02-04 17:05:46,678 - INFO - Epoch 177: val_loss=3.9271, val_acc=0.00%
2025-02-04 17:05:46,681 - INFO - Epoch 177: EPOCH_AVG_TRAIN_LOSS=0.6031
2025-02-04 17:05:46,684 - INFO - #################### Training epoch 178 ####################
2025-02-04 17:05:46,684 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:47,001 - INFO - Epoch 178: train_loss=0.5364
2025-02-04 17:05:47,244 - INFO - Epoch 178: train_loss=0.5643
2025-02-04 17:05:47,548 - INFO - Epoch 178: val_loss=3.9097, val_acc=0.00%
2025-02-04 17:05:47,551 - INFO - Epoch 178: EPOCH_AVG_TRAIN_LOSS=0.5504
2025-02-04 17:05:47,554 - INFO - #################### Training epoch 179 ####################
2025-02-04 17:05:47,554 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:47,867 - INFO - Epoch 179: train_loss=0.5835
2025-02-04 17:05:48,111 - INFO - Epoch 179: train_loss=0.4758
2025-02-04 17:05:48,420 - INFO - Epoch 179: val_loss=3.9274, val_acc=0.00%
2025-02-04 17:05:48,423 - INFO - Epoch 179: EPOCH_AVG_TRAIN_LOSS=0.5296
2025-02-04 17:05:48,426 - INFO - #################### Training epoch 180 ####################
2025-02-04 17:05:48,426 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:48,741 - INFO - Epoch 180: train_loss=0.6381
2025-02-04 17:05:48,985 - INFO - Epoch 180: train_loss=0.3557
2025-02-04 17:05:49,294 - INFO - Epoch 180: val_loss=3.9268, val_acc=0.00%
2025-02-04 17:05:49,298 - INFO - Epoch 180: EPOCH_AVG_TRAIN_LOSS=0.4969
2025-02-04 17:05:49,300 - INFO - #################### Training epoch 181 ####################
2025-02-04 17:05:49,300 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:49,614 - INFO - Epoch 181: train_loss=0.5855
2025-02-04 17:05:49,858 - INFO - Epoch 181: train_loss=0.4730
2025-02-04 17:05:50,169 - INFO - Epoch 181: val_loss=3.9297, val_acc=0.00%
2025-02-04 17:05:50,173 - INFO - Epoch 181: EPOCH_AVG_TRAIN_LOSS=0.5293
2025-02-04 17:05:50,175 - INFO - #################### Training epoch 182 ####################
2025-02-04 17:05:50,175 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:50,492 - INFO - Epoch 182: train_loss=0.6307
2025-02-04 17:05:50,735 - INFO - Epoch 182: train_loss=0.3819
2025-02-04 17:05:51,047 - INFO - Epoch 182: val_loss=3.9345, val_acc=0.00%
2025-02-04 17:05:51,051 - INFO - Epoch 182: EPOCH_AVG_TRAIN_LOSS=0.5063
2025-02-04 17:05:51,053 - INFO - #################### Training epoch 183 ####################
2025-02-04 17:05:51,053 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:51,372 - INFO - Epoch 183: train_loss=0.6070
2025-02-04 17:05:51,615 - INFO - Epoch 183: train_loss=0.4359
2025-02-04 17:05:51,926 - INFO - Epoch 183: val_loss=3.9334, val_acc=0.00%
2025-02-04 17:05:51,929 - INFO - Epoch 183: EPOCH_AVG_TRAIN_LOSS=0.5214
2025-02-04 17:05:51,932 - INFO - #################### Training epoch 184 ####################
2025-02-04 17:05:51,932 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:52,246 - INFO - Epoch 184: train_loss=0.5735
2025-02-04 17:05:52,489 - INFO - Epoch 184: train_loss=0.4933
2025-02-04 17:05:52,794 - INFO - Epoch 184: val_loss=3.9307, val_acc=0.00%
2025-02-04 17:05:52,798 - INFO - Epoch 184: EPOCH_AVG_TRAIN_LOSS=0.5334
2025-02-04 17:05:52,800 - INFO - #################### Training epoch 185 ####################
2025-02-04 17:05:52,800 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:53,119 - INFO - Epoch 185: train_loss=0.4107
2025-02-04 17:05:53,363 - INFO - Epoch 185: train_loss=0.8221
2025-02-04 17:05:53,672 - INFO - Epoch 185: val_loss=3.9235, val_acc=0.00%
2025-02-04 17:05:53,676 - INFO - Epoch 185: EPOCH_AVG_TRAIN_LOSS=0.6164
2025-02-04 17:05:53,678 - INFO - #################### Training epoch 186 ####################
2025-02-04 17:05:53,678 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:54,000 - INFO - Epoch 186: train_loss=0.5065
2025-02-04 17:05:54,243 - INFO - Epoch 186: train_loss=0.6357
2025-02-04 17:05:54,549 - INFO - Epoch 186: val_loss=3.9268, val_acc=0.00%
2025-02-04 17:05:54,552 - INFO - Epoch 186: EPOCH_AVG_TRAIN_LOSS=0.5711
2025-02-04 17:05:54,555 - INFO - #################### Training epoch 187 ####################
2025-02-04 17:05:54,555 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:54,868 - INFO - Epoch 187: train_loss=0.3882
2025-02-04 17:05:55,112 - INFO - Epoch 187: train_loss=0.8702
2025-02-04 17:05:55,426 - INFO - Epoch 187: val_loss=3.9349, val_acc=0.00%
2025-02-04 17:05:55,430 - INFO - Epoch 187: EPOCH_AVG_TRAIN_LOSS=0.6292
2025-02-04 17:05:55,432 - INFO - #################### Training epoch 188 ####################
2025-02-04 17:05:55,432 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:55,747 - INFO - Epoch 188: train_loss=0.4031
2025-02-04 17:05:55,991 - INFO - Epoch 188: train_loss=0.8410
2025-02-04 17:05:56,303 - INFO - Epoch 188: val_loss=3.9337, val_acc=0.00%
2025-02-04 17:05:56,307 - INFO - Epoch 188: EPOCH_AVG_TRAIN_LOSS=0.6221
2025-02-04 17:05:56,309 - INFO - #################### Training epoch 189 ####################
2025-02-04 17:05:56,310 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:56,628 - INFO - Epoch 189: train_loss=0.5713
2025-02-04 17:05:56,872 - INFO - Epoch 189: train_loss=0.5010
2025-02-04 17:05:57,180 - INFO - Epoch 189: val_loss=3.9288, val_acc=0.00%
2025-02-04 17:05:57,183 - INFO - Epoch 189: EPOCH_AVG_TRAIN_LOSS=0.5362
2025-02-04 17:05:57,186 - INFO - #################### Training epoch 190 ####################
2025-02-04 17:05:57,186 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:57,501 - INFO - Epoch 190: train_loss=0.4546
2025-02-04 17:05:57,745 - INFO - Epoch 190: train_loss=0.7316
2025-02-04 17:05:58,055 - INFO - Epoch 190: val_loss=3.9334, val_acc=0.00%
2025-02-04 17:05:58,059 - INFO - Epoch 190: EPOCH_AVG_TRAIN_LOSS=0.5931
2025-02-04 17:05:58,061 - INFO - #################### Training epoch 191 ####################
2025-02-04 17:05:58,061 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:58,378 - INFO - Epoch 191: train_loss=0.4465
2025-02-04 17:05:58,622 - INFO - Epoch 191: train_loss=0.7497
2025-02-04 17:05:58,934 - INFO - Epoch 191: val_loss=3.9211, val_acc=0.00%
2025-02-04 17:05:58,938 - INFO - Epoch 191: EPOCH_AVG_TRAIN_LOSS=0.5981
2025-02-04 17:05:58,941 - INFO - #################### Training epoch 192 ####################
2025-02-04 17:05:58,941 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:05:59,257 - INFO - Epoch 192: train_loss=0.4340
2025-02-04 17:05:59,501 - INFO - Epoch 192: train_loss=0.7781
2025-02-04 17:05:59,812 - INFO - Epoch 192: val_loss=3.9360, val_acc=0.00%
2025-02-04 17:05:59,816 - INFO - Epoch 192: EPOCH_AVG_TRAIN_LOSS=0.6061
2025-02-04 17:05:59,819 - INFO - #################### Training epoch 193 ####################
2025-02-04 17:05:59,819 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:06:00,130 - INFO - Epoch 193: train_loss=0.4933
2025-02-04 17:06:00,374 - INFO - Epoch 193: train_loss=0.6618
2025-02-04 17:06:00,682 - INFO - Epoch 193: val_loss=3.9246, val_acc=0.00%
2025-02-04 17:06:00,686 - INFO - Epoch 193: EPOCH_AVG_TRAIN_LOSS=0.5775
2025-02-04 17:06:00,688 - INFO - #################### Training epoch 194 ####################
2025-02-04 17:06:00,688 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:06:01,003 - INFO - Epoch 194: train_loss=0.5140
2025-02-04 17:06:01,247 - INFO - Epoch 194: train_loss=0.6122
2025-02-04 17:06:01,558 - INFO - Epoch 194: val_loss=3.9228, val_acc=0.00%
2025-02-04 17:06:01,562 - INFO - Epoch 194: EPOCH_AVG_TRAIN_LOSS=0.5631
2025-02-04 17:06:01,564 - INFO - #################### Training epoch 195 ####################
2025-02-04 17:06:01,564 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:06:01,880 - INFO - Epoch 195: train_loss=0.5593
2025-02-04 17:06:02,124 - INFO - Epoch 195: train_loss=0.5183
2025-02-04 17:06:02,431 - INFO - Epoch 195: val_loss=3.9072, val_acc=0.00%
2025-02-04 17:06:02,435 - INFO - Epoch 195: EPOCH_AVG_TRAIN_LOSS=0.5388
2025-02-04 17:06:02,437 - INFO - #################### Training epoch 196 ####################
2025-02-04 17:06:02,438 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:06:02,755 - INFO - Epoch 196: train_loss=0.5168
2025-02-04 17:06:02,999 - INFO - Epoch 196: train_loss=0.6083
2025-02-04 17:06:03,312 - INFO - Epoch 196: val_loss=3.9322, val_acc=0.00%
2025-02-04 17:06:03,316 - INFO - Epoch 196: EPOCH_AVG_TRAIN_LOSS=0.5626
2025-02-04 17:06:03,319 - INFO - #################### Training epoch 197 ####################
2025-02-04 17:06:03,319 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:06:03,635 - INFO - Epoch 197: train_loss=0.6536
2025-02-04 17:06:03,879 - INFO - Epoch 197: train_loss=0.3289
2025-02-04 17:06:04,190 - INFO - Epoch 197: val_loss=3.9332, val_acc=0.00%
2025-02-04 17:06:04,194 - INFO - Epoch 197: EPOCH_AVG_TRAIN_LOSS=0.4912
2025-02-04 17:06:04,196 - INFO - #################### Training epoch 198 ####################
2025-02-04 17:06:04,196 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:06:04,513 - INFO - Epoch 198: train_loss=0.4927
2025-02-04 17:06:04,757 - INFO - Epoch 198: train_loss=0.6637
2025-02-04 17:06:05,067 - INFO - Epoch 198: val_loss=3.9347, val_acc=0.00%
2025-02-04 17:06:05,071 - INFO - Epoch 198: EPOCH_AVG_TRAIN_LOSS=0.5782
2025-02-04 17:06:05,073 - INFO - #################### Training epoch 199 ####################
2025-02-04 17:06:05,073 - INFO - Current Learning Rate: 1.525879e-08
2025-02-04 17:06:05,389 - INFO - Epoch 199: train_loss=0.5821
2025-02-04 17:06:05,633 - INFO - Epoch 199: train_loss=0.4784
2025-02-04 17:06:05,948 - INFO - Epoch 199: val_loss=3.9401, val_acc=0.00%
2025-02-04 17:06:05,952 - INFO - Epoch 199: EPOCH_AVG_TRAIN_LOSS=0.5302
2025-02-04 17:06:06,120 - INFO - Model saved.
