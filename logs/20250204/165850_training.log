2025-02-04 16:58:57,987 - INFO - Starting training with the following parameters:
2025-02-04 16:58:57,988 - INFO - | Parameter       | Value               |
|-----------------|---------------------|
| attention       | Scaled Dot-Product|
| d_model         | 128            |
| n_heads         | 8              |
| d_f             | 128            |
| num_layers      | 3              |
| d_input         | 32             |
| num_classes     | 3              |
| dropout         | 0.1            |
| learning_rate   | 0.01           |
| epochs          | 500            |
| batch_size      | 32             |

2025-02-04 16:58:58,672 - INFO - Epoch 0: val_loss=1.1543, val_acc=33.33%
2025-02-04 16:58:58,822 - INFO - #################### Training epoch 0 ####################
2025-02-04 16:58:58,822 - INFO - Current Learning Rate: 1.000000e-02
2025-02-04 16:58:59,043 - INFO - Epoch 0: train_loss=1.2733
2025-02-04 16:58:59,642 - INFO - Epoch 0: val_loss=2.6059, val_acc=33.33%
2025-02-04 16:58:59,646 - INFO - Epoch 0: EPOCH_AVG_TRAIN_LOSS=1.2733
2025-02-04 16:58:59,687 - INFO - #################### Training epoch 1 ####################
2025-02-04 16:58:59,687 - INFO - Current Learning Rate: 1.000000e-02
2025-02-04 16:59:00,078 - INFO - Epoch 1: train_loss=2.7928
2025-02-04 16:59:00,682 - INFO - Epoch 1: val_loss=2.7515, val_acc=33.33%
2025-02-04 16:59:00,686 - INFO - Epoch 1: EPOCH_AVG_TRAIN_LOSS=2.7928
2025-02-04 16:59:00,710 - INFO - #################### Training epoch 2 ####################
2025-02-04 16:59:00,710 - INFO - Current Learning Rate: 1.000000e-02
2025-02-04 16:59:01,154 - INFO - Epoch 2: train_loss=2.8503
2025-02-04 16:59:01,734 - INFO - Epoch 2: val_loss=2.0142, val_acc=33.33%
2025-02-04 16:59:01,737 - INFO - Epoch 2: EPOCH_AVG_TRAIN_LOSS=2.8503
2025-02-04 16:59:01,763 - INFO - #################### Training epoch 3 ####################
2025-02-04 16:59:01,763 - INFO - Current Learning Rate: 1.000000e-02
2025-02-04 16:59:02,138 - INFO - Epoch 3: train_loss=1.9871
2025-02-04 16:59:02,654 - INFO - Epoch 3: val_loss=1.6184, val_acc=66.67%
2025-02-04 16:59:02,657 - INFO - Epoch 3: EPOCH_AVG_TRAIN_LOSS=1.9871
2025-02-04 16:59:02,685 - INFO - #################### Training epoch 4 ####################
2025-02-04 16:59:02,685 - INFO - Current Learning Rate: 1.000000e-02
2025-02-04 16:59:03,081 - INFO - Epoch 4: train_loss=1.9533
2025-02-04 16:59:03,673 - INFO - Epoch 4: val_loss=1.3966, val_acc=66.67%
2025-02-04 16:59:03,677 - INFO - Epoch 4: EPOCH_AVG_TRAIN_LOSS=1.9533
2025-02-04 16:59:03,704 - INFO - #################### Training epoch 5 ####################
2025-02-04 16:59:03,704 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:59:04,110 - INFO - Epoch 5: train_loss=1.7867
2025-02-04 16:59:04,698 - INFO - Epoch 5: val_loss=1.3262, val_acc=66.67%
2025-02-04 16:59:04,701 - INFO - Epoch 5: EPOCH_AVG_TRAIN_LOSS=1.7867
2025-02-04 16:59:04,730 - INFO - #################### Training epoch 6 ####################
2025-02-04 16:59:04,730 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:59:05,112 - INFO - Epoch 6: train_loss=1.7012
2025-02-04 16:59:05,602 - INFO - Epoch 6: val_loss=1.2675, val_acc=66.67%
2025-02-04 16:59:05,605 - INFO - Epoch 6: EPOCH_AVG_TRAIN_LOSS=1.7012
2025-02-04 16:59:05,634 - INFO - #################### Training epoch 7 ####################
2025-02-04 16:59:05,634 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:59:06,025 - INFO - Epoch 7: train_loss=1.6226
2025-02-04 16:59:06,577 - INFO - Epoch 7: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:06,581 - INFO - Epoch 7: EPOCH_AVG_TRAIN_LOSS=1.6226
2025-02-04 16:59:06,583 - INFO - #################### Training epoch 8 ####################
2025-02-04 16:59:06,583 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:59:06,980 - INFO - Epoch 8: train_loss=nan
2025-02-04 16:59:07,597 - INFO - Epoch 8: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:07,601 - INFO - Epoch 8: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:07,603 - INFO - #################### Training epoch 9 ####################
2025-02-04 16:59:07,603 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:59:08,011 - INFO - Epoch 9: train_loss=nan
2025-02-04 16:59:08,616 - INFO - Epoch 9: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:08,619 - INFO - Epoch 9: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:08,622 - INFO - #################### Training epoch 10 ####################
2025-02-04 16:59:08,622 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:59:09,049 - INFO - Epoch 10: train_loss=nan
2025-02-04 16:59:09,622 - INFO - Epoch 10: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:09,626 - INFO - Epoch 10: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:09,628 - INFO - #################### Training epoch 11 ####################
2025-02-04 16:59:09,628 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:59:10,003 - INFO - Epoch 11: train_loss=nan
2025-02-04 16:59:10,523 - INFO - Epoch 11: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:10,526 - INFO - Epoch 11: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:10,529 - INFO - #################### Training epoch 12 ####################
2025-02-04 16:59:10,529 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:59:10,919 - INFO - Epoch 12: train_loss=nan
2025-02-04 16:59:11,464 - INFO - Epoch 12: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:11,467 - INFO - Epoch 12: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:11,470 - INFO - #################### Training epoch 13 ####################
2025-02-04 16:59:11,470 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:59:11,865 - INFO - Epoch 13: train_loss=nan
2025-02-04 16:59:12,485 - INFO - Epoch 13: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:12,488 - INFO - Epoch 13: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:12,491 - INFO - #################### Training epoch 14 ####################
2025-02-04 16:59:12,491 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:59:12,894 - INFO - Epoch 14: train_loss=nan
2025-02-04 16:59:13,498 - INFO - Epoch 14: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:13,502 - INFO - Epoch 14: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:13,504 - INFO - #################### Training epoch 15 ####################
2025-02-04 16:59:13,504 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:59:13,913 - INFO - Epoch 15: train_loss=nan
2025-02-04 16:59:14,468 - INFO - Epoch 15: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:14,472 - INFO - Epoch 15: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:14,474 - INFO - #################### Training epoch 16 ####################
2025-02-04 16:59:14,474 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:59:14,846 - INFO - Epoch 16: train_loss=nan
2025-02-04 16:59:15,362 - INFO - Epoch 16: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:15,366 - INFO - Epoch 16: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:15,368 - INFO - #################### Training epoch 17 ####################
2025-02-04 16:59:15,368 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:59:15,756 - INFO - Epoch 17: train_loss=nan
2025-02-04 16:59:16,300 - INFO - Epoch 17: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:16,303 - INFO - Epoch 17: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:16,305 - INFO - #################### Training epoch 18 ####################
2025-02-04 16:59:16,305 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:59:16,700 - INFO - Epoch 18: train_loss=nan
2025-02-04 16:59:17,296 - INFO - Epoch 18: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:17,299 - INFO - Epoch 18: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:17,302 - INFO - #################### Training epoch 19 ####################
2025-02-04 16:59:17,302 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:59:17,704 - INFO - Epoch 19: train_loss=nan
2025-02-04 16:59:18,311 - INFO - Epoch 19: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:18,315 - INFO - Epoch 19: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:18,317 - INFO - #################### Training epoch 20 ####################
2025-02-04 16:59:18,317 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:59:18,787 - INFO - Epoch 20: train_loss=nan
2025-02-04 16:59:19,377 - INFO - Epoch 20: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:19,381 - INFO - Epoch 20: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:19,383 - INFO - #################### Training epoch 21 ####################
2025-02-04 16:59:19,383 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:59:19,756 - INFO - Epoch 21: train_loss=nan
2025-02-04 16:59:20,273 - INFO - Epoch 21: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:20,276 - INFO - Epoch 21: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:20,278 - INFO - #################### Training epoch 22 ####################
2025-02-04 16:59:20,278 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:59:20,670 - INFO - Epoch 22: train_loss=nan
2025-02-04 16:59:21,222 - INFO - Epoch 22: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:21,225 - INFO - Epoch 22: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:21,228 - INFO - #################### Training epoch 23 ####################
2025-02-04 16:59:21,228 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:59:21,622 - INFO - Epoch 23: train_loss=nan
2025-02-04 16:59:22,236 - INFO - Epoch 23: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:22,240 - INFO - Epoch 23: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:22,242 - INFO - #################### Training epoch 24 ####################
2025-02-04 16:59:22,242 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:59:22,647 - INFO - Epoch 24: train_loss=nan
2025-02-04 16:59:23,243 - INFO - Epoch 24: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:23,246 - INFO - Epoch 24: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:23,249 - INFO - #################### Training epoch 25 ####################
2025-02-04 16:59:23,249 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 16:59:23,638 - INFO - Epoch 25: train_loss=nan
2025-02-04 16:59:24,175 - INFO - Epoch 25: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:24,178 - INFO - Epoch 25: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:24,180 - INFO - #################### Training epoch 26 ####################
2025-02-04 16:59:24,180 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 16:59:24,550 - INFO - Epoch 26: train_loss=nan
2025-02-04 16:59:25,063 - INFO - Epoch 26: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:25,066 - INFO - Epoch 26: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:25,069 - INFO - #################### Training epoch 27 ####################
2025-02-04 16:59:25,069 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 16:59:25,455 - INFO - Epoch 27: train_loss=nan
2025-02-04 16:59:25,990 - INFO - Epoch 27: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:25,994 - INFO - Epoch 27: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:25,997 - INFO - #################### Training epoch 28 ####################
2025-02-04 16:59:25,997 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 16:59:26,388 - INFO - Epoch 28: train_loss=nan
2025-02-04 16:59:26,945 - INFO - Epoch 28: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:26,949 - INFO - Epoch 28: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:26,952 - INFO - #################### Training epoch 29 ####################
2025-02-04 16:59:26,952 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 16:59:27,344 - INFO - Epoch 29: train_loss=nan
2025-02-04 16:59:27,964 - INFO - Epoch 29: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:27,967 - INFO - Epoch 29: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:27,970 - INFO - #################### Training epoch 30 ####################
2025-02-04 16:59:27,970 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 16:59:28,537 - INFO - Epoch 30: train_loss=nan
2025-02-04 16:59:29,288 - INFO - Epoch 30: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:29,296 - INFO - Epoch 30: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:29,307 - INFO - #################### Training epoch 31 ####################
2025-02-04 16:59:29,307 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 16:59:29,712 - INFO - Epoch 31: train_loss=nan
2025-02-04 16:59:30,400 - INFO - Epoch 31: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:30,409 - INFO - Epoch 31: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:30,428 - INFO - #################### Training epoch 32 ####################
2025-02-04 16:59:30,429 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 16:59:30,794 - INFO - Epoch 32: train_loss=nan
2025-02-04 16:59:31,398 - INFO - Epoch 32: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:31,403 - INFO - Epoch 32: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:31,413 - INFO - #################### Training epoch 33 ####################
2025-02-04 16:59:31,413 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 16:59:31,941 - INFO - Epoch 33: train_loss=nan
2025-02-04 16:59:32,584 - INFO - Epoch 33: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:32,589 - INFO - Epoch 33: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:32,593 - INFO - #################### Training epoch 34 ####################
2025-02-04 16:59:32,593 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 16:59:33,147 - INFO - Epoch 34: train_loss=nan
2025-02-04 16:59:33,884 - INFO - Epoch 34: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:33,889 - INFO - Epoch 34: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:33,895 - INFO - #################### Training epoch 35 ####################
2025-02-04 16:59:33,895 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 16:59:34,441 - INFO - Epoch 35: train_loss=nan
2025-02-04 16:59:35,177 - INFO - Epoch 35: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:35,182 - INFO - Epoch 35: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:35,190 - INFO - #################### Training epoch 36 ####################
2025-02-04 16:59:35,190 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 16:59:35,810 - INFO - Epoch 36: train_loss=nan
2025-02-04 16:59:36,487 - INFO - Epoch 36: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:36,492 - INFO - Epoch 36: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:36,503 - INFO - #################### Training epoch 37 ####################
2025-02-04 16:59:36,503 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 16:59:37,088 - INFO - Epoch 37: train_loss=nan
2025-02-04 16:59:37,723 - INFO - Epoch 37: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:37,727 - INFO - Epoch 37: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:37,730 - INFO - #################### Training epoch 38 ####################
2025-02-04 16:59:37,730 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 16:59:38,307 - INFO - Epoch 38: train_loss=nan
2025-02-04 16:59:38,957 - INFO - Epoch 38: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:38,962 - INFO - Epoch 38: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:38,970 - INFO - #################### Training epoch 39 ####################
2025-02-04 16:59:38,970 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 16:59:39,550 - INFO - Epoch 39: train_loss=nan
2025-02-04 16:59:40,189 - INFO - Epoch 39: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:40,194 - INFO - Epoch 39: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:40,197 - INFO - #################### Training epoch 40 ####################
2025-02-04 16:59:40,197 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 16:59:40,840 - INFO - Epoch 40: train_loss=nan
2025-02-04 16:59:41,481 - INFO - Epoch 40: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:41,485 - INFO - Epoch 40: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:41,496 - INFO - #################### Training epoch 41 ####################
2025-02-04 16:59:41,496 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 16:59:42,065 - INFO - Epoch 41: train_loss=nan
2025-02-04 16:59:42,786 - INFO - Epoch 41: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:42,791 - INFO - Epoch 41: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:42,801 - INFO - #################### Training epoch 42 ####################
2025-02-04 16:59:42,801 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 16:59:43,335 - INFO - Epoch 42: train_loss=nan
2025-02-04 16:59:44,067 - INFO - Epoch 42: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:44,073 - INFO - Epoch 42: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:44,083 - INFO - #################### Training epoch 43 ####################
2025-02-04 16:59:44,083 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 16:59:44,628 - INFO - Epoch 43: train_loss=nan
2025-02-04 16:59:45,338 - INFO - Epoch 43: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:45,342 - INFO - Epoch 43: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:45,353 - INFO - #################### Training epoch 44 ####################
2025-02-04 16:59:45,353 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 16:59:45,989 - INFO - Epoch 44: train_loss=nan
2025-02-04 16:59:46,607 - INFO - Epoch 44: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:46,614 - INFO - Epoch 44: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:46,625 - INFO - #################### Training epoch 45 ####################
2025-02-04 16:59:46,625 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 16:59:47,257 - INFO - Epoch 45: train_loss=nan
2025-02-04 16:59:47,896 - INFO - Epoch 45: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:47,901 - INFO - Epoch 45: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:47,912 - INFO - #################### Training epoch 46 ####################
2025-02-04 16:59:47,912 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 16:59:48,586 - INFO - Epoch 46: train_loss=nan
2025-02-04 16:59:49,190 - INFO - Epoch 46: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:49,196 - INFO - Epoch 46: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:49,207 - INFO - #################### Training epoch 47 ####################
2025-02-04 16:59:49,207 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 16:59:49,810 - INFO - Epoch 47: train_loss=nan
2025-02-04 16:59:50,468 - INFO - Epoch 47: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:50,472 - INFO - Epoch 47: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:50,483 - INFO - #################### Training epoch 48 ####################
2025-02-04 16:59:50,483 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 16:59:51,088 - INFO - Epoch 48: train_loss=nan
2025-02-04 16:59:51,770 - INFO - Epoch 48: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:51,775 - INFO - Epoch 48: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:51,785 - INFO - #################### Training epoch 49 ####################
2025-02-04 16:59:51,785 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 16:59:52,416 - INFO - Epoch 49: train_loss=nan
2025-02-04 16:59:53,150 - INFO - Epoch 49: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:53,155 - INFO - Epoch 49: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:53,166 - INFO - #################### Training epoch 50 ####################
2025-02-04 16:59:53,166 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 16:59:53,760 - INFO - Epoch 50: train_loss=nan
2025-02-04 16:59:54,379 - INFO - Epoch 50: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:54,384 - INFO - Epoch 50: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:54,395 - INFO - #################### Training epoch 51 ####################
2025-02-04 16:59:54,395 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 16:59:54,972 - INFO - Epoch 51: train_loss=nan
2025-02-04 16:59:55,645 - INFO - Epoch 51: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:55,650 - INFO - Epoch 51: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:55,660 - INFO - #################### Training epoch 52 ####################
2025-02-04 16:59:55,660 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 16:59:56,116 - INFO - Epoch 52: train_loss=nan
2025-02-04 16:59:56,833 - INFO - Epoch 52: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:56,838 - INFO - Epoch 52: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:56,848 - INFO - #################### Training epoch 53 ####################
2025-02-04 16:59:56,848 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 16:59:57,232 - INFO - Epoch 53: train_loss=nan
2025-02-04 16:59:57,911 - INFO - Epoch 53: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:57,916 - INFO - Epoch 53: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:57,927 - INFO - #################### Training epoch 54 ####################
2025-02-04 16:59:57,927 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 16:59:58,333 - INFO - Epoch 54: train_loss=nan
2025-02-04 16:59:58,991 - INFO - Epoch 54: val_loss=nan, val_acc=66.67%
2025-02-04 16:59:58,996 - INFO - Epoch 54: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:59:59,007 - INFO - #################### Training epoch 55 ####################
2025-02-04 16:59:59,008 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 16:59:59,526 - INFO - Epoch 55: train_loss=nan
2025-02-04 17:00:00,137 - INFO - Epoch 55: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:00,142 - INFO - Epoch 55: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:00,152 - INFO - #################### Training epoch 56 ####################
2025-02-04 17:00:00,152 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 17:00:00,690 - INFO - Epoch 56: train_loss=nan
2025-02-04 17:00:01,408 - INFO - Epoch 56: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:01,412 - INFO - Epoch 56: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:01,423 - INFO - #################### Training epoch 57 ####################
2025-02-04 17:00:01,423 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 17:00:01,889 - INFO - Epoch 57: train_loss=nan
2025-02-04 17:00:02,651 - INFO - Epoch 57: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:02,656 - INFO - Epoch 57: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:02,666 - INFO - #################### Training epoch 58 ####################
2025-02-04 17:00:02,666 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 17:00:03,157 - INFO - Epoch 58: train_loss=nan
2025-02-04 17:00:03,898 - INFO - Epoch 58: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:03,902 - INFO - Epoch 58: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:03,913 - INFO - #################### Training epoch 59 ####################
2025-02-04 17:00:03,913 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 17:00:04,320 - INFO - Epoch 59: train_loss=nan
2025-02-04 17:00:05,039 - INFO - Epoch 59: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:05,048 - INFO - Epoch 59: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:05,067 - INFO - #################### Training epoch 60 ####################
2025-02-04 17:00:05,067 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 17:00:05,453 - INFO - Epoch 60: train_loss=nan
2025-02-04 17:00:06,065 - INFO - Epoch 60: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:06,070 - INFO - Epoch 60: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:06,080 - INFO - #################### Training epoch 61 ####################
2025-02-04 17:00:06,080 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 17:00:06,552 - INFO - Epoch 61: train_loss=nan
2025-02-04 17:00:07,156 - INFO - Epoch 61: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:07,161 - INFO - Epoch 61: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:07,171 - INFO - #################### Training epoch 62 ####################
2025-02-04 17:00:07,171 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 17:00:07,712 - INFO - Epoch 62: train_loss=nan
2025-02-04 17:00:08,393 - INFO - Epoch 62: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:08,397 - INFO - Epoch 62: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:08,400 - INFO - #################### Training epoch 63 ####################
2025-02-04 17:00:08,400 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 17:00:08,884 - INFO - Epoch 63: train_loss=nan
2025-02-04 17:00:09,652 - INFO - Epoch 63: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:09,657 - INFO - Epoch 63: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:09,667 - INFO - #################### Training epoch 64 ####################
2025-02-04 17:00:09,667 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 17:00:10,152 - INFO - Epoch 64: train_loss=nan
2025-02-04 17:00:10,897 - INFO - Epoch 64: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:10,902 - INFO - Epoch 64: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:10,913 - INFO - #################### Training epoch 65 ####################
2025-02-04 17:00:10,913 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 17:00:11,318 - INFO - Epoch 65: train_loss=nan
2025-02-04 17:00:12,049 - INFO - Epoch 65: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:12,058 - INFO - Epoch 65: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:12,077 - INFO - #################### Training epoch 66 ####################
2025-02-04 17:00:12,077 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 17:00:12,468 - INFO - Epoch 66: train_loss=nan
2025-02-04 17:00:13,133 - INFO - Epoch 66: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:13,138 - INFO - Epoch 66: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:13,151 - INFO - #################### Training epoch 67 ####################
2025-02-04 17:00:13,151 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 17:00:13,570 - INFO - Epoch 67: train_loss=nan
2025-02-04 17:00:14,178 - INFO - Epoch 67: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:14,182 - INFO - Epoch 67: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:14,193 - INFO - #################### Training epoch 68 ####################
2025-02-04 17:00:14,193 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 17:00:14,727 - INFO - Epoch 68: train_loss=nan
2025-02-04 17:00:15,376 - INFO - Epoch 68: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:15,381 - INFO - Epoch 68: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:15,392 - INFO - #################### Training epoch 69 ####################
2025-02-04 17:00:15,392 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 17:00:15,892 - INFO - Epoch 69: train_loss=nan
2025-02-04 17:00:16,629 - INFO - Epoch 69: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:16,633 - INFO - Epoch 69: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:16,644 - INFO - #################### Training epoch 70 ####################
2025-02-04 17:00:16,644 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 17:00:17,063 - INFO - Epoch 70: train_loss=nan
2025-02-04 17:00:17,896 - INFO - Epoch 70: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:17,901 - INFO - Epoch 70: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:17,911 - INFO - #################### Training epoch 71 ####################
2025-02-04 17:00:17,911 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 17:00:18,316 - INFO - Epoch 71: train_loss=nan
2025-02-04 17:00:19,034 - INFO - Epoch 71: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:19,043 - INFO - Epoch 71: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:19,062 - INFO - #################### Training epoch 72 ####################
2025-02-04 17:00:19,062 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 17:00:19,439 - INFO - Epoch 72: train_loss=nan
2025-02-04 17:00:20,034 - INFO - Epoch 72: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:20,038 - INFO - Epoch 72: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:20,049 - INFO - #################### Training epoch 73 ####################
2025-02-04 17:00:20,049 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 17:00:20,543 - INFO - Epoch 73: train_loss=nan
2025-02-04 17:00:21,182 - INFO - Epoch 73: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:21,187 - INFO - Epoch 73: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:21,197 - INFO - #################### Training epoch 74 ####################
2025-02-04 17:00:21,197 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 17:00:21,779 - INFO - Epoch 74: train_loss=nan
2025-02-04 17:00:22,420 - INFO - Epoch 74: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:22,425 - INFO - Epoch 74: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:22,435 - INFO - #################### Training epoch 75 ####################
2025-02-04 17:00:22,435 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 17:00:22,988 - INFO - Epoch 75: train_loss=nan
2025-02-04 17:00:23,614 - INFO - Epoch 75: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:23,619 - INFO - Epoch 75: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:23,630 - INFO - #################### Training epoch 76 ####################
2025-02-04 17:00:23,630 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 17:00:24,169 - INFO - Epoch 76: train_loss=nan
2025-02-04 17:00:24,905 - INFO - Epoch 76: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:24,909 - INFO - Epoch 76: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:24,914 - INFO - #################### Training epoch 77 ####################
2025-02-04 17:00:24,914 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:25,391 - INFO - Epoch 77: train_loss=nan
2025-02-04 17:00:26,122 - INFO - Epoch 77: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:26,126 - INFO - Epoch 77: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:26,137 - INFO - #################### Training epoch 78 ####################
2025-02-04 17:00:26,137 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:26,615 - INFO - Epoch 78: train_loss=nan
2025-02-04 17:00:27,306 - INFO - Epoch 78: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:27,311 - INFO - Epoch 78: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:27,322 - INFO - #################### Training epoch 79 ####################
2025-02-04 17:00:27,322 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:27,794 - INFO - Epoch 79: train_loss=nan
2025-02-04 17:00:28,510 - INFO - Epoch 79: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:28,515 - INFO - Epoch 79: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:28,525 - INFO - #################### Training epoch 80 ####################
2025-02-04 17:00:28,525 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:28,912 - INFO - Epoch 80: train_loss=nan
2025-02-04 17:00:29,596 - INFO - Epoch 80: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:29,601 - INFO - Epoch 80: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:29,612 - INFO - #################### Training epoch 81 ####################
2025-02-04 17:00:29,612 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:30,013 - INFO - Epoch 81: train_loss=nan
2025-02-04 17:00:30,639 - INFO - Epoch 81: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:30,643 - INFO - Epoch 81: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:30,646 - INFO - #################### Training epoch 82 ####################
2025-02-04 17:00:30,646 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:31,222 - INFO - Epoch 82: train_loss=nan
2025-02-04 17:00:31,852 - INFO - Epoch 82: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:31,856 - INFO - Epoch 82: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:31,861 - INFO - #################### Training epoch 83 ####################
2025-02-04 17:00:31,861 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:32,412 - INFO - Epoch 83: train_loss=nan
2025-02-04 17:00:33,153 - INFO - Epoch 83: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:33,156 - INFO - Epoch 83: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:33,159 - INFO - #################### Training epoch 84 ####################
2025-02-04 17:00:33,159 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:33,639 - INFO - Epoch 84: train_loss=nan
2025-02-04 17:00:34,377 - INFO - Epoch 84: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:34,382 - INFO - Epoch 84: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:34,392 - INFO - #################### Training epoch 85 ####################
2025-02-04 17:00:34,392 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:34,900 - INFO - Epoch 85: train_loss=nan
2025-02-04 17:00:35,570 - INFO - Epoch 85: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:35,575 - INFO - Epoch 85: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:35,586 - INFO - #################### Training epoch 86 ####################
2025-02-04 17:00:35,586 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:36,034 - INFO - Epoch 86: train_loss=nan
2025-02-04 17:00:36,765 - INFO - Epoch 86: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:36,771 - INFO - Epoch 86: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:36,781 - INFO - #################### Training epoch 87 ####################
2025-02-04 17:00:36,781 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:37,166 - INFO - Epoch 87: train_loss=nan
2025-02-04 17:00:37,844 - INFO - Epoch 87: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:37,849 - INFO - Epoch 87: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:37,860 - INFO - #################### Training epoch 88 ####################
2025-02-04 17:00:37,860 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:38,267 - INFO - Epoch 88: train_loss=nan
2025-02-04 17:00:38,896 - INFO - Epoch 88: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:38,900 - INFO - Epoch 88: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:38,902 - INFO - #################### Training epoch 89 ####################
2025-02-04 17:00:38,902 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:39,481 - INFO - Epoch 89: train_loss=nan
2025-02-04 17:00:40,099 - INFO - Epoch 89: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:40,103 - INFO - Epoch 89: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:40,113 - INFO - #################### Training epoch 90 ####################
2025-02-04 17:00:40,114 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:40,657 - INFO - Epoch 90: train_loss=nan
2025-02-04 17:00:41,388 - INFO - Epoch 90: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:41,392 - INFO - Epoch 90: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:41,395 - INFO - #################### Training epoch 91 ####################
2025-02-04 17:00:41,395 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:41,874 - INFO - Epoch 91: train_loss=nan
2025-02-04 17:00:42,634 - INFO - Epoch 91: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:42,638 - INFO - Epoch 91: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:42,648 - INFO - #################### Training epoch 92 ####################
2025-02-04 17:00:42,648 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:43,146 - INFO - Epoch 92: train_loss=nan
2025-02-04 17:00:43,890 - INFO - Epoch 92: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:43,894 - INFO - Epoch 92: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:43,905 - INFO - #################### Training epoch 93 ####################
2025-02-04 17:00:43,905 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:44,309 - INFO - Epoch 93: train_loss=nan
2025-02-04 17:00:45,039 - INFO - Epoch 93: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:45,048 - INFO - Epoch 93: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:45,067 - INFO - #################### Training epoch 94 ####################
2025-02-04 17:00:45,067 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:45,453 - INFO - Epoch 94: train_loss=nan
2025-02-04 17:00:46,084 - INFO - Epoch 94: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:46,088 - INFO - Epoch 94: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:46,100 - INFO - #################### Training epoch 95 ####################
2025-02-04 17:00:46,100 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:46,553 - INFO - Epoch 95: train_loss=nan
2025-02-04 17:00:47,159 - INFO - Epoch 95: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:47,164 - INFO - Epoch 95: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:47,174 - INFO - #################### Training epoch 96 ####################
2025-02-04 17:00:47,174 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:47,712 - INFO - Epoch 96: train_loss=nan
2025-02-04 17:00:48,376 - INFO - Epoch 96: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:48,380 - INFO - Epoch 96: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:48,383 - INFO - #################### Training epoch 97 ####################
2025-02-04 17:00:48,383 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:48,867 - INFO - Epoch 97: train_loss=nan
2025-02-04 17:00:49,652 - INFO - Epoch 97: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:49,656 - INFO - Epoch 97: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:49,666 - INFO - #################### Training epoch 98 ####################
2025-02-04 17:00:49,666 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:50,138 - INFO - Epoch 98: train_loss=nan
2025-02-04 17:00:50,888 - INFO - Epoch 98: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:50,893 - INFO - Epoch 98: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:50,904 - INFO - #################### Training epoch 99 ####################
2025-02-04 17:00:50,904 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:51,311 - INFO - Epoch 99: train_loss=nan
2025-02-04 17:00:52,024 - INFO - Epoch 99: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:52,033 - INFO - Epoch 99: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:52,052 - INFO - #################### Training epoch 100 ####################
2025-02-04 17:00:52,052 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:52,441 - INFO - Epoch 100: train_loss=nan
2025-02-04 17:00:53,053 - INFO - Epoch 100: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:53,058 - INFO - Epoch 100: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:53,069 - INFO - #################### Training epoch 101 ####################
2025-02-04 17:00:53,069 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:53,540 - INFO - Epoch 101: train_loss=nan
2025-02-04 17:00:54,147 - INFO - Epoch 101: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:54,152 - INFO - Epoch 101: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:54,162 - INFO - #################### Training epoch 102 ####################
2025-02-04 17:00:54,162 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:54,700 - INFO - Epoch 102: train_loss=nan
2025-02-04 17:00:55,366 - INFO - Epoch 102: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:55,370 - INFO - Epoch 102: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:55,374 - INFO - #################### Training epoch 103 ####################
2025-02-04 17:00:55,374 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:55,863 - INFO - Epoch 103: train_loss=nan
2025-02-04 17:00:56,640 - INFO - Epoch 103: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:56,645 - INFO - Epoch 103: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:56,655 - INFO - #################### Training epoch 104 ####################
2025-02-04 17:00:56,655 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:57,133 - INFO - Epoch 104: train_loss=nan
2025-02-04 17:00:57,889 - INFO - Epoch 104: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:57,894 - INFO - Epoch 104: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:57,904 - INFO - #################### Training epoch 105 ####################
2025-02-04 17:00:57,904 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:58,314 - INFO - Epoch 105: train_loss=nan
2025-02-04 17:00:59,119 - INFO - Epoch 105: val_loss=nan, val_acc=66.67%
2025-02-04 17:00:59,128 - INFO - Epoch 105: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:00:59,147 - INFO - #################### Training epoch 106 ####################
2025-02-04 17:00:59,147 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:59,560 - INFO - Epoch 106: train_loss=nan
2025-02-04 17:01:00,279 - INFO - Epoch 106: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:00,288 - INFO - Epoch 106: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:00,307 - INFO - #################### Training epoch 107 ####################
2025-02-04 17:01:00,307 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:00,702 - INFO - Epoch 107: train_loss=nan
2025-02-04 17:01:01,420 - INFO - Epoch 107: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:01,429 - INFO - Epoch 107: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:01,448 - INFO - #################### Training epoch 108 ####################
2025-02-04 17:01:01,448 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:01,840 - INFO - Epoch 108: train_loss=nan
2025-02-04 17:01:02,572 - INFO - Epoch 108: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:02,581 - INFO - Epoch 108: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:02,600 - INFO - #################### Training epoch 109 ####################
2025-02-04 17:01:02,600 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:02,998 - INFO - Epoch 109: train_loss=nan
2025-02-04 17:01:03,770 - INFO - Epoch 109: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:03,775 - INFO - Epoch 109: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:03,785 - INFO - #################### Training epoch 110 ####################
2025-02-04 17:01:03,785 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:04,188 - INFO - Epoch 110: train_loss=nan
2025-02-04 17:01:04,898 - INFO - Epoch 110: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:04,907 - INFO - Epoch 110: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:04,926 - INFO - #################### Training epoch 111 ####################
2025-02-04 17:01:04,926 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:05,322 - INFO - Epoch 111: train_loss=nan
2025-02-04 17:01:05,948 - INFO - Epoch 111: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:05,957 - INFO - Epoch 111: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:05,976 - INFO - #################### Training epoch 112 ####################
2025-02-04 17:01:05,976 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:06,433 - INFO - Epoch 112: train_loss=nan
2025-02-04 17:01:07,068 - INFO - Epoch 112: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:07,072 - INFO - Epoch 112: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:07,083 - INFO - #################### Training epoch 113 ####################
2025-02-04 17:01:07,083 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:07,592 - INFO - Epoch 113: train_loss=nan
2025-02-04 17:01:08,258 - INFO - Epoch 113: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:08,263 - INFO - Epoch 113: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:08,273 - INFO - #################### Training epoch 114 ####################
2025-02-04 17:01:08,273 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:08,729 - INFO - Epoch 114: train_loss=nan
2025-02-04 17:01:09,511 - INFO - Epoch 114: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:09,516 - INFO - Epoch 114: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:09,526 - INFO - #################### Training epoch 115 ####################
2025-02-04 17:01:09,526 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:09,957 - INFO - Epoch 115: train_loss=nan
2025-02-04 17:01:10,804 - INFO - Epoch 115: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:10,808 - INFO - Epoch 115: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:10,819 - INFO - #################### Training epoch 116 ####################
2025-02-04 17:01:10,819 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:11,230 - INFO - Epoch 116: train_loss=nan
2025-02-04 17:01:12,012 - INFO - Epoch 116: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:12,021 - INFO - Epoch 116: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:12,040 - INFO - #################### Training epoch 117 ####################
2025-02-04 17:01:12,040 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:12,444 - INFO - Epoch 117: train_loss=nan
2025-02-04 17:01:13,155 - INFO - Epoch 117: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:13,165 - INFO - Epoch 117: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:13,184 - INFO - #################### Training epoch 118 ####################
2025-02-04 17:01:13,184 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:13,577 - INFO - Epoch 118: train_loss=nan
2025-02-04 17:01:14,287 - INFO - Epoch 118: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:14,296 - INFO - Epoch 118: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:14,315 - INFO - #################### Training epoch 119 ####################
2025-02-04 17:01:14,315 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:14,712 - INFO - Epoch 119: train_loss=nan
2025-02-04 17:01:15,361 - INFO - Epoch 119: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:15,370 - INFO - Epoch 119: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:15,389 - INFO - #################### Training epoch 120 ####################
2025-02-04 17:01:15,389 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:15,824 - INFO - Epoch 120: train_loss=nan
2025-02-04 17:01:16,462 - INFO - Epoch 120: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:16,466 - INFO - Epoch 120: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:16,477 - INFO - #################### Training epoch 121 ####################
2025-02-04 17:01:16,477 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:16,981 - INFO - Epoch 121: train_loss=nan
2025-02-04 17:01:17,631 - INFO - Epoch 121: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:17,637 - INFO - Epoch 121: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:17,647 - INFO - #################### Training epoch 122 ####################
2025-02-04 17:01:17,647 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:18,120 - INFO - Epoch 122: train_loss=nan
2025-02-04 17:01:18,901 - INFO - Epoch 122: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:18,906 - INFO - Epoch 122: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:18,916 - INFO - #################### Training epoch 123 ####################
2025-02-04 17:01:18,916 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:19,408 - INFO - Epoch 123: train_loss=nan
2025-02-04 17:01:20,166 - INFO - Epoch 123: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:20,173 - INFO - Epoch 123: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:20,184 - INFO - #################### Training epoch 124 ####################
2025-02-04 17:01:20,184 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:20,602 - INFO - Epoch 124: train_loss=nan
2025-02-04 17:01:21,400 - INFO - Epoch 124: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:21,409 - INFO - Epoch 124: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:21,428 - INFO - #################### Training epoch 125 ####################
2025-02-04 17:01:21,428 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:21,842 - INFO - Epoch 125: train_loss=nan
2025-02-04 17:01:22,573 - INFO - Epoch 125: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:22,582 - INFO - Epoch 125: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:22,601 - INFO - #################### Training epoch 126 ####################
2025-02-04 17:01:22,601 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:23,001 - INFO - Epoch 126: train_loss=nan
2025-02-04 17:01:23,715 - INFO - Epoch 126: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:23,724 - INFO - Epoch 126: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:23,743 - INFO - #################### Training epoch 127 ####################
2025-02-04 17:01:23,743 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:24,143 - INFO - Epoch 127: train_loss=nan
2025-02-04 17:01:24,956 - INFO - Epoch 127: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:24,961 - INFO - Epoch 127: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:24,971 - INFO - #################### Training epoch 128 ####################
2025-02-04 17:01:24,971 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:25,382 - INFO - Epoch 128: train_loss=nan
2025-02-04 17:01:26,086 - INFO - Epoch 128: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:26,090 - INFO - Epoch 128: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:26,101 - INFO - #################### Training epoch 129 ####################
2025-02-04 17:01:26,101 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:26,476 - INFO - Epoch 129: train_loss=nan
2025-02-04 17:01:27,120 - INFO - Epoch 129: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:27,125 - INFO - Epoch 129: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:27,134 - INFO - #################### Training epoch 130 ####################
2025-02-04 17:01:27,134 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:27,629 - INFO - Epoch 130: train_loss=nan
2025-02-04 17:01:28,285 - INFO - Epoch 130: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:28,290 - INFO - Epoch 130: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:28,301 - INFO - #################### Training epoch 131 ####################
2025-02-04 17:01:28,301 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:28,804 - INFO - Epoch 131: train_loss=nan
2025-02-04 17:01:29,608 - INFO - Epoch 131: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:29,612 - INFO - Epoch 131: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:29,622 - INFO - #################### Training epoch 132 ####################
2025-02-04 17:01:29,623 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:30,048 - INFO - Epoch 132: train_loss=nan
2025-02-04 17:01:30,930 - INFO - Epoch 132: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:30,934 - INFO - Epoch 132: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:30,945 - INFO - #################### Training epoch 133 ####################
2025-02-04 17:01:30,945 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:31,364 - INFO - Epoch 133: train_loss=nan
2025-02-04 17:01:32,168 - INFO - Epoch 133: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:32,177 - INFO - Epoch 133: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:32,196 - INFO - #################### Training epoch 134 ####################
2025-02-04 17:01:32,196 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:32,604 - INFO - Epoch 134: train_loss=nan
2025-02-04 17:01:33,316 - INFO - Epoch 134: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:33,326 - INFO - Epoch 134: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:33,344 - INFO - #################### Training epoch 135 ####################
2025-02-04 17:01:33,345 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:33,739 - INFO - Epoch 135: train_loss=nan
2025-02-04 17:01:34,446 - INFO - Epoch 135: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:34,455 - INFO - Epoch 135: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:34,474 - INFO - #################### Training epoch 136 ####################
2025-02-04 17:01:34,474 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:34,869 - INFO - Epoch 136: train_loss=nan
2025-02-04 17:01:35,488 - INFO - Epoch 136: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:35,497 - INFO - Epoch 136: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:35,516 - INFO - #################### Training epoch 137 ####################
2025-02-04 17:01:35,516 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:35,981 - INFO - Epoch 137: train_loss=nan
2025-02-04 17:01:36,616 - INFO - Epoch 137: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:36,620 - INFO - Epoch 137: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:36,631 - INFO - #################### Training epoch 138 ####################
2025-02-04 17:01:36,631 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:37,139 - INFO - Epoch 138: train_loss=nan
2025-02-04 17:01:37,819 - INFO - Epoch 138: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:37,824 - INFO - Epoch 138: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:37,835 - INFO - #################### Training epoch 139 ####################
2025-02-04 17:01:37,835 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:38,274 - INFO - Epoch 139: train_loss=nan
2025-02-04 17:01:39,069 - INFO - Epoch 139: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:39,074 - INFO - Epoch 139: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:39,084 - INFO - #################### Training epoch 140 ####################
2025-02-04 17:01:39,084 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:39,513 - INFO - Epoch 140: train_loss=nan
2025-02-04 17:01:40,366 - INFO - Epoch 140: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:40,371 - INFO - Epoch 140: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:40,381 - INFO - #################### Training epoch 141 ####################
2025-02-04 17:01:40,381 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:40,792 - INFO - Epoch 141: train_loss=nan
2025-02-04 17:01:41,584 - INFO - Epoch 141: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:41,593 - INFO - Epoch 141: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:41,612 - INFO - #################### Training epoch 142 ####################
2025-02-04 17:01:41,612 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:42,023 - INFO - Epoch 142: train_loss=nan
2025-02-04 17:01:42,748 - INFO - Epoch 142: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:42,757 - INFO - Epoch 142: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:42,776 - INFO - #################### Training epoch 143 ####################
2025-02-04 17:01:42,776 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:43,177 - INFO - Epoch 143: train_loss=nan
2025-02-04 17:01:43,908 - INFO - Epoch 143: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:43,917 - INFO - Epoch 143: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:43,936 - INFO - #################### Training epoch 144 ####################
2025-02-04 17:01:43,936 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:44,338 - INFO - Epoch 144: train_loss=nan
2025-02-04 17:01:45,106 - INFO - Epoch 144: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:45,110 - INFO - Epoch 144: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:45,120 - INFO - #################### Training epoch 145 ####################
2025-02-04 17:01:45,120 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:45,523 - INFO - Epoch 145: train_loss=nan
2025-02-04 17:01:46,192 - INFO - Epoch 145: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:46,196 - INFO - Epoch 145: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:46,206 - INFO - #################### Training epoch 146 ####################
2025-02-04 17:01:46,207 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:46,618 - INFO - Epoch 146: train_loss=nan
2025-02-04 17:01:47,255 - INFO - Epoch 146: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:47,259 - INFO - Epoch 146: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:47,270 - INFO - #################### Training epoch 147 ####################
2025-02-04 17:01:47,270 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:47,771 - INFO - Epoch 147: train_loss=nan
2025-02-04 17:01:48,453 - INFO - Epoch 147: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:48,457 - INFO - Epoch 147: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:48,468 - INFO - #################### Training epoch 148 ####################
2025-02-04 17:01:48,468 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:48,895 - INFO - Epoch 148: train_loss=nan
2025-02-04 17:01:49,695 - INFO - Epoch 148: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:49,699 - INFO - Epoch 148: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:49,709 - INFO - #################### Training epoch 149 ####################
2025-02-04 17:01:49,709 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:50,140 - INFO - Epoch 149: train_loss=nan
2025-02-04 17:01:50,997 - INFO - Epoch 149: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:51,002 - INFO - Epoch 149: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:51,012 - INFO - #################### Training epoch 150 ####################
2025-02-04 17:01:51,012 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:51,423 - INFO - Epoch 150: train_loss=nan
2025-02-04 17:01:52,222 - INFO - Epoch 150: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:52,231 - INFO - Epoch 150: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:52,250 - INFO - #################### Training epoch 151 ####################
2025-02-04 17:01:52,250 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:52,659 - INFO - Epoch 151: train_loss=nan
2025-02-04 17:01:53,371 - INFO - Epoch 151: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:53,380 - INFO - Epoch 151: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:53,399 - INFO - #################### Training epoch 152 ####################
2025-02-04 17:01:53,399 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:53,789 - INFO - Epoch 152: train_loss=nan
2025-02-04 17:01:54,407 - INFO - Epoch 152: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:54,412 - INFO - Epoch 152: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:54,429 - INFO - #################### Training epoch 153 ####################
2025-02-04 17:01:54,429 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:54,899 - INFO - Epoch 153: train_loss=nan
2025-02-04 17:01:55,533 - INFO - Epoch 153: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:55,538 - INFO - Epoch 153: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:55,548 - INFO - #################### Training epoch 154 ####################
2025-02-04 17:01:55,548 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:56,057 - INFO - Epoch 154: train_loss=nan
2025-02-04 17:01:56,731 - INFO - Epoch 154: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:56,735 - INFO - Epoch 154: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:56,746 - INFO - #################### Training epoch 155 ####################
2025-02-04 17:01:56,746 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:57,193 - INFO - Epoch 155: train_loss=nan
2025-02-04 17:01:57,981 - INFO - Epoch 155: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:57,986 - INFO - Epoch 155: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:57,996 - INFO - #################### Training epoch 156 ####################
2025-02-04 17:01:57,996 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:58,473 - INFO - Epoch 156: train_loss=nan
2025-02-04 17:01:59,231 - INFO - Epoch 156: val_loss=nan, val_acc=66.67%
2025-02-04 17:01:59,236 - INFO - Epoch 156: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:01:59,247 - INFO - #################### Training epoch 157 ####################
2025-02-04 17:01:59,247 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:59,658 - INFO - Epoch 157: train_loss=nan
2025-02-04 17:02:00,459 - INFO - Epoch 157: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:00,468 - INFO - Epoch 157: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:00,487 - INFO - #################### Training epoch 158 ####################
2025-02-04 17:02:00,487 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:00,900 - INFO - Epoch 158: train_loss=nan
2025-02-04 17:02:01,619 - INFO - Epoch 158: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:01,628 - INFO - Epoch 158: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:01,647 - INFO - #################### Training epoch 159 ####################
2025-02-04 17:02:01,647 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:02,046 - INFO - Epoch 159: train_loss=nan
2025-02-04 17:02:02,756 - INFO - Epoch 159: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:02,765 - INFO - Epoch 159: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:02,784 - INFO - #################### Training epoch 160 ####################
2025-02-04 17:02:02,784 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:03,178 - INFO - Epoch 160: train_loss=nan
2025-02-04 17:02:03,826 - INFO - Epoch 160: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:03,835 - INFO - Epoch 160: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:03,854 - INFO - #################### Training epoch 161 ####################
2025-02-04 17:02:03,854 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:04,289 - INFO - Epoch 161: train_loss=nan
2025-02-04 17:02:04,924 - INFO - Epoch 161: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:04,928 - INFO - Epoch 161: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:04,939 - INFO - #################### Training epoch 162 ####################
2025-02-04 17:02:04,939 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:05,447 - INFO - Epoch 162: train_loss=nan
2025-02-04 17:02:06,100 - INFO - Epoch 162: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:06,105 - INFO - Epoch 162: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:06,116 - INFO - #################### Training epoch 163 ####################
2025-02-04 17:02:06,116 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:06,585 - INFO - Epoch 163: train_loss=nan
2025-02-04 17:02:07,365 - INFO - Epoch 163: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:07,370 - INFO - Epoch 163: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:07,380 - INFO - #################### Training epoch 164 ####################
2025-02-04 17:02:07,380 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:07,897 - INFO - Epoch 164: train_loss=nan
2025-02-04 17:02:08,580 - INFO - Epoch 164: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:08,584 - INFO - Epoch 164: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:08,595 - INFO - #################### Training epoch 165 ####################
2025-02-04 17:02:08,595 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:09,023 - INFO - Epoch 165: train_loss=nan
2025-02-04 17:02:09,736 - INFO - Epoch 165: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:09,741 - INFO - Epoch 165: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:09,755 - INFO - #################### Training epoch 166 ####################
2025-02-04 17:02:09,755 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:10,119 - INFO - Epoch 166: train_loss=nan
2025-02-04 17:02:10,743 - INFO - Epoch 166: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:10,748 - INFO - Epoch 166: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:10,758 - INFO - #################### Training epoch 167 ####################
2025-02-04 17:02:10,758 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:11,290 - INFO - Epoch 167: train_loss=nan
2025-02-04 17:02:11,931 - INFO - Epoch 167: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:11,936 - INFO - Epoch 167: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:11,946 - INFO - #################### Training epoch 168 ####################
2025-02-04 17:02:11,946 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:12,538 - INFO - Epoch 168: train_loss=nan
2025-02-04 17:02:13,163 - INFO - Epoch 168: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:13,168 - INFO - Epoch 168: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:13,179 - INFO - #################### Training epoch 169 ####################
2025-02-04 17:02:13,179 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:13,741 - INFO - Epoch 169: train_loss=nan
2025-02-04 17:02:14,481 - INFO - Epoch 169: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:14,485 - INFO - Epoch 169: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:14,487 - INFO - #################### Training epoch 170 ####################
2025-02-04 17:02:14,487 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:14,969 - INFO - Epoch 170: train_loss=nan
2025-02-04 17:02:15,688 - INFO - Epoch 170: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:15,692 - INFO - Epoch 170: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:15,703 - INFO - #################### Training epoch 171 ####################
2025-02-04 17:02:15,703 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:16,128 - INFO - Epoch 171: train_loss=nan
2025-02-04 17:02:16,932 - INFO - Epoch 171: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:16,937 - INFO - Epoch 171: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:16,948 - INFO - #################### Training epoch 172 ####################
2025-02-04 17:02:16,948 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:17,386 - INFO - Epoch 172: train_loss=nan
2025-02-04 17:02:18,123 - INFO - Epoch 172: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:18,127 - INFO - Epoch 172: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:18,139 - INFO - #################### Training epoch 173 ####################
2025-02-04 17:02:18,139 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:18,533 - INFO - Epoch 173: train_loss=nan
2025-02-04 17:02:19,226 - INFO - Epoch 173: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:19,231 - INFO - Epoch 173: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:19,243 - INFO - #################### Training epoch 174 ####################
2025-02-04 17:02:19,243 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:19,626 - INFO - Epoch 174: train_loss=nan
2025-02-04 17:02:20,235 - INFO - Epoch 174: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:20,240 - INFO - Epoch 174: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:20,250 - INFO - #################### Training epoch 175 ####################
2025-02-04 17:02:20,250 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:20,779 - INFO - Epoch 175: train_loss=nan
2025-02-04 17:02:21,401 - INFO - Epoch 175: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:21,406 - INFO - Epoch 175: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:21,417 - INFO - #################### Training epoch 176 ####################
2025-02-04 17:02:21,417 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:21,952 - INFO - Epoch 176: train_loss=nan
2025-02-04 17:02:22,681 - INFO - Epoch 176: val_loss=nan, val_acc=66.67%
2025-02-04 17:02:22,685 - INFO - Epoch 176: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 17:02:22,690 - INFO - #################### Training epoch 177 ####################
2025-02-04 17:02:22,690 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:23,167 - INFO - Epoch 177: train_loss=nan
