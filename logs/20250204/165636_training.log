2025-02-04 16:56:46,505 - INFO - Starting training with the following parameters:
2025-02-04 16:56:46,505 - INFO - | Parameter       | Value               |
|-----------------|---------------------|
| attention       | XFormers       |
| d_model         | 128            |
| n_heads         | 8              |
| d_f             | 128            |
| num_layers      | 3              |
| d_input         | 32             |
| num_classes     | 3              |
| dropout         | 0.1            |
| learning_rate   | 0.01           |
| epochs          | 500            |
| batch_size      | 32             |

2025-02-04 16:56:47,064 - INFO - Epoch 0: val_loss=1.8379, val_acc=0.00%
2025-02-04 16:56:47,200 - INFO - #################### Training epoch 0 ####################
2025-02-04 16:56:47,200 - INFO - Current Learning Rate: 1.000000e-02
2025-02-04 16:56:47,279 - INFO - Epoch 0: train_loss=1.3109
2025-02-04 16:56:47,604 - INFO - Epoch 0: val_loss=0.6601, val_acc=66.67%
2025-02-04 16:56:47,607 - INFO - Epoch 0: EPOCH_AVG_TRAIN_LOSS=1.3109
2025-02-04 16:56:47,635 - INFO - #################### Training epoch 1 ####################
2025-02-04 16:56:47,635 - INFO - Current Learning Rate: 1.000000e-02
2025-02-04 16:56:47,861 - INFO - Epoch 1: train_loss=2.9154
2025-02-04 16:56:48,152 - INFO - Epoch 1: val_loss=2.1780, val_acc=33.33%
2025-02-04 16:56:48,156 - INFO - Epoch 1: EPOCH_AVG_TRAIN_LOSS=2.9154
2025-02-04 16:56:48,180 - INFO - #################### Training epoch 2 ####################
2025-02-04 16:56:48,181 - INFO - Current Learning Rate: 1.000000e-02
2025-02-04 16:56:48,410 - INFO - Epoch 2: train_loss=3.2463
2025-02-04 16:56:48,704 - INFO - Epoch 2: val_loss=1.4502, val_acc=33.33%
2025-02-04 16:56:48,707 - INFO - Epoch 2: EPOCH_AVG_TRAIN_LOSS=3.2463
2025-02-04 16:56:48,739 - INFO - #################### Training epoch 3 ####################
2025-02-04 16:56:48,740 - INFO - Current Learning Rate: 1.000000e-02
2025-02-04 16:56:48,968 - INFO - Epoch 3: train_loss=2.1395
2025-02-04 16:56:49,252 - INFO - Epoch 3: val_loss=0.8259, val_acc=33.33%
2025-02-04 16:56:49,256 - INFO - Epoch 3: EPOCH_AVG_TRAIN_LOSS=2.1395
2025-02-04 16:56:49,282 - INFO - #################### Training epoch 4 ####################
2025-02-04 16:56:49,282 - INFO - Current Learning Rate: 1.000000e-02
2025-02-04 16:56:49,513 - INFO - Epoch 4: train_loss=1.3300
2025-02-04 16:56:49,801 - INFO - Epoch 4: val_loss=1.2262, val_acc=33.33%
2025-02-04 16:56:49,805 - INFO - Epoch 4: EPOCH_AVG_TRAIN_LOSS=1.3300
2025-02-04 16:56:49,832 - INFO - #################### Training epoch 5 ####################
2025-02-04 16:56:49,832 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:56:50,059 - INFO - Epoch 5: train_loss=1.2101
2025-02-04 16:56:50,343 - INFO - Epoch 5: val_loss=1.4423, val_acc=0.00%
2025-02-04 16:56:50,347 - INFO - Epoch 5: EPOCH_AVG_TRAIN_LOSS=1.2101
2025-02-04 16:56:50,350 - INFO - #################### Training epoch 6 ####################
2025-02-04 16:56:50,350 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:56:50,582 - INFO - Epoch 6: train_loss=1.2466
2025-02-04 16:56:50,864 - INFO - Epoch 6: val_loss=1.4484, val_acc=0.00%
2025-02-04 16:56:50,868 - INFO - Epoch 6: EPOCH_AVG_TRAIN_LOSS=1.2466
2025-02-04 16:56:50,871 - INFO - #################### Training epoch 7 ####################
2025-02-04 16:56:50,871 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:56:51,106 - INFO - Epoch 7: train_loss=1.2129
2025-02-04 16:56:51,390 - INFO - Epoch 7: val_loss=1.4470, val_acc=0.00%
2025-02-04 16:56:51,393 - INFO - Epoch 7: EPOCH_AVG_TRAIN_LOSS=1.2129
2025-02-04 16:56:51,397 - INFO - #################### Training epoch 8 ####################
2025-02-04 16:56:51,397 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:56:51,623 - INFO - Epoch 8: train_loss=1.1893
2025-02-04 16:56:51,907 - INFO - Epoch 8: val_loss=1.3316, val_acc=0.00%
2025-02-04 16:56:51,911 - INFO - Epoch 8: EPOCH_AVG_TRAIN_LOSS=1.1893
2025-02-04 16:56:51,914 - INFO - #################### Training epoch 9 ####################
2025-02-04 16:56:51,914 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:56:52,143 - INFO - Epoch 9: train_loss=1.1373
2025-02-04 16:56:52,429 - INFO - Epoch 9: val_loss=1.1359, val_acc=0.00%
2025-02-04 16:56:52,432 - INFO - Epoch 9: EPOCH_AVG_TRAIN_LOSS=1.1373
2025-02-04 16:56:52,459 - INFO - #################### Training epoch 10 ####################
2025-02-04 16:56:52,459 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:56:52,692 - INFO - Epoch 10: train_loss=1.0911
2025-02-04 16:56:52,976 - INFO - Epoch 10: val_loss=0.9548, val_acc=100.00%
2025-02-04 16:56:52,980 - INFO - Epoch 10: EPOCH_AVG_TRAIN_LOSS=1.0911
2025-02-04 16:56:53,006 - INFO - #################### Training epoch 11 ####################
2025-02-04 16:56:53,006 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:56:53,237 - INFO - Epoch 11: train_loss=1.0976
2025-02-04 16:56:53,523 - INFO - Epoch 11: val_loss=0.8891, val_acc=66.67%
2025-02-04 16:56:53,527 - INFO - Epoch 11: EPOCH_AVG_TRAIN_LOSS=1.0976
2025-02-04 16:56:53,553 - INFO - #################### Training epoch 12 ####################
2025-02-04 16:56:53,553 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:56:53,794 - INFO - Epoch 12: train_loss=1.1169
2025-02-04 16:56:54,080 - INFO - Epoch 12: val_loss=0.8964, val_acc=66.67%
2025-02-04 16:56:54,084 - INFO - Epoch 12: EPOCH_AVG_TRAIN_LOSS=1.1169
2025-02-04 16:56:54,087 - INFO - #################### Training epoch 13 ####################
2025-02-04 16:56:54,087 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:56:54,318 - INFO - Epoch 13: train_loss=1.1142
2025-02-04 16:56:54,603 - INFO - Epoch 13: val_loss=0.9589, val_acc=100.00%
2025-02-04 16:56:54,607 - INFO - Epoch 13: EPOCH_AVG_TRAIN_LOSS=1.1142
2025-02-04 16:56:54,611 - INFO - #################### Training epoch 14 ####################
2025-02-04 16:56:54,611 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:56:54,841 - INFO - Epoch 14: train_loss=1.0959
2025-02-04 16:56:55,128 - INFO - Epoch 14: val_loss=1.0683, val_acc=33.33%
2025-02-04 16:56:55,132 - INFO - Epoch 14: EPOCH_AVG_TRAIN_LOSS=1.0959
2025-02-04 16:56:55,135 - INFO - #################### Training epoch 15 ####################
2025-02-04 16:56:55,135 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:56:55,369 - INFO - Epoch 15: train_loss=1.0858
2025-02-04 16:56:55,657 - INFO - Epoch 15: val_loss=1.1184, val_acc=0.00%
2025-02-04 16:56:55,661 - INFO - Epoch 15: EPOCH_AVG_TRAIN_LOSS=1.0858
2025-02-04 16:56:55,665 - INFO - #################### Training epoch 16 ####################
2025-02-04 16:56:55,665 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:56:55,896 - INFO - Epoch 16: train_loss=1.0879
2025-02-04 16:56:56,183 - INFO - Epoch 16: val_loss=1.1453, val_acc=0.00%
2025-02-04 16:56:56,187 - INFO - Epoch 16: EPOCH_AVG_TRAIN_LOSS=1.0879
2025-02-04 16:56:56,190 - INFO - #################### Training epoch 17 ####################
2025-02-04 16:56:56,190 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:56:56,418 - INFO - Epoch 17: train_loss=1.0914
2025-02-04 16:56:56,703 - INFO - Epoch 17: val_loss=1.1430, val_acc=0.00%
2025-02-04 16:56:56,707 - INFO - Epoch 17: EPOCH_AVG_TRAIN_LOSS=1.0914
2025-02-04 16:56:56,710 - INFO - #################### Training epoch 18 ####################
2025-02-04 16:56:56,711 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:56:56,942 - INFO - Epoch 18: train_loss=1.0922
2025-02-04 16:56:57,227 - INFO - Epoch 18: val_loss=1.1170, val_acc=0.00%
2025-02-04 16:56:57,231 - INFO - Epoch 18: EPOCH_AVG_TRAIN_LOSS=1.0922
2025-02-04 16:56:57,235 - INFO - #################### Training epoch 19 ####################
2025-02-04 16:56:57,235 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:56:57,468 - INFO - Epoch 19: train_loss=1.0904
2025-02-04 16:56:57,758 - INFO - Epoch 19: val_loss=1.0796, val_acc=33.33%
2025-02-04 16:56:57,761 - INFO - Epoch 19: EPOCH_AVG_TRAIN_LOSS=1.0904
2025-02-04 16:56:57,765 - INFO - #################### Training epoch 20 ####################
2025-02-04 16:56:57,765 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:56:57,995 - INFO - Epoch 20: train_loss=1.0873
2025-02-04 16:56:58,281 - INFO - Epoch 20: val_loss=1.0612, val_acc=33.33%
2025-02-04 16:56:58,284 - INFO - Epoch 20: EPOCH_AVG_TRAIN_LOSS=1.0873
2025-02-04 16:56:58,288 - INFO - #################### Training epoch 21 ####################
2025-02-04 16:56:58,288 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:56:58,519 - INFO - Epoch 21: train_loss=1.0863
2025-02-04 16:56:58,805 - INFO - Epoch 21: val_loss=1.0452, val_acc=66.67%
2025-02-04 16:56:58,809 - INFO - Epoch 21: EPOCH_AVG_TRAIN_LOSS=1.0863
2025-02-04 16:56:58,812 - INFO - #################### Training epoch 22 ####################
2025-02-04 16:56:58,812 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:56:59,041 - INFO - Epoch 22: train_loss=1.0858
2025-02-04 16:56:59,326 - INFO - Epoch 22: val_loss=1.0329, val_acc=66.67%
2025-02-04 16:56:59,330 - INFO - Epoch 22: EPOCH_AVG_TRAIN_LOSS=1.0858
2025-02-04 16:56:59,333 - INFO - #################### Training epoch 23 ####################
2025-02-04 16:56:59,333 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:56:59,564 - INFO - Epoch 23: train_loss=1.0856
2025-02-04 16:56:59,853 - INFO - Epoch 23: val_loss=1.0252, val_acc=66.67%
2025-02-04 16:56:59,856 - INFO - Epoch 23: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:56:59,860 - INFO - #################### Training epoch 24 ####################
2025-02-04 16:56:59,860 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:57:00,089 - INFO - Epoch 24: train_loss=1.0857
2025-02-04 16:57:00,374 - INFO - Epoch 24: val_loss=1.0218, val_acc=100.00%
2025-02-04 16:57:00,378 - INFO - Epoch 24: EPOCH_AVG_TRAIN_LOSS=1.0857
2025-02-04 16:57:00,381 - INFO - #################### Training epoch 25 ####################
2025-02-04 16:57:00,381 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:57:00,612 - INFO - Epoch 25: train_loss=1.0861
2025-02-04 16:57:00,900 - INFO - Epoch 25: val_loss=1.0221, val_acc=66.67%
2025-02-04 16:57:00,904 - INFO - Epoch 25: EPOCH_AVG_TRAIN_LOSS=1.0861
2025-02-04 16:57:00,907 - INFO - #################### Training epoch 26 ####################
2025-02-04 16:57:00,907 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:57:01,137 - INFO - Epoch 26: train_loss=1.0867
2025-02-04 16:57:01,419 - INFO - Epoch 26: val_loss=1.0250, val_acc=66.67%
2025-02-04 16:57:01,423 - INFO - Epoch 26: EPOCH_AVG_TRAIN_LOSS=1.0867
2025-02-04 16:57:01,426 - INFO - #################### Training epoch 27 ####################
2025-02-04 16:57:01,426 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:57:01,660 - INFO - Epoch 27: train_loss=1.0870
2025-02-04 16:57:01,948 - INFO - Epoch 27: val_loss=1.0295, val_acc=66.67%
2025-02-04 16:57:01,952 - INFO - Epoch 27: EPOCH_AVG_TRAIN_LOSS=1.0870
2025-02-04 16:57:01,955 - INFO - #################### Training epoch 28 ####################
2025-02-04 16:57:01,955 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:57:02,184 - INFO - Epoch 28: train_loss=1.0868
2025-02-04 16:57:02,474 - INFO - Epoch 28: val_loss=1.0323, val_acc=66.67%
2025-02-04 16:57:02,478 - INFO - Epoch 28: EPOCH_AVG_TRAIN_LOSS=1.0868
2025-02-04 16:57:02,481 - INFO - #################### Training epoch 29 ####################
2025-02-04 16:57:02,481 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:57:02,709 - INFO - Epoch 29: train_loss=1.0864
2025-02-04 16:57:02,993 - INFO - Epoch 29: val_loss=1.0355, val_acc=66.67%
2025-02-04 16:57:02,996 - INFO - Epoch 29: EPOCH_AVG_TRAIN_LOSS=1.0864
2025-02-04 16:57:02,999 - INFO - #################### Training epoch 30 ####################
2025-02-04 16:57:02,999 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:57:03,233 - INFO - Epoch 30: train_loss=1.0860
2025-02-04 16:57:03,520 - INFO - Epoch 30: val_loss=1.0390, val_acc=100.00%
2025-02-04 16:57:03,526 - INFO - Epoch 30: EPOCH_AVG_TRAIN_LOSS=1.0860
2025-02-04 16:57:03,529 - INFO - #################### Training epoch 31 ####################
2025-02-04 16:57:03,529 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:57:03,759 - INFO - Epoch 31: train_loss=1.0858
2025-02-04 16:57:04,040 - INFO - Epoch 31: val_loss=1.0429, val_acc=100.00%
2025-02-04 16:57:04,044 - INFO - Epoch 31: EPOCH_AVG_TRAIN_LOSS=1.0858
2025-02-04 16:57:04,047 - INFO - #################### Training epoch 32 ####################
2025-02-04 16:57:04,047 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:57:04,275 - INFO - Epoch 32: train_loss=1.0856
2025-02-04 16:57:04,559 - INFO - Epoch 32: val_loss=1.0449, val_acc=100.00%
2025-02-04 16:57:04,563 - INFO - Epoch 32: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:57:04,566 - INFO - #################### Training epoch 33 ####################
2025-02-04 16:57:04,566 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:57:04,795 - INFO - Epoch 33: train_loss=1.0854
2025-02-04 16:57:05,079 - INFO - Epoch 33: val_loss=1.0468, val_acc=66.67%
2025-02-04 16:57:05,082 - INFO - Epoch 33: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:57:05,085 - INFO - #################### Training epoch 34 ####################
2025-02-04 16:57:05,085 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:57:05,312 - INFO - Epoch 34: train_loss=1.0853
2025-02-04 16:57:05,596 - INFO - Epoch 34: val_loss=1.0489, val_acc=66.67%
2025-02-04 16:57:05,600 - INFO - Epoch 34: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:05,603 - INFO - #################### Training epoch 35 ####################
2025-02-04 16:57:05,603 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:57:05,832 - INFO - Epoch 35: train_loss=1.0858
2025-02-04 16:57:06,115 - INFO - Epoch 35: val_loss=1.0508, val_acc=66.67%
2025-02-04 16:57:06,119 - INFO - Epoch 35: EPOCH_AVG_TRAIN_LOSS=1.0858
2025-02-04 16:57:06,122 - INFO - #################### Training epoch 36 ####################
2025-02-04 16:57:06,122 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:57:06,352 - INFO - Epoch 36: train_loss=1.0848
2025-02-04 16:57:06,635 - INFO - Epoch 36: val_loss=1.0526, val_acc=33.33%
2025-02-04 16:57:06,639 - INFO - Epoch 36: EPOCH_AVG_TRAIN_LOSS=1.0848
2025-02-04 16:57:06,642 - INFO - #################### Training epoch 37 ####################
2025-02-04 16:57:06,642 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:57:06,868 - INFO - Epoch 37: train_loss=1.0851
2025-02-04 16:57:07,151 - INFO - Epoch 37: val_loss=1.0544, val_acc=33.33%
2025-02-04 16:57:07,155 - INFO - Epoch 37: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:57:07,158 - INFO - #################### Training epoch 38 ####################
2025-02-04 16:57:07,158 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:57:07,388 - INFO - Epoch 38: train_loss=1.0853
2025-02-04 16:57:07,671 - INFO - Epoch 38: val_loss=1.0560, val_acc=33.33%
2025-02-04 16:57:07,674 - INFO - Epoch 38: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:07,677 - INFO - #################### Training epoch 39 ####################
2025-02-04 16:57:07,678 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:57:07,905 - INFO - Epoch 39: train_loss=1.0849
2025-02-04 16:57:08,190 - INFO - Epoch 39: val_loss=1.0574, val_acc=33.33%
2025-02-04 16:57:08,193 - INFO - Epoch 39: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 16:57:08,197 - INFO - #################### Training epoch 40 ####################
2025-02-04 16:57:08,197 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:57:08,427 - INFO - Epoch 40: train_loss=1.0855
2025-02-04 16:57:08,710 - INFO - Epoch 40: val_loss=1.0586, val_acc=33.33%
2025-02-04 16:57:08,714 - INFO - Epoch 40: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:57:08,717 - INFO - #################### Training epoch 41 ####################
2025-02-04 16:57:08,717 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 16:57:08,945 - INFO - Epoch 41: train_loss=1.0853
2025-02-04 16:57:09,231 - INFO - Epoch 41: val_loss=1.0590, val_acc=33.33%
2025-02-04 16:57:09,235 - INFO - Epoch 41: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:09,238 - INFO - #################### Training epoch 42 ####################
2025-02-04 16:57:09,238 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 16:57:09,469 - INFO - Epoch 42: train_loss=1.0854
2025-02-04 16:57:09,753 - INFO - Epoch 42: val_loss=1.0594, val_acc=33.33%
2025-02-04 16:57:09,756 - INFO - Epoch 42: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:57:09,760 - INFO - #################### Training epoch 43 ####################
2025-02-04 16:57:09,760 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 16:57:09,989 - INFO - Epoch 43: train_loss=1.0850
2025-02-04 16:57:10,275 - INFO - Epoch 43: val_loss=1.0596, val_acc=33.33%
2025-02-04 16:57:10,279 - INFO - Epoch 43: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:10,282 - INFO - #################### Training epoch 44 ####################
2025-02-04 16:57:10,282 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 16:57:10,510 - INFO - Epoch 44: train_loss=1.0853
2025-02-04 16:57:10,796 - INFO - Epoch 44: val_loss=1.0597, val_acc=33.33%
2025-02-04 16:57:10,799 - INFO - Epoch 44: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:10,802 - INFO - #################### Training epoch 45 ####################
2025-02-04 16:57:10,802 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 16:57:11,031 - INFO - Epoch 45: train_loss=1.0855
2025-02-04 16:57:11,311 - INFO - Epoch 45: val_loss=1.0597, val_acc=33.33%
2025-02-04 16:57:11,314 - INFO - Epoch 45: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:57:11,318 - INFO - #################### Training epoch 46 ####################
2025-02-04 16:57:11,318 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 16:57:11,549 - INFO - Epoch 46: train_loss=1.0850
2025-02-04 16:57:11,833 - INFO - Epoch 46: val_loss=1.0597, val_acc=33.33%
2025-02-04 16:57:11,836 - INFO - Epoch 46: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:11,839 - INFO - #################### Training epoch 47 ####################
2025-02-04 16:57:11,839 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 16:57:12,068 - INFO - Epoch 47: train_loss=1.0854
2025-02-04 16:57:12,350 - INFO - Epoch 47: val_loss=1.0596, val_acc=33.33%
2025-02-04 16:57:12,353 - INFO - Epoch 47: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:57:12,356 - INFO - #################### Training epoch 48 ####################
2025-02-04 16:57:12,357 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 16:57:12,585 - INFO - Epoch 48: train_loss=1.0852
2025-02-04 16:57:12,872 - INFO - Epoch 48: val_loss=1.0595, val_acc=33.33%
2025-02-04 16:57:12,876 - INFO - Epoch 48: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:12,879 - INFO - #################### Training epoch 49 ####################
2025-02-04 16:57:12,879 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 16:57:13,107 - INFO - Epoch 49: train_loss=1.0855
2025-02-04 16:57:13,393 - INFO - Epoch 49: val_loss=1.0594, val_acc=33.33%
2025-02-04 16:57:13,396 - INFO - Epoch 49: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:57:13,399 - INFO - #################### Training epoch 50 ####################
2025-02-04 16:57:13,399 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 16:57:13,628 - INFO - Epoch 50: train_loss=1.0850
2025-02-04 16:57:13,911 - INFO - Epoch 50: val_loss=1.0593, val_acc=33.33%
2025-02-04 16:57:13,915 - INFO - Epoch 50: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:13,918 - INFO - #################### Training epoch 51 ####################
2025-02-04 16:57:13,918 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 16:57:14,147 - INFO - Epoch 51: train_loss=1.0852
2025-02-04 16:57:14,432 - INFO - Epoch 51: val_loss=1.0591, val_acc=33.33%
2025-02-04 16:57:14,435 - INFO - Epoch 51: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:14,439 - INFO - #################### Training epoch 52 ####################
2025-02-04 16:57:14,439 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 16:57:14,667 - INFO - Epoch 52: train_loss=1.0852
2025-02-04 16:57:14,950 - INFO - Epoch 52: val_loss=1.0590, val_acc=33.33%
2025-02-04 16:57:14,954 - INFO - Epoch 52: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:14,957 - INFO - #################### Training epoch 53 ####################
2025-02-04 16:57:14,957 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 16:57:15,185 - INFO - Epoch 53: train_loss=1.0847
2025-02-04 16:57:15,469 - INFO - Epoch 53: val_loss=1.0589, val_acc=33.33%
2025-02-04 16:57:15,472 - INFO - Epoch 53: EPOCH_AVG_TRAIN_LOSS=1.0847
2025-02-04 16:57:15,476 - INFO - #################### Training epoch 54 ####################
2025-02-04 16:57:15,476 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 16:57:15,704 - INFO - Epoch 54: train_loss=1.0853
2025-02-04 16:57:15,989 - INFO - Epoch 54: val_loss=1.0588, val_acc=33.33%
2025-02-04 16:57:15,993 - INFO - Epoch 54: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:15,996 - INFO - #################### Training epoch 55 ####################
2025-02-04 16:57:15,996 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 16:57:16,225 - INFO - Epoch 55: train_loss=1.0851
2025-02-04 16:57:16,509 - INFO - Epoch 55: val_loss=1.0587, val_acc=33.33%
2025-02-04 16:57:16,512 - INFO - Epoch 55: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:57:16,516 - INFO - #################### Training epoch 56 ####################
2025-02-04 16:57:16,516 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 16:57:16,747 - INFO - Epoch 56: train_loss=1.0851
2025-02-04 16:57:17,033 - INFO - Epoch 56: val_loss=1.0586, val_acc=33.33%
2025-02-04 16:57:17,037 - INFO - Epoch 56: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:57:17,040 - INFO - #################### Training epoch 57 ####################
2025-02-04 16:57:17,040 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 16:57:17,271 - INFO - Epoch 57: train_loss=1.0850
2025-02-04 16:57:17,555 - INFO - Epoch 57: val_loss=1.0585, val_acc=33.33%
2025-02-04 16:57:17,558 - INFO - Epoch 57: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:17,562 - INFO - #################### Training epoch 58 ####################
2025-02-04 16:57:17,562 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 16:57:17,793 - INFO - Epoch 58: train_loss=1.0850
2025-02-04 16:57:18,079 - INFO - Epoch 58: val_loss=1.0585, val_acc=33.33%
2025-02-04 16:57:18,083 - INFO - Epoch 58: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:18,086 - INFO - #################### Training epoch 59 ####################
2025-02-04 16:57:18,086 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 16:57:18,315 - INFO - Epoch 59: train_loss=1.0847
2025-02-04 16:57:18,600 - INFO - Epoch 59: val_loss=1.0584, val_acc=33.33%
2025-02-04 16:57:18,603 - INFO - Epoch 59: EPOCH_AVG_TRAIN_LOSS=1.0847
2025-02-04 16:57:18,607 - INFO - #################### Training epoch 60 ####################
2025-02-04 16:57:18,607 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 16:57:18,841 - INFO - Epoch 60: train_loss=1.0850
2025-02-04 16:57:19,127 - INFO - Epoch 60: val_loss=1.0584, val_acc=33.33%
2025-02-04 16:57:19,131 - INFO - Epoch 60: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:19,135 - INFO - #################### Training epoch 61 ####################
2025-02-04 16:57:19,135 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 16:57:19,364 - INFO - Epoch 61: train_loss=1.0852
2025-02-04 16:57:19,647 - INFO - Epoch 61: val_loss=1.0583, val_acc=33.33%
2025-02-04 16:57:19,651 - INFO - Epoch 61: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:19,654 - INFO - #################### Training epoch 62 ####################
2025-02-04 16:57:19,654 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 16:57:19,886 - INFO - Epoch 62: train_loss=1.0854
2025-02-04 16:57:20,170 - INFO - Epoch 62: val_loss=1.0583, val_acc=33.33%
2025-02-04 16:57:20,174 - INFO - Epoch 62: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:57:20,177 - INFO - #################### Training epoch 63 ####################
2025-02-04 16:57:20,177 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 16:57:20,407 - INFO - Epoch 63: train_loss=1.0855
2025-02-04 16:57:20,692 - INFO - Epoch 63: val_loss=1.0582, val_acc=33.33%
2025-02-04 16:57:20,696 - INFO - Epoch 63: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:57:20,700 - INFO - #################### Training epoch 64 ####################
2025-02-04 16:57:20,700 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 16:57:20,927 - INFO - Epoch 64: train_loss=1.0855
2025-02-04 16:57:21,212 - INFO - Epoch 64: val_loss=1.0582, val_acc=33.33%
2025-02-04 16:57:21,216 - INFO - Epoch 64: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:57:21,219 - INFO - #################### Training epoch 65 ####################
2025-02-04 16:57:21,219 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 16:57:21,452 - INFO - Epoch 65: train_loss=1.0851
2025-02-04 16:57:21,737 - INFO - Epoch 65: val_loss=1.0582, val_acc=33.33%
2025-02-04 16:57:21,740 - INFO - Epoch 65: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:57:21,744 - INFO - #################### Training epoch 66 ####################
2025-02-04 16:57:21,744 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 16:57:21,972 - INFO - Epoch 66: train_loss=1.0850
2025-02-04 16:57:22,256 - INFO - Epoch 66: val_loss=1.0582, val_acc=33.33%
2025-02-04 16:57:22,259 - INFO - Epoch 66: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:22,263 - INFO - #################### Training epoch 67 ####################
2025-02-04 16:57:22,263 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 16:57:22,491 - INFO - Epoch 67: train_loss=1.0856
2025-02-04 16:57:22,776 - INFO - Epoch 67: val_loss=1.0582, val_acc=33.33%
2025-02-04 16:57:22,780 - INFO - Epoch 67: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:57:22,783 - INFO - #################### Training epoch 68 ####################
2025-02-04 16:57:22,783 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 16:57:23,011 - INFO - Epoch 68: train_loss=1.0853
2025-02-04 16:57:23,297 - INFO - Epoch 68: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:23,301 - INFO - Epoch 68: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:23,304 - INFO - #################### Training epoch 69 ####################
2025-02-04 16:57:23,304 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 16:57:23,533 - INFO - Epoch 69: train_loss=1.0853
2025-02-04 16:57:23,812 - INFO - Epoch 69: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:23,816 - INFO - Epoch 69: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:23,820 - INFO - #################### Training epoch 70 ####################
2025-02-04 16:57:23,820 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 16:57:24,050 - INFO - Epoch 70: train_loss=1.0857
2025-02-04 16:57:24,335 - INFO - Epoch 70: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:24,339 - INFO - Epoch 70: EPOCH_AVG_TRAIN_LOSS=1.0857
2025-02-04 16:57:24,342 - INFO - #################### Training epoch 71 ####################
2025-02-04 16:57:24,342 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 16:57:24,572 - INFO - Epoch 71: train_loss=1.0855
2025-02-04 16:57:24,854 - INFO - Epoch 71: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:24,858 - INFO - Epoch 71: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:57:24,861 - INFO - #################### Training epoch 72 ####################
2025-02-04 16:57:24,861 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 16:57:25,091 - INFO - Epoch 72: train_loss=1.0851
2025-02-04 16:57:25,376 - INFO - Epoch 72: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:25,379 - INFO - Epoch 72: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:57:25,383 - INFO - #################### Training epoch 73 ####################
2025-02-04 16:57:25,383 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 16:57:25,612 - INFO - Epoch 73: train_loss=1.0860
2025-02-04 16:57:25,898 - INFO - Epoch 73: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:25,901 - INFO - Epoch 73: EPOCH_AVG_TRAIN_LOSS=1.0860
2025-02-04 16:57:25,905 - INFO - #################### Training epoch 74 ####################
2025-02-04 16:57:25,905 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 16:57:26,135 - INFO - Epoch 74: train_loss=1.0852
2025-02-04 16:57:26,416 - INFO - Epoch 74: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:26,420 - INFO - Epoch 74: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:26,423 - INFO - #################### Training epoch 75 ####################
2025-02-04 16:57:26,423 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 16:57:26,652 - INFO - Epoch 75: train_loss=1.0851
2025-02-04 16:57:26,935 - INFO - Epoch 75: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:26,939 - INFO - Epoch 75: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:57:26,942 - INFO - #################### Training epoch 76 ####################
2025-02-04 16:57:26,942 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 16:57:27,173 - INFO - Epoch 76: train_loss=1.0855
2025-02-04 16:57:27,458 - INFO - Epoch 76: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:27,461 - INFO - Epoch 76: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:57:27,464 - INFO - #################### Training epoch 77 ####################
2025-02-04 16:57:27,464 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 16:57:27,692 - INFO - Epoch 77: train_loss=1.0848
2025-02-04 16:57:27,978 - INFO - Epoch 77: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:27,981 - INFO - Epoch 77: EPOCH_AVG_TRAIN_LOSS=1.0848
2025-02-04 16:57:27,985 - INFO - #################### Training epoch 78 ####################
2025-02-04 16:57:27,985 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 16:57:28,216 - INFO - Epoch 78: train_loss=1.0853
2025-02-04 16:57:28,501 - INFO - Epoch 78: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:28,504 - INFO - Epoch 78: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:28,508 - INFO - #################### Training epoch 79 ####################
2025-02-04 16:57:28,508 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 16:57:28,738 - INFO - Epoch 79: train_loss=1.0854
2025-02-04 16:57:29,022 - INFO - Epoch 79: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:29,026 - INFO - Epoch 79: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:57:29,029 - INFO - #################### Training epoch 80 ####################
2025-02-04 16:57:29,029 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 16:57:29,259 - INFO - Epoch 80: train_loss=1.0851
2025-02-04 16:57:29,544 - INFO - Epoch 80: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:29,548 - INFO - Epoch 80: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:57:29,551 - INFO - #################### Training epoch 81 ####################
2025-02-04 16:57:29,551 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 16:57:29,782 - INFO - Epoch 81: train_loss=1.0852
2025-02-04 16:57:30,064 - INFO - Epoch 81: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:30,068 - INFO - Epoch 81: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:30,071 - INFO - #################### Training epoch 82 ####################
2025-02-04 16:57:30,071 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 16:57:30,301 - INFO - Epoch 82: train_loss=1.0853
2025-02-04 16:57:30,586 - INFO - Epoch 82: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:30,590 - INFO - Epoch 82: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:30,593 - INFO - #################### Training epoch 83 ####################
2025-02-04 16:57:30,593 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 16:57:30,824 - INFO - Epoch 83: train_loss=1.0854
2025-02-04 16:57:31,109 - INFO - Epoch 83: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:31,112 - INFO - Epoch 83: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:57:31,116 - INFO - #################### Training epoch 84 ####################
2025-02-04 16:57:31,116 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 16:57:31,347 - INFO - Epoch 84: train_loss=1.0853
2025-02-04 16:57:31,629 - INFO - Epoch 84: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:31,632 - INFO - Epoch 84: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:31,636 - INFO - #################### Training epoch 85 ####################
2025-02-04 16:57:31,636 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 16:57:31,867 - INFO - Epoch 85: train_loss=1.0852
2025-02-04 16:57:32,150 - INFO - Epoch 85: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:32,153 - INFO - Epoch 85: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:32,157 - INFO - #################### Training epoch 86 ####################
2025-02-04 16:57:32,157 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 16:57:32,387 - INFO - Epoch 86: train_loss=1.0856
2025-02-04 16:57:32,673 - INFO - Epoch 86: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:32,677 - INFO - Epoch 86: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:57:32,680 - INFO - #################### Training epoch 87 ####################
2025-02-04 16:57:32,680 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 16:57:32,909 - INFO - Epoch 87: train_loss=1.0852
2025-02-04 16:57:33,194 - INFO - Epoch 87: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:33,197 - INFO - Epoch 87: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:33,201 - INFO - #################### Training epoch 88 ####################
2025-02-04 16:57:33,201 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 16:57:33,432 - INFO - Epoch 88: train_loss=1.0847
2025-02-04 16:57:33,723 - INFO - Epoch 88: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:33,727 - INFO - Epoch 88: EPOCH_AVG_TRAIN_LOSS=1.0847
2025-02-04 16:57:33,731 - INFO - #################### Training epoch 89 ####################
2025-02-04 16:57:33,731 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 16:57:33,960 - INFO - Epoch 89: train_loss=1.0853
2025-02-04 16:57:34,246 - INFO - Epoch 89: val_loss=1.0581, val_acc=33.33%
2025-02-04 16:57:34,249 - INFO - Epoch 89: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:34,253 - INFO - #################### Training epoch 90 ####################
2025-02-04 16:57:34,253 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 16:57:34,489 - INFO - Epoch 90: train_loss=1.0855
2025-02-04 16:57:34,773 - INFO - Epoch 90: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:34,776 - INFO - Epoch 90: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:57:34,780 - INFO - #################### Training epoch 91 ####################
2025-02-04 16:57:34,780 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 16:57:35,008 - INFO - Epoch 91: train_loss=1.0855
2025-02-04 16:57:35,295 - INFO - Epoch 91: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:35,299 - INFO - Epoch 91: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:57:35,302 - INFO - #################### Training epoch 92 ####################
2025-02-04 16:57:35,302 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 16:57:35,533 - INFO - Epoch 92: train_loss=1.0850
2025-02-04 16:57:35,817 - INFO - Epoch 92: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:35,821 - INFO - Epoch 92: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:35,824 - INFO - #################### Training epoch 93 ####################
2025-02-04 16:57:35,824 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 16:57:36,053 - INFO - Epoch 93: train_loss=1.0851
2025-02-04 16:57:36,337 - INFO - Epoch 93: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:36,341 - INFO - Epoch 93: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:57:36,344 - INFO - #################### Training epoch 94 ####################
2025-02-04 16:57:36,344 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:36,575 - INFO - Epoch 94: train_loss=1.0850
2025-02-04 16:57:36,865 - INFO - Epoch 94: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:36,869 - INFO - Epoch 94: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:36,872 - INFO - #################### Training epoch 95 ####################
2025-02-04 16:57:36,872 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:37,103 - INFO - Epoch 95: train_loss=1.0849
2025-02-04 16:57:37,388 - INFO - Epoch 95: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:37,391 - INFO - Epoch 95: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 16:57:37,394 - INFO - #################### Training epoch 96 ####################
2025-02-04 16:57:37,395 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:37,625 - INFO - Epoch 96: train_loss=1.0852
2025-02-04 16:57:37,912 - INFO - Epoch 96: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:37,915 - INFO - Epoch 96: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:37,919 - INFO - #################### Training epoch 97 ####################
2025-02-04 16:57:37,919 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:38,148 - INFO - Epoch 97: train_loss=1.0856
2025-02-04 16:57:38,430 - INFO - Epoch 97: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:38,434 - INFO - Epoch 97: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:57:38,437 - INFO - #################### Training epoch 98 ####################
2025-02-04 16:57:38,437 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:38,667 - INFO - Epoch 98: train_loss=1.0852
2025-02-04 16:57:38,952 - INFO - Epoch 98: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:38,956 - INFO - Epoch 98: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:38,959 - INFO - #################### Training epoch 99 ####################
2025-02-04 16:57:38,959 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:39,190 - INFO - Epoch 99: train_loss=1.0853
2025-02-04 16:57:39,476 - INFO - Epoch 99: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:39,479 - INFO - Epoch 99: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:39,483 - INFO - #################### Training epoch 100 ####################
2025-02-04 16:57:39,483 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:39,711 - INFO - Epoch 100: train_loss=1.0852
2025-02-04 16:57:39,995 - INFO - Epoch 100: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:39,998 - INFO - Epoch 100: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:40,001 - INFO - #################### Training epoch 101 ####################
2025-02-04 16:57:40,002 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:40,231 - INFO - Epoch 101: train_loss=1.0856
2025-02-04 16:57:40,515 - INFO - Epoch 101: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:40,519 - INFO - Epoch 101: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:57:40,522 - INFO - #################### Training epoch 102 ####################
2025-02-04 16:57:40,522 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:40,753 - INFO - Epoch 102: train_loss=1.0853
2025-02-04 16:57:41,036 - INFO - Epoch 102: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:41,040 - INFO - Epoch 102: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:41,043 - INFO - #################### Training epoch 103 ####################
2025-02-04 16:57:41,043 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:41,273 - INFO - Epoch 103: train_loss=1.0851
2025-02-04 16:57:41,558 - INFO - Epoch 103: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:41,562 - INFO - Epoch 103: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:57:41,566 - INFO - #################### Training epoch 104 ####################
2025-02-04 16:57:41,566 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:41,794 - INFO - Epoch 104: train_loss=1.0849
2025-02-04 16:57:42,079 - INFO - Epoch 104: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:42,083 - INFO - Epoch 104: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 16:57:42,086 - INFO - #################### Training epoch 105 ####################
2025-02-04 16:57:42,086 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:42,319 - INFO - Epoch 105: train_loss=1.0850
2025-02-04 16:57:42,605 - INFO - Epoch 105: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:42,609 - INFO - Epoch 105: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:42,612 - INFO - #################### Training epoch 106 ####################
2025-02-04 16:57:42,612 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:42,842 - INFO - Epoch 106: train_loss=1.0856
2025-02-04 16:57:43,128 - INFO - Epoch 106: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:43,132 - INFO - Epoch 106: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:57:43,135 - INFO - #################### Training epoch 107 ####################
2025-02-04 16:57:43,135 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:43,367 - INFO - Epoch 107: train_loss=1.0854
2025-02-04 16:57:43,654 - INFO - Epoch 107: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:43,658 - INFO - Epoch 107: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:57:43,661 - INFO - #################### Training epoch 108 ####################
2025-02-04 16:57:43,661 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:43,894 - INFO - Epoch 108: train_loss=1.0852
2025-02-04 16:57:44,176 - INFO - Epoch 108: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:44,180 - INFO - Epoch 108: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:44,183 - INFO - #################### Training epoch 109 ####################
2025-02-04 16:57:44,183 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:44,412 - INFO - Epoch 109: train_loss=1.0852
2025-02-04 16:57:44,696 - INFO - Epoch 109: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:44,700 - INFO - Epoch 109: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:44,703 - INFO - #################### Training epoch 110 ####################
2025-02-04 16:57:44,703 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:44,933 - INFO - Epoch 110: train_loss=1.0854
2025-02-04 16:57:45,219 - INFO - Epoch 110: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:45,223 - INFO - Epoch 110: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:57:45,226 - INFO - #################### Training epoch 111 ####################
2025-02-04 16:57:45,226 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:45,458 - INFO - Epoch 111: train_loss=1.0850
2025-02-04 16:57:45,743 - INFO - Epoch 111: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:45,747 - INFO - Epoch 111: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:45,750 - INFO - #################### Training epoch 112 ####################
2025-02-04 16:57:45,750 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:45,983 - INFO - Epoch 112: train_loss=1.0852
2025-02-04 16:57:46,268 - INFO - Epoch 112: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:46,271 - INFO - Epoch 112: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:46,275 - INFO - #################### Training epoch 113 ####################
2025-02-04 16:57:46,275 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:46,508 - INFO - Epoch 113: train_loss=1.0858
2025-02-04 16:57:46,793 - INFO - Epoch 113: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:46,797 - INFO - Epoch 113: EPOCH_AVG_TRAIN_LOSS=1.0858
2025-02-04 16:57:46,800 - INFO - #################### Training epoch 114 ####################
2025-02-04 16:57:46,800 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:47,033 - INFO - Epoch 114: train_loss=1.0850
2025-02-04 16:57:47,316 - INFO - Epoch 114: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:47,320 - INFO - Epoch 114: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:47,323 - INFO - #################### Training epoch 115 ####################
2025-02-04 16:57:47,323 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:47,552 - INFO - Epoch 115: train_loss=1.0858
2025-02-04 16:57:47,837 - INFO - Epoch 115: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:47,841 - INFO - Epoch 115: EPOCH_AVG_TRAIN_LOSS=1.0858
2025-02-04 16:57:47,844 - INFO - #################### Training epoch 116 ####################
2025-02-04 16:57:47,844 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:48,073 - INFO - Epoch 116: train_loss=1.0851
2025-02-04 16:57:48,355 - INFO - Epoch 116: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:48,358 - INFO - Epoch 116: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:57:48,362 - INFO - #################### Training epoch 117 ####################
2025-02-04 16:57:48,362 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:48,592 - INFO - Epoch 117: train_loss=1.0853
2025-02-04 16:57:48,873 - INFO - Epoch 117: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:48,877 - INFO - Epoch 117: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:48,880 - INFO - #################### Training epoch 118 ####################
2025-02-04 16:57:48,880 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:49,111 - INFO - Epoch 118: train_loss=1.0853
2025-02-04 16:57:49,392 - INFO - Epoch 118: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:49,395 - INFO - Epoch 118: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:49,399 - INFO - #################### Training epoch 119 ####################
2025-02-04 16:57:49,399 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:49,630 - INFO - Epoch 119: train_loss=1.0846
2025-02-04 16:57:49,916 - INFO - Epoch 119: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:49,920 - INFO - Epoch 119: EPOCH_AVG_TRAIN_LOSS=1.0846
2025-02-04 16:57:49,923 - INFO - #################### Training epoch 120 ####################
2025-02-04 16:57:49,923 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:50,152 - INFO - Epoch 120: train_loss=1.0849
2025-02-04 16:57:50,440 - INFO - Epoch 120: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:50,443 - INFO - Epoch 120: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 16:57:50,447 - INFO - #################### Training epoch 121 ####################
2025-02-04 16:57:50,447 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:50,676 - INFO - Epoch 121: train_loss=1.0851
2025-02-04 16:57:50,959 - INFO - Epoch 121: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:50,963 - INFO - Epoch 121: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:57:50,966 - INFO - #################### Training epoch 122 ####################
2025-02-04 16:57:50,966 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:51,194 - INFO - Epoch 122: train_loss=1.0854
2025-02-04 16:57:51,479 - INFO - Epoch 122: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:51,482 - INFO - Epoch 122: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:57:51,486 - INFO - #################### Training epoch 123 ####################
2025-02-04 16:57:51,486 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:51,715 - INFO - Epoch 123: train_loss=1.0854
2025-02-04 16:57:51,999 - INFO - Epoch 123: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:52,002 - INFO - Epoch 123: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:57:52,006 - INFO - #################### Training epoch 124 ####################
2025-02-04 16:57:52,006 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:52,237 - INFO - Epoch 124: train_loss=1.0852
2025-02-04 16:57:52,521 - INFO - Epoch 124: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:52,525 - INFO - Epoch 124: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:52,529 - INFO - #################### Training epoch 125 ####################
2025-02-04 16:57:52,529 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:52,758 - INFO - Epoch 125: train_loss=1.0852
2025-02-04 16:57:53,045 - INFO - Epoch 125: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:53,049 - INFO - Epoch 125: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:53,052 - INFO - #################### Training epoch 126 ####################
2025-02-04 16:57:53,052 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:53,281 - INFO - Epoch 126: train_loss=1.0850
2025-02-04 16:57:53,567 - INFO - Epoch 126: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:53,571 - INFO - Epoch 126: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:53,574 - INFO - #################### Training epoch 127 ####################
2025-02-04 16:57:53,574 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:53,806 - INFO - Epoch 127: train_loss=1.0855
2025-02-04 16:57:54,091 - INFO - Epoch 127: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:54,095 - INFO - Epoch 127: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:57:54,098 - INFO - #################### Training epoch 128 ####################
2025-02-04 16:57:54,098 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:54,328 - INFO - Epoch 128: train_loss=1.0854
2025-02-04 16:57:54,615 - INFO - Epoch 128: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:54,618 - INFO - Epoch 128: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:57:54,622 - INFO - #################### Training epoch 129 ####################
2025-02-04 16:57:54,622 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:54,855 - INFO - Epoch 129: train_loss=1.0851
2025-02-04 16:57:55,141 - INFO - Epoch 129: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:55,145 - INFO - Epoch 129: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:57:55,148 - INFO - #################### Training epoch 130 ####################
2025-02-04 16:57:55,148 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:55,377 - INFO - Epoch 130: train_loss=1.0850
2025-02-04 16:57:55,664 - INFO - Epoch 130: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:55,667 - INFO - Epoch 130: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:55,671 - INFO - #################### Training epoch 131 ####################
2025-02-04 16:57:55,671 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:55,900 - INFO - Epoch 131: train_loss=1.0851
2025-02-04 16:57:56,185 - INFO - Epoch 131: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:56,188 - INFO - Epoch 131: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:57:56,192 - INFO - #################### Training epoch 132 ####################
2025-02-04 16:57:56,192 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:56,422 - INFO - Epoch 132: train_loss=1.0852
2025-02-04 16:57:56,707 - INFO - Epoch 132: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:56,710 - INFO - Epoch 132: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:57:56,713 - INFO - #################### Training epoch 133 ####################
2025-02-04 16:57:56,713 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:56,942 - INFO - Epoch 133: train_loss=1.0854
2025-02-04 16:57:57,229 - INFO - Epoch 133: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:57,232 - INFO - Epoch 133: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:57:57,235 - INFO - #################### Training epoch 134 ####################
2025-02-04 16:57:57,235 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:57,467 - INFO - Epoch 134: train_loss=1.0850
2025-02-04 16:57:57,752 - INFO - Epoch 134: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:57,756 - INFO - Epoch 134: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:57,759 - INFO - #################### Training epoch 135 ####################
2025-02-04 16:57:57,759 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:57,989 - INFO - Epoch 135: train_loss=1.0854
2025-02-04 16:57:58,271 - INFO - Epoch 135: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:58,275 - INFO - Epoch 135: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:57:58,278 - INFO - #################### Training epoch 136 ####################
2025-02-04 16:57:58,278 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:58,506 - INFO - Epoch 136: train_loss=1.0850
2025-02-04 16:57:58,793 - INFO - Epoch 136: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:58,796 - INFO - Epoch 136: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:57:58,799 - INFO - #################### Training epoch 137 ####################
2025-02-04 16:57:58,799 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:59,028 - INFO - Epoch 137: train_loss=1.0853
2025-02-04 16:57:59,308 - INFO - Epoch 137: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:59,312 - INFO - Epoch 137: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:57:59,315 - INFO - #################### Training epoch 138 ####################
2025-02-04 16:57:59,315 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:57:59,544 - INFO - Epoch 138: train_loss=1.0855
2025-02-04 16:57:59,828 - INFO - Epoch 138: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:57:59,832 - INFO - Epoch 138: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:57:59,835 - INFO - #################### Training epoch 139 ####################
2025-02-04 16:57:59,835 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:00,062 - INFO - Epoch 139: train_loss=1.0855
2025-02-04 16:58:00,340 - INFO - Epoch 139: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:00,344 - INFO - Epoch 139: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:58:00,347 - INFO - #################### Training epoch 140 ####################
2025-02-04 16:58:00,347 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:00,579 - INFO - Epoch 140: train_loss=1.0853
2025-02-04 16:58:00,864 - INFO - Epoch 140: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:00,867 - INFO - Epoch 140: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:00,871 - INFO - #################### Training epoch 141 ####################
2025-02-04 16:58:00,871 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:01,097 - INFO - Epoch 141: train_loss=1.0856
2025-02-04 16:58:01,382 - INFO - Epoch 141: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:01,385 - INFO - Epoch 141: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:58:01,389 - INFO - #################### Training epoch 142 ####################
2025-02-04 16:58:01,389 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:01,620 - INFO - Epoch 142: train_loss=1.0851
2025-02-04 16:58:01,898 - INFO - Epoch 142: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:01,901 - INFO - Epoch 142: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:01,905 - INFO - #################### Training epoch 143 ####################
2025-02-04 16:58:01,905 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:02,136 - INFO - Epoch 143: train_loss=1.0852
2025-02-04 16:58:02,422 - INFO - Epoch 143: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:02,425 - INFO - Epoch 143: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:02,429 - INFO - #################### Training epoch 144 ####################
2025-02-04 16:58:02,429 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:02,661 - INFO - Epoch 144: train_loss=1.0853
2025-02-04 16:58:02,946 - INFO - Epoch 144: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:02,949 - INFO - Epoch 144: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:02,953 - INFO - #################### Training epoch 145 ####################
2025-02-04 16:58:02,953 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:03,183 - INFO - Epoch 145: train_loss=1.0854
2025-02-04 16:58:03,468 - INFO - Epoch 145: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:03,472 - INFO - Epoch 145: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:03,475 - INFO - #################### Training epoch 146 ####################
2025-02-04 16:58:03,475 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:03,704 - INFO - Epoch 146: train_loss=1.0850
2025-02-04 16:58:03,990 - INFO - Epoch 146: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:03,993 - INFO - Epoch 146: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:58:03,997 - INFO - #################### Training epoch 147 ####################
2025-02-04 16:58:03,997 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:04,228 - INFO - Epoch 147: train_loss=1.0853
2025-02-04 16:58:04,514 - INFO - Epoch 147: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:04,517 - INFO - Epoch 147: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:04,520 - INFO - #################### Training epoch 148 ####################
2025-02-04 16:58:04,520 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:04,748 - INFO - Epoch 148: train_loss=1.0853
2025-02-04 16:58:05,035 - INFO - Epoch 148: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:05,038 - INFO - Epoch 148: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:05,042 - INFO - #################### Training epoch 149 ####################
2025-02-04 16:58:05,042 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:05,270 - INFO - Epoch 149: train_loss=1.0848
2025-02-04 16:58:05,555 - INFO - Epoch 149: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:05,558 - INFO - Epoch 149: EPOCH_AVG_TRAIN_LOSS=1.0848
2025-02-04 16:58:05,561 - INFO - #################### Training epoch 150 ####################
2025-02-04 16:58:05,561 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:05,791 - INFO - Epoch 150: train_loss=1.0847
2025-02-04 16:58:06,074 - INFO - Epoch 150: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:06,078 - INFO - Epoch 150: EPOCH_AVG_TRAIN_LOSS=1.0847
2025-02-04 16:58:06,081 - INFO - #################### Training epoch 151 ####################
2025-02-04 16:58:06,081 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:06,311 - INFO - Epoch 151: train_loss=1.0850
2025-02-04 16:58:06,595 - INFO - Epoch 151: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:06,599 - INFO - Epoch 151: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:58:06,602 - INFO - #################### Training epoch 152 ####################
2025-02-04 16:58:06,602 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:06,832 - INFO - Epoch 152: train_loss=1.0851
2025-02-04 16:58:07,116 - INFO - Epoch 152: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:07,120 - INFO - Epoch 152: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:07,123 - INFO - #################### Training epoch 153 ####################
2025-02-04 16:58:07,123 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:07,353 - INFO - Epoch 153: train_loss=1.0851
2025-02-04 16:58:07,637 - INFO - Epoch 153: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:07,640 - INFO - Epoch 153: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:07,644 - INFO - #################### Training epoch 154 ####################
2025-02-04 16:58:07,644 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:07,873 - INFO - Epoch 154: train_loss=1.0847
2025-02-04 16:58:08,157 - INFO - Epoch 154: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:08,160 - INFO - Epoch 154: EPOCH_AVG_TRAIN_LOSS=1.0847
2025-02-04 16:58:08,164 - INFO - #################### Training epoch 155 ####################
2025-02-04 16:58:08,164 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:08,393 - INFO - Epoch 155: train_loss=1.0852
2025-02-04 16:58:08,679 - INFO - Epoch 155: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:08,683 - INFO - Epoch 155: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:08,686 - INFO - #################### Training epoch 156 ####################
2025-02-04 16:58:08,686 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:08,915 - INFO - Epoch 156: train_loss=1.0854
2025-02-04 16:58:09,200 - INFO - Epoch 156: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:09,204 - INFO - Epoch 156: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:09,207 - INFO - #################### Training epoch 157 ####################
2025-02-04 16:58:09,207 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:09,436 - INFO - Epoch 157: train_loss=1.0852
2025-02-04 16:58:09,723 - INFO - Epoch 157: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:09,727 - INFO - Epoch 157: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:09,730 - INFO - #################### Training epoch 158 ####################
2025-02-04 16:58:09,730 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:09,959 - INFO - Epoch 158: train_loss=1.0854
2025-02-04 16:58:10,243 - INFO - Epoch 158: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:10,246 - INFO - Epoch 158: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:10,249 - INFO - #################### Training epoch 159 ####################
2025-02-04 16:58:10,249 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:10,479 - INFO - Epoch 159: train_loss=1.0851
2025-02-04 16:58:10,766 - INFO - Epoch 159: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:10,769 - INFO - Epoch 159: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:10,773 - INFO - #################### Training epoch 160 ####################
2025-02-04 16:58:10,773 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:11,003 - INFO - Epoch 160: train_loss=1.0852
2025-02-04 16:58:11,289 - INFO - Epoch 160: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:11,292 - INFO - Epoch 160: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:11,295 - INFO - #################### Training epoch 161 ####################
2025-02-04 16:58:11,295 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:11,525 - INFO - Epoch 161: train_loss=1.0854
2025-02-04 16:58:11,811 - INFO - Epoch 161: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:11,814 - INFO - Epoch 161: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:11,817 - INFO - #################### Training epoch 162 ####################
2025-02-04 16:58:11,817 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:12,046 - INFO - Epoch 162: train_loss=1.0851
2025-02-04 16:58:12,333 - INFO - Epoch 162: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:12,337 - INFO - Epoch 162: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:12,340 - INFO - #################### Training epoch 163 ####################
2025-02-04 16:58:12,340 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:12,569 - INFO - Epoch 163: train_loss=1.0850
2025-02-04 16:58:12,854 - INFO - Epoch 163: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:12,858 - INFO - Epoch 163: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:58:12,861 - INFO - #################### Training epoch 164 ####################
2025-02-04 16:58:12,861 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:13,091 - INFO - Epoch 164: train_loss=1.0851
2025-02-04 16:58:13,379 - INFO - Epoch 164: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:13,382 - INFO - Epoch 164: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:13,385 - INFO - #################### Training epoch 165 ####################
2025-02-04 16:58:13,385 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:13,617 - INFO - Epoch 165: train_loss=1.0854
2025-02-04 16:58:13,902 - INFO - Epoch 165: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:13,906 - INFO - Epoch 165: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:13,909 - INFO - #################### Training epoch 166 ####################
2025-02-04 16:58:13,909 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:14,138 - INFO - Epoch 166: train_loss=1.0851
2025-02-04 16:58:14,421 - INFO - Epoch 166: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:14,425 - INFO - Epoch 166: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:14,428 - INFO - #################### Training epoch 167 ####################
2025-02-04 16:58:14,428 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:14,658 - INFO - Epoch 167: train_loss=1.0851
2025-02-04 16:58:14,941 - INFO - Epoch 167: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:14,945 - INFO - Epoch 167: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:14,948 - INFO - #################### Training epoch 168 ####################
2025-02-04 16:58:14,948 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:15,180 - INFO - Epoch 168: train_loss=1.0852
2025-02-04 16:58:15,465 - INFO - Epoch 168: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:15,469 - INFO - Epoch 168: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:15,472 - INFO - #################### Training epoch 169 ####################
2025-02-04 16:58:15,472 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:15,703 - INFO - Epoch 169: train_loss=1.0847
2025-02-04 16:58:15,988 - INFO - Epoch 169: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:15,992 - INFO - Epoch 169: EPOCH_AVG_TRAIN_LOSS=1.0847
2025-02-04 16:58:15,995 - INFO - #################### Training epoch 170 ####################
2025-02-04 16:58:15,995 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:16,372 - INFO - Epoch 170: train_loss=1.0854
2025-02-04 16:58:16,659 - INFO - Epoch 170: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:16,662 - INFO - Epoch 170: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:16,665 - INFO - #################### Training epoch 171 ####################
2025-02-04 16:58:16,665 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:16,900 - INFO - Epoch 171: train_loss=1.0855
2025-02-04 16:58:17,187 - INFO - Epoch 171: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:17,191 - INFO - Epoch 171: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:58:17,194 - INFO - #################### Training epoch 172 ####################
2025-02-04 16:58:17,194 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:17,427 - INFO - Epoch 172: train_loss=1.0853
2025-02-04 16:58:17,714 - INFO - Epoch 172: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:17,718 - INFO - Epoch 172: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:17,721 - INFO - #################### Training epoch 173 ####################
2025-02-04 16:58:17,721 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:17,952 - INFO - Epoch 173: train_loss=1.0853
2025-02-04 16:58:18,241 - INFO - Epoch 173: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:18,245 - INFO - Epoch 173: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:18,248 - INFO - #################### Training epoch 174 ####################
2025-02-04 16:58:18,248 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:18,483 - INFO - Epoch 174: train_loss=1.0853
2025-02-04 16:58:18,772 - INFO - Epoch 174: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:18,775 - INFO - Epoch 174: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:18,778 - INFO - #################### Training epoch 175 ####################
2025-02-04 16:58:18,778 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:19,009 - INFO - Epoch 175: train_loss=1.0850
2025-02-04 16:58:19,299 - INFO - Epoch 175: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:19,303 - INFO - Epoch 175: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:58:19,306 - INFO - #################### Training epoch 176 ####################
2025-02-04 16:58:19,306 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:19,539 - INFO - Epoch 176: train_loss=1.0854
2025-02-04 16:58:19,828 - INFO - Epoch 176: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:19,832 - INFO - Epoch 176: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:19,835 - INFO - #################### Training epoch 177 ####################
2025-02-04 16:58:19,835 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:20,070 - INFO - Epoch 177: train_loss=1.0851
2025-02-04 16:58:20,356 - INFO - Epoch 177: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:20,359 - INFO - Epoch 177: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:20,362 - INFO - #################### Training epoch 178 ####################
2025-02-04 16:58:20,362 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:20,596 - INFO - Epoch 178: train_loss=1.0854
2025-02-04 16:58:20,883 - INFO - Epoch 178: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:20,886 - INFO - Epoch 178: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:20,890 - INFO - #################### Training epoch 179 ####################
2025-02-04 16:58:20,890 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:21,123 - INFO - Epoch 179: train_loss=1.0854
2025-02-04 16:58:21,412 - INFO - Epoch 179: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:21,416 - INFO - Epoch 179: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:21,419 - INFO - #################### Training epoch 180 ####################
2025-02-04 16:58:21,419 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:21,650 - INFO - Epoch 180: train_loss=1.0853
2025-02-04 16:58:21,937 - INFO - Epoch 180: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:21,941 - INFO - Epoch 180: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:21,944 - INFO - #################### Training epoch 181 ####################
2025-02-04 16:58:21,944 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:22,176 - INFO - Epoch 181: train_loss=1.0851
2025-02-04 16:58:22,465 - INFO - Epoch 181: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:22,469 - INFO - Epoch 181: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:22,472 - INFO - #################### Training epoch 182 ####################
2025-02-04 16:58:22,472 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:22,706 - INFO - Epoch 182: train_loss=1.0854
2025-02-04 16:58:22,993 - INFO - Epoch 182: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:22,997 - INFO - Epoch 182: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:23,000 - INFO - #################### Training epoch 183 ####################
2025-02-04 16:58:23,000 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:23,233 - INFO - Epoch 183: train_loss=1.0854
2025-02-04 16:58:23,520 - INFO - Epoch 183: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:23,524 - INFO - Epoch 183: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:23,527 - INFO - #################### Training epoch 184 ####################
2025-02-04 16:58:23,527 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:23,759 - INFO - Epoch 184: train_loss=1.0851
2025-02-04 16:58:24,047 - INFO - Epoch 184: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:24,050 - INFO - Epoch 184: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:24,054 - INFO - #################### Training epoch 185 ####################
2025-02-04 16:58:24,054 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:24,286 - INFO - Epoch 185: train_loss=1.0852
2025-02-04 16:58:24,568 - INFO - Epoch 185: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:24,571 - INFO - Epoch 185: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:24,575 - INFO - #################### Training epoch 186 ####################
2025-02-04 16:58:24,575 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:24,807 - INFO - Epoch 186: train_loss=1.0856
2025-02-04 16:58:25,093 - INFO - Epoch 186: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:25,097 - INFO - Epoch 186: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:58:25,100 - INFO - #################### Training epoch 187 ####################
2025-02-04 16:58:25,100 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:25,333 - INFO - Epoch 187: train_loss=1.0850
2025-02-04 16:58:25,621 - INFO - Epoch 187: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:25,625 - INFO - Epoch 187: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:58:25,628 - INFO - #################### Training epoch 188 ####################
2025-02-04 16:58:25,628 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:25,865 - INFO - Epoch 188: train_loss=1.0851
2025-02-04 16:58:26,151 - INFO - Epoch 188: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:26,155 - INFO - Epoch 188: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:26,158 - INFO - #################### Training epoch 189 ####################
2025-02-04 16:58:26,159 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:26,389 - INFO - Epoch 189: train_loss=1.0855
2025-02-04 16:58:26,677 - INFO - Epoch 189: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:26,680 - INFO - Epoch 189: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:58:26,684 - INFO - #################### Training epoch 190 ####################
2025-02-04 16:58:26,684 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:26,918 - INFO - Epoch 190: train_loss=1.0851
2025-02-04 16:58:27,206 - INFO - Epoch 190: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:27,210 - INFO - Epoch 190: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:27,213 - INFO - #################### Training epoch 191 ####################
2025-02-04 16:58:27,213 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:27,446 - INFO - Epoch 191: train_loss=1.0853
2025-02-04 16:58:27,734 - INFO - Epoch 191: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:27,738 - INFO - Epoch 191: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:27,741 - INFO - #################### Training epoch 192 ####################
2025-02-04 16:58:27,741 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:27,972 - INFO - Epoch 192: train_loss=1.0854
2025-02-04 16:58:28,260 - INFO - Epoch 192: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:28,263 - INFO - Epoch 192: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:28,267 - INFO - #################### Training epoch 193 ####################
2025-02-04 16:58:28,267 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:28,500 - INFO - Epoch 193: train_loss=1.0852
2025-02-04 16:58:28,786 - INFO - Epoch 193: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:28,790 - INFO - Epoch 193: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:28,793 - INFO - #################### Training epoch 194 ####################
2025-02-04 16:58:28,793 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:29,027 - INFO - Epoch 194: train_loss=1.0852
2025-02-04 16:58:29,315 - INFO - Epoch 194: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:29,318 - INFO - Epoch 194: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:29,321 - INFO - #################### Training epoch 195 ####################
2025-02-04 16:58:29,322 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:29,559 - INFO - Epoch 195: train_loss=1.0854
2025-02-04 16:58:29,845 - INFO - Epoch 195: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:29,849 - INFO - Epoch 195: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:29,852 - INFO - #################### Training epoch 196 ####################
2025-02-04 16:58:29,852 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:30,085 - INFO - Epoch 196: train_loss=1.0853
2025-02-04 16:58:30,375 - INFO - Epoch 196: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:30,378 - INFO - Epoch 196: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:30,381 - INFO - #################### Training epoch 197 ####################
2025-02-04 16:58:30,381 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:30,611 - INFO - Epoch 197: train_loss=1.0854
2025-02-04 16:58:30,899 - INFO - Epoch 197: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:30,903 - INFO - Epoch 197: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:30,906 - INFO - #################### Training epoch 198 ####################
2025-02-04 16:58:30,906 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:31,137 - INFO - Epoch 198: train_loss=1.0856
2025-02-04 16:58:31,425 - INFO - Epoch 198: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:31,429 - INFO - Epoch 198: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:58:31,432 - INFO - #################### Training epoch 199 ####################
2025-02-04 16:58:31,432 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:31,666 - INFO - Epoch 199: train_loss=1.0847
2025-02-04 16:58:31,953 - INFO - Epoch 199: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:31,956 - INFO - Epoch 199: EPOCH_AVG_TRAIN_LOSS=1.0847
2025-02-04 16:58:31,960 - INFO - #################### Training epoch 200 ####################
2025-02-04 16:58:31,960 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:32,192 - INFO - Epoch 200: train_loss=1.0848
2025-02-04 16:58:32,480 - INFO - Epoch 200: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:32,484 - INFO - Epoch 200: EPOCH_AVG_TRAIN_LOSS=1.0848
2025-02-04 16:58:32,487 - INFO - #################### Training epoch 201 ####################
2025-02-04 16:58:32,487 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:32,719 - INFO - Epoch 201: train_loss=1.0853
2025-02-04 16:58:33,005 - INFO - Epoch 201: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:33,009 - INFO - Epoch 201: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:33,012 - INFO - #################### Training epoch 202 ####################
2025-02-04 16:58:33,012 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:33,244 - INFO - Epoch 202: train_loss=1.0849
2025-02-04 16:58:33,532 - INFO - Epoch 202: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:33,535 - INFO - Epoch 202: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 16:58:33,538 - INFO - #################### Training epoch 203 ####################
2025-02-04 16:58:33,538 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:33,771 - INFO - Epoch 203: train_loss=1.0849
2025-02-04 16:58:34,060 - INFO - Epoch 203: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:34,063 - INFO - Epoch 203: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 16:58:34,067 - INFO - #################### Training epoch 204 ####################
2025-02-04 16:58:34,067 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:34,299 - INFO - Epoch 204: train_loss=1.0856
2025-02-04 16:58:34,588 - INFO - Epoch 204: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:34,592 - INFO - Epoch 204: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:58:34,595 - INFO - #################### Training epoch 205 ####################
2025-02-04 16:58:34,595 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:34,830 - INFO - Epoch 205: train_loss=1.0851
2025-02-04 16:58:35,118 - INFO - Epoch 205: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:35,122 - INFO - Epoch 205: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:35,125 - INFO - #################### Training epoch 206 ####################
2025-02-04 16:58:35,125 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:35,356 - INFO - Epoch 206: train_loss=1.0854
2025-02-04 16:58:35,643 - INFO - Epoch 206: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:35,646 - INFO - Epoch 206: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:35,649 - INFO - #################### Training epoch 207 ####################
2025-02-04 16:58:35,649 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:35,884 - INFO - Epoch 207: train_loss=1.0850
2025-02-04 16:58:36,171 - INFO - Epoch 207: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:36,175 - INFO - Epoch 207: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:58:36,178 - INFO - #################### Training epoch 208 ####################
2025-02-04 16:58:36,178 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:36,410 - INFO - Epoch 208: train_loss=1.0853
2025-02-04 16:58:36,699 - INFO - Epoch 208: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:36,702 - INFO - Epoch 208: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:36,706 - INFO - #################### Training epoch 209 ####################
2025-02-04 16:58:36,706 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:36,940 - INFO - Epoch 209: train_loss=1.0855
2025-02-04 16:58:37,224 - INFO - Epoch 209: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:37,228 - INFO - Epoch 209: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:58:37,231 - INFO - #################### Training epoch 210 ####################
2025-02-04 16:58:37,231 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:37,461 - INFO - Epoch 210: train_loss=1.0854
2025-02-04 16:58:37,749 - INFO - Epoch 210: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:37,752 - INFO - Epoch 210: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:37,756 - INFO - #################### Training epoch 211 ####################
2025-02-04 16:58:37,756 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:37,988 - INFO - Epoch 211: train_loss=1.0853
2025-02-04 16:58:38,275 - INFO - Epoch 211: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:38,279 - INFO - Epoch 211: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:38,282 - INFO - #################### Training epoch 212 ####################
2025-02-04 16:58:38,282 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:38,513 - INFO - Epoch 212: train_loss=1.0852
2025-02-04 16:58:38,800 - INFO - Epoch 212: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:38,804 - INFO - Epoch 212: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:38,807 - INFO - #################### Training epoch 213 ####################
2025-02-04 16:58:38,807 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:39,038 - INFO - Epoch 213: train_loss=1.0852
2025-02-04 16:58:39,325 - INFO - Epoch 213: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:39,329 - INFO - Epoch 213: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:39,332 - INFO - #################### Training epoch 214 ####################
2025-02-04 16:58:39,332 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:39,566 - INFO - Epoch 214: train_loss=1.0848
2025-02-04 16:58:39,853 - INFO - Epoch 214: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:39,857 - INFO - Epoch 214: EPOCH_AVG_TRAIN_LOSS=1.0848
2025-02-04 16:58:39,860 - INFO - #################### Training epoch 215 ####################
2025-02-04 16:58:39,860 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:40,094 - INFO - Epoch 215: train_loss=1.0851
2025-02-04 16:58:40,381 - INFO - Epoch 215: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:40,385 - INFO - Epoch 215: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:40,388 - INFO - #################### Training epoch 216 ####################
2025-02-04 16:58:40,388 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:40,624 - INFO - Epoch 216: train_loss=1.0854
2025-02-04 16:58:40,911 - INFO - Epoch 216: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:40,915 - INFO - Epoch 216: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:40,918 - INFO - #################### Training epoch 217 ####################
2025-02-04 16:58:40,919 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:41,149 - INFO - Epoch 217: train_loss=1.0851
2025-02-04 16:58:41,434 - INFO - Epoch 217: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:41,438 - INFO - Epoch 217: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:41,441 - INFO - #################### Training epoch 218 ####################
2025-02-04 16:58:41,441 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:41,673 - INFO - Epoch 218: train_loss=1.0850
2025-02-04 16:58:41,961 - INFO - Epoch 218: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:41,964 - INFO - Epoch 218: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:58:41,967 - INFO - #################### Training epoch 219 ####################
2025-02-04 16:58:41,967 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:42,202 - INFO - Epoch 219: train_loss=1.0856
2025-02-04 16:58:42,491 - INFO - Epoch 219: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:42,494 - INFO - Epoch 219: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:58:42,498 - INFO - #################### Training epoch 220 ####################
2025-02-04 16:58:42,498 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:42,730 - INFO - Epoch 220: train_loss=1.0852
2025-02-04 16:58:43,017 - INFO - Epoch 220: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:43,021 - INFO - Epoch 220: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:43,024 - INFO - #################### Training epoch 221 ####################
2025-02-04 16:58:43,024 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:43,255 - INFO - Epoch 221: train_loss=1.0852
2025-02-04 16:58:43,544 - INFO - Epoch 221: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:43,548 - INFO - Epoch 221: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:43,551 - INFO - #################### Training epoch 222 ####################
2025-02-04 16:58:43,551 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:43,781 - INFO - Epoch 222: train_loss=1.0853
2025-02-04 16:58:44,066 - INFO - Epoch 222: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:44,070 - INFO - Epoch 222: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:44,073 - INFO - #################### Training epoch 223 ####################
2025-02-04 16:58:44,074 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:44,306 - INFO - Epoch 223: train_loss=1.0853
2025-02-04 16:58:44,599 - INFO - Epoch 223: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:44,603 - INFO - Epoch 223: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:44,606 - INFO - #################### Training epoch 224 ####################
2025-02-04 16:58:44,606 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:44,840 - INFO - Epoch 224: train_loss=1.0851
2025-02-04 16:58:45,129 - INFO - Epoch 224: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:45,133 - INFO - Epoch 224: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:45,136 - INFO - #################### Training epoch 225 ####################
2025-02-04 16:58:45,136 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:45,369 - INFO - Epoch 225: train_loss=1.0852
2025-02-04 16:58:45,657 - INFO - Epoch 225: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:45,661 - INFO - Epoch 225: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:45,664 - INFO - #################### Training epoch 226 ####################
2025-02-04 16:58:45,664 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:45,898 - INFO - Epoch 226: train_loss=1.0855
2025-02-04 16:58:46,181 - INFO - Epoch 226: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:46,185 - INFO - Epoch 226: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:58:46,188 - INFO - #################### Training epoch 227 ####################
2025-02-04 16:58:46,188 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:46,422 - INFO - Epoch 227: train_loss=1.0850
2025-02-04 16:58:46,711 - INFO - Epoch 227: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:46,714 - INFO - Epoch 227: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:58:46,717 - INFO - #################### Training epoch 228 ####################
2025-02-04 16:58:46,718 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:46,948 - INFO - Epoch 228: train_loss=1.0850
2025-02-04 16:58:47,235 - INFO - Epoch 228: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:47,238 - INFO - Epoch 228: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:58:47,242 - INFO - #################### Training epoch 229 ####################
2025-02-04 16:58:47,242 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:47,474 - INFO - Epoch 229: train_loss=1.0853
2025-02-04 16:58:47,761 - INFO - Epoch 229: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:47,765 - INFO - Epoch 229: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:47,768 - INFO - #################### Training epoch 230 ####################
2025-02-04 16:58:47,768 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:48,001 - INFO - Epoch 230: train_loss=1.0854
2025-02-04 16:58:48,290 - INFO - Epoch 230: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:48,293 - INFO - Epoch 230: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:48,297 - INFO - #################### Training epoch 231 ####################
2025-02-04 16:58:48,297 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:48,529 - INFO - Epoch 231: train_loss=1.0853
2025-02-04 16:58:48,816 - INFO - Epoch 231: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:48,819 - INFO - Epoch 231: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:58:48,822 - INFO - #################### Training epoch 232 ####################
2025-02-04 16:58:48,822 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:49,053 - INFO - Epoch 232: train_loss=1.0855
2025-02-04 16:58:49,336 - INFO - Epoch 232: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:49,340 - INFO - Epoch 232: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:58:49,343 - INFO - #################### Training epoch 233 ####################
2025-02-04 16:58:49,343 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:49,576 - INFO - Epoch 233: train_loss=1.0849
2025-02-04 16:58:49,856 - INFO - Epoch 233: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:49,860 - INFO - Epoch 233: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 16:58:49,863 - INFO - #################### Training epoch 234 ####################
2025-02-04 16:58:49,863 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:50,093 - INFO - Epoch 234: train_loss=1.0849
2025-02-04 16:58:50,380 - INFO - Epoch 234: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:50,384 - INFO - Epoch 234: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 16:58:50,387 - INFO - #################### Training epoch 235 ####################
2025-02-04 16:58:50,387 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:50,659 - INFO - Epoch 235: train_loss=1.0848
2025-02-04 16:58:50,946 - INFO - Epoch 235: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:50,950 - INFO - Epoch 235: EPOCH_AVG_TRAIN_LOSS=1.0848
2025-02-04 16:58:50,953 - INFO - #################### Training epoch 236 ####################
2025-02-04 16:58:50,953 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:51,185 - INFO - Epoch 236: train_loss=1.0847
2025-02-04 16:58:51,470 - INFO - Epoch 236: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:51,473 - INFO - Epoch 236: EPOCH_AVG_TRAIN_LOSS=1.0847
2025-02-04 16:58:51,477 - INFO - #################### Training epoch 237 ####################
2025-02-04 16:58:51,477 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:51,709 - INFO - Epoch 237: train_loss=1.0857
2025-02-04 16:58:51,999 - INFO - Epoch 237: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:52,003 - INFO - Epoch 237: EPOCH_AVG_TRAIN_LOSS=1.0857
2025-02-04 16:58:52,006 - INFO - #################### Training epoch 238 ####################
2025-02-04 16:58:52,006 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:52,236 - INFO - Epoch 238: train_loss=1.0852
2025-02-04 16:58:52,521 - INFO - Epoch 238: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:52,525 - INFO - Epoch 238: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:52,528 - INFO - #################### Training epoch 239 ####################
2025-02-04 16:58:52,528 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:52,766 - INFO - Epoch 239: train_loss=1.0852
2025-02-04 16:58:53,051 - INFO - Epoch 239: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:53,055 - INFO - Epoch 239: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:53,058 - INFO - #################### Training epoch 240 ####################
2025-02-04 16:58:53,058 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:53,292 - INFO - Epoch 240: train_loss=1.0851
2025-02-04 16:58:53,581 - INFO - Epoch 240: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:53,585 - INFO - Epoch 240: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:53,588 - INFO - #################### Training epoch 241 ####################
2025-02-04 16:58:53,588 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:53,824 - INFO - Epoch 241: train_loss=1.0857
2025-02-04 16:58:54,113 - INFO - Epoch 241: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:54,117 - INFO - Epoch 241: EPOCH_AVG_TRAIN_LOSS=1.0857
2025-02-04 16:58:54,120 - INFO - #################### Training epoch 242 ####################
2025-02-04 16:58:54,120 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:54,354 - INFO - Epoch 242: train_loss=1.0856
2025-02-04 16:58:54,639 - INFO - Epoch 242: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:54,643 - INFO - Epoch 242: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:58:54,646 - INFO - #################### Training epoch 243 ####################
2025-02-04 16:58:54,646 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:54,877 - INFO - Epoch 243: train_loss=1.0844
2025-02-04 16:58:55,163 - INFO - Epoch 243: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:55,166 - INFO - Epoch 243: EPOCH_AVG_TRAIN_LOSS=1.0844
2025-02-04 16:58:55,170 - INFO - #################### Training epoch 244 ####################
2025-02-04 16:58:55,170 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:55,402 - INFO - Epoch 244: train_loss=1.0854
2025-02-04 16:58:55,689 - INFO - Epoch 244: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:55,692 - INFO - Epoch 244: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:55,696 - INFO - #################### Training epoch 245 ####################
2025-02-04 16:58:55,696 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:55,927 - INFO - Epoch 245: train_loss=1.0854
2025-02-04 16:58:56,212 - INFO - Epoch 245: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:56,216 - INFO - Epoch 245: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:56,219 - INFO - #################### Training epoch 246 ####################
2025-02-04 16:58:56,219 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:56,442 - INFO - Epoch 246: train_loss=1.0856
2025-02-04 16:58:56,726 - INFO - Epoch 246: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:56,730 - INFO - Epoch 246: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:58:56,733 - INFO - #################### Training epoch 247 ####################
2025-02-04 16:58:56,733 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:56,965 - INFO - Epoch 247: train_loss=1.0854
2025-02-04 16:58:57,255 - INFO - Epoch 247: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:57,259 - INFO - Epoch 247: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:58:57,262 - INFO - #################### Training epoch 248 ####################
2025-02-04 16:58:57,262 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:57,493 - INFO - Epoch 248: train_loss=1.0852
2025-02-04 16:58:57,783 - INFO - Epoch 248: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:57,786 - INFO - Epoch 248: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:58:57,790 - INFO - #################### Training epoch 249 ####################
2025-02-04 16:58:57,790 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:58,019 - INFO - Epoch 249: train_loss=1.0851
2025-02-04 16:58:58,314 - INFO - Epoch 249: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:58,319 - INFO - Epoch 249: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:58:58,323 - INFO - #################### Training epoch 250 ####################
2025-02-04 16:58:58,324 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:58,599 - INFO - Epoch 250: train_loss=1.0857
2025-02-04 16:58:58,960 - INFO - Epoch 250: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:58,965 - INFO - Epoch 250: EPOCH_AVG_TRAIN_LOSS=1.0857
2025-02-04 16:58:58,975 - INFO - #################### Training epoch 251 ####################
2025-02-04 16:58:58,975 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:58:59,404 - INFO - Epoch 251: train_loss=1.0847
2025-02-04 16:58:59,696 - INFO - Epoch 251: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:58:59,700 - INFO - Epoch 251: EPOCH_AVG_TRAIN_LOSS=1.0847
2025-02-04 16:58:59,703 - INFO - #################### Training epoch 252 ####################
2025-02-04 16:58:59,703 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:00,250 - INFO - Epoch 252: train_loss=1.0854
2025-02-04 16:59:00,637 - INFO - Epoch 252: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:00,640 - INFO - Epoch 252: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:59:00,644 - INFO - #################### Training epoch 253 ####################
2025-02-04 16:59:00,644 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:00,877 - INFO - Epoch 253: train_loss=1.0854
2025-02-04 16:59:01,362 - INFO - Epoch 253: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:01,367 - INFO - Epoch 253: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:59:01,377 - INFO - #################### Training epoch 254 ####################
2025-02-04 16:59:01,377 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:01,612 - INFO - Epoch 254: train_loss=1.0850
2025-02-04 16:59:01,925 - INFO - Epoch 254: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:01,929 - INFO - Epoch 254: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:59:01,933 - INFO - #################### Training epoch 255 ####################
2025-02-04 16:59:01,933 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:02,449 - INFO - Epoch 255: train_loss=1.0855
2025-02-04 16:59:02,737 - INFO - Epoch 255: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:02,740 - INFO - Epoch 255: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:59:02,744 - INFO - #################### Training epoch 256 ####################
2025-02-04 16:59:02,744 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:03,307 - INFO - Epoch 256: train_loss=1.0857
2025-02-04 16:59:03,683 - INFO - Epoch 256: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:03,686 - INFO - Epoch 256: EPOCH_AVG_TRAIN_LOSS=1.0857
2025-02-04 16:59:03,690 - INFO - #################### Training epoch 257 ####################
2025-02-04 16:59:03,690 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:04,110 - INFO - Epoch 257: train_loss=1.0846
2025-02-04 16:59:04,489 - INFO - Epoch 257: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:04,492 - INFO - Epoch 257: EPOCH_AVG_TRAIN_LOSS=1.0846
2025-02-04 16:59:04,495 - INFO - #################### Training epoch 258 ####################
2025-02-04 16:59:04,495 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:04,739 - INFO - Epoch 258: train_loss=1.0854
2025-02-04 16:59:05,112 - INFO - Epoch 258: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:05,117 - INFO - Epoch 258: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:59:05,127 - INFO - #################### Training epoch 259 ####################
2025-02-04 16:59:05,127 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:05,424 - INFO - Epoch 259: train_loss=1.0856
2025-02-04 16:59:05,715 - INFO - Epoch 259: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:05,719 - INFO - Epoch 259: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:59:05,722 - INFO - #################### Training epoch 260 ####################
2025-02-04 16:59:05,722 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:06,293 - INFO - Epoch 260: train_loss=1.0853
2025-02-04 16:59:06,620 - INFO - Epoch 260: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:06,623 - INFO - Epoch 260: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:59:06,627 - INFO - #################### Training epoch 261 ####################
2025-02-04 16:59:06,627 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:07,178 - INFO - Epoch 261: train_loss=1.0851
2025-02-04 16:59:07,595 - INFO - Epoch 261: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:07,598 - INFO - Epoch 261: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:59:07,602 - INFO - #################### Training epoch 262 ####################
2025-02-04 16:59:07,602 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:08,147 - INFO - Epoch 262: train_loss=1.0849
2025-02-04 16:59:08,533 - INFO - Epoch 262: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:08,536 - INFO - Epoch 262: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 16:59:08,540 - INFO - #################### Training epoch 263 ####################
2025-02-04 16:59:08,540 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:08,776 - INFO - Epoch 263: train_loss=1.0852
2025-02-04 16:59:09,252 - INFO - Epoch 263: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:09,256 - INFO - Epoch 263: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:59:09,267 - INFO - #################### Training epoch 264 ####################
2025-02-04 16:59:09,267 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:09,501 - INFO - Epoch 264: train_loss=1.0851
2025-02-04 16:59:09,816 - INFO - Epoch 264: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:09,821 - INFO - Epoch 264: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:59:09,824 - INFO - #################### Training epoch 265 ####################
2025-02-04 16:59:09,824 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:10,319 - INFO - Epoch 265: train_loss=1.0850
2025-02-04 16:59:10,608 - INFO - Epoch 265: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:10,612 - INFO - Epoch 265: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:59:10,615 - INFO - #################### Training epoch 266 ####################
2025-02-04 16:59:10,615 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:11,187 - INFO - Epoch 266: train_loss=1.0850
2025-02-04 16:59:11,509 - INFO - Epoch 266: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:11,513 - INFO - Epoch 266: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:59:11,516 - INFO - #################### Training epoch 267 ####################
2025-02-04 16:59:11,516 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:12,066 - INFO - Epoch 267: train_loss=1.0851
2025-02-04 16:59:12,473 - INFO - Epoch 267: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:12,478 - INFO - Epoch 267: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:59:12,483 - INFO - #################### Training epoch 268 ####################
2025-02-04 16:59:12,483 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:13,014 - INFO - Epoch 268: train_loss=1.0852
2025-02-04 16:59:13,399 - INFO - Epoch 268: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:13,402 - INFO - Epoch 268: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:59:13,406 - INFO - #################### Training epoch 269 ####################
2025-02-04 16:59:13,406 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:13,642 - INFO - Epoch 269: train_loss=1.0847
2025-02-04 16:59:14,101 - INFO - Epoch 269: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:14,105 - INFO - Epoch 269: EPOCH_AVG_TRAIN_LOSS=1.0847
2025-02-04 16:59:14,116 - INFO - #################### Training epoch 270 ####################
2025-02-04 16:59:14,116 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:14,347 - INFO - Epoch 270: train_loss=1.0855
2025-02-04 16:59:14,658 - INFO - Epoch 270: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:14,662 - INFO - Epoch 270: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:59:14,665 - INFO - #################### Training epoch 271 ####################
2025-02-04 16:59:14,665 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:15,162 - INFO - Epoch 271: train_loss=1.0856
2025-02-04 16:59:15,453 - INFO - Epoch 271: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:15,457 - INFO - Epoch 271: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:59:15,460 - INFO - #################### Training epoch 272 ####################
2025-02-04 16:59:15,460 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:16,026 - INFO - Epoch 272: train_loss=1.0852
2025-02-04 16:59:16,349 - INFO - Epoch 272: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:16,353 - INFO - Epoch 272: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:59:16,356 - INFO - #################### Training epoch 273 ####################
2025-02-04 16:59:16,356 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:16,921 - INFO - Epoch 273: train_loss=1.0852
2025-02-04 16:59:17,302 - INFO - Epoch 273: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:17,305 - INFO - Epoch 273: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:59:17,309 - INFO - #################### Training epoch 274 ####################
2025-02-04 16:59:17,309 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:17,876 - INFO - Epoch 274: train_loss=1.0850
2025-02-04 16:59:18,265 - INFO - Epoch 274: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:18,269 - INFO - Epoch 274: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 16:59:18,273 - INFO - #################### Training epoch 275 ####################
2025-02-04 16:59:18,273 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:18,512 - INFO - Epoch 275: train_loss=1.0855
2025-02-04 16:59:19,004 - INFO - Epoch 275: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:19,009 - INFO - Epoch 275: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:59:19,019 - INFO - #################### Training epoch 276 ####################
2025-02-04 16:59:19,019 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:19,254 - INFO - Epoch 276: train_loss=1.0854
2025-02-04 16:59:19,572 - INFO - Epoch 276: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:19,576 - INFO - Epoch 276: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:59:19,587 - INFO - #################### Training epoch 277 ####################
2025-02-04 16:59:19,587 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:20,071 - INFO - Epoch 277: train_loss=1.0854
2025-02-04 16:59:20,361 - INFO - Epoch 277: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:20,364 - INFO - Epoch 277: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:59:20,368 - INFO - #################### Training epoch 278 ####################
2025-02-04 16:59:20,368 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:20,928 - INFO - Epoch 278: train_loss=1.0849
2025-02-04 16:59:21,263 - INFO - Epoch 278: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:21,266 - INFO - Epoch 278: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 16:59:21,270 - INFO - #################### Training epoch 279 ####################
2025-02-04 16:59:21,270 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:21,815 - INFO - Epoch 279: train_loss=1.0851
2025-02-04 16:59:22,225 - INFO - Epoch 279: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:22,230 - INFO - Epoch 279: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:59:22,235 - INFO - #################### Training epoch 280 ####################
2025-02-04 16:59:22,235 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:22,743 - INFO - Epoch 280: train_loss=1.0854
2025-02-04 16:59:23,125 - INFO - Epoch 280: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:23,129 - INFO - Epoch 280: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:59:23,132 - INFO - #################### Training epoch 281 ####################
2025-02-04 16:59:23,132 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:23,370 - INFO - Epoch 281: train_loss=1.0857
2025-02-04 16:59:23,811 - INFO - Epoch 281: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:23,816 - INFO - Epoch 281: EPOCH_AVG_TRAIN_LOSS=1.0857
2025-02-04 16:59:23,826 - INFO - #################### Training epoch 282 ####################
2025-02-04 16:59:23,826 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:24,056 - INFO - Epoch 282: train_loss=1.0854
2025-02-04 16:59:24,356 - INFO - Epoch 282: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:24,360 - INFO - Epoch 282: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:59:24,363 - INFO - #################### Training epoch 283 ####################
2025-02-04 16:59:24,363 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:24,865 - INFO - Epoch 283: train_loss=1.0851
2025-02-04 16:59:25,156 - INFO - Epoch 283: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:25,160 - INFO - Epoch 283: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:59:25,163 - INFO - #################### Training epoch 284 ####################
2025-02-04 16:59:25,163 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:25,734 - INFO - Epoch 284: train_loss=1.0857
2025-02-04 16:59:26,054 - INFO - Epoch 284: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:26,057 - INFO - Epoch 284: EPOCH_AVG_TRAIN_LOSS=1.0857
2025-02-04 16:59:26,061 - INFO - #################### Training epoch 285 ####################
2025-02-04 16:59:26,061 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:26,644 - INFO - Epoch 285: train_loss=1.0853
2025-02-04 16:59:26,981 - INFO - Epoch 285: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:26,984 - INFO - Epoch 285: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:59:26,987 - INFO - #################### Training epoch 286 ####################
2025-02-04 16:59:26,987 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:27,538 - INFO - Epoch 286: train_loss=1.0853
2025-02-04 16:59:27,939 - INFO - Epoch 286: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:27,943 - INFO - Epoch 286: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:59:27,951 - INFO - #################### Training epoch 287 ####################
2025-02-04 16:59:27,951 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:28,369 - INFO - Epoch 287: train_loss=1.0853
2025-02-04 16:59:28,945 - INFO - Epoch 287: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:28,949 - INFO - Epoch 287: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:59:28,959 - INFO - #################### Training epoch 288 ####################
2025-02-04 16:59:28,959 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:29,373 - INFO - Epoch 288: train_loss=1.0851
2025-02-04 16:59:29,753 - INFO - Epoch 288: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:29,757 - INFO - Epoch 288: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:59:29,767 - INFO - #################### Training epoch 289 ####################
2025-02-04 16:59:29,768 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:30,396 - INFO - Epoch 289: train_loss=1.0854
2025-02-04 16:59:30,883 - INFO - Epoch 289: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:30,888 - INFO - Epoch 289: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:59:30,898 - INFO - #################### Training epoch 290 ####################
2025-02-04 16:59:30,898 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:31,145 - INFO - Epoch 290: train_loss=1.0855
2025-02-04 16:59:31,676 - INFO - Epoch 290: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:31,685 - INFO - Epoch 290: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:59:31,704 - INFO - #################### Training epoch 291 ####################
2025-02-04 16:59:31,704 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:32,276 - INFO - Epoch 291: train_loss=1.0856
2025-02-04 16:59:32,664 - INFO - Epoch 291: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:32,669 - INFO - Epoch 291: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:59:32,680 - INFO - #################### Training epoch 292 ####################
2025-02-04 16:59:32,680 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:33,354 - INFO - Epoch 292: train_loss=1.0848
2025-02-04 16:59:33,892 - INFO - Epoch 292: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:33,895 - INFO - Epoch 292: EPOCH_AVG_TRAIN_LOSS=1.0848
2025-02-04 16:59:33,907 - INFO - #################### Training epoch 293 ####################
2025-02-04 16:59:33,907 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:34,620 - INFO - Epoch 293: train_loss=1.0851
2025-02-04 16:59:35,018 - INFO - Epoch 293: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:35,022 - INFO - Epoch 293: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:59:35,025 - INFO - #################### Training epoch 294 ####################
2025-02-04 16:59:35,025 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:35,731 - INFO - Epoch 294: train_loss=1.0849
2025-02-04 16:59:36,167 - INFO - Epoch 294: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:36,170 - INFO - Epoch 294: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 16:59:36,174 - INFO - #################### Training epoch 295 ####################
2025-02-04 16:59:36,174 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:36,826 - INFO - Epoch 295: train_loss=1.0851
2025-02-04 16:59:37,374 - INFO - Epoch 295: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:37,378 - INFO - Epoch 295: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:59:37,388 - INFO - #################### Training epoch 296 ####################
2025-02-04 16:59:37,388 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:38,046 - INFO - Epoch 296: train_loss=1.0852
2025-02-04 16:59:38,591 - INFO - Epoch 296: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:38,595 - INFO - Epoch 296: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:59:38,606 - INFO - #################### Training epoch 297 ####################
2025-02-04 16:59:38,606 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:39,216 - INFO - Epoch 297: train_loss=1.0852
2025-02-04 16:59:39,820 - INFO - Epoch 297: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:39,824 - INFO - Epoch 297: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:59:39,835 - INFO - #################### Training epoch 298 ####################
2025-02-04 16:59:39,835 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:40,401 - INFO - Epoch 298: train_loss=1.0854
2025-02-04 16:59:40,955 - INFO - Epoch 298: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:40,960 - INFO - Epoch 298: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:59:40,977 - INFO - #################### Training epoch 299 ####################
2025-02-04 16:59:40,977 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:41,203 - INFO - Epoch 299: train_loss=1.0848
2025-02-04 16:59:41,582 - INFO - Epoch 299: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:41,586 - INFO - Epoch 299: EPOCH_AVG_TRAIN_LOSS=1.0848
2025-02-04 16:59:41,597 - INFO - #################### Training epoch 300 ####################
2025-02-04 16:59:41,597 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:42,294 - INFO - Epoch 300: train_loss=1.0849
2025-02-04 16:59:42,805 - INFO - Epoch 300: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:42,810 - INFO - Epoch 300: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 16:59:42,820 - INFO - #################### Training epoch 301 ####################
2025-02-04 16:59:42,820 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:43,513 - INFO - Epoch 301: train_loss=1.0856
2025-02-04 16:59:43,973 - INFO - Epoch 301: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:43,977 - INFO - Epoch 301: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 16:59:43,985 - INFO - #################### Training epoch 302 ####################
2025-02-04 16:59:43,985 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:44,667 - INFO - Epoch 302: train_loss=1.0853
2025-02-04 16:59:45,063 - INFO - Epoch 302: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:45,067 - INFO - Epoch 302: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:59:45,070 - INFO - #################### Training epoch 303 ####################
2025-02-04 16:59:45,070 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:45,760 - INFO - Epoch 303: train_loss=1.0851
2025-02-04 16:59:46,303 - INFO - Epoch 303: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:46,307 - INFO - Epoch 303: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:59:46,310 - INFO - #################### Training epoch 304 ####################
2025-02-04 16:59:46,310 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:46,965 - INFO - Epoch 304: train_loss=1.0851
2025-02-04 16:59:47,556 - INFO - Epoch 304: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:47,561 - INFO - Epoch 304: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 16:59:47,571 - INFO - #################### Training epoch 305 ####################
2025-02-04 16:59:47,571 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:48,170 - INFO - Epoch 305: train_loss=1.0853
2025-02-04 16:59:48,761 - INFO - Epoch 305: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:48,766 - INFO - Epoch 305: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:59:48,776 - INFO - #################### Training epoch 306 ####################
2025-02-04 16:59:48,777 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:49,007 - INFO - Epoch 306: train_loss=1.0853
2025-02-04 16:59:49,379 - INFO - Epoch 306: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:49,384 - INFO - Epoch 306: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:59:49,394 - INFO - #################### Training epoch 307 ####################
2025-02-04 16:59:49,394 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:50,089 - INFO - Epoch 307: train_loss=1.0852
2025-02-04 16:59:50,539 - INFO - Epoch 307: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:50,543 - INFO - Epoch 307: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:59:50,554 - INFO - #################### Training epoch 308 ####################
2025-02-04 16:59:50,554 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:51,282 - INFO - Epoch 308: train_loss=1.0852
2025-02-04 16:59:51,694 - INFO - Epoch 308: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:51,698 - INFO - Epoch 308: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:59:51,703 - INFO - #################### Training epoch 309 ####################
2025-02-04 16:59:51,703 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:52,423 - INFO - Epoch 309: train_loss=1.0853
2025-02-04 16:59:52,828 - INFO - Epoch 309: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:52,832 - INFO - Epoch 309: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:59:52,835 - INFO - #################### Training epoch 310 ####################
2025-02-04 16:59:52,835 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:53,500 - INFO - Epoch 310: train_loss=1.0849
2025-02-04 16:59:54,049 - INFO - Epoch 310: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:54,054 - INFO - Epoch 310: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 16:59:54,064 - INFO - #################### Training epoch 311 ####################
2025-02-04 16:59:54,064 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:54,721 - INFO - Epoch 311: train_loss=1.0846
2025-02-04 16:59:55,234 - INFO - Epoch 311: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:55,238 - INFO - Epoch 311: EPOCH_AVG_TRAIN_LOSS=1.0846
2025-02-04 16:59:55,249 - INFO - #################### Training epoch 312 ####################
2025-02-04 16:59:55,249 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:55,804 - INFO - Epoch 312: train_loss=1.0854
2025-02-04 16:59:56,290 - INFO - Epoch 312: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:56,294 - INFO - Epoch 312: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 16:59:56,307 - INFO - #################### Training epoch 313 ####################
2025-02-04 16:59:56,307 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:56,898 - INFO - Epoch 313: train_loss=1.0855
2025-02-04 16:59:57,358 - INFO - Epoch 313: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:57,363 - INFO - Epoch 313: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 16:59:57,373 - INFO - #################### Training epoch 314 ####################
2025-02-04 16:59:57,373 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:57,946 - INFO - Epoch 314: train_loss=1.0853
2025-02-04 16:59:58,501 - INFO - Epoch 314: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:58,506 - INFO - Epoch 314: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 16:59:58,517 - INFO - #################### Training epoch 315 ####################
2025-02-04 16:59:58,517 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:58,933 - INFO - Epoch 315: train_loss=1.0852
2025-02-04 16:59:59,475 - INFO - Epoch 315: val_loss=1.0580, val_acc=33.33%
2025-02-04 16:59:59,480 - INFO - Epoch 315: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 16:59:59,490 - INFO - #################### Training epoch 316 ####################
2025-02-04 16:59:59,490 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:59:59,871 - INFO - Epoch 316: train_loss=1.0852
2025-02-04 17:00:00,250 - INFO - Epoch 316: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:00,255 - INFO - Epoch 316: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:00:00,265 - INFO - #################### Training epoch 317 ####################
2025-02-04 17:00:00,265 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:00,927 - INFO - Epoch 317: train_loss=1.0854
2025-02-04 17:00:01,412 - INFO - Epoch 317: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:01,417 - INFO - Epoch 317: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:00:01,427 - INFO - #################### Training epoch 318 ####################
2025-02-04 17:00:01,428 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:02,076 - INFO - Epoch 318: train_loss=1.0852
2025-02-04 17:00:02,551 - INFO - Epoch 318: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:02,560 - INFO - Epoch 318: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:00:02,574 - INFO - #################### Training epoch 319 ####################
2025-02-04 17:00:02,574 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:02,843 - INFO - Epoch 319: train_loss=1.0854
2025-02-04 17:00:03,365 - INFO - Epoch 319: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:03,375 - INFO - Epoch 319: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:00:03,394 - INFO - #################### Training epoch 320 ####################
2025-02-04 17:00:03,394 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:03,980 - INFO - Epoch 320: train_loss=1.0853
2025-02-04 17:00:04,357 - INFO - Epoch 320: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:04,361 - INFO - Epoch 320: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:00:04,372 - INFO - #################### Training epoch 321 ####################
2025-02-04 17:00:04,372 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:05,014 - INFO - Epoch 321: train_loss=1.0854
2025-02-04 17:00:05,522 - INFO - Epoch 321: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:05,527 - INFO - Epoch 321: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:00:05,537 - INFO - #################### Training epoch 322 ####################
2025-02-04 17:00:05,537 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:06,128 - INFO - Epoch 322: train_loss=1.0854
2025-02-04 17:00:06,633 - INFO - Epoch 322: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:06,638 - INFO - Epoch 322: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:00:06,648 - INFO - #################### Training epoch 323 ####################
2025-02-04 17:00:06,649 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:06,908 - INFO - Epoch 323: train_loss=1.0850
2025-02-04 17:00:07,349 - INFO - Epoch 323: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:07,354 - INFO - Epoch 323: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:00:07,368 - INFO - #################### Training epoch 324 ####################
2025-02-04 17:00:07,369 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:08,002 - INFO - Epoch 324: train_loss=1.0849
2025-02-04 17:00:08,441 - INFO - Epoch 324: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:08,445 - INFO - Epoch 324: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 17:00:08,455 - INFO - #################### Training epoch 325 ####################
2025-02-04 17:00:08,456 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:09,084 - INFO - Epoch 325: train_loss=1.0855
2025-02-04 17:00:09,577 - INFO - Epoch 325: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:09,582 - INFO - Epoch 325: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 17:00:09,592 - INFO - #################### Training epoch 326 ####################
2025-02-04 17:00:09,593 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:09,839 - INFO - Epoch 326: train_loss=1.0858
2025-02-04 17:00:10,369 - INFO - Epoch 326: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:10,378 - INFO - Epoch 326: EPOCH_AVG_TRAIN_LOSS=1.0858
2025-02-04 17:00:10,397 - INFO - #################### Training epoch 327 ####################
2025-02-04 17:00:10,397 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:10,976 - INFO - Epoch 327: train_loss=1.0849
2025-02-04 17:00:11,350 - INFO - Epoch 327: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:11,355 - INFO - Epoch 327: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 17:00:11,365 - INFO - #################### Training epoch 328 ####################
2025-02-04 17:00:11,365 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:12,011 - INFO - Epoch 328: train_loss=1.0853
2025-02-04 17:00:12,530 - INFO - Epoch 328: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:12,534 - INFO - Epoch 328: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:00:12,545 - INFO - #################### Training epoch 329 ####################
2025-02-04 17:00:12,545 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:13,143 - INFO - Epoch 329: train_loss=1.0852
2025-02-04 17:00:13,625 - INFO - Epoch 329: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:13,631 - INFO - Epoch 329: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:00:13,641 - INFO - #################### Training epoch 330 ####################
2025-02-04 17:00:13,642 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:13,924 - INFO - Epoch 330: train_loss=1.0850
2025-02-04 17:00:14,437 - INFO - Epoch 330: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:14,446 - INFO - Epoch 330: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:00:14,465 - INFO - #################### Training epoch 331 ####################
2025-02-04 17:00:14,465 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:15,061 - INFO - Epoch 331: train_loss=1.0856
2025-02-04 17:00:15,444 - INFO - Epoch 331: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:15,448 - INFO - Epoch 331: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 17:00:15,459 - INFO - #################### Training epoch 332 ####################
2025-02-04 17:00:15,459 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:16,105 - INFO - Epoch 332: train_loss=1.0850
2025-02-04 17:00:16,614 - INFO - Epoch 332: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:16,619 - INFO - Epoch 332: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:00:16,630 - INFO - #################### Training epoch 333 ####################
2025-02-04 17:00:16,630 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:17,141 - INFO - Epoch 333: train_loss=1.0854
2025-02-04 17:00:17,653 - INFO - Epoch 333: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:17,656 - INFO - Epoch 333: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:00:17,667 - INFO - #################### Training epoch 334 ####################
2025-02-04 17:00:17,668 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:18,004 - INFO - Epoch 334: train_loss=1.0852
2025-02-04 17:00:18,382 - INFO - Epoch 334: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:18,387 - INFO - Epoch 334: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:00:18,397 - INFO - #################### Training epoch 335 ####################
2025-02-04 17:00:18,397 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:19,022 - INFO - Epoch 335: train_loss=1.0858
2025-02-04 17:00:19,548 - INFO - Epoch 335: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:19,553 - INFO - Epoch 335: EPOCH_AVG_TRAIN_LOSS=1.0858
2025-02-04 17:00:19,563 - INFO - #################### Training epoch 336 ####################
2025-02-04 17:00:19,563 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:20,150 - INFO - Epoch 336: train_loss=1.0855
2025-02-04 17:00:20,757 - INFO - Epoch 336: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:20,762 - INFO - Epoch 336: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 17:00:20,773 - INFO - #################### Training epoch 337 ####################
2025-02-04 17:00:20,773 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:21,341 - INFO - Epoch 337: train_loss=1.0851
2025-02-04 17:00:21,944 - INFO - Epoch 337: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:21,952 - INFO - Epoch 337: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 17:00:21,963 - INFO - #################### Training epoch 338 ####################
2025-02-04 17:00:21,963 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:22,189 - INFO - Epoch 338: train_loss=1.0858
2025-02-04 17:00:22,712 - INFO - Epoch 338: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:22,721 - INFO - Epoch 338: EPOCH_AVG_TRAIN_LOSS=1.0858
2025-02-04 17:00:22,740 - INFO - #################### Training epoch 339 ####################
2025-02-04 17:00:22,740 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:23,325 - INFO - Epoch 339: train_loss=1.0851
2025-02-04 17:00:23,696 - INFO - Epoch 339: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:23,701 - INFO - Epoch 339: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 17:00:23,711 - INFO - #################### Training epoch 340 ####################
2025-02-04 17:00:23,711 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:24,374 - INFO - Epoch 340: train_loss=1.0853
2025-02-04 17:00:24,905 - INFO - Epoch 340: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:24,909 - INFO - Epoch 340: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:00:24,916 - INFO - #################### Training epoch 341 ####################
2025-02-04 17:00:24,916 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:25,554 - INFO - Epoch 341: train_loss=1.0853
2025-02-04 17:00:26,065 - INFO - Epoch 341: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:26,070 - INFO - Epoch 341: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:00:26,088 - INFO - #################### Training epoch 342 ####################
2025-02-04 17:00:26,088 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:26,524 - INFO - Epoch 342: train_loss=1.0850
2025-02-04 17:00:27,030 - INFO - Epoch 342: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:27,035 - INFO - Epoch 342: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:00:27,045 - INFO - #################### Training epoch 343 ####################
2025-02-04 17:00:27,045 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:27,479 - INFO - Epoch 343: train_loss=1.0855
2025-02-04 17:00:27,967 - INFO - Epoch 343: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:27,971 - INFO - Epoch 343: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 17:00:27,988 - INFO - #################### Training epoch 344 ####################
2025-02-04 17:00:27,988 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:28,573 - INFO - Epoch 344: train_loss=1.0855
2025-02-04 17:00:29,038 - INFO - Epoch 344: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:29,042 - INFO - Epoch 344: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 17:00:29,053 - INFO - #################### Training epoch 345 ####################
2025-02-04 17:00:29,053 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:29,625 - INFO - Epoch 345: train_loss=1.0853
2025-02-04 17:00:30,185 - INFO - Epoch 345: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:30,190 - INFO - Epoch 345: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:00:30,201 - INFO - #################### Training epoch 346 ####################
2025-02-04 17:00:30,201 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:30,645 - INFO - Epoch 346: train_loss=1.0852
2025-02-04 17:00:31,219 - INFO - Epoch 346: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:31,223 - INFO - Epoch 346: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:00:31,234 - INFO - #################### Training epoch 347 ####################
2025-02-04 17:00:31,234 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:31,567 - INFO - Epoch 347: train_loss=1.0850
2025-02-04 17:00:31,932 - INFO - Epoch 347: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:31,937 - INFO - Epoch 347: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:00:31,947 - INFO - #################### Training epoch 348 ####################
2025-02-04 17:00:31,947 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:32,620 - INFO - Epoch 348: train_loss=1.0849
2025-02-04 17:00:33,154 - INFO - Epoch 348: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:33,157 - INFO - Epoch 348: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 17:00:33,167 - INFO - #################### Training epoch 349 ####################
2025-02-04 17:00:33,167 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:33,816 - INFO - Epoch 349: train_loss=1.0851
2025-02-04 17:00:34,330 - INFO - Epoch 349: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:34,339 - INFO - Epoch 349: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 17:00:34,351 - INFO - #################### Training epoch 350 ####################
2025-02-04 17:00:34,351 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:34,721 - INFO - Epoch 350: train_loss=1.0852
2025-02-04 17:00:35,253 - INFO - Epoch 350: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:35,258 - INFO - Epoch 350: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:00:35,269 - INFO - #################### Training epoch 351 ####################
2025-02-04 17:00:35,269 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:35,723 - INFO - Epoch 351: train_loss=1.0855
2025-02-04 17:00:36,191 - INFO - Epoch 351: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:36,195 - INFO - Epoch 351: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 17:00:36,206 - INFO - #################### Training epoch 352 ####################
2025-02-04 17:00:36,206 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:36,807 - INFO - Epoch 352: train_loss=1.0852
2025-02-04 17:00:37,285 - INFO - Epoch 352: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:37,290 - INFO - Epoch 352: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:00:37,300 - INFO - #################### Training epoch 353 ####################
2025-02-04 17:00:37,300 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:37,879 - INFO - Epoch 353: train_loss=1.0851
2025-02-04 17:00:38,446 - INFO - Epoch 353: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:38,451 - INFO - Epoch 353: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 17:00:38,462 - INFO - #################### Training epoch 354 ####################
2025-02-04 17:00:38,462 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:38,939 - INFO - Epoch 354: train_loss=1.0850
2025-02-04 17:00:39,481 - INFO - Epoch 354: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:39,485 - INFO - Epoch 354: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:00:39,495 - INFO - #################### Training epoch 355 ####################
2025-02-04 17:00:39,495 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:39,827 - INFO - Epoch 355: train_loss=1.0852
2025-02-04 17:00:40,205 - INFO - Epoch 355: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:40,210 - INFO - Epoch 355: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:00:40,220 - INFO - #################### Training epoch 356 ####################
2025-02-04 17:00:40,220 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:40,894 - INFO - Epoch 356: train_loss=1.0856
2025-02-04 17:00:41,393 - INFO - Epoch 356: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:41,399 - INFO - Epoch 356: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 17:00:41,409 - INFO - #################### Training epoch 357 ####################
2025-02-04 17:00:41,409 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:42,065 - INFO - Epoch 357: train_loss=1.0855
2025-02-04 17:00:42,566 - INFO - Epoch 357: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:42,570 - INFO - Epoch 357: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 17:00:42,581 - INFO - #################### Training epoch 358 ####################
2025-02-04 17:00:42,581 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:42,831 - INFO - Epoch 358: train_loss=1.0847
2025-02-04 17:00:43,367 - INFO - Epoch 358: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:43,376 - INFO - Epoch 358: EPOCH_AVG_TRAIN_LOSS=1.0847
2025-02-04 17:00:43,395 - INFO - #################### Training epoch 359 ####################
2025-02-04 17:00:43,395 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:43,968 - INFO - Epoch 359: train_loss=1.0854
2025-02-04 17:00:44,348 - INFO - Epoch 359: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:44,353 - INFO - Epoch 359: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:00:44,363 - INFO - #################### Training epoch 360 ####################
2025-02-04 17:00:44,363 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:45,008 - INFO - Epoch 360: train_loss=1.0848
2025-02-04 17:00:45,520 - INFO - Epoch 360: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:45,524 - INFO - Epoch 360: EPOCH_AVG_TRAIN_LOSS=1.0848
2025-02-04 17:00:45,535 - INFO - #################### Training epoch 361 ####################
2025-02-04 17:00:45,535 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:46,133 - INFO - Epoch 361: train_loss=1.0851
2025-02-04 17:00:46,637 - INFO - Epoch 361: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:46,641 - INFO - Epoch 361: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 17:00:46,652 - INFO - #################### Training epoch 362 ####################
2025-02-04 17:00:46,652 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:46,908 - INFO - Epoch 362: train_loss=1.0850
2025-02-04 17:00:47,379 - INFO - Epoch 362: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:47,388 - INFO - Epoch 362: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:00:47,407 - INFO - #################### Training epoch 363 ####################
2025-02-04 17:00:47,407 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:48,013 - INFO - Epoch 363: train_loss=1.0852
2025-02-04 17:00:48,437 - INFO - Epoch 363: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:48,442 - INFO - Epoch 363: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:00:48,452 - INFO - #################### Training epoch 364 ####################
2025-02-04 17:00:48,452 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:49,080 - INFO - Epoch 364: train_loss=1.0859
2025-02-04 17:00:49,574 - INFO - Epoch 364: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:49,579 - INFO - Epoch 364: EPOCH_AVG_TRAIN_LOSS=1.0859
2025-02-04 17:00:49,590 - INFO - #################### Training epoch 365 ####################
2025-02-04 17:00:49,590 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:49,827 - INFO - Epoch 365: train_loss=1.0850
2025-02-04 17:00:50,342 - INFO - Epoch 365: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:50,351 - INFO - Epoch 365: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:00:50,371 - INFO - #################### Training epoch 366 ####################
2025-02-04 17:00:50,371 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:50,963 - INFO - Epoch 366: train_loss=1.0853
2025-02-04 17:00:51,349 - INFO - Epoch 366: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:51,354 - INFO - Epoch 366: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:00:51,364 - INFO - #################### Training epoch 367 ####################
2025-02-04 17:00:51,364 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:52,006 - INFO - Epoch 367: train_loss=1.0852
2025-02-04 17:00:52,508 - INFO - Epoch 367: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:52,513 - INFO - Epoch 367: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:00:52,524 - INFO - #################### Training epoch 368 ####################
2025-02-04 17:00:52,524 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:53,116 - INFO - Epoch 368: train_loss=1.0848
2025-02-04 17:00:53,624 - INFO - Epoch 368: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:53,629 - INFO - Epoch 368: EPOCH_AVG_TRAIN_LOSS=1.0848
2025-02-04 17:00:53,639 - INFO - #################### Training epoch 369 ####################
2025-02-04 17:00:53,639 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:53,896 - INFO - Epoch 369: train_loss=1.0853
2025-02-04 17:00:54,376 - INFO - Epoch 369: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:54,385 - INFO - Epoch 369: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:00:54,405 - INFO - #################### Training epoch 370 ####################
2025-02-04 17:00:54,405 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:55,015 - INFO - Epoch 370: train_loss=1.0850
2025-02-04 17:00:55,428 - INFO - Epoch 370: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:55,432 - INFO - Epoch 370: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:00:55,442 - INFO - #################### Training epoch 371 ####################
2025-02-04 17:00:55,442 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:56,076 - INFO - Epoch 371: train_loss=1.0852
2025-02-04 17:00:56,574 - INFO - Epoch 371: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:56,579 - INFO - Epoch 371: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:00:56,590 - INFO - #################### Training epoch 372 ####################
2025-02-04 17:00:56,590 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:56,822 - INFO - Epoch 372: train_loss=1.0852
2025-02-04 17:00:57,357 - INFO - Epoch 372: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:57,366 - INFO - Epoch 372: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:00:57,385 - INFO - #################### Training epoch 373 ####################
2025-02-04 17:00:57,386 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:57,959 - INFO - Epoch 373: train_loss=1.0851
2025-02-04 17:00:58,320 - INFO - Epoch 373: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:58,325 - INFO - Epoch 373: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 17:00:58,335 - INFO - #################### Training epoch 374 ####################
2025-02-04 17:00:58,335 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:00:59,001 - INFO - Epoch 374: train_loss=1.0856
2025-02-04 17:00:59,551 - INFO - Epoch 374: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:00:59,555 - INFO - Epoch 374: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 17:00:59,560 - INFO - #################### Training epoch 375 ####################
2025-02-04 17:00:59,560 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:00,213 - INFO - Epoch 375: train_loss=1.0856
2025-02-04 17:01:00,702 - INFO - Epoch 375: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:00,710 - INFO - Epoch 375: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 17:01:00,723 - INFO - #################### Training epoch 376 ####################
2025-02-04 17:01:00,723 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:01,306 - INFO - Epoch 376: train_loss=1.0850
2025-02-04 17:01:01,828 - INFO - Epoch 376: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:01,833 - INFO - Epoch 376: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:01:01,841 - INFO - #################### Training epoch 377 ####################
2025-02-04 17:01:01,841 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:02,438 - INFO - Epoch 377: train_loss=1.0850
2025-02-04 17:01:02,926 - INFO - Epoch 377: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:02,930 - INFO - Epoch 377: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:01:02,943 - INFO - #################### Training epoch 378 ####################
2025-02-04 17:01:02,943 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:03,537 - INFO - Epoch 378: train_loss=1.0855
2025-02-04 17:01:04,199 - INFO - Epoch 378: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:04,204 - INFO - Epoch 378: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 17:01:04,215 - INFO - #################### Training epoch 379 ####################
2025-02-04 17:01:04,215 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:04,825 - INFO - Epoch 379: train_loss=1.0849
2025-02-04 17:01:05,322 - INFO - Epoch 379: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:05,328 - INFO - Epoch 379: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 17:01:05,343 - INFO - #################### Training epoch 380 ####################
2025-02-04 17:01:05,343 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:05,937 - INFO - Epoch 380: train_loss=1.0853
2025-02-04 17:01:06,430 - INFO - Epoch 380: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:06,434 - INFO - Epoch 380: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:01:06,445 - INFO - #################### Training epoch 381 ####################
2025-02-04 17:01:06,445 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:06,783 - INFO - Epoch 381: train_loss=1.0856
2025-02-04 17:01:07,260 - INFO - Epoch 381: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:07,265 - INFO - Epoch 381: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 17:01:07,280 - INFO - #################### Training epoch 382 ####################
2025-02-04 17:01:07,280 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:07,882 - INFO - Epoch 382: train_loss=1.0854
2025-02-04 17:01:08,321 - INFO - Epoch 382: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:08,326 - INFO - Epoch 382: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:01:08,336 - INFO - #################### Training epoch 383 ####################
2025-02-04 17:01:08,336 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:08,947 - INFO - Epoch 383: train_loss=1.0853
2025-02-04 17:01:09,502 - INFO - Epoch 383: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:09,506 - INFO - Epoch 383: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:01:09,517 - INFO - #################### Training epoch 384 ####################
2025-02-04 17:01:09,517 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:10,014 - INFO - Epoch 384: train_loss=1.0851
2025-02-04 17:01:10,538 - INFO - Epoch 384: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:10,543 - INFO - Epoch 384: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 17:01:10,553 - INFO - #################### Training epoch 385 ####################
2025-02-04 17:01:10,553 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:10,886 - INFO - Epoch 385: train_loss=1.0850
2025-02-04 17:01:11,266 - INFO - Epoch 385: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:11,271 - INFO - Epoch 385: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:01:11,281 - INFO - #################### Training epoch 386 ####################
2025-02-04 17:01:11,281 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:11,945 - INFO - Epoch 386: train_loss=1.0852
2025-02-04 17:01:12,445 - INFO - Epoch 386: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:12,451 - INFO - Epoch 386: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:01:12,461 - INFO - #################### Training epoch 387 ####################
2025-02-04 17:01:12,461 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:13,097 - INFO - Epoch 387: train_loss=1.0849
2025-02-04 17:01:13,577 - INFO - Epoch 387: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:13,584 - INFO - Epoch 387: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 17:01:13,599 - INFO - #################### Training epoch 388 ####################
2025-02-04 17:01:13,600 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:14,183 - INFO - Epoch 388: train_loss=1.0854
2025-02-04 17:01:14,701 - INFO - Epoch 388: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:14,705 - INFO - Epoch 388: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:01:14,714 - INFO - #################### Training epoch 389 ####################
2025-02-04 17:01:14,714 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:15,304 - INFO - Epoch 389: train_loss=1.0849
2025-02-04 17:01:15,795 - INFO - Epoch 389: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:15,800 - INFO - Epoch 389: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 17:01:15,810 - INFO - #################### Training epoch 390 ####################
2025-02-04 17:01:15,810 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:16,173 - INFO - Epoch 390: train_loss=1.0854
2025-02-04 17:01:16,655 - INFO - Epoch 390: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:16,659 - INFO - Epoch 390: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:01:16,677 - INFO - #################### Training epoch 391 ####################
2025-02-04 17:01:16,677 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:17,282 - INFO - Epoch 391: train_loss=1.0850
2025-02-04 17:01:17,706 - INFO - Epoch 391: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:17,711 - INFO - Epoch 391: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:01:17,721 - INFO - #################### Training epoch 392 ####################
2025-02-04 17:01:17,721 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:18,332 - INFO - Epoch 392: train_loss=1.0853
2025-02-04 17:01:18,842 - INFO - Epoch 392: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:18,847 - INFO - Epoch 392: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:01:18,864 - INFO - #################### Training epoch 393 ####################
2025-02-04 17:01:18,864 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:19,096 - INFO - Epoch 393: train_loss=1.0855
2025-02-04 17:01:19,643 - INFO - Epoch 393: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:19,652 - INFO - Epoch 393: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 17:01:19,671 - INFO - #################### Training epoch 394 ####################
2025-02-04 17:01:19,671 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:20,233 - INFO - Epoch 394: train_loss=1.0851
2025-02-04 17:01:20,603 - INFO - Epoch 394: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:20,608 - INFO - Epoch 394: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 17:01:20,619 - INFO - #################### Training epoch 395 ####################
2025-02-04 17:01:20,619 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:21,289 - INFO - Epoch 395: train_loss=1.0851
2025-02-04 17:01:21,828 - INFO - Epoch 395: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:21,832 - INFO - Epoch 395: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 17:01:21,839 - INFO - #################### Training epoch 396 ####################
2025-02-04 17:01:21,840 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:22,480 - INFO - Epoch 396: train_loss=1.0854
2025-02-04 17:01:22,997 - INFO - Epoch 396: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:23,003 - INFO - Epoch 396: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:01:23,019 - INFO - #################### Training epoch 397 ####################
2025-02-04 17:01:23,019 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:23,604 - INFO - Epoch 397: train_loss=1.0854
2025-02-04 17:01:24,130 - INFO - Epoch 397: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:24,136 - INFO - Epoch 397: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:01:24,143 - INFO - #################### Training epoch 398 ####################
2025-02-04 17:01:24,143 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:24,710 - INFO - Epoch 398: train_loss=1.0849
2025-02-04 17:01:25,124 - INFO - Epoch 398: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:25,128 - INFO - Epoch 398: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 17:01:25,131 - INFO - #################### Training epoch 399 ####################
2025-02-04 17:01:25,131 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:25,815 - INFO - Epoch 399: train_loss=1.0852
2025-02-04 17:01:26,290 - INFO - Epoch 399: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:26,294 - INFO - Epoch 399: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:01:26,305 - INFO - #################### Training epoch 400 ####################
2025-02-04 17:01:26,306 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:26,820 - INFO - Epoch 400: train_loss=1.0854
2025-02-04 17:01:27,317 - INFO - Epoch 400: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:27,321 - INFO - Epoch 400: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:01:27,340 - INFO - #################### Training epoch 401 ####################
2025-02-04 17:01:27,341 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:27,926 - INFO - Epoch 401: train_loss=1.0852
2025-02-04 17:01:28,362 - INFO - Epoch 401: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:28,368 - INFO - Epoch 401: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:01:28,379 - INFO - #################### Training epoch 402 ####################
2025-02-04 17:01:28,380 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:29,018 - INFO - Epoch 402: train_loss=1.0855
2025-02-04 17:01:29,596 - INFO - Epoch 402: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:29,600 - INFO - Epoch 402: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 17:01:29,611 - INFO - #################### Training epoch 403 ####################
2025-02-04 17:01:29,611 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:30,110 - INFO - Epoch 403: train_loss=1.0856
2025-02-04 17:01:30,631 - INFO - Epoch 403: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:30,637 - INFO - Epoch 403: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 17:01:30,648 - INFO - #################### Training epoch 404 ####################
2025-02-04 17:01:30,648 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:30,990 - INFO - Epoch 404: train_loss=1.0850
2025-02-04 17:01:31,364 - INFO - Epoch 404: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:31,368 - INFO - Epoch 404: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:01:31,378 - INFO - #################### Training epoch 405 ####################
2025-02-04 17:01:31,378 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:32,057 - INFO - Epoch 405: train_loss=1.0854
2025-02-04 17:01:32,600 - INFO - Epoch 405: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:32,604 - INFO - Epoch 405: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:01:32,616 - INFO - #################### Training epoch 406 ####################
2025-02-04 17:01:32,616 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:33,258 - INFO - Epoch 406: train_loss=1.0854
2025-02-04 17:01:33,740 - INFO - Epoch 406: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:33,747 - INFO - Epoch 406: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:01:33,763 - INFO - #################### Training epoch 407 ####################
2025-02-04 17:01:33,763 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:34,344 - INFO - Epoch 407: train_loss=1.0854
2025-02-04 17:01:34,860 - INFO - Epoch 407: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:34,864 - INFO - Epoch 407: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:01:34,872 - INFO - #################### Training epoch 408 ####################
2025-02-04 17:01:34,872 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:35,447 - INFO - Epoch 408: train_loss=1.0849
2025-02-04 17:01:35,944 - INFO - Epoch 408: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:35,949 - INFO - Epoch 408: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 17:01:35,959 - INFO - #################### Training epoch 409 ####################
2025-02-04 17:01:35,959 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:36,330 - INFO - Epoch 409: train_loss=1.0854
2025-02-04 17:01:36,802 - INFO - Epoch 409: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:36,807 - INFO - Epoch 409: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:01:36,817 - INFO - #################### Training epoch 410 ####################
2025-02-04 17:01:36,817 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:37,426 - INFO - Epoch 410: train_loss=1.0853
2025-02-04 17:01:37,885 - INFO - Epoch 410: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:37,889 - INFO - Epoch 410: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:01:37,899 - INFO - #################### Training epoch 411 ####################
2025-02-04 17:01:37,900 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:38,489 - INFO - Epoch 411: train_loss=1.0857
2025-02-04 17:01:39,057 - INFO - Epoch 411: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:39,062 - INFO - Epoch 411: EPOCH_AVG_TRAIN_LOSS=1.0857
2025-02-04 17:01:39,072 - INFO - #################### Training epoch 412 ####################
2025-02-04 17:01:39,072 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:39,561 - INFO - Epoch 412: train_loss=1.0853
2025-02-04 17:01:40,094 - INFO - Epoch 412: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:40,099 - INFO - Epoch 412: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:01:40,109 - INFO - #################### Training epoch 413 ####################
2025-02-04 17:01:40,109 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:40,442 - INFO - Epoch 413: train_loss=1.0855
2025-02-04 17:01:40,798 - INFO - Epoch 413: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:40,803 - INFO - Epoch 413: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 17:01:40,813 - INFO - #################### Training epoch 414 ####################
2025-02-04 17:01:40,813 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:41,479 - INFO - Epoch 414: train_loss=1.0856
2025-02-04 17:01:42,019 - INFO - Epoch 414: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:42,023 - INFO - Epoch 414: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 17:01:42,031 - INFO - #################### Training epoch 415 ####################
2025-02-04 17:01:42,031 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:42,661 - INFO - Epoch 415: train_loss=1.0848
2025-02-04 17:01:43,173 - INFO - Epoch 415: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:43,177 - INFO - Epoch 415: EPOCH_AVG_TRAIN_LOSS=1.0848
2025-02-04 17:01:43,193 - INFO - #################### Training epoch 416 ####################
2025-02-04 17:01:43,193 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:43,763 - INFO - Epoch 416: train_loss=1.0855
2025-02-04 17:01:44,254 - INFO - Epoch 416: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:44,259 - INFO - Epoch 416: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 17:01:44,269 - INFO - #################### Training epoch 417 ####################
2025-02-04 17:01:44,270 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:44,851 - INFO - Epoch 417: train_loss=1.0853
2025-02-04 17:01:45,263 - INFO - Epoch 417: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:45,266 - INFO - Epoch 417: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:01:45,270 - INFO - #################### Training epoch 418 ####################
2025-02-04 17:01:45,270 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:45,940 - INFO - Epoch 418: train_loss=1.0847
2025-02-04 17:01:46,479 - INFO - Epoch 418: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:46,484 - INFO - Epoch 418: EPOCH_AVG_TRAIN_LOSS=1.0847
2025-02-04 17:01:46,494 - INFO - #################### Training epoch 419 ####################
2025-02-04 17:01:46,494 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:46,963 - INFO - Epoch 419: train_loss=1.0850
2025-02-04 17:01:47,437 - INFO - Epoch 419: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:47,441 - INFO - Epoch 419: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:01:47,452 - INFO - #################### Training epoch 420 ####################
2025-02-04 17:01:47,452 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:48,056 - INFO - Epoch 420: train_loss=1.0853
2025-02-04 17:01:48,511 - INFO - Epoch 420: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:48,516 - INFO - Epoch 420: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:01:48,526 - INFO - #################### Training epoch 421 ####################
2025-02-04 17:01:48,526 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:49,108 - INFO - Epoch 421: train_loss=1.0852
2025-02-04 17:01:49,680 - INFO - Epoch 421: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:49,685 - INFO - Epoch 421: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:01:49,695 - INFO - #################### Training epoch 422 ####################
2025-02-04 17:01:49,696 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:50,141 - INFO - Epoch 422: train_loss=1.0857
2025-02-04 17:01:50,713 - INFO - Epoch 422: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:50,717 - INFO - Epoch 422: EPOCH_AVG_TRAIN_LOSS=1.0857
2025-02-04 17:01:50,723 - INFO - #################### Training epoch 423 ####################
2025-02-04 17:01:50,723 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:51,062 - INFO - Epoch 423: train_loss=1.0851
2025-02-04 17:01:51,424 - INFO - Epoch 423: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:51,429 - INFO - Epoch 423: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 17:01:51,440 - INFO - #################### Training epoch 424 ####################
2025-02-04 17:01:51,440 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:52,116 - INFO - Epoch 424: train_loss=1.0852
2025-02-04 17:01:52,654 - INFO - Epoch 424: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:52,658 - INFO - Epoch 424: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:01:52,664 - INFO - #################### Training epoch 425 ####################
2025-02-04 17:01:52,664 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:53,312 - INFO - Epoch 425: train_loss=1.0849
2025-02-04 17:01:53,805 - INFO - Epoch 425: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:53,814 - INFO - Epoch 425: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 17:01:53,826 - INFO - #################### Training epoch 426 ####################
2025-02-04 17:01:53,826 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:54,412 - INFO - Epoch 426: train_loss=1.0851
2025-02-04 17:01:54,897 - INFO - Epoch 426: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:54,903 - INFO - Epoch 426: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 17:01:54,913 - INFO - #################### Training epoch 427 ####################
2025-02-04 17:01:54,913 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:55,249 - INFO - Epoch 427: train_loss=1.0855
2025-02-04 17:01:55,723 - INFO - Epoch 427: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:55,727 - INFO - Epoch 427: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 17:01:55,740 - INFO - #################### Training epoch 428 ####################
2025-02-04 17:01:55,740 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:56,346 - INFO - Epoch 428: train_loss=1.0852
2025-02-04 17:01:56,791 - INFO - Epoch 428: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:56,796 - INFO - Epoch 428: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:01:56,806 - INFO - #################### Training epoch 429 ####################
2025-02-04 17:01:56,806 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:57,406 - INFO - Epoch 429: train_loss=1.0855
2025-02-04 17:01:57,915 - INFO - Epoch 429: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:57,920 - INFO - Epoch 429: EPOCH_AVG_TRAIN_LOSS=1.0855
2025-02-04 17:01:57,931 - INFO - #################### Training epoch 430 ####################
2025-02-04 17:01:57,931 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:58,162 - INFO - Epoch 430: train_loss=1.0849
2025-02-04 17:01:58,695 - INFO - Epoch 430: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:58,704 - INFO - Epoch 430: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 17:01:58,724 - INFO - #################### Training epoch 431 ####################
2025-02-04 17:01:58,724 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:59,298 - INFO - Epoch 431: train_loss=1.0853
2025-02-04 17:01:59,662 - INFO - Epoch 431: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:01:59,667 - INFO - Epoch 431: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:01:59,677 - INFO - #################### Training epoch 432 ####################
2025-02-04 17:01:59,677 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:00,345 - INFO - Epoch 432: train_loss=1.0851
2025-02-04 17:02:00,888 - INFO - Epoch 432: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:00,893 - INFO - Epoch 432: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 17:02:00,898 - INFO - #################### Training epoch 433 ####################
2025-02-04 17:02:00,898 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:01,546 - INFO - Epoch 433: train_loss=1.0853
2025-02-04 17:02:02,042 - INFO - Epoch 433: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:02,046 - INFO - Epoch 433: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:02:02,062 - INFO - #################### Training epoch 434 ####################
2025-02-04 17:02:02,062 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:02,652 - INFO - Epoch 434: train_loss=1.0849
2025-02-04 17:02:03,167 - INFO - Epoch 434: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:03,173 - INFO - Epoch 434: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 17:02:03,180 - INFO - #################### Training epoch 435 ####################
2025-02-04 17:02:03,180 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:03,769 - INFO - Epoch 435: train_loss=1.0857
2025-02-04 17:02:04,275 - INFO - Epoch 435: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:04,280 - INFO - Epoch 435: EPOCH_AVG_TRAIN_LOSS=1.0857
2025-02-04 17:02:04,287 - INFO - #################### Training epoch 436 ####################
2025-02-04 17:02:04,287 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:04,638 - INFO - Epoch 436: train_loss=1.0849
2025-02-04 17:02:05,128 - INFO - Epoch 436: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:05,138 - INFO - Epoch 436: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 17:02:05,157 - INFO - #################### Training epoch 437 ####################
2025-02-04 17:02:05,157 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:05,743 - INFO - Epoch 437: train_loss=1.0850
2025-02-04 17:02:06,175 - INFO - Epoch 437: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:06,179 - INFO - Epoch 437: EPOCH_AVG_TRAIN_LOSS=1.0850
2025-02-04 17:02:06,190 - INFO - #################### Training epoch 438 ####################
2025-02-04 17:02:06,190 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:06,803 - INFO - Epoch 438: train_loss=1.0851
2025-02-04 17:02:07,337 - INFO - Epoch 438: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:07,342 - INFO - Epoch 438: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 17:02:07,352 - INFO - #################### Training epoch 439 ####################
2025-02-04 17:02:07,352 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:07,718 - INFO - Epoch 439: train_loss=1.0856
2025-02-04 17:02:08,291 - INFO - Epoch 439: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:08,296 - INFO - Epoch 439: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 17:02:08,303 - INFO - #################### Training epoch 440 ####################
2025-02-04 17:02:08,303 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:08,713 - INFO - Epoch 440: train_loss=1.0856
2025-02-04 17:02:09,108 - INFO - Epoch 440: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:09,113 - INFO - Epoch 440: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 17:02:09,123 - INFO - #################### Training epoch 441 ####################
2025-02-04 17:02:09,123 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:09,743 - INFO - Epoch 441: train_loss=1.0853
2025-02-04 17:02:10,302 - INFO - Epoch 441: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:10,306 - INFO - Epoch 441: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:02:10,316 - INFO - #################### Training epoch 442 ####################
2025-02-04 17:02:10,317 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:10,887 - INFO - Epoch 442: train_loss=1.0854
2025-02-04 17:02:11,501 - INFO - Epoch 442: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:11,506 - INFO - Epoch 442: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:02:11,516 - INFO - #################### Training epoch 443 ####################
2025-02-04 17:02:11,516 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:11,962 - INFO - Epoch 443: train_loss=1.0852
2025-02-04 17:02:12,536 - INFO - Epoch 443: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:12,542 - INFO - Epoch 443: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:02:12,552 - INFO - #################### Training epoch 444 ####################
2025-02-04 17:02:12,552 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:12,883 - INFO - Epoch 444: train_loss=1.0847
2025-02-04 17:02:13,256 - INFO - Epoch 444: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:13,262 - INFO - Epoch 444: EPOCH_AVG_TRAIN_LOSS=1.0847
2025-02-04 17:02:13,273 - INFO - #################### Training epoch 445 ####################
2025-02-04 17:02:13,273 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:13,951 - INFO - Epoch 445: train_loss=1.0852
2025-02-04 17:02:14,483 - INFO - Epoch 445: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:14,487 - INFO - Epoch 445: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:02:14,498 - INFO - #################### Training epoch 446 ####################
2025-02-04 17:02:14,498 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:15,134 - INFO - Epoch 446: train_loss=1.0853
2025-02-04 17:02:15,636 - INFO - Epoch 446: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:15,645 - INFO - Epoch 446: EPOCH_AVG_TRAIN_LOSS=1.0853
2025-02-04 17:02:15,659 - INFO - #################### Training epoch 447 ####################
2025-02-04 17:02:15,659 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:16,261 - INFO - Epoch 447: train_loss=1.0849
2025-02-04 17:02:16,739 - INFO - Epoch 447: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:16,744 - INFO - Epoch 447: EPOCH_AVG_TRAIN_LOSS=1.0849
2025-02-04 17:02:16,754 - INFO - #################### Training epoch 448 ####################
2025-02-04 17:02:16,754 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:17,076 - INFO - Epoch 448: train_loss=1.0854
2025-02-04 17:02:17,534 - INFO - Epoch 448: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:17,538 - INFO - Epoch 448: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:02:17,549 - INFO - #################### Training epoch 449 ####################
2025-02-04 17:02:17,549 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:18,169 - INFO - Epoch 449: train_loss=1.0856
2025-02-04 17:02:18,612 - INFO - Epoch 449: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:18,617 - INFO - Epoch 449: EPOCH_AVG_TRAIN_LOSS=1.0856
2025-02-04 17:02:18,627 - INFO - #################### Training epoch 450 ####################
2025-02-04 17:02:18,627 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:19,236 - INFO - Epoch 450: train_loss=1.0852
2025-02-04 17:02:19,727 - INFO - Epoch 450: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:19,731 - INFO - Epoch 450: EPOCH_AVG_TRAIN_LOSS=1.0852
2025-02-04 17:02:19,742 - INFO - #################### Training epoch 451 ####################
2025-02-04 17:02:19,742 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:19,980 - INFO - Epoch 451: train_loss=1.0854
2025-02-04 17:02:20,522 - INFO - Epoch 451: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:20,531 - INFO - Epoch 451: EPOCH_AVG_TRAIN_LOSS=1.0854
2025-02-04 17:02:20,550 - INFO - #################### Training epoch 452 ####################
2025-02-04 17:02:20,550 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:21,117 - INFO - Epoch 452: train_loss=1.0851
2025-02-04 17:02:21,496 - INFO - Epoch 452: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:21,501 - INFO - Epoch 452: EPOCH_AVG_TRAIN_LOSS=1.0851
2025-02-04 17:02:21,511 - INFO - #################### Training epoch 453 ####################
2025-02-04 17:02:21,511 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:22,171 - INFO - Epoch 453: train_loss=1.0858
2025-02-04 17:02:22,682 - INFO - Epoch 453: val_loss=1.0580, val_acc=33.33%
2025-02-04 17:02:22,686 - INFO - Epoch 453: EPOCH_AVG_TRAIN_LOSS=1.0858
2025-02-04 17:02:22,697 - INFO - #################### Training epoch 454 ####################
2025-02-04 17:02:22,697 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:23,334 - INFO - Epoch 454: train_loss=1.0852
