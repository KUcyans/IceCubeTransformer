nohup: ignoring input
wandb: Currently logged in as: cyans (cyans-k-benhavns-universitet) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.5
wandb: Run data is saved locally in /lustre/hpc/icecube/cyan/factory/IceCubeTransformer/wandb/run-20250204_163508-3nidyxib
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-bird-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/cyans-k-benhavns-universitet/%5B20250204_163459%5D%20Flavour%20Classification
wandb: üöÄ View run at https://wandb.ai/cyans-k-benhavns-universitet/%5B20250204_163459%5D%20Flavour%20Classification/runs/3nidyxib
/groups/icecube/cyan/.local/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.9 TrainingDebuggingYard.py --date 20250204 --time 1 ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/groups/icecube/cyan/.local/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/groups/icecube/cyan/.local/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /lustre/hpc/icecube/cyan/factory/IceCubeTransformer/checkpoints/20250204 exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
/groups/icecube/cyan/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(

  | Name                        | Type       | Params | Mode 
-------------------------------------------------------------------
0 | input_projection            | Linear     | 4.2 K  | train
1 | encoder_blocks              | ModuleList | 297 K  | train
2 | classification_output_layer | Linear     | 387    | train
-------------------------------------------------------------------
301 K     Trainable params
0         Non-trainable params
301 K     Total params
1.207     Total estimated model params size (MB)
Number of GPUs available: 2
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
Number of GPUs available: 2
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
------------- Multi-Flavour Shard (Energy Band: ER_1_PEV_100_PEV, Part: 1, Shard: 1) -------------
------------- Multi-Flavour Shard (Energy Band: ER_1_PEV_100_PEV, Part: 1, Shard: 1) -------------
Config validation passed.
Dataset split into train (24), val (3), and test (3)
Class weights: tensor([0.1250, 0.1250, 0.1250])
Sanity Checking: |          | 0/? [00:00<?, ?it/s]/groups/icecube/cyan/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.47it/s]                                                                           Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]  mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 0: train_loss=1.3488
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.61it/s]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.61it/s, v_num=yxib]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.38it/s][A
                                                                      [AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.55it/s, v_num=yxib]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.55it/s, v_num=yxib]Metric val_acc improved. New best score: 0.667
Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib]        Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 12 because it has no valid entries in the mask.
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 12 because it has no valid entries in the mask.
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 12 because it has no valid entries in the mask.
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 0: train_loss=1.0999
Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=yxib]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=yxib, epoch_avg_train_loss=1.350]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 64.48it/s][A
                                                                      [AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.350]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.350]Metric val_acc improved by 0.667 >= min_delta = 0.0. New best score: 0.000
Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.350]        Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.350] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 0: train_loss=1.1715
Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=yxib, epoch_avg_train_loss=1.350]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.100]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.39it/s][A
                                                                      [AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=yxib, epoch_avg_train_loss=1.100]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=yxib, epoch_avg_train_loss=1.100]Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.100]        Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.100] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 0: train_loss=1.1763
Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s, v_num=yxib, epoch_avg_train_loss=1.100]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s, v_num=yxib, epoch_avg_train_loss=1.170]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.83it/s][A
                                                                      [AEpoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.170]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.170]Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.170]        Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.170] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 0: train_loss=1.1230
Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s, v_num=yxib, epoch_avg_train_loss=1.170]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.36it/s, v_num=yxib, epoch_avg_train_loss=1.180]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.36it/s][A
                                                                      [AEpoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.180]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.180]Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.180]        Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.180] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 0: train_loss=1.0890
Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=yxib, epoch_avg_train_loss=1.180]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=yxib, epoch_avg_train_loss=1.120]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.03it/s][A
                                                                      [AEpoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.120]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.120]Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.120]        Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.120] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 0: train_loss=1.0946
Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=yxib, epoch_avg_train_loss=1.120]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=yxib, epoch_avg_train_loss=1.090]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 64.84it/s][A
                                                                      [AEpoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.84it/s, v_num=yxib, epoch_avg_train_loss=1.090]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.84it/s, v_num=yxib, epoch_avg_train_loss=1.090]Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.090]        Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.090] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 0: train_loss=1.0995
Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=yxib, epoch_avg_train_loss=1.090]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=yxib, epoch_avg_train_loss=1.090]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.29it/s][A
                                                                      [AEpoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=yxib, epoch_avg_train_loss=1.090]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=yxib, epoch_avg_train_loss=1.090]Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.090]        Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.090] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 8: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 9: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 8: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 9: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 8: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 9: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
Batch 0: train_loss=1.0957
Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=yxib, epoch_avg_train_loss=1.090]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=yxib, epoch_avg_train_loss=1.100]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.78it/s][A
                                                                      [AEpoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.100]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=yxib, epoch_avg_train_loss=1.100]Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.100]        Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.100] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 12 because it has no valid entries in the mask.
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 12 because it has no valid entries in the mask.
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 12 because it has no valid entries in the mask.
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 0: train_loss=1.0812
Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s, v_num=yxib, epoch_avg_train_loss=1.100]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.36it/s, v_num=yxib, epoch_avg_train_loss=1.100]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.43it/s][A
                                                                      [AEpoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.100]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.100]Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.100]        Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.100] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 0: train_loss=1.0766
Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.100]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=yxib, epoch_avg_train_loss=1.080]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 62.11it/s][A
                                                                      [AEpoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.080]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.080]Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.080]        Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.080] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 0: train_loss=1.0810
Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.080]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=yxib, epoch_avg_train_loss=1.080]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.51it/s][A
                                                                      [AEpoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.080]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.080]Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.080]        Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.080] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 0: train_loss=1.0799
Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=yxib, epoch_avg_train_loss=1.080]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=yxib, epoch_avg_train_loss=1.080]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 64.63it/s][A
                                                                      [AEpoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.080]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.080]Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.080]        Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.080] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 0: train_loss=1.0721
Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=yxib, epoch_avg_train_loss=1.080]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.080]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.51it/s][A
                                                                      [AEpoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.080]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.080]Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.080]        Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.080] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 0: train_loss=1.0675
Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=yxib, epoch_avg_train_loss=1.080]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=yxib, epoch_avg_train_loss=1.070]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.91it/s][A
                                                                      [AEpoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.070]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.070]Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.070]        Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.070] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 0: train_loss=1.0646
Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=yxib, epoch_avg_train_loss=1.070]Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=yxib, epoch_avg_train_loss=1.070]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.36it/s][A
                                                                      [AEpoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.070]Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.070]Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.070]        Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.070] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 12 because it has no valid entries in the mask.
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 12 because it has no valid entries in the mask.
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 12 because it has no valid entries in the mask.
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0630
Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=yxib, epoch_avg_train_loss=1.070]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=yxib, epoch_avg_train_loss=1.060]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.70it/s][A
                                                                      [AEpoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.060]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.060]Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.060]        Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.060] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0622
Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.57it/s, v_num=yxib, epoch_avg_train_loss=1.060]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.56it/s, v_num=yxib, epoch_avg_train_loss=1.060]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.30it/s][A
                                                                      [AEpoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.060]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.060]Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.060]        Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.060] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 0: train_loss=1.0604
Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=yxib, epoch_avg_train_loss=1.060]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=yxib, epoch_avg_train_loss=1.060]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.14it/s][A
                                                                      [AEpoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.060]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.060]Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.060]        Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.060] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 21: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 21: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 21: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 0: train_loss=1.0569
Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.55it/s, v_num=yxib, epoch_avg_train_loss=1.060]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s, v_num=yxib, epoch_avg_train_loss=1.060]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.89it/s][A
                                                                      [AEpoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.060]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.060]Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.060]        Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.060] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 0: train_loss=1.0547
Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.55it/s, v_num=yxib, epoch_avg_train_loss=1.060]Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.55it/s, v_num=yxib, epoch_avg_train_loss=1.060]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.19it/s][A
                                                                      [AEpoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.060]Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.060]Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.060]        Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.060] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 0: train_loss=1.0511
Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s, v_num=yxib, epoch_avg_train_loss=1.060]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s, v_num=yxib, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 60.50it/s][A
                                                                      [AEpoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.050]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.050]Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.050]        Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 13 because it has no valid entries in the mask.
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 13 because it has no valid entries in the mask.
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 13 because it has no valid entries in the mask.
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0490
Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.36it/s, v_num=yxib, epoch_avg_train_loss=1.050]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.36it/s, v_num=yxib, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.27it/s][A
                                                                      [AEpoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.050]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.050]Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.050]        Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 0: train_loss=1.0477
Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=yxib, epoch_avg_train_loss=1.050]Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=yxib, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 64.44it/s][A
                                                                      [AEpoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.050]Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.050]Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.050]        Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 5: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 5: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 5: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
Batch 0: train_loss=1.0443
Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=yxib, epoch_avg_train_loss=1.050]Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.61it/s][A
                                                                      [AEpoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.050]Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.050]Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.050]        Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.0422
Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s, v_num=yxib, epoch_avg_train_loss=1.050]Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s, v_num=yxib, epoch_avg_train_loss=1.040]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 64.19it/s][A
                                                                      [AEpoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.040]        Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.040] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 0: train_loss=1.0419
Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=yxib, epoch_avg_train_loss=1.040]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 63.66it/s][A
                                                                      [AEpoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.040]        Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.040] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 0: train_loss=1.0410
Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=yxib, epoch_avg_train_loss=1.040]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.51it/s][A
                                                                      [AEpoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.040]        Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.040] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 0: train_loss=1.0385
Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=yxib, epoch_avg_train_loss=1.040]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.02it/s][A
                                                                      [AEpoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.040]        Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.040] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 13 because it has no valid entries in the mask.
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 13 because it has no valid entries in the mask.
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 13 because it has no valid entries in the mask.
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 0: train_loss=1.0376
Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=yxib, epoch_avg_train_loss=1.040]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.82it/s][A
                                                                      [AEpoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.040]        Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.040] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
Batch 0: train_loss=1.0355
Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=yxib, epoch_avg_train_loss=1.040]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.70it/s][A
                                                                      [AEpoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.040]        Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.040] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 6: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 6: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 6: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 0: train_loss=1.0330
Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.040]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.31it/s][A
                                                                      [AEpoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.040]        Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.040] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 0: train_loss=1.0319
Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=yxib, epoch_avg_train_loss=1.040]Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=yxib, epoch_avg_train_loss=1.030]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 64.82it/s][A
                                                                      [AEpoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030]        Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0321
Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=yxib, epoch_avg_train_loss=1.030]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.60it/s][A
                                                                      [AEpoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030]        Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 10 because it has no valid entries in the mask.
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 10 because it has no valid entries in the mask.
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 10 because it has no valid entries in the mask.
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 0: train_loss=1.0325
Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=yxib, epoch_avg_train_loss=1.030]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 64.74it/s][A
                                                                      [AEpoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030]        Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0303
Epoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=yxib, epoch_avg_train_loss=1.030]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.10it/s][A
                                                                      [AEpoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030]        Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 0: train_loss=1.0297
Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=yxib, epoch_avg_train_loss=1.030]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.77it/s][A
                                                                      [AEpoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030]        Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
Batch 0: train_loss=1.0290
Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=yxib, epoch_avg_train_loss=1.030]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.96it/s][A
                                                                      [AEpoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030]        Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 13 because it has no valid entries in the mask.
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 13 because it has no valid entries in the mask.
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 13 because it has no valid entries in the mask.
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0286
Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=yxib, epoch_avg_train_loss=1.030]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.29it/s][A
                                                                      [AEpoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030]        Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 19: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 19: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 19: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.0276
Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=yxib, epoch_avg_train_loss=1.030]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.32it/s][A
                                                                      [AEpoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030]        Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 0: train_loss=1.0268
Epoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, v_num=yxib, epoch_avg_train_loss=1.030]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.71it/s][A
                                                                      [AEpoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030]        Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.0262
Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.030]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.56it/s][A
                                                                      [AEpoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030]        Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 0: train_loss=1.0275
Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.030]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.36it/s][A
                                                                      [AEpoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030]        Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 13: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 13: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 13: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 0: train_loss=1.0264
Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s, v_num=yxib, epoch_avg_train_loss=1.030]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.23it/s][A
                                                                      [AEpoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030]        Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 0: train_loss=1.0247
Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s, v_num=yxib, epoch_avg_train_loss=1.030]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.12it/s][A
                                                                      [AEpoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030]        Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 0: train_loss=1.0251
Epoch 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.86it/s][A
                                                                      [AEpoch 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 0: train_loss=1.0245
Epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s, v_num=yxib, epoch_avg_train_loss=1.030]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.66it/s][A
                                                                      [AEpoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030]        Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 0: train_loss=1.0247
Epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.86it/s][A
                                                                      [AEpoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 0: train_loss=1.0251
Epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.07it/s][A
                                                                      [AEpoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 1: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 1: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 1: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
Batch 0: train_loss=1.0242
Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, v_num=yxib, epoch_avg_train_loss=1.030]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.96it/s][A
                                                                      [AEpoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030]        Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.030] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 19: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 23: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 19: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 23: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 19: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 23: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 0: train_loss=1.0244
Epoch 50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s, v_num=yxib, epoch_avg_train_loss=1.030]Epoch 50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.43it/s][A
                                                                      [AEpoch 50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 0: train_loss=1.0236
Epoch 51: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 51: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.66it/s][A
                                                                      [AEpoch 51: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 51: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 10 because it has no valid entries in the mask.
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 10 because it has no valid entries in the mask.
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 10 because it has no valid entries in the mask.
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0235
Epoch 52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.76it/s][A
                                                                      [AEpoch 52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 0: train_loss=1.0243
Epoch 53: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 53: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.05it/s][A
                                                                      [AEpoch 53: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 53: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0243
Epoch 54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.92it/s][A
                                                                      [AEpoch 54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0236
Epoch 55: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 55: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.06it/s][A
                                                                      [AEpoch 55: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 55: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0242
Epoch 56: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 56: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.31it/s][A
                                                                      [AEpoch 56: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 56: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 21 because it has no valid entries in the mask.
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 21 because it has no valid entries in the mask.
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 21 because it has no valid entries in the mask.
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 0: train_loss=1.0232
Epoch 57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.95it/s][A
                                                                      [AEpoch 57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 10 because it has no valid entries in the mask.
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 20: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 10 because it has no valid entries in the mask.
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 20: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 10 because it has no valid entries in the mask.
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 20: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 0: train_loss=1.0230
Epoch 58: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 58: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.13it/s][A
                                                                      [AEpoch 58: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 58: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 0: train_loss=1.0238
Epoch 59: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 59: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.67it/s][A
                                                                      [AEpoch 59: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 59: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 0: train_loss=1.0227
Epoch 60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.69it/s][A
                                                                      [AEpoch 60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 0: train_loss=1.0232
Epoch 61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.90it/s][A
                                                                      [AEpoch 61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 0: train_loss=1.0234
Epoch 62: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 62: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.74it/s][A
                                                                      [AEpoch 62: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 62: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 0: train_loss=1.0227
Epoch 63: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 63: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.04it/s][A
                                                                      [AEpoch 63: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 63: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 0: train_loss=1.0229
Epoch 64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.59it/s][A
                                                                      [AEpoch 64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.0226
Epoch 65: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 65: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.44it/s][A
                                                                      [AEpoch 65: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 65: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 0: train_loss=1.0230
Epoch 66: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 66: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.02it/s][A
                                                                      [AEpoch 66: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 66: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 0: train_loss=1.0225
Epoch 67: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 67: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.05it/s][A
                                                                      [AEpoch 67: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 67: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 0: train_loss=1.0231
Epoch 68: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 68: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.70it/s][A
                                                                      [AEpoch 68: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 68: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 0: train_loss=1.0223
Epoch 69: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 69: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.39it/s][A
                                                                      [AEpoch 69: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 69: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 12 because it has no valid entries in the mask.
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 12 because it has no valid entries in the mask.
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 12 because it has no valid entries in the mask.
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.0219
Epoch 70: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 70: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.33it/s][A
                                                                      [AEpoch 70: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 70: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 0: train_loss=1.0225
Epoch 71: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 71: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.13it/s][A
                                                                      [AEpoch 71: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 71: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 0: train_loss=1.0219
Epoch 72: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 72: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.48it/s][A
                                                                      [AEpoch 72: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 72: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.0224
Epoch 73: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 73: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.49it/s][A
                                                                      [AEpoch 73: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 73: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 0: train_loss=1.0229
Epoch 74: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 74: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 64.52it/s][A
                                                                      [AEpoch 74: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 74: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 12 because it has no valid entries in the mask.
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 12 because it has no valid entries in the mask.
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 12 because it has no valid entries in the mask.
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 0: train_loss=1.0221
Epoch 75: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 75: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.92it/s][A
                                                                      [AEpoch 75: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 75: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 21 because it has no valid entries in the mask.
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 21 because it has no valid entries in the mask.
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 3 because it has no valid entries in the mask.
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 21 because it has no valid entries in the mask.
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 0: train_loss=1.0229
Epoch 76: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 76: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.04it/s][A
                                                                      [AEpoch 76: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.91it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 76: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 0: train_loss=1.0230
Epoch 77: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.36it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 77: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.35it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.35it/s][A
                                                                      [AEpoch 77: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 77: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 16 because it has no valid entries in the mask.
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 0: train_loss=1.0224
Epoch 78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.26it/s][A
                                                                      [AEpoch 78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 8: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 8: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 8: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 0: train_loss=1.0227
Epoch 79: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 79: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.62it/s][A
                                                                      [AEpoch 79: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 79: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 0: train_loss=1.0225
Epoch 80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.64it/s][A
                                                                      [AEpoch 80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 8 because it has no valid entries in the mask.
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 0: train_loss=1.0232
Epoch 81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 62.07it/s][A
                                                                      [AEpoch 81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 10 because it has no valid entries in the mask.
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 10 because it has no valid entries in the mask.
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 10 because it has no valid entries in the mask.
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 0: train_loss=1.0223
Epoch 82: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 82: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.07it/s][A
                                                                      [AEpoch 82: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 82: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 21 because it has no valid entries in the mask.
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 21 because it has no valid entries in the mask.
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 21 because it has no valid entries in the mask.
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 0: train_loss=1.0229
Epoch 83: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 83: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.20it/s][A
                                                                      [AEpoch 83: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 83: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 21 because it has no valid entries in the mask.
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 21 because it has no valid entries in the mask.
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 21 because it has no valid entries in the mask.
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 0: train_loss=1.0229
Epoch 84: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 84: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.27it/s][A
                                                                      [AEpoch 84: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 84: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 18 because it has no valid entries in the mask.
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 0: train_loss=1.0225
Epoch 85: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 85: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.07it/s][A
                                                                      [AEpoch 85: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 85: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 1: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 1: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 1: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.0227
Epoch 86: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 86: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.30it/s][A
                                                                      [AEpoch 86: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 86: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
Batch 0: train_loss=1.0225
Epoch 87: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 87: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.05it/s][A
                                                                      [AEpoch 87: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 87: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 22 because it has no valid entries in the mask.
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 0: train_loss=1.0224
Epoch 88: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 88: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.95it/s][A
                                                                      [AEpoch 88: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 88: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
Batch 0: train_loss=1.0217
Epoch 89: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 89: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.30it/s][A
                                                                      [AEpoch 89: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 89: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 21 because it has no valid entries in the mask.
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 21 because it has no valid entries in the mask.
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 19 because it has no valid entries in the mask.
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 21 because it has no valid entries in the mask.
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 0: train_loss=1.0228
Epoch 90: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 90: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.20it/s][A
                                                                      [AEpoch 90: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 90: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 15 because it has no valid entries in the mask.
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 0: train_loss=1.0228
Epoch 91: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 91: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.81it/s][A
                                                                      [AEpoch 91: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 91: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 14: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 14: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 14: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Skipping batch 23 because it has no valid entries in the mask.
Batch 0: train_loss=1.0222
Epoch 92: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 92: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.56it/s][A
                                                                      [AEpoch 92: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 92: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0228
Epoch 93: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 93: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.63it/s][A
                                                                      [AEpoch 93: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 93: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Skipping batch 7 because it has no valid entries in the mask.
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Skipping batch 20 because it has no valid entries in the mask.
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 0: train_loss=1.0219
Epoch 94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 2 because it has no valid entries in the mask.

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.32it/s][A
                                                                      [AEpoch 94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Skipping batch 11 because it has no valid entries in the mask.
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.0232
Epoch 95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 64.23it/s][A
                                                                      [AEpoch 95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.91it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 4 because it has no valid entries in the mask.
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 17 because it has no valid entries in the mask.
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 0: train_loss=1.0228
Epoch 96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Skipping batch 1 because it has no valid entries in the mask.
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.58it/s][A
                                                                      [AEpoch 96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 5 because it has no valid entries in the mask.
Batch 6: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Skipping batch 14 because it has no valid entries in the mask.
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 0: train_loss=1.0232
Epoch 97: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 97: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.52it/s][A
                                                                      [AEpoch 97: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 97: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 11: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 13 because it has no valid entries in the mask.
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 11: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 13 because it has no valid entries in the mask.
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Skipping batch 9 because it has no valid entries in the mask.
Batch 10: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 11: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Skipping batch 13 because it has no valid entries in the mask.
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.0232
Epoch 98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.89it/s][A
                                                                      [AEpoch 98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020]        Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=yxib, epoch_avg_train_loss=1.020] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 13 because it has no valid entries in the mask.
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 13 because it has no valid entries in the mask.
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Skipping batch 6 because it has no valid entries in the mask.
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Skipping batch 13 because it has no valid entries in the mask.
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 0: train_loss=1.0224
Epoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.56it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.55it/s, v_num=yxib, epoch_avg_train_loss=1.020]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Skipping batch 0 because it has no valid entries in the mask.
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.07it/s][A
                                                                      [AEpoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=yxib, epoch_avg_train_loss=1.020]Epoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=yxib, epoch_avg_train_loss=1.020]`Trainer.fit` stopped: `max_epochs=100` reached.
Epoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=yxib, epoch_avg_train_loss=1.020]FIT Profiler Report
Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.configure_callbacks
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 module.py:936(configure_callbacks)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningDataModule]PMTfiedDataModule.setup
         2063 function calls (2047 primitive calls) in 0.012 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.012    0.012 PMTfiedDataModule.py:25(setup)
        1    0.000    0.000    0.009    0.009 PMTfiedDataModule.py:37(<listcomp>)
       25    0.000    0.000    0.009    0.000 dataset.py:409(__getitem__)
       24    0.000    0.000    0.009    0.000 DatasetMultiFlavourShard_Micro.py:50(__getitem__)
       24    0.001    0.000    0.009    0.000 DatasetMonoFlavourShard_Micro.py:55(__getitem__)
       24    0.002    0.000    0.004    0.000 DatasetMonoFlavourShard_Micro.py:153(_extract_features)
      121    0.002    0.000    0.002    0.000 {built-in method torch.tensor}
      4/1    0.000    0.000    0.002    0.002 _tensor.py:982(__format__)
        1    0.000    0.000    0.002    0.002 {function Tensor.__format__ at 0x7f88c723ec10}
        1    0.000    0.000    0.002    0.002 _tensor.py:457(__repr__)
        1    0.000    0.000    0.002    0.002 _tensor_str.py:695(_str)
       24    0.002    0.000    0.002    0.000 DatasetMonoFlavourShard_Micro.py:166(<listcomp>)
        3    0.000    0.000    0.001    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.001    0.001 contextlib.py:114(__enter__)
        2    0.000    0.000    0.001    0.001 _python_dispatch.py:201(_disable_current_modes)
        1    0.000    0.000    0.001    0.001 <frozen importlib._bootstrap>:1002(_find_and_load)
        1    0.000    0.000    0.001    0.001 <frozen importlib._bootstrap>:967(_find_and_load_unlocked)
        1    0.000    0.000    0.001    0.001 <frozen importlib._bootstrap>:659(_load_unlocked)
       24    0.001    0.000    0.001    0.000 shape_base.py:372(stack)
        1    0.000    0.000    0.001    0.001 <frozen importlib._bootstrap_external>:844(exec_module)
       48    0.001    0.000    0.001    0.000 {built-in method numpy.zeros}
        1    0.000    0.000    0.001    0.001 _tensor_str.py:391(_str_intern)
        1    0.000    0.000    0.001    0.001 _tensor_str.py:303(_tensor_str)
        1    0.000    0.000    0.001    0.001 <frozen importlib._bootstrap_external>:916(get_code)
        1    0.000    0.000    0.001    0.001 _tensor_str.py:123(__init__)
        1    0.000    0.000    0.001    0.001 <frozen importlib._bootstrap_external>:1036(get_data)
        1    0.000    0.000    0.000    0.000 {built-in method io.open_code}
       24    0.000    0.000    0.000    0.000 {built-in method torch.argmax}
        2    0.000    0.000    0.000    0.000 {built-in method builtins.print}
        4    0.000    0.000    0.000    0.000 redirect.py:644(write)
        4    0.000    0.000    0.000    0.000 wandb_run.py:2304(<lambda>)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:220(_call_with_frames_removed)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.exec}
        4    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
        1    0.000    0.000    0.000    0.000 schema_check_mode.py:3(<module>)
        1    0.000    0.000    0.000    0.000 dataset.py:426(random_split)
        4    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
        4    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
        2    0.000    0.000    0.000    0.000 __init__.py:345(namedtuple)
        1    0.000    0.000    0.000    0.000 {built-in method torch.randperm}
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}
        1    0.000    0.000    0.000    0.000 {built-in method torch.isfinite}
        1    0.000    0.000    0.000    0.000 {built-in method torch.bincount}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:901(_find_spec)
       24    0.000    0.000    0.000    0.000 shape_base.py:455(<listcomp>)
        4    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
       24    0.000    0.000    0.000    0.000 shape_base.py:443(<listcomp>)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1415(find_spec)
        4    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1383(_get_spec)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:645(_compile_bytecode)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1514(find_spec)
        4    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
        1    0.000    0.000    0.000    0.000 {built-in method marshal.loads}
        2    0.000    0.000    0.000    0.000 {built-in method builtins.eval}
       27    0.000    0.000    0.000    0.000 {method 'item' of 'torch._C.TensorBase' objects}
        4    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        1    0.000    0.000    0.000    0.000 _tensor.py:35(wrapped)
        4    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:135(_path_stat)
        3    0.000    0.000    0.000    0.000 {built-in method posix.stat}
        1    0.000    0.000    0.000    0.000 {built-in method torch.masked_select}
        1    0.000    0.000    0.000    0.000 _tensor.py:964(__rdiv__)
       24    0.000    0.000    0.000    0.000 shape_base.py:447(<setcomp>)
        2    0.000    0.000    0.000    0.000 _tensor.py:1033(__iter__)
       24    0.000    0.000    0.000    0.000 DatasetMultiFlavourShard_Micro.py:72(_global_to_local_index)
        4    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
        2    0.000    0.000    0.000    0.000 {method 'unbind' of 'torch._C.TensorBase' objects}
        4    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
       24    0.000    0.000    0.000    0.000 DatasetMonoFlavourShard_Micro.py:160(<listcomp>)
      768    0.000    0.000    0.000    0.000 {built-in method numpy.asanyarray}
    62/50    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        1    0.000    0.000    0.000    0.000 {method 'read' of '_io.BufferedReader' objects}
        1    0.000    0.000    0.000    0.000 {built-in method torch.ceil}
        1    0.000    0.000    0.000    0.000 {method 'reshape' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:154(_path_isfile)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:145(_path_is_mode_type)
        4    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
       24    0.000    0.000    0.000    0.000 shape_base.py:362(_stack_dispatcher)
        2    0.000    0.000    0.000    0.000 DatasetMultiFlavourShard_Micro.py:47(__len__)
        3    0.000    0.000    0.000    0.000 _tensor_str.py:117(tensor_totype)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:558(module_from_spec)
        1    0.000    0.000    0.000    0.000 _tensor_str.py:266(_tensor_str_with_formatter)
        3    0.000    0.000    0.000    0.000 {method 'to' of 'torch._C.TensorBase' objects}
        4    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:486(_init_module_attrs)
        1    0.000    0.000    0.000    0.000 _tensor_str.py:221(_vector_str)
        5    0.000    0.000    0.000    0.000 {built-in method builtins.sum}
    59/58    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        8    0.000    0.000    0.000    0.000 DatasetMultiFlavourShard_Micro.py:48(<genexpr>)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1077(path_stats)
        1    0.000    0.000    0.000    0.000 {method 'reciprocal' of 'torch._C.TensorBase' objects}
       24    0.000    0.000    0.000    0.000 shape_base.py:207(_arrays_for_stack_dispatcher)
        1    0.000    0.000    0.000    0.000 {method 'min' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 {method 'tolist' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:361(cache_from_source)
       24    0.000    0.000    0.000    0.000 __init__.py:819(is_tensor)
        1    0.000    0.000    0.000    0.000 {method 'max' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:385(cached)
        6    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}
        6    0.000    0.000    0.000    0.000 DatasetMonoFlavourShard_Micro.py:52(__len__)
        1    0.000    0.000    0.000    0.000 {method 'ne' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:491(_get_cached)
        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:121(_path_join)
        1    0.000    0.000    0.000    0.000 {method 'float' of 'torch._C.TensorBase' objects}
       24    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}
        2    0.000    0.000    0.000    0.000 grad_mode.py:80(__enter__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.__build_class__}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:156(__enter__)
        2    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        4    0.000    0.000    0.000    0.000 grad_mode.py:184(__init__)
        4    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
        4    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        1    0.000    0.000    0.000    0.000 _tensor_str.py:254(<listcomp>)
       29    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        4    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        1    0.000    0.000    0.000    0.000 _ops.py:544(_len_torch_dispatch_stack_pre_dispatch)
        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:127(_path_split)
        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:123(<listcomp>)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:166(_get_module_lock)
        1    0.000    0.000    0.000    0.000 {built-in method torch._C._functorch.is_functorch_wrapped_tensor}
        3    0.000    0.000    0.000    0.000 _tensor_str.py:232(_val_formatter)
       15    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        1    0.000    0.000    0.000    0.000 dataset.py:486(<listcomp>)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1509(_get_spec)
        1    0.000    0.000    0.000    0.000 {built-in method torch._is_functional_tensor}
        1    0.000    0.000    0.000    0.000 {method 'abs' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        2    0.000    0.000    0.000    0.000 {built-in method torch._C._get_tracing_state}
        2    0.000    0.000    0.000    0.000 grad_mode.py:75(__init__)
        6    0.000    0.000    0.000    0.000 {built-in method builtins.max}
        2    0.000    0.000    0.000    0.000 grad_mode.py:84(__exit__)
        8    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 forward_ad.py:144(unpack_dual)
        4    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        3    0.000    0.000    0.000    0.000 _tensor_str.py:193(format)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 parameter.py:8(__instancecheck__)
       16    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:560(_classify_pyc)
        1    0.000    0.000    0.000    0.000 contextlib.py:261(helper)
        4    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:696(spec_from_file_location)
        2    0.000    0.000    0.000    0.000 _contextlib.py:154(__new__)
        1    0.000    0.000    0.000    0.000 {method 'manual_seed' of 'torch._C.Generator' objects}
        1    0.000    0.000    0.000    0.000 _ops.py:441(count)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:79(_unpack_uint32)
        1    0.000    0.000    0.000    0.000 import_hooks.py:233(find_spec)
        1    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)
        1    0.000    0.000    0.000    0.000 _tensor_str.py:354(_add_suffixes)
        4    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:160(__exit__)
        4    0.000    0.000    0.000    0.000 {built-in method utcnow}
        3    0.000    0.000    0.000    0.000 {method '__format__' of 'float' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:87(acquire)
       24    0.000    0.000    0.000    0.000 multiarray.py:153(concatenate)
        1    0.000    0.000    0.000    0.000 {built-in method torch._C._get_default_device}
        3    0.000    0.000    0.000    0.000 dataset.py:405(__init__)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:593(_validate_timestamp_pyc)
        4    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
        1    0.000    0.000    0.000    0.000 {method '_is_zerotensor' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:112(release)
        4    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:129(<genexpr>)
        4    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}
        1    0.000    0.000    0.000    0.000 typing.py:271(inner)
        4    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        1    0.000    0.000    0.000    0.000 {method 'is_neg' of 'torch._C.TensorBase' objects}
        7    0.000    0.000    0.000    0.000 {built-in method sys.intern}
        5    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:874(__enter__)
        5    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:878(__exit__)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:58(__init__)
        7    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:231(_verbose_message)
        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1033(_handle_fromlist)
        1    0.000    0.000    0.000    0.000 __init__.py:82(find_spec)
       14    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}
        6    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}
        4    0.000    0.000    0.000    0.000 {method 'numel' of 'torch._C.TensorBase' objects}
        6    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:811(find_spec)
        7    0.000    0.000    0.000    0.000 __init__.py:419(<genexpr>)
        7    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:185(cb)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1346(_path_importer_cache)
        1    0.000    0.000    0.000    0.000 <string>:1(<lambda>)
        7    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}
        3    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f88d25377c0}
        6    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:523(_check_name_wrapper)
        6    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        7    0.000    0.000    0.000    0.000 {method '__contains__' of 'frozenset' objects}
        3    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:35(_new_module)
        4    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
        5    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:398(parent)
        1    0.000    0.000    0.000    0.000 _tensor_str.py:259(<listcomp>)
        7    0.000    0.000    0.000    0.000 {built-in method _imp.release_lock}
        1    0.000    0.000    0.000    0.000 {built-in method torch._C._len_torch_dispatch_stack}
        7    0.000    0.000    0.000    0.000 {built-in method _imp.acquire_lock}
        5    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        1    0.000    0.000    0.000    0.000 schema_check_mode.py:61(SchemaCheckMode)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:351(__init__)
        2    0.000    0.000    0.000    0.000 {built-in method torch.get_default_dtype}
        1    0.000    0.000    0.000    0.000 {built-in method _imp.is_frozen}
        1    0.000    0.000    0.000    0.000 {built-in method math.isclose}
        1    0.000    0.000    0.000    0.000 _tensor_str.py:256(<listcomp>)
        1    0.000    0.000    0.000    0.000 _ops.py:442(<listcomp>)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1006(__init__)
        5    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
        2    0.000    0.000    0.000    0.000 {method 'has_names' of 'torch._C.TensorBase' objects}
        3    0.000    0.000    0.000    0.000 {built-in method from_bytes}
        2    0.000    0.000    0.000    0.000 {built-in method sys._getframe}
        1    0.000    0.000    0.000    0.000 _ops.py:575(mode_stack_state_for_pre_dispatch)
        2    0.000    0.000    0.000    0.000 {built-in method builtins.iter}
        4    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
        2    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
        4    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
        1    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:406(has_location)
        1    0.000    0.000    0.000    0.000 {built-in method math.floor}
        2    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}
        1    0.000    0.000    0.000    0.000 _tensor_str.py:190(width)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:841(create_module)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:152(__init__)
        2    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
        3    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1031(get_filename)
        1    0.000    0.000    0.000    0.000 _python_dispatch.py:212(<listcomp>)
        1    0.000    0.000    0.000    0.000 typing.py:1375(cast)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:736(find_spec)
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:68(_relax_case)
        1    0.000    0.000    0.000    0.000 {built-in method _imp._fix_co_filename}
        1    0.000    0.000    0.000    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f88c4c64040}
        1    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 _python_dispatch.py:229(<listcomp>)



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.setup
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 early_stopping.py:135(setup)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.setup
         13 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 progress_bar.py:171(setup)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1241(is_global_zero)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        2    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 single_device.py:81(is_global_zero)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.setup
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:58(setup)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.setup
         1491 function calls in 0.002 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.002    0.002 model_checkpoint.py:267(setup)
        1    0.000    0.000    0.002    0.002 model_checkpoint.py:650(__warn_if_dir_not_empty)
        1    0.000    0.000    0.001    0.001 local.py:58(ls)
        1    0.001    0.001    0.001    0.001 local.py:63(<listcomp>)
        1    0.000    0.000    0.001    0.001 cloud_io.py:105(_is_dir)
        1    0.000    0.000    0.001    0.001 cloud_io.py:83(_is_object_storage)
        3    0.000    0.000    0.001    0.000 imports.py:45(module_available)
        3    0.000    0.000    0.000    0.000 imports.py:29(package_available)
        3    0.000    0.000    0.000    0.000 util.py:73(find_spec)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:901(_find_spec)
       13    0.000    0.000    0.000    0.000 local.py:71(info)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1415(find_spec)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1383(_get_spec)
       21    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1514(find_spec)
       12    0.000    0.000    0.000    0.000 {method 'stat' of 'posix.DirEntry' objects}
        1    0.000    0.000    0.000    0.000 {built-in method posix.scandir}
        1    0.000    0.000    0.000    0.000 rank_zero.py:36(wrapped_fn)
        1    0.000    0.000    0.000    0.000 rank_zero.py:76(rank_zero_warn)
        1    0.000    0.000    0.000    0.000 rank_zero.py:72(_warn)
       23    0.000    0.000    0.000    0.000 {built-in method posix.stat}
        1    0.000    0.000    0.000    0.000 {built-in method _warnings.warn}
       21    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:135(_path_stat)
        1    0.000    0.000    0.000    0.000 warnings.py:96(_showwarnmsg)
        1    0.000    0.000    0.000    0.000 warnings.py:20(_showwarnmsg_impl)
      105    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:121(_path_join)
        1    0.000    0.000    0.000    0.000 redirect.py:644(write)
        1    0.000    0.000    0.000    0.000 cloud_io.py:60(get_filesystem)
        1    0.000    0.000    0.000    0.000 core.py:362(url_to_fs)
        1    0.000    0.000    0.000    0.000 wandb_run.py:2310(<lambda>)
        1    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
        1    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
      105    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:123(<listcomp>)
        1    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
       16    0.000    0.000    0.000    0.000 local.py:230(_strip_protocol)
        1    0.000    0.000    0.000    0.000 warnings.py:117(_formatwarnmsg)
        1    0.000    0.000    0.000    0.000 warnings.py:40(_custom_format_warning)
        1    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
        1    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
        1    0.000    0.000    0.000    0.000 pathlib.py:1079(__new__)
        1    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
        1    0.000    0.000    0.000    0.000 core.py:331(_un_chain)
        1    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        1    0.000    0.000    0.000    0.000 registry.py:289(filesystem)
        1    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
        1    0.000    0.000    0.000    0.000 pathlib.py:702(_from_parts)
        1    0.000    0.000    0.000    0.000 spec.py:65(__call__)
        1    0.000    0.000    0.000    0.000 pathlib.py:682(_parse_args)
        1    0.000    0.000    0.000    0.000 local.py:133(isdir)
        1    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
        1    0.000    0.000    0.000    0.000 pathlib.py:64(parse_parts)
      226    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}
        1    0.000    0.000    0.000    0.000 genericpath.py:39(isdir)
        1    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
       86    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        1    0.000    0.000    0.000    0.000 utils.py:306(tokenize)
      105    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:231(_verbose_message)
        1    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
      106    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
       16    0.000    0.000    0.000    0.000 local.py:280(make_path_posix)
       84    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}
        1    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
        3    0.000    0.000    0.000    0.000 __init__.py:82(find_spec)
       27    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:874(__enter__)
       27    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:878(__exit__)
       18    0.000    0.000    0.000    0.000 utils.py:327(stringify_path)
        1    0.000    0.000    0.000    0.000 warnings.py:57(_is_path_in_lightning)
        1    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
       76    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        1    0.000    0.000    0.000    0.000 {built-in method _hashlib.openssl_md5}
        1    0.000    0.000    0.000    0.000 enums.py:81(__eq__)
       24    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1346(_path_importer_cache)
        3    0.000    0.000    0.000    0.000 __init__.py:57(find_spec)
       10    0.000    0.000    0.000    0.000 {built-in method sys.intern}
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        2    0.000    0.000    0.000    0.000 registry.py:222(get_filesystem_class)
        1    0.000    0.000    0.000    0.000 re.py:250(compile)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:736(find_spec)
        1    0.000    0.000    0.000    0.000 pathlib.py:742(__str__)
       21    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        3    0.000    0.000    0.000    0.000 import_hooks.py:233(find_spec)
        3    0.000    0.000    0.000    0.000 __init__.py:24(_module_matches_namespace)
        3    0.000    0.000    0.000    0.000 __init__.py:66(find_spec)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
       24    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}
        1    0.000    0.000    0.000    0.000 re.py:289(_compile)
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
       27    0.000    0.000    0.000    0.000 {built-in method _imp.acquire_lock}
       27    0.000    0.000    0.000    0.000 {built-in method _imp.release_lock}
        1    0.000    0.000    0.000    0.000 pathlib.py:303(splitroot)
        1    0.000    0.000    0.000    0.000 {method 'hexdigest' of '_hashlib.HASH' objects}
        1    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        1    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        1    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 core.py:541(split_protocol)
       12    0.000    0.000    0.000    0.000 {method 'is_file' of 'posix.DirEntry' objects}
        3    0.000    0.000    0.000    0.000 {built-in method _imp.is_builtin}
       21    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:68(_relax_case)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:811(find_spec)
        3    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}
        1    0.000    0.000    0.000    0.000 types.py:171(__get__)
       12    0.000    0.000    0.000    0.000 {method 'is_dir' of 'posix.DirEntry' objects}
        1    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        3    0.000    0.000    0.000    0.000 {built-in method builtins.locals}
        1    0.000    0.000    0.000    0.000 warnings.py:403(__init__)
        1    0.000    0.000    0.000    0.000 config.py:99(apply_config)
        3    0.000    0.000    0.000    0.000 __init__.py:33(_module_matches_namespace)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:351(__init__)
       12    0.000    0.000    0.000    0.000 {method 'is_symlink' of 'posix.DirEntry' objects}
        2    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 pathlib.py:725(_format_parsed_parts)
        4    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}
        3    0.000    0.000    0.000    0.000 six.py:194(find_spec)
        1    0.000    0.000    0.000    0.000 pathlib.py:1193(absolute)
        2    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
       13    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 pathlib.py:1089(_init)
        1    0.000    0.000    0.000    0.000 local.py:68(<listcomp>)
        1    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 {built-in method utcnow}
        1    0.000    0.000    0.000    0.000 trainer.py:1241(is_global_zero)
        1    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
        6    0.000    0.000    0.000    0.000 {method 'partition' of 'str' objects}
        6    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        1    0.000    0.000    0.000    0.000 pathlib.py:1001(is_absolute)
        1    0.000    0.000    0.000    0.000 model_checkpoint.py:608(__resolve_ckpt_dir)
        3    0.000    0.000    0.000    0.000 {built-in method _imp.is_frozen}
        3    0.000    0.000    0.000    0.000 _compat.py:52(find_spec)
        2    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        3    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        3    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 enum.py:792(value)
        3    0.000    0.000    0.000    0.000 __init__.py:89(<lambda>)
        3    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f88d25377c0}
        2    0.000    0.000    0.000    0.000 {method 'reverse' of 'list' objects}
        1    0.000    0.000    0.000    0.000 core.py:395(<dictcomp>)
        2    0.000    0.000    0.000    0.000 {built-in method _stat.S_ISDIR}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {method 'lstrip' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {built-in method _stat.S_ISLNK}
        1    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        1    0.000    0.000    0.000    0.000 {method '__exit__' of 'posix.ScandirIterator' objects}
        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 spec.py:214(_get_kwargs_from_urls)
        1    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
        1    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
        1    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        1    0.000    0.000    0.000    0.000 spec.py:67(<genexpr>)
        1    0.000    0.000    0.000    0.000 single_device.py:81(is_global_zero)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 single_device.py:90(broadcast)



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.setup
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 hooks.py:420(setup)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.configure_optimizers
         1231 function calls (1036 primitive calls) in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.001    0.001 FlavourClassificationTransformerEncoder.py:66(configure_optimizers)
        1    0.000    0.000    0.001    0.001 lr_scheduler.py:1260(__init__)
        1    0.000    0.000    0.001    0.001 lr_scheduler.py:57(_check_verbose_deprecated_warning)
        1    0.000    0.000    0.001    0.001 {built-in method _warnings.warn}
        1    0.000    0.000    0.001    0.001 warnings.py:96(_showwarnmsg)
        1    0.000    0.000    0.001    0.001 warnings.py:20(_showwarnmsg_impl)
        1    0.000    0.000    0.001    0.001 warnings.py:117(_formatwarnmsg)
        1    0.000    0.000    0.001    0.001 warnings.py:40(_custom_format_warning)
        1    0.000    0.000    0.001    0.001 warnings.py:15(formatwarning)
        1    0.000    0.000    0.001    0.001 warnings.py:35(_formatwarnmsg_impl)
        1    0.000    0.000    0.001    0.001 linecache.py:26(getline)
        1    0.000    0.000    0.001    0.001 vararg_kernel.py:127(_monkey_patched_getlines)
        1    0.000    0.000    0.001    0.001 linecache.py:36(getlines)
        1    0.000    0.000    0.001    0.001 linecache.py:80(updatecache)
        1    0.000    0.000    0.000    0.000 adam.py:31(__init__)
        1    0.000    0.000    0.000    0.000 tokenize.py:388(open)
        1    0.000    0.000    0.000    0.000 optimizer.py:339(__init__)
        1    0.000    0.000    0.000    0.000 {built-in method io.open}
        1    0.000    0.000    0.000    0.000 {method 'readlines' of '_io._IOBase' objects}
       41    0.000    0.000    0.000    0.000 module.py:2233(parameters)
       41    0.000    0.000    0.000    0.000 module.py:2258(named_parameters)
       41    0.000    0.000    0.000    0.000 module.py:2219(_named_members)
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}
        1    0.000    0.000    0.000    0.000 _compile.py:21(inner)
   242/47    0.000    0.000    0.000    0.000 module.py:2395(named_modules)
        1    0.000    0.000    0.000    0.000 redirect.py:644(write)
        1    0.000    0.000    0.000    0.000 eval_frame.py:596(_fn)
        1    0.000    0.000    0.000    0.000 wandb_run.py:2310(<lambda>)
        1    0.000    0.000    0.000    0.000 optimizer.py:987(add_param_group)
        1    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
        1    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
        1    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
       12    0.000    0.000    0.000    0.000 codecs.py:319(decode)
       12    0.000    0.000    0.000    0.000 {built-in method _codecs.utf_8_decode}
        1    0.000    0.000    0.000    0.000 decorators.py:38(disable)
        1    0.000    0.000    0.000    0.000 {built-in method posix.stat}
        1    0.000    0.000    0.000    0.000 tokenize.py:295(detect_encoding)
        1    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
      160    0.000    0.000    0.000    0.000 _tensor.py:1055(__hash__)
        1    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
        1    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
        1    0.000    0.000    0.000    0.000 optimizer.py:514(_patch_step_function)
        1    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        1    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
        1    0.000    0.000    0.000    0.000 pathlib.py:1079(__new__)
       86    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
        2    0.000    0.000    0.000    0.000 tokenize.py:319(read_or_stop)
        1    0.000    0.000    0.000    0.000 pathlib.py:702(_from_parts)
        1    0.000    0.000    0.000    0.000 optimizer.py:462(profile_hook_step)
        2    0.000    0.000    0.000    0.000 {method 'readline' of '_io.BufferedReader' objects}
        1    0.000    0.000    0.000    0.000 pathlib.py:682(_parse_args)
        1    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
        1    0.000    0.000    0.000    0.000 eval_frame.py:562(__init__)
        1    0.000    0.000    0.000    0.000 pathlib.py:64(parse_parts)
       46    0.000    0.000    0.000    0.000 module.py:2286(<lambda>)
        1    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
        1    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
        1    0.000    0.000    0.000    0.000 eval_frame.py:265(__init__)
        1    0.000    0.000    0.000    0.000 eval_frame.py:565(__call__)
        2    0.000    0.000    0.000    0.000 functools.py:35(update_wrapper)
      161    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        1    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
        1    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
       92    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
       52    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        2    0.000    0.000    0.000    0.000 tokenize.py:325(find_cookie)
        1    0.000    0.000    0.000    0.000 warnings.py:57(_is_path_in_lightning)
       18    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
       10    0.000    0.000    0.000    0.000 {built-in method sys.intern}
        1    0.000    0.000    0.000    0.000 pathlib.py:742(__str__)
        1    0.000    0.000    0.000    0.000 typing_extensions.py:1710(args)
        3    0.000    0.000    0.000    0.000 eval_frame.py:240(innermost_fn)
        1    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}
        3    0.000    0.000    0.000    0.000 {method 'match' of 're.Pattern' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
       41    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)
        1    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        1    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
        1    0.000    0.000    0.000    0.000 pathlib.py:303(splitroot)
       11    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 pathlib.py:725(_format_parsed_parts)
        2    0.000    0.000    0.000    0.000 warnings.py:403(__init__)
        1    0.000    0.000    0.000    0.000 typing_extensions.py:1569(__init__)
        1    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        2    0.000    0.000    0.000    0.000 functools.py:65(wraps)
       10    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}
        2    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 typing_extensions.py:1714(kwargs)
        9    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        1    0.000    0.000    0.000    0.000 {built-in method utcnow}
        1    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
        1    0.000    0.000    0.000    0.000 lr_scheduler.py:1368(_init_is_better)
       13    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'seek' of '_io.BufferedReader' objects}
        1    0.000    0.000    0.000    0.000 pathlib.py:1193(absolute)
        1    0.000    0.000    0.000    0.000 {method 'startswith' of 'bytes' objects}
        2    0.000    0.000    0.000    0.000 {built-in method torch._C._dynamo.eval_frame.set_eval_frame}
        1    0.000    0.000    0.000    0.000 lr_scheduler.py:1310(_reset)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        1    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}
        1    0.000    0.000    0.000    0.000 inspect.py:73(isclass)
        1    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)
        2    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
        1    0.000    0.000    0.000    0.000 typing_extensions.py:1592(__init__)
        2    0.000    0.000    0.000    0.000 {method 'decode' of 'bytes' objects}
        1    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}
        1    0.000    0.000    0.000    0.000 lr_scheduler.py:1304(<listcomp>)
        1    0.000    0.000    0.000    0.000 pathlib.py:1001(is_absolute)
        1    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {method 'isdisjoint' of 'set' objects}
        3    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
        1    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}
        1    0.000    0.000    0.000    0.000 eval_frame.py:232(nothing)
        1    0.000    0.000    0.000    0.000 typing.py:1375(cast)
        1    0.000    0.000    0.000    0.000 pathlib.py:1089(_init)
        1    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f88d25377c0}
        1    0.000    0.000    0.000    0.000 {method 'lstrip' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'reverse' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        1    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.ord}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_fit_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:64(on_fit_start)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)



Profile stats for: [Callback]TQDMProgressBar.on_fit_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:64(on_fit_start)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_fit_start
         6711 function calls (5397 primitive calls) in 0.002 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.002    0.002 model_summary.py:59(on_fit_start)
       12    0.000    0.000    0.002    0.000 {built-in method builtins.sum}
      250    0.000    0.000    0.001    0.000 module.py:2233(parameters)
      250    0.000    0.000    0.001    0.000 module.py:2258(named_parameters)
      250    0.000    0.000    0.001    0.000 module.py:2219(_named_members)
        1    0.000    0.000    0.001    0.001 model_summary.py:312(_get_summary_data)
        3    0.000    0.000    0.001    0.000 model_summary.py:255(total_parameters)
      123    0.000    0.000    0.001    0.000 model_summary.py:257(<genexpr>)
        2    0.000    0.000    0.001    0.000 model_summary.py:247(param_nums)
        2    0.000    0.000    0.001    0.000 model_summary.py:249(<listcomp>)
        6    0.000    0.000    0.001    0.000 model_summary.py:135(num_parameters)
       86    0.000    0.000    0.001    0.000 model_summary.py:138(<genexpr>)
      240    0.000    0.000    0.000    0.000 model_summary.py:457(_is_lazy_weight_tensor)
 1358/284    0.000    0.000    0.000    0.000 module.py:2395(named_modules)
  493/253    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        1    0.000    0.000    0.000    0.000 model_summary.py:259(trainable_parameters)
        1    0.000    0.000    0.000    0.000 model_summary.py:265(total_layer_params)
        1    0.000    0.000    0.000    0.000 model_summary.py:269(model_size)
       41    0.000    0.000    0.000    0.000 model_summary.py:261(<genexpr>)
      240    0.000    0.000    0.000    0.000 parameter.py:8(__instancecheck__)
        1    0.000    0.000    0.000    0.000 model_summary.py:80(summarize)
      517    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
      480    0.000    0.000    0.000    0.000 _tensor.py:1055(__hash__)
        1    0.000    0.000    0.000    0.000 __init__.py:1436(info)
        1    0.000    0.000    0.000    0.000 __init__.py:1565(_log)
        1    0.000    0.000    0.000    0.000 model_summary.py:73(_summary)
      274    0.000    0.000    0.000    0.000 module.py:2286(<lambda>)
        1    0.000    0.000    0.000    0.000 __init__.py:1591(handle)
        1    0.000    0.000    0.000    0.000 __init__.py:1645(callHandlers)
        1    0.000    0.000    0.000    0.000 model_summary.py:470(summarize)
        1    0.000    0.000    0.000    0.000 __init__.py:939(handle)
        1    0.000    0.000    0.000    0.000 model_summary.py:204(__init__)
        1    0.000    0.000    0.000    0.000 __init__.py:1071(emit)
        1    0.000    0.000    0.000    0.000 model_summary.py:273(summarize)
        1    0.000    0.000    0.000    0.000 redirect.py:644(write)
        1    0.000    0.000    0.000    0.000 model_summary.py:371(_format_summary_table)
        1    0.000    0.000    0.000    0.000 wandb_run.py:2310(<lambda>)
        1    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
        1    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
        1    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
      480    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        4    0.000    0.000    0.000    0.000 model_summary.py:274(<genexpr>)
      549    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
        3    0.000    0.000    0.000    0.000 model_summary.py:71(__init__)
        3    0.000    0.000    0.000    0.000 model_summary.py:81(_register_hook)
        1    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
      160    0.000    0.000    0.000    0.000 {method 'numel' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
        1    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
      240    0.000    0.000    0.000    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f88c4c64040}
      242    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        6    0.000    0.000    0.000    0.000 model_summary.py:419(get_human_readable_count)
        3    0.000    0.000    0.000    0.000 module.py:1463(register_forward_hook)
        1    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        1    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
        1    0.000    0.000    0.000    0.000 __init__.py:1550(makeRecord)
        1    0.000    0.000    0.000    0.000 __init__.py:282(__init__)
        1    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
       80    0.000    0.000    0.000    0.000 {built-in method math.prod}
        3    0.000    0.000    0.000    0.000 hooks.py:24(__init__)
        1    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
       10    0.000    0.000    0.000    0.000 {built-in method builtins.max}
       24    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}
        1    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
        1    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
        6    0.000    0.000    0.000    0.000 model_summary.py:113(detach_hook)
        1    0.000    0.000    0.000    0.000 model_summary.py:218(named_modules)
        1    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
        1    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
        1    0.000    0.000    0.000    0.000 __init__.py:916(format)
        1    0.000    0.000    0.000    0.000 __init__.py:1514(findCaller)
       20    0.000    0.000    0.000    0.000 model_summary.py:385(<genexpr>)
        1    0.000    0.000    0.000    0.000 __init__.py:650(format)
        4    0.000    0.000    0.000    0.000 module.py:2348(named_children)
        6    0.000    0.000    0.000    0.000 hooks.py:35(remove)
        1    0.000    0.000    0.000    0.000 model_summary.py:235(layer_types)
        1    0.000    0.000    0.000    0.000 model_summary.py:392(<listcomp>)
        3    0.000    0.000    0.000    0.000 model_summary.py:78(__del__)
        1    0.000    0.000    0.000    0.000 __init__.py:1060(flush)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 model_summary.py:237(<listcomp>)
        1    0.000    0.000    0.000    0.000 posixpath.py:140(basename)
        1    0.000    0.000    0.000    0.000 model_summary.py:231(layer_names)
        1    0.000    0.000    0.000    0.000 posixpath.py:117(splitext)
        1    0.000    0.000    0.000    0.000 model_summary.py:251(training_modes)
        1    0.000    0.000    0.000    0.000 model_summary.py:282(<listcomp>)
       31    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        1    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        1    0.000    0.000    0.000    0.000 __init__.py:628(usesTime)
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
        1    0.000    0.000    0.000    0.000 model_summary.py:253(<listcomp>)
        1    0.000    0.000    0.000    0.000 __init__.py:218(_acquireLock)
        1    0.000    0.000    0.000    0.000 __init__.py:634(formatMessage)
        9    0.000    0.000    0.000    0.000 hooks.py:33(<genexpr>)
        5    0.000    0.000    0.000    0.000 {built-in method math.log10}
        1    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)
        1    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 genericpath.py:121(_splitext)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
       20    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 trainer.py:1195(precision)
        6    0.000    0.000    0.000    0.000 {built-in method builtins.min}
        2    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        2    0.000    0.000    0.000    0.000 __init__.py:896(acquire)
        3    0.000    0.000    0.000    0.000 model_summary.py:130(layer_type)
        1    0.000    0.000    0.000    0.000 posixpath.py:52(normcase)
        1    0.000    0.000    0.000    0.000 __init__.py:421(usesTime)
        1    0.000    0.000    0.000    0.000 __init__.py:160(<lambda>)
        1    0.000    0.000    0.000    0.000 model_summary.py:415(get_formatted_model_size)
        1    0.000    0.000    0.000    0.000 trainer.py:1241(is_global_zero)
        2    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
        1    0.000    0.000    0.000    0.000 __init__.py:432(format)
        1    0.000    0.000    0.000    0.000 __init__.py:227(_releaseLock)
        1    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
        2    0.000    0.000    0.000    0.000 __init__.py:903(release)
        2    0.000    0.000    0.000    0.000 __init__.py:791(filter)
        1    0.000    0.000    0.000    0.000 __init__.py:119(getLevelName)
        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}
        3    0.000    0.000    0.000    0.000 {method 'count' of 'str' objects}
        3    0.000    0.000    0.000    0.000 model_summary.py:140(training)
        3    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 threading.py:1358(current_thread)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        4    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        4    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 __init__.py:429(_format)
        6    0.000    0.000    0.000    0.000 {built-in method math.ceil}
        2    0.000    0.000    0.000    0.000 module.py:243(example_input_array)
        1    0.000    0.000    0.000    0.000 {built-in method utcnow}
        3    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}
        4    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)
        5    0.000    0.000    0.000    0.000 {built-in method math.floor}
        1    0.000    0.000    0.000    0.000 __init__.py:358(getMessage)
        5    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}
        1    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        1    0.000    0.000    0.000    0.000 threading.py:1093(name)
        2    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
        1    0.000    0.000    0.000    0.000 __init__.py:1675(getEffectiveLevel)
        1    0.000    0.000    0.000    0.000 module.py:215(trainer)
        3    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        3    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 {built-in method time.time}
        1    0.000    0.000    0.000    0.000 model_summary.py:323(<listcomp>)
        1    0.000    0.000    0.000    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
        1    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {built-in method sys._getframe}
        1    0.000    0.000    0.000    0.000 __init__.py:1276(disable)
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        1    0.000    0.000    0.000    0.000 process.py:189(name)
        1    0.000    0.000    0.000    0.000 process.py:37(current_process)
        1    0.000    0.000    0.000    0.000 {method 'keys' of 'collections.OrderedDict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
        1    0.000    0.000    0.000    0.000 single_device.py:81(is_global_zero)
        1    0.000    0.000    0.000    0.000 strategy.py:93(precision_plugin)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.ord}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:64(on_fit_start)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_fit_start
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 hooks.py:30(on_fit_start)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_sanity_check_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:70(on_sanity_check_start)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_sanity_check_start
         684 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.001    0.001 tqdm_progress.py:249(on_sanity_check_start)
        1    0.000    0.000    0.001    0.001 tqdm_progress.py:185(init_sanity_tqdm)
        2    0.000    0.000    0.001    0.000 std.py:663(__new__)
        2    0.000    0.000    0.001    0.000 std.py:760(get_lock)
        1    0.000    0.000    0.001    0.001 std.py:90(__init__)
        1    0.000    0.000    0.001    0.001 std.py:116(create_mp_lock)
        1    0.000    0.000    0.001    0.001 context.py:70(RLock)
        2    0.000    0.000    0.000    0.000 tqdm_progress.py:40(__init__)
        2    0.000    0.000    0.000    0.000 std.py:952(__init__)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1002(_find_and_load)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:967(_find_and_load_unlocked)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:659(_load_unlocked)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:844(exec_module)
        1    0.000    0.000    0.000    0.000 std.py:1325(refresh)
        1    0.000    0.000    0.000    0.000 std.py:1464(display)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:916(get_code)
        1    0.000    0.000    0.000    0.000 _monitor.py:30(__init__)
        2    0.000    0.000    0.000    0.000 utils.py:333(_screen_shape_linux)
        1    0.000    0.000    0.000    0.000 synchronize.py:186(__init__)
        1    0.000    0.000    0.000    0.000 synchronize.py:50(__init__)
        1    0.000    0.000    0.000    0.000 threading.py:880(start)
        1    0.000    0.000    0.000    0.000 std.py:457(print_status)
        2    0.000    0.000    0.000    0.000 utils.py:347(<listcomp>)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:220(_call_with_frames_removed)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.exec}
        1    0.000    0.000    0.000    0.000 synchronize.py:10(<module>)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:645(_compile_bytecode)
        1    0.000    0.000    0.000    0.000 {built-in method marshal.loads}
        8    0.000    0.000    0.000    0.000 {built-in method builtins.__build_class__}
        1    0.000    0.000    0.000    0.000 std.py:451(fp_write)
        2    0.000    0.000    0.000    0.000 utils.py:194(inner)
        1    0.000    0.000    0.000    0.000 threading.py:563(wait)
        1    0.000    0.000    0.000    0.000 threading.py:280(wait)
        1    0.000    0.000    0.000    0.000 redirect.py:644(write)
        1    0.000    0.000    0.000    0.000 std.py:1150(__str__)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 synchronize.py:114(_make_name)
        1    0.000    0.000    0.000    0.000 wandb_run.py:2304(<lambda>)
        1    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
        1    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:901(_find_spec)
        1    0.000    0.000    0.000    0.000 tempfile.py:149(__next__)
        1    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
        4    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 {built-in method _thread.start_new_thread}
        1    0.000    0.000    0.000    0.000 utils.py:378(disp_len)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1415(find_spec)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1383(_get_spec)
        1    0.000    0.000    0.000    0.000 std.py:464(format_meter)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1036(get_data)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1514(find_spec)
        1    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
        1    0.000    0.000    0.000    0.000 utils.py:374(_text_width)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.sum}
        1    0.000    0.000    0.000    0.000 tempfile.py:138(rng)
        1    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
       51    0.000    0.000    0.000    0.000 utils.py:375(<genexpr>)
        1    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:558(module_from_spec)
        1    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        1    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:486(_init_module_attrs)
        1    0.000    0.000    0.000    0.000 std.py:679(_get_free_pos)
        1    0.000    0.000    0.000    0.000 threading.py:802(__init__)
        1    0.000    0.000    0.000    0.000 random.py:117(__init__)
        3    0.000    0.000    0.000    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
        1    0.000    0.000    0.000    0.000 std.py:438(status_printer)
        1    0.000    0.000    0.000    0.000 random.py:126(seed)
        2    0.000    0.000    0.000    0.000 {built-in method fcntl.ioctl}
        1    0.000    0.000    0.000    0.000 std.py:1446(format_dict)
        1    0.000    0.000    0.000    0.000 {method 'read' of '_io.BufferedReader' objects}
       50    0.000    0.000    0.000    0.000 {built-in method unicodedata.east_asian_width}
        1    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
        2    0.000    0.000    0.000    0.000 utils.py:213(__init__)
        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:385(cached)
        1    0.000    0.000    0.000    0.000 tempfile.py:152(<listcomp>)
        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:361(cache_from_source)
        1    0.000    0.000    0.000    0.000 {function Random.seed at 0x7f88d0bbea60}
        1    0.000    0.000    0.000    0.000 std.py:682(<setcomp>)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:135(_path_stat)
        8    0.000    0.000    0.000    0.000 random.py:343(choice)
        1    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
        1    0.000    0.000    0.000    0.000 util.py:171(register_after_fork)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:491(_get_cached)
        3    0.000    0.000    0.000    0.000 _weakrefset.py:63(__iter__)
        2    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}
        3    0.000    0.000    0.000    0.000 {built-in method posix.stat}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:156(__enter__)
        1    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
        1    0.000    0.000    0.000    0.000 {built-in method io.open_code}
        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:121(_path_join)
        4    0.000    0.000    0.000    0.000 std.py:110(__enter__)
        4    0.000    0.000    0.000    0.000 std.py:113(__exit__)
        5    0.000    0.000    0.000    0.000 std.py:106(release)
        5    0.000    0.000    0.000    0.000 std.py:102(acquire)
        8    0.000    0.000    0.000    0.000 random.py:237(_randbelow_with_getrandbits)
        2    0.000    0.000    0.000    0.000 os.py:674(__getitem__)
        2    0.000    0.000    0.000    0.000 threading.py:528(__init__)
        1    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
        1    0.000    0.000    0.000    0.000 weakref.py:165(__setitem__)
        3    0.000    0.000    0.000    0.000 _weakrefset.py:86(add)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:166(_get_module_lock)
       23    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        5    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:398(parent)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1509(_get_spec)
        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:127(_path_split)
        1    0.000    0.000    0.000    0.000 std.py:186(__format__)
        4    0.000    0.000    0.000    0.000 utils.py:187(disable_on_exception)
        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:123(<listcomp>)
        1    0.000    0.000    0.000    0.000 _weakrefset.py:111(remove)
        2    0.000    0.000    0.000    0.000 threading.py:228(__init__)
        1    0.000    0.000    0.000    0.000 synchronize.py:360(Barrier)
       17    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}
        2    0.000    0.000    0.000    0.000 functools.py:392(__get__)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:154(_path_isfile)
        2    0.000    0.000    0.000    0.000 utils.py:266(_supports_unicode)
        2    0.000    0.000    0.000    0.000 {method 'remove' of 'set' objects}
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:173(is_disabled)
        1    0.000    0.000    0.000    0.000 _weakrefset.py:27(__exit__)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:145(_path_is_mode_type)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1077(path_stats)
        4    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1033(_handle_fromlist)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:560(_classify_pyc)
        4    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
        3    0.000    0.000    0.000    0.000 {built-in method builtins.max}
        1    0.000    0.000    0.000    0.000 synchronize.py:46(SemLock)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 std.py:400(format_interval)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:696(spec_from_file_location)
        2    0.000    0.000    0.000    0.000 utils.py:156(__init__)
        2    0.000    0.000    0.000    0.000 os.py:754(encode)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:87(acquire)
        1    0.000    0.000    0.000    0.000 threading.py:271(_is_owned)
        1    0.000    0.000    0.000    0.000 utils.py:125(__eq__)
        1    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:79(_unpack_uint32)
        5    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
        1    0.000    0.000    0.000    0.000 synchronize.py:142(BoundedSemaphore)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:160(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method fromtimestamp}
        6    0.000    0.000    0.000    0.000 utils.py:152(wrapper_setattr)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:593(_validate_timestamp_pyc)
        1    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        2    0.000    0.000    0.000    0.000 {method 'setter' of 'property' objects}
        1    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
       10    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}
        1    0.000    0.000    0.000    0.000 std.py:153(__init__)
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 _weakrefset.py:53(_commit_removals)
        4    0.000    0.000    0.000    0.000 utils.py:222(__eq__)
        3    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
        1    0.000    0.000    0.000    0.000 utils.py:273(_is_ascii)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:112(release)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:58(__init__)
        4    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:129(<genexpr>)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        5    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:878(__exit__)
        5    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:874(__enter__)
       13    0.000    0.000    0.000    0.000 {method 'getrandbits' of '_random.Random' objects}
       16    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        3    0.000    0.000    0.000    0.000 std.py:226(__init__)
        7    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:231(_verbose_message)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:169(is_enabled)
        1    0.000    0.000    0.000    0.000 import_hooks.py:233(find_spec)
        9    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:185(cb)
       12    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        3    0.000    0.000    0.000    0.000 std.py:1157(__hash__)
        1    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 threading.py:256(__enter__)
        1    0.000    0.000    0.000    0.000 _weakrefset.py:17(__init__)
       14    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
        1    0.000    0.000    0.000    0.000 threading.py:1229(_make_invoke_excepthook)
        1    0.000    0.000    0.000    0.000 weakref.py:348(__new__)
        2    0.000    0.000    0.000    0.000 std.py:1153(_comparable)
        2    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:811(find_spec)
        1    0.000    0.000    0.000    0.000 threading.py:1162(daemon)
        6    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 synchronize.py:210(Condition)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:523(_check_name_wrapper)
        1    0.000    0.000    0.000    0.000 threading.py:268(_acquire_restore)
        1    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
        1    0.000    0.000    0.000    0.000 {built-in method atexit.register}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1346(_path_importer_cache)
        1    0.000    0.000    0.000    0.000 __init__.py:82(find_spec)
        1    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
        1    0.000    0.000    0.000    0.000 _weakrefset.py:21(__enter__)
        1    0.000    0.000    0.000    0.000 threading.py:265(_release_save)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1006(__init__)
        1    0.000    0.000    0.000    0.000 utils.py:252(_is_utf)
        4    0.000    0.000    0.000    0.000 {built-in method _weakref.proxy}
        1    0.000    0.000    0.000    0.000 synchronize.py:90(_make_methods)
        1    0.000    0.000    0.000    0.000 _monitor.py:94(report)
        3    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
        1    0.000    0.000    0.000    0.000 weakref.py:353(__init__)
        1    0.000    0.000    0.000    0.000 threading.py:757(_newname)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:35(_new_module)
        3    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f88d25377c0}
        1    0.000    0.000    0.000    0.000 utils.py:108(__init__)
        5    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
        1    0.000    0.000    0.000    0.000 context.py:233(get_context)
        1    0.000    0.000    0.000    0.000 utils.py:282(_screen_shape_wrapper)
        5    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}
        1    0.000    0.000    0.000    0.000 util.py:48(debug)
        7    0.000    0.000    0.000    0.000 {built-in method _imp.release_lock}
        6    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 threading.py:1358(current_thread)
        7    0.000    0.000    0.000    0.000 {built-in method _imp.acquire_lock}
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:137(val_progress_bar)
        8    0.000    0.000    0.000    0.000 {method 'bit_length' of 'int' objects}
        4    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        5    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 std.py:167(colour)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:351(__init__)
        1    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        1    0.000    0.000    0.000    0.000 synchronize.py:321(Event)
        4    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        1    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        3    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
        1    0.000    0.000    0.000    0.000 std.py:231(__call__)
        1    0.000    0.000    0.000    0.000 threading.py:259(__exit__)
        1    0.000    0.000    0.000    0.000 threading.py:1147(daemon)
        3    0.000    0.000    0.000    0.000 {built-in method from_bytes}
        1    0.000    0.000    0.000    0.000 {method 'difference' of 'set' objects}
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:165(process_position)
        1    0.000    0.000    0.000    0.000 std.py:98(<listcomp>)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}
        1    0.000    0.000    0.000    0.000 synchronize.py:123(Semaphore)
        1    0.000    0.000    0.000    0.000 {built-in method utcnow}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:152(__init__)
        1    0.000    0.000    0.000    0.000 utils.py:112(__format__)
        4    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
        1    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}
        3    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.min}
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:127(train_progress_bar)
        3    0.000    0.000    0.000    0.000 threading.py:536(is_set)
        2    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 {built-in method _imp.is_frozen}
        3    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        1    0.000    0.000    0.000    0.000 synchronize.py:159(Lock)
        1    0.000    0.000    0.000    0.000 context.py:197(get_start_method)
        1    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:406(has_location)
        1    0.000    0.000    0.000    0.000 synchronize.py:184(RLock)
        1    0.000    0.000    0.000    0.000 progress_bar.py:60(sanity_check_description)
        1    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1031(get_filename)
        1    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:161(refresh_rate)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:841(create_module)
        1    0.000    0.000    0.000    0.000 {built-in method _imp._fix_co_filename}
        1    0.000    0.000    0.000    0.000 {built-in method time.time}
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 std.py:163(colour)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:736(find_spec)
        1    0.000    0.000    0.000    0.000 process.py:37(current_process)
        1    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
        1    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:68(_relax_case)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)



Profile stats for: [Callback]ModelSummary.on_sanity_check_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:70(on_sanity_check_start)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 callback.py:70(on_sanity_check_start)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningDataModule]PMTfiedDataModule.val_dataloader
         605 function calls (516 primitive calls) in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 PMTfiedDataModule.py:77(val_dataloader)
      2/1    0.000    0.000    0.000    0.000 data.py:287(wrapper)
        1    0.000    0.000    0.000    0.000 dataloader.py:227(__init__)
        2    0.000    0.000    0.000    0.000 inspect.py:3111(signature)
        2    0.000    0.000    0.000    0.000 inspect.py:2859(from_callable)
        2    0.000    0.000    0.000    0.000 inspect.py:2246(_signature_from_callable)
        2    0.000    0.000    0.000    0.000 inspect.py:2152(_signature_from_function)
       37    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        3    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)
        3    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}
     45/3    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)
     45/3    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}
    23/22    0.000    0.000    0.000    0.000 data.py:334(wrapper)
       21    0.000    0.000    0.000    0.000 inspect.py:2498(__init__)
       21    0.000    0.000    0.000    0.000 data.py:295(<genexpr>)
    20/19    0.000    0.000    0.000    0.000 dataloader.py:418(__setattr__)
        2    0.000    0.000    0.000    0.000 inspect.py:2781(__init__)
       21    0.000    0.000    0.000    0.000 enum.py:358(__call__)
        1    0.000    0.000    0.000    0.000 dataloader.py:487(check_worker_number_rationality)
        1    0.000    0.000    0.000    0.000 sampler.py:133(__init__)
        1    0.000    0.000    0.000    0.000 sampler.py:262(__init__)
       23    0.000    0.000    0.000    0.000 inspect.py:2830(<genexpr>)
        2    0.000    0.000    0.000    0.000 inspect.py:494(unwrap)
        1    0.000    0.000    0.000    0.000 {built-in method posix.sched_getaffinity}
       50    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
       61    0.000    0.000    0.000    0.000 inspect.py:2548(name)
        2    0.000    0.000    0.000    0.000 data.py:303(<dictcomp>)
       37    0.000    0.000    0.000    0.000 _collections_abc.py:262(__subclasshook__)
       27    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        2    0.000    0.000    0.000    0.000 sampler.py:146(num_samples)
       21    0.000    0.000    0.000    0.000 enum.py:670(__new__)
       21    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}
      8/6    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        1    0.000    0.000    0.000    0.000 dataloader.py:394(multiprocessing_context)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 {built-in method torch.set_vital}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
       21    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        2    0.000    0.000    0.000    0.000 inspect.py:514(_is_wrapper)
       19    0.000    0.000    0.000    0.000 inspect.py:2560(kind)
       19    0.000    0.000    0.000    0.000 inspect.py:2552(default)
        4    0.000    0.000    0.000    0.000 inspect.py:159(isfunction)
        5    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        2    0.000    0.000    0.000    0.000 dataset.py:422(__len__)
        2    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}
        2    0.000    0.000    0.000    0.000 {built-in method sys.getrecursionlimit}
        2    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        2    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
        1    0.000    0.000    0.000    0.000 dataloader.py:442(_auto_collation)
        2    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
        2    0.000    0.000    0.000    0.000 inspect.py:2865(parameters)
        1    0.000    0.000    0.000    0.000 {method 'index' of 'tuple' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_model_eval
         79891 function calls (70700 primitive calls) in 0.043 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.002    0.000    0.042    0.000 hooks.py:162(on_validation_model_eval)
      101    0.000    0.000    0.039    0.000 module.py:2459(eval)
 4646/101    0.008    0.000    0.039    0.000 module.py:2437(train)
     4646    0.016    0.000    0.022    0.000 module.py:1731(__setattr__)
     9191    0.003    0.000    0.008    0.000 module.py:2339(children)
18584/13938    0.003    0.000    0.006    0.000 {built-in method builtins.isinstance}
     9191    0.004    0.000    0.005    0.000 module.py:2348(named_children)
     4646    0.002    0.000    0.004    0.000 parameter.py:8(__instancecheck__)
    14039    0.001    0.000    0.001    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.001    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.001    0.000 trainer.py:1203(model)
     4646    0.000    0.000    0.000    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f88c4c64040}
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
     4545    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
      101    0.000    0.000    0.000    0.000 module.py:215(trainer)
     4646    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:351(model)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_validation_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 callback.py:211(on_validation_start)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_validation_start
         36019 function calls in 0.171 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.003    0.000    0.171    0.002 tqdm_progress.py:287(on_validation_start)
      100    0.008    0.000    0.165    0.002 tqdm_progress.py:223(init_validation_tqdm)
      100    0.001    0.000    0.148    0.001 tqdm_progress.py:40(__init__)
      100    0.007    0.000    0.146    0.001 std.py:952(__init__)
      100    0.000    0.000    0.122    0.001 std.py:1325(refresh)
      100    0.000    0.000    0.121    0.001 std.py:1464(display)
      600    0.001    0.000    0.099    0.000 utils.py:194(inner)
      200    0.001    0.000    0.086    0.000 std.py:1441(moveto)
      300    0.002    0.000    0.080    0.000 redirect.py:644(write)
      300    0.002    0.000    0.078    0.000 wandb_run.py:2304(<lambda>)
      300    0.001    0.000    0.076    0.000 wandb_run.py:390(wrapper_fn)
      300    0.003    0.000    0.075    0.000 wandb_run.py:1429(_console_raw_callback)
      300    0.008    0.000    0.071    0.000 interface.py:749(publish_output_raw)
      300    0.003    0.000    0.053    0.000 interface_shared.py:76(_publish_output_raw)
      300    0.002    0.000    0.050    0.000 interface_sock.py:45(_publish)
      300    0.002    0.000    0.047    0.000 sock_client.py:219(send_record_publish)
      300    0.000    0.000    0.045    0.000 sock_client.py:153(send_server_request)
      300    0.002    0.000    0.044    0.000 sock_client.py:145(_send_message)
      300    0.001    0.000    0.040    0.000 sock_client.py:121(_sendall_with_error_handle)
      300    0.039    0.000    0.039    0.000 {method 'send' of '_socket.socket' objects}
      100    0.001    0.000    0.019    0.000 std.py:457(print_status)
      500    0.019    0.000    0.019    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      100    0.002    0.000    0.016    0.000 std.py:1150(__str__)
      100    0.000    0.000    0.014    0.000 std.py:451(fp_write)
      200    0.005    0.000    0.013    0.000 utils.py:333(_screen_shape_linux)
      100    0.006    0.000    0.010    0.000 std.py:464(format_meter)
      100    0.003    0.000    0.008    0.000 std.py:663(__new__)
      300    0.002    0.000    0.007    0.000 well_known_types.py:172(GetCurrentTime)
      300    0.003    0.000    0.005    0.000 well_known_types.py:242(FromDatetime)
      200    0.004    0.000    0.004    0.000 {built-in method fcntl.ioctl}
      100    0.001    0.000    0.004    0.000 utils.py:378(disp_len)
      100    0.001    0.000    0.004    0.000 std.py:1446(format_dict)
      200    0.001    0.000    0.003    0.000 utils.py:347(<listcomp>)
      100    0.000    0.000    0.003    0.000 utils.py:374(_text_width)
      100    0.000    0.000    0.003    0.000 {built-in method builtins.sum}
      300    0.003    0.000    0.003    0.000 enum_type_wrapper.py:92(__getattr__)
     4600    0.002    0.000    0.003    0.000 utils.py:375(<genexpr>)
      100    0.001    0.000    0.002    0.000 utils.py:213(__init__)
      200    0.001    0.000    0.002    0.000 std.py:110(__enter__)
      100    0.002    0.000    0.002    0.000 tqdm_progress.py:137(val_progress_bar)
      200    0.002    0.000    0.002    0.000 os.py:674(__getitem__)
      300    0.001    0.000    0.001    0.000 std.py:102(acquire)
      100    0.001    0.000    0.001    0.000 std.py:438(status_printer)
      200    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}
      100    0.000    0.000    0.001    0.000 _weakrefset.py:86(add)
      300    0.001    0.000    0.001    0.000 calendar.py:655(timegm)
      100    0.001    0.000    0.001    0.000 tqdm_progress.py:173(is_disabled)
      200    0.001    0.000    0.001    0.000 utils.py:266(_supports_unicode)
      200    0.000    0.000    0.001    0.000 std.py:113(__exit__)
      300    0.001    0.000    0.001    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
      300    0.001    0.000    0.001    0.000 std.py:106(release)
      300    0.001    0.000    0.001    0.000 interface_sock.py:41(_assign)
      100    0.001    0.000    0.001    0.000 functools.py:392(__get__)
      100    0.001    0.000    0.001    0.000 {built-in method fromtimestamp}
      101    0.000    0.000    0.001    0.000 trainer.py:1429(sanity_checking)
      100    0.001    0.000    0.001    0.000 {method 'add' of 'set' objects}
      300    0.001    0.000    0.001    0.000 std.py:226(__init__)
      101    0.000    0.000    0.001    0.000 enums.py:81(__eq__)
      100    0.000    0.000    0.001    0.000 utils.py:156(__init__)
      500    0.001    0.000    0.001    0.000 {built-in method builtins.hasattr}
     1900    0.001    0.000    0.001    0.000 {built-in method builtins.getattr}
      300    0.001    0.000    0.001    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
      100    0.000    0.000    0.001    0.000 utils.py:273(_is_ascii)
     4500    0.001    0.000    0.001    0.000 {built-in method unicodedata.east_asian_width}
      200    0.000    0.000    0.001    0.000 utils.py:187(disable_on_exception)
      600    0.001    0.000    0.001    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
      300    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
      100    0.000    0.000    0.000    0.000 std.py:400(format_interval)
      100    0.000    0.000    0.000    0.000 std.py:153(__init__)
      300    0.000    0.000    0.000    0.000 utils.py:152(wrapper_setattr)
      202    0.000    0.000    0.000    0.000 types.py:171(__get__)
      100    0.000    0.000    0.000    0.000 std.py:1147(__del__)
      300    0.000    0.000    0.000    0.000 {built-in method utcnow}
      200    0.000    0.000    0.000    0.000 os.py:754(encode)
      300    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
      100    0.000    0.000    0.000    0.000 tqdm_progress.py:169(is_enabled)
      300    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      100    0.000    0.000    0.000    0.000 std.py:186(__format__)
      100    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 tqdm_progress.py:165(process_position)
      100    0.000    0.000    0.000    0.000 _monitor.py:94(report)
      202    0.000    0.000    0.000    0.000 enum.py:792(value)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 std.py:760(get_lock)
      400    0.000    0.000    0.000    0.000 utils.py:222(__eq__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      300    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
      300    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      300    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      700    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      200    0.000    0.000    0.000    0.000 utils.py:370(_term_move_up)
      300    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
      100    0.000    0.000    0.000    0.000 utils.py:252(_is_utf)
      100    0.000    0.000    0.000    0.000 std.py:231(__call__)
      401    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      100    0.000    0.000    0.000    0.000 utils.py:282(_screen_shape_wrapper)
      200    0.000    0.000    0.000    0.000 {built-in method _weakref.proxy}
      100    0.000    0.000    0.000    0.000 threading.py:536(is_set)
      300    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      300    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      100    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f88d25377c0}
      100    0.000    0.000    0.000    0.000 std.py:1157(__hash__)
      300    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      300    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
      300    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      300    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
      300    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
      100    0.000    0.000    0.000    0.000 progress_bar.py:54(trainer)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.max}
      100    0.000    0.000    0.000    0.000 utils.py:108(__init__)
      100    0.000    0.000    0.000    0.000 utils.py:112(__format__)
      300    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
      100    0.000    0.000    0.000    0.000 {built-in method time.time}
      100    0.000    0.000    0.000    0.000 std.py:167(colour)
      202    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 std.py:163(colour)
      100    0.000    0.000    0.000    0.000 tqdm_progress.py:161(refresh_rate)
      100    0.000    0.000    0.000    0.000 std.py:1265(close)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.id}
      100    0.000    0.000    0.000    0.000 progress_bar.py:68(validation_description)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_validation_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:211(on_validation_start)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:211(on_validation_start)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_start
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 hooks.py:50(on_validation_start)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Strategy]SingleDeviceStrategy.on_validation_start
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 strategy.py:549(on_validation_start)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_validation_epoch_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 callback.py:124(on_validation_epoch_start)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_validation_epoch_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:124(on_validation_epoch_start)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_validation_epoch_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 callback.py:124(on_validation_epoch_start)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:124(on_validation_epoch_start)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_epoch_start
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 hooks.py:241(on_validation_epoch_start)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [_EvaluationLoop].val_next
         9407 function calls (8599 primitive calls) in 0.056 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
  404/101    0.000    0.000    0.055    0.001 {built-in method builtins.next}
      101    0.000    0.000    0.055    0.001 combined_loader.py:339(__next__)
      101    0.001    0.000    0.055    0.001 combined_loader.py:128(__next__)
      101    0.004    0.000    0.054    0.001 dataloader.py:625(__next__)
      101    0.001    0.000    0.016    0.000 dataloader.py:1297(_next_data)
      101    0.003    0.000    0.016    0.000 profiler.py:693(__exit__)
      101    0.001    0.000    0.015    0.000 dataloader.py:1264(_get_data)
      101    0.000    0.000    0.014    0.000 dataloader.py:1118(_try_get_data)
      101    0.001    0.000    0.014    0.000 queue.py:154(get)
      101    0.003    0.000    0.013    0.000 profiler.py:687(__enter__)
      101    0.001    0.000    0.013    0.000 _ops.py:887(__call__)
      206    0.013    0.000    0.013    0.000 {method 'acquire' of '_thread.lock' objects}
        1    0.000    0.000    0.013    0.013 threading.py:280(wait)
      101    0.001    0.000    0.010    0.000 _ops.py:1047(__call__)
      101    0.001    0.000    0.010    0.000 _ops.py:943(_must_dispatch_in_python)
      101    0.009    0.000    0.009    0.000 {built-in method torch._ops.profiler._record_function_enter_new}
      101    0.001    0.000    0.008    0.000 _pytree.py:1181(tree_any)
      101    0.000    0.000    0.007    0.000 {built-in method builtins.any}
  707/202    0.003    0.000    0.005    0.000 _pytree.py:874(tree_iter)
      101    0.003    0.000    0.004    0.000 profiler.py:676(__init__)
      101    0.002    0.000    0.002    0.000 {built-in method torch._ops.profiler.}
      404    0.000    0.000    0.002    0.000 _pytree.py:656(_is_leaf)
      707    0.000    0.000    0.002    0.000 _pytree.py:649(_get_node_type)
      101    0.002    0.000    0.002    0.000 _ops.py:945(<lambda>)
      707    0.001    0.000    0.001    0.000 _pytree.py:638(_is_namedtuple_instance)
      101    0.001    0.000    0.001    0.000 typing.py:271(inner)
      101    0.000    0.000    0.001    0.000 evaluation_loop.py:346(_on_after_fetch)
      101    0.001    0.000    0.001    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 threading.py:1133(is_alive)
      101    0.000    0.000    0.000    0.000 dataloader.py:1366(_process_data)
      101    0.000    0.000    0.000    0.000 dataloader.py:1346(_try_put_index)
      101    0.000    0.000    0.000    0.000 _pytree.py:423(_dict_flatten)
      101    0.000    0.000    0.000    0.000 threading.py:1066(_wait_for_tstate_lock)
      101    0.000    0.000    0.000    0.000 threading.py:351(notify)
     1112    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      101    0.000    0.000    0.000    0.000 threading.py:256(__enter__)
      101    0.000    0.000    0.000    0.000 queue.py:217(_get)
      202    0.000    0.000    0.000    0.000 _pytree.py:395(_tuple_flatten)
      102    0.000    0.000    0.000    0.000 queue.py:209(_qsize)
      101    0.000    0.000    0.000    0.000 dataloader.py:619(_next_index)
      303    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      102    0.000    0.000    0.000    0.000 threading.py:271(_is_owned)
      101    0.000    0.000    0.000    0.000 threading.py:259(__exit__)
      101    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}
      101    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
      101    0.000    0.000    0.000    0.000 states.py:67(dataloader_prefix)
      101    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 __init__.py:129(annotate)
      101    0.000    0.000    0.000    0.000 threading.py:536(is_set)
      102    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      101    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      101    0.000    0.000    0.000    0.000 {method '__exit__' of 'torch._C.DisableTorchFunctionSubclass' objects}
      101    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 threading.py:265(_release_save)
        1    0.000    0.000    0.000    0.000 threading.py:268(_acquire_restore)
        1    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}
        1    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_before_batch_transfer
         1407 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      201    0.000    0.000    0.001    0.000 contextlib.py:123(__exit__)
      201    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      201    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      201    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      201    0.000    0.000    0.000    0.000 hooks.py:613(on_before_batch_transfer)
      201    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      201    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Strategy]SingleDeviceStrategy.batch_to_device
         10251 function calls (10050 primitive calls) in 0.107 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      201    0.003    0.000    0.107    0.001 strategy.py:262(batch_to_device)
      201    0.000    0.000    0.103    0.001 module.py:354(_apply_batch_transfer_handler)
      201    0.000    0.000    0.103    0.001 module.py:336(_call_batch_hook)
      201    0.001    0.000    0.100    0.000 call.py:137(_call_lightning_module_hook)
      201    0.000    0.000    0.098    0.000 contextlib.py:114(__enter__)
      201    0.000    0.000    0.098    0.000 {built-in method builtins.next}
      201    0.000    0.000    0.098    0.000 profiler.py:55(profile)
      201    0.001    0.000    0.098    0.000 advanced.py:65(start)
      201    0.097    0.000    0.097    0.000 {method 'enable' of '_lsprof.Profiler' objects}
      201    0.000    0.000    0.003    0.000 data_connector.py:349(get_instance)
      402    0.001    0.000    0.002    0.000 model_helpers.py:29(is_overridden)
      402    0.001    0.000    0.002    0.000 overrides.py:10(is_overridden)
      201    0.000    0.000    0.001    0.000 module.py:1731(__setattr__)
      201    0.001    0.000    0.001    0.000 single_device.py:71(root_device)
2211/2010    0.000    0.000    0.001    0.000 {built-in method builtins.isinstance}
     1206    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      201    0.000    0.000    0.000    0.000 contextlib.py:261(helper)
      201    0.000    0.000    0.000    0.000 parameter.py:8(__instancecheck__)
      201    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)
      804    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      201    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      201    0.000    0.000    0.000    0.000 __init__.py:1424(debug)
      201    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
      603    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      402    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      201    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      201    0.000    0.000    0.000    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f88c4c64040}
      201    0.000    0.000    0.000    0.000 {built-in method builtins.callable}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.transfer_batch_to_device
         10060 function calls in 0.090 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      201    0.001    0.000    0.090    0.000 hooks.py:564(transfer_batch_to_device)
      201    0.004    0.000    0.088    0.000 apply_func.py:71(move_data_to_device)
      201    0.002    0.000    0.084    0.000 apply_func.py:23(apply_to_collection)
      201    0.001    0.000    0.079    0.000 apply_func.py:70(<dictcomp>)
      804    0.007    0.000    0.078    0.000 apply_func.py:91(batch_to)
      804    0.071    0.000    0.071    0.000 {method 'to' of 'torch._C.TensorBase' objects}
     2814    0.001    0.000    0.003    0.000 {built-in method builtins.isinstance}
     1005    0.000    0.000    0.002    0.000 abc.py:117(__instancecheck__)
     1005    0.002    0.000    0.002    0.000 {built-in method _abc._abc_instancecheck}
      201    0.000    0.000    0.002    0.000 {built-in method builtins.all}
     1005    0.000    0.000    0.001    0.000 apply_func.py:69(<genexpr>)
      201    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      201    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      201    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      201    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      201    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      201    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      201    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)
      201    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}
        2    0.000    0.000    0.000    0.000 apply_func.py:63(__subclasshook__)
        2    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        2    0.000    0.000    0.000    0.000 {built-in method builtins.callable}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_after_batch_transfer
         1407 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      201    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      201    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      201    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      201    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      201    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      201    0.000    0.000    0.000    0.000 hooks.py:641(on_after_batch_transfer)
      201    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_validation_batch_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 callback.py:142(on_validation_batch_start)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_validation_batch_start
         79555 function calls in 0.057 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.002    0.000    0.057    0.001 tqdm_progress.py:292(on_validation_batch_start)
      202    0.000    0.000    0.051    0.000 std.py:1325(refresh)
      202    0.001    0.000    0.050    0.000 std.py:1464(display)
      101    0.001    0.000    0.029    0.000 std.py:1360(reset)
     1204    0.001    0.000    0.028    0.000 utils.py:194(inner)
      602    0.001    0.000    0.024    0.000 redirect.py:644(write)
      602    0.000    0.000    0.023    0.000 wandb_run.py:2304(<lambda>)
      101    0.000    0.000    0.023    0.000 std.py:1382(set_description)
      602    0.000    0.000    0.023    0.000 wandb_run.py:390(wrapper_fn)
      602    0.001    0.000    0.022    0.000 wandb_run.py:1429(_console_raw_callback)
      602    0.003    0.000    0.021    0.000 interface.py:749(publish_output_raw)
      400    0.001    0.000    0.018    0.000 std.py:1441(moveto)
      202    0.001    0.000    0.017    0.000 std.py:457(print_status)
      202    0.001    0.000    0.014    0.000 std.py:1150(__str__)
      602    0.001    0.000    0.014    0.000 interface_shared.py:76(_publish_output_raw)
      602    0.001    0.000    0.013    0.000 interface_sock.py:45(_publish)
      602    0.001    0.000    0.012    0.000 sock_client.py:219(send_record_publish)
      202    0.000    0.000    0.011    0.000 std.py:451(fp_write)
      602    0.000    0.000    0.011    0.000 sock_client.py:153(send_server_request)
      602    0.001    0.000    0.010    0.000 sock_client.py:145(_send_message)
      202    0.004    0.000    0.010    0.000 std.py:464(format_meter)
      602    0.001    0.000    0.008    0.000 sock_client.py:121(_sendall_with_error_handle)
      602    0.007    0.000    0.007    0.000 {method 'send' of '_socket.socket' objects}
      202    0.000    0.000    0.005    0.000 utils.py:378(disp_len)
      202    0.000    0.000    0.005    0.000 utils.py:374(_text_width)
      202    0.001    0.000    0.004    0.000 {built-in method builtins.sum}
      202    0.001    0.000    0.004    0.000 std.py:1446(format_dict)
      602    0.004    0.000    0.004    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      602    0.000    0.000    0.004    0.000 well_known_types.py:172(GetCurrentTime)
    11423    0.002    0.000    0.003    0.000 utils.py:375(<genexpr>)
      202    0.001    0.000    0.003    0.000 utils.py:333(_screen_shape_linux)
      404    0.002    0.000    0.003    0.000 utils.py:273(_is_ascii)
      602    0.001    0.000    0.003    0.000 well_known_types.py:242(FromDatetime)
      101    0.001    0.000    0.002    0.000 progress_bar.py:90(total_val_batches_current_dataloader)
      404    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}
      202    0.001    0.000    0.001    0.000 {built-in method fcntl.ioctl}
    17974    0.001    0.000    0.001    0.000 {built-in method builtins.ord}
      303    0.000    0.000    0.001    0.000 enums.py:81(__eq__)
      602    0.001    0.000    0.001    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
      203    0.000    0.000    0.001    0.000 trainer.py:1429(sanity_checking)
    11221    0.001    0.000    0.001    0.000 {built-in method unicodedata.east_asian_width}
      602    0.001    0.000    0.001    0.000 enum_type_wrapper.py:92(__getattr__)
      602    0.001    0.000    0.001    0.000 calendar.py:655(timegm)
      202    0.000    0.000    0.001    0.000 utils.py:347(<listcomp>)
      602    0.001    0.000    0.001    0.000 interface_sock.py:41(_assign)
      602    0.001    0.000    0.001    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
      202    0.000    0.000    0.001    0.000 os.py:674(__getitem__)
      202    0.000    0.000    0.001    0.000 std.py:186(__format__)
     1204    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
      202    0.000    0.000    0.000    0.000 std.py:400(format_interval)
      100    0.000    0.000    0.000    0.000 trainer.py:1535(num_val_batches)
      606    0.000    0.000    0.000    0.000 types.py:171(__get__)
      202    0.000    0.000    0.000    0.000 std.py:102(acquire)
     1806    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      602    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
      202    0.000    0.000    0.000    0.000 std.py:106(release)
      602    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
      602    0.000    0.000    0.000    0.000 {built-in method utcnow}
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      202    0.000    0.000    0.000    0.000 std.py:153(__init__)
      101    0.000    0.000    0.000    0.000 tqdm_progress.py:440(convert_inf)
      602    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
      202    0.000    0.000    0.000    0.000 {built-in method fromtimestamp}
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      202    0.000    0.000    0.000    0.000 os.py:754(encode)
     1010    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
     1406    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      202    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      202    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
      400    0.000    0.000    0.000    0.000 utils.py:370(_term_move_up)
      404    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      202    0.000    0.000    0.000    0.000 std.py:231(__call__)
      303    0.000    0.000    0.000    0.000 tqdm_progress.py:131(val_progress_bar)
      303    0.000    0.000    0.000    0.000 std.py:226(__init__)
      602    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      101    0.000    0.000    0.000    0.000 progress_bar.py:145(has_dataloader_changed)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      602    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      602    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      606    0.000    0.000    0.000    0.000 enum.py:792(value)
      202    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
      202    0.000    0.000    0.000    0.000 utils.py:112(__format__)
      606    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
      202    0.000    0.000    0.000    0.000 {built-in method builtins.max}
      101    0.000    0.000    0.000    0.000 {built-in method math.isinf}
      606    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      404    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      602    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
      202    0.000    0.000    0.000    0.000 progress_bar.py:54(trainer)
      303    0.000    0.000    0.000    0.000 {built-in method time.time}
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      202    0.000    0.000    0.000    0.000 utils.py:108(__init__)
      202    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      202    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      202    0.000    0.000    0.000    0.000 std.py:167(colour)
      202    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
      202    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      202    0.000    0.000    0.000    0.000 std.py:163(colour)
      202    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      101    0.000    0.000    0.000    0.000 {built-in method math.isnan}
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 progress_bar.py:68(validation_description)
        1    0.000    0.000    0.000    0.000 trainer.py:1528(num_sanity_val_batches)
        1    0.000    0.000    0.000    0.000 evaluation_loop.py:84(max_batches)
        1    0.000    0.000    0.000    0.000 evaluation_loop.py:90(<listcomp>)
        2    0.000    0.000    0.000    0.000 {built-in method builtins.min}
        1    0.000    0.000    0.000    0.000 trainer.py:1533(<listcomp>)
        1    0.000    0.000    0.000    0.000 progress_bar.py:60(sanity_check_description)



Profile stats for: [Callback]ModelSummary.on_validation_batch_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 callback.py:142(on_validation_batch_start)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:142(on_validation_batch_start)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_batch_start
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 hooks.py:93(on_validation_batch_start)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Strategy]SingleDeviceStrategy.validation_step
         767614 function calls (745001 primitive calls) in 3.557 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.001    0.000    3.560    0.035 strategy.py:400(validation_step)
      101    0.007    0.000    3.554    0.035 FlavourClassificationTransformerEncoder.py:97(validation_step)
 4848/101    0.004    0.000    2.683    0.027 module.py:1549(_wrapped_call_impl)
 4848/101    0.021    0.000    2.682    0.027 module.py:1555(_call_impl)
      101    0.005    0.000    2.680    0.027 FlavourClassificationTransformerEncoder.py:55(forward)
      303    0.117    0.000    2.555    0.008 EncoderBlock.py:57(forward)
      606    0.318    0.001    1.563    0.003 LayerNormalisation.py:17(forward)
      808    0.916    0.001    0.916    0.001 {method 'mean' of 'torch._C.TensorBase' objects}
      101    0.002    0.000    0.661    0.007 functional.py:3014(cross_entropy)
      101    0.658    0.007    0.658    0.007 {built-in method torch._C._nn.cross_entropy_loss}
      303    0.091    0.000    0.614    0.002 XFormersAttention.py:31(forward)
     6363    0.010    0.000    0.394    0.000 __init__.py:1436(info)
     6363    0.009    0.000    0.382    0.000 __init__.py:1565(_log)
     6363    0.005    0.000    0.262    0.000 __init__.py:1591(handle)
     6363    0.010    0.000    0.255    0.000 __init__.py:1645(callHandlers)
     6363    0.007    0.000    0.245    0.000 __init__.py:939(handle)
     6363    0.004    0.000    0.230    0.000 __init__.py:1178(emit)
     6363    0.007    0.000    0.226    0.000 __init__.py:1071(emit)
      303    0.017    0.000    0.172    0.001 FFN.py:16(forward)
     2020    0.005    0.000    0.172    0.000 linear.py:116(forward)
     2020    0.166    0.000    0.166    0.000 {built-in method torch._C._nn.linear}
     6363    0.007    0.000    0.154    0.000 __init__.py:1060(flush)
     6363    0.139    0.000    0.139    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
     1818    0.003    0.000    0.135    0.000 {built-in method builtins.print}
     3636    0.003    0.000    0.132    0.000 redirect.py:644(write)
     3636    0.002    0.000    0.128    0.000 wandb_run.py:2304(<lambda>)
     3636    0.003    0.000    0.126    0.000 wandb_run.py:390(wrapper_fn)
      606    0.004    0.000    0.125    0.000 __init__.py:189(memory_efficient_attention)
     3636    0.005    0.000    0.123    0.000 wandb_run.py:1429(_console_raw_callback)
      606    0.002    0.000    0.119    0.000 __init__.py:457(_memory_efficient_attention)
      606    0.004    0.000    0.117    0.000 __init__.py:475(_memory_efficient_attention_forward)
     3636    0.013    0.000    0.115    0.000 interface.py:749(publish_output_raw)
      303    0.103    0.000    0.103    0.000 {built-in method torch.argmax}
     6363    0.006    0.000    0.089    0.000 __init__.py:1550(makeRecord)
     6363    0.039    0.000    0.084    0.000 __init__.py:282(__init__)
      202    0.007    0.000    0.082    0.000 module.py:382(log)
     4646    0.082    0.000    0.082    0.000 {method 'any' of 'torch._C.TensorBase' objects}
     3636    0.005    0.000    0.077    0.000 interface_shared.py:76(_publish_output_raw)
     3636    0.003    0.000    0.070    0.000 interface_sock.py:45(_publish)
     6363    0.003    0.000    0.065    0.000 __init__.py:916(format)
     3636    0.004    0.000    0.065    0.000 sock_client.py:219(send_record_publish)
     4646    0.064    0.000    0.064    0.000 {built-in method torch.isnan}
     6363    0.009    0.000    0.062    0.000 __init__.py:650(format)
     3636    0.001    0.000    0.059    0.000 sock_client.py:153(send_server_request)
      606    0.001    0.000    0.059    0.000 dispatch.py:126(_dispatch_fw)
     3636    0.008    0.000    0.058    0.000 sock_client.py:145(_send_message)
      606    0.003    0.000    0.053    0.000 dispatch.py:55(_run_priority_list)
      202    0.006    0.000    0.047    0.000 result.py:355(log)
     3636    0.004    0.000    0.042    0.000 sock_client.py:121(_sendall_with_error_handle)
      606    0.004    0.000    0.039    0.000 attn_bias.py:707(from_seqlens)
      606    0.002    0.000    0.038    0.000 cutlass.py:211(apply)
     3636    0.037    0.000    0.037    0.000 {method 'send' of '_socket.socket' objects}
      606    0.004    0.000    0.036    0.000 cutlass.py:275(apply_bmhk)
     1212    0.014    0.000    0.034    0.000 common.py:348(not_supported_reasons)
     6363    0.010    0.000    0.034    0.000 __init__.py:582(formatTime)
      707    0.001    0.000    0.031    0.000 _ops.py:1047(__call__)
  818/810    0.002    0.000    0.029    0.000 apply_func.py:23(apply_to_collection)
      606    0.003    0.000    0.028    0.000 flash.py:629(not_supported_reasons)
      606    0.027    0.000    0.027    0.000 {built-in method torch._ops.aten._efficient_attention_forward}
      606    0.002    0.000    0.025    0.000 attn_bias.py:350(from_seqlens)
      606    0.002    0.000    0.023    0.000 cutlass.py:326(not_supported_reasons)
      606    0.002    0.000    0.022    0.000 attn_bias.py:329(_get_seqstart)
     1212    0.022    0.000    0.022    0.000 {method 'reshape' of 'torch._C.TensorBase' objects}
44087/41611    0.007    0.000    0.022    0.000 {built-in method builtins.isinstance}
     6363    0.013    0.000    0.021    0.000 __init__.py:1514(findCaller)
      606    0.021    0.000    0.021    0.000 {built-in method torch.sqrt}
      303    0.001    0.000    0.021    0.000 activation.py:103(forward)
     3636    0.003    0.000    0.020    0.000 well_known_types.py:172(GetCurrentTime)
     1818    0.002    0.000    0.020    0.000 __init__.py:438(get_device_capability)
      202    0.000    0.000    0.020    0.000 result.py:424(update_metrics)
      303    0.001    0.000    0.019    0.000 functional.py:1489(relu)
      610    0.019    0.000    0.019    0.000 {built-in method torch.tensor}
      202    0.001    0.000    0.019    0.000 result.py:264(forward)
  544/406    0.002    0.000    0.019    0.000 apply_func.py:84(_apply_to_collection_slow)
      303    0.019    0.000    0.019    0.000 {built-in method torch.relu}
      909    0.019    0.000    0.019    0.000 {method 'sum' of 'torch._C.TensorBase' objects}
      202    0.004    0.000    0.018    0.000 metric.py:476(wrapped_func)
     1818    0.003    0.000    0.018    0.000 __init__.py:455(get_device_properties)
     1414    0.018    0.000    0.018    0.000 {method 'item' of 'torch._C.TensorBase' objects}
      909    0.018    0.000    0.018    0.000 {built-in method torch.zeros}
     3636    0.005    0.000    0.016    0.000 well_known_types.py:242(FromDatetime)
     6363    0.006    0.000    0.015    0.000 posixpath.py:117(splitext)
      303    0.014    0.000    0.014    0.000 {built-in method torch.all}
      202    0.010    0.000    0.013    0.000 result.py:207(update)
     6363    0.013    0.000    0.013    0.000 {built-in method time.strftime}
      909    0.002    0.000    0.012    0.000 dropout.py:58(forward)
      606    0.007    0.000    0.012    0.000 common.py:120(validate_inputs)
      606    0.012    0.000    0.012    0.000 {method 'var' of 'torch._C.TensorBase' objects}
     6363    0.006    0.000    0.012    0.000 posixpath.py:140(basename)
      303    0.011    0.000    0.011    0.000 {built-in method torch.stack}
     6363    0.011    0.000    0.011    0.000 {built-in method time.localtime}
      909    0.004    0.000    0.011    0.000 functional.py:1279(dropout)
     6363    0.003    0.000    0.010    0.000 __init__.py:634(formatMessage)
     1818    0.005    0.000    0.009    0.000 _utils.py:9(_get_device_index)
     1212    0.003    0.000    0.009    0.000 attn_bias.py:90(_get_default_bias_device)
     6363    0.006    0.000    0.008    0.000 genericpath.py:121(_splitext)
        2    0.000    0.000    0.008    0.004 result.py:415(register_key)
     1238    0.002    0.000    0.008    0.000 typing.py:719(__instancecheck__)
5788/1255    0.001    0.000    0.008    0.000 abc.py:121(__subclasscheck__)
5788/1255    0.007    0.000    0.008    0.000 {built-in method _abc._abc_subclasscheck}
      606    0.002    0.000    0.008    0.000 cutlass.py:49(_minimum_gemm_alignment)
     6363    0.002    0.000    0.007    0.000 __init__.py:432(format)
    12726    0.004    0.000    0.007    0.000 __init__.py:896(acquire)
      750    0.000    0.000    0.007    0.000 abc.py:117(__instancecheck__)
      202    0.001    0.000    0.007    0.000 module.py:654(__to_tensor)
      750    0.002    0.000    0.006    0.000 {built-in method _abc._abc_instancecheck}
      606    0.001    0.000    0.006    0.000 __init__.py:115(is_available)
     6363    0.002    0.000    0.006    0.000 __init__.py:628(usesTime)
     9999    0.006    0.000    0.006    0.000 {built-in method posix.getpid}
     9797    0.006    0.000    0.006    0.000 module.py:1716(__getattr__)
        2    0.000    0.000    0.006    0.003 result.py:306(to)
     1238    0.002    0.000    0.006    0.000 typing.py:848(__subclasscheck__)
      303    0.001    0.000    0.006    0.000 _tensor.py:982(__format__)
     1919    0.001    0.000    0.005    0.000 {built-in method builtins.any}
     3636    0.005    0.000    0.005    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
     6363    0.005    0.000    0.005    0.000 __init__.py:429(_format)
      909    0.005    0.000    0.005    0.000 {built-in method torch.dropout}
     3636    0.005    0.000    0.005    0.000 enum_type_wrapper.py:92(__getattr__)
    12726    0.004    0.000    0.005    0.000 __init__.py:903(release)
     3636    0.004    0.000    0.005    0.000 calendar.py:655(timegm)
      101    0.000    0.000    0.005    0.000 functional.py:1858(softmax)
      606    0.005    0.000    0.005    0.000 {method 'transpose' of 'torch._C.TensorBase' objects}
      101    0.005    0.000    0.005    0.000 {method 'softmax' of 'torch._C.TensorBase' objects}
     6363    0.003    0.000    0.004    0.000 __init__.py:160(<lambda>)
      606    0.004    0.000    0.004    0.000 dispatch.py:79(_dispatch_fw_priority_list)
    19089    0.004    0.000    0.004    0.000 {method 'rfind' of 'str' objects}
    22629    0.004    0.000    0.004    0.000 {built-in method builtins.hasattr}
      101    0.001    0.000    0.004    0.000 profiler.py:693(__exit__)
     4848    0.004    0.000    0.004    0.000 {built-in method torch._C._get_tracing_state}
     6363    0.002    0.000    0.004    0.000 __init__.py:421(usesTime)
      202    0.001    0.000    0.004    0.000 result.py:340(_extract_batch_size)
      606    0.001    0.000    0.004    0.000 __init__.py:111(_nvml_based_avail)
     1818    0.003    0.000    0.004    0.000 _utils.py:764(_get_device_index)
     1252    0.000    0.000    0.004    0.000 {built-in method builtins.issubclass}
     1818    0.004    0.000    0.004    0.000 {method 'unsqueeze' of 'torch._C.TensorBase' objects}
      101    0.000    0.000    0.003    0.000 _ops.py:887(__call__)
      101    0.001    0.000    0.003    0.000 data.py:61(extract_batch_size)
      202    0.003    0.000    0.003    0.000 {method 'clone' of 'torch._C.TensorBase' objects}
      202    0.000    0.000    0.003    0.000 trainer.py:1642(_results)
    12726    0.003    0.000    0.003    0.000 __init__.py:791(filter)
      202    0.000    0.000    0.003    0.000 trainer.py:1564(_active_loop)
      606    0.001    0.000    0.003    0.000 os.py:771(getenv)
      101    0.000    0.000    0.003    0.000 profiler.py:687(__enter__)
     6363    0.003    0.000    0.003    0.000 threading.py:1358(current_thread)
     3636    0.003    0.000    0.003    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
     6363    0.002    0.000    0.003    0.000 __init__.py:119(getLevelName)
    12727    0.003    0.000    0.003    0.000 {method 'acquire' of '_thread.RLock' objects}
      303    0.000    0.000    0.003    0.000 {built-in method builtins.next}
      909    0.003    0.000    0.003    0.000 {method 'view' of 'torch._C.TensorBase' objects}
     1818    0.002    0.000    0.003    0.000 common.py:482(check_lastdim_alignment_stride1)
     9784    0.003    0.000    0.003    0.000 {built-in method builtins.getattr}
     6363    0.003    0.000    0.003    0.000 __init__.py:1689(isEnabledFor)
     7272    0.003    0.000    0.003    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
     1818    0.003    0.000    0.003    0.000 {built-in method torch.cuda._get_device_properties}
     6363    0.002    0.000    0.003    0.000 posixpath.py:41(_get_sep)
      101    0.003    0.000    0.003    0.000 {built-in method torch._ops.profiler._record_function_enter_new}
     6363    0.002    0.000    0.003    0.000 posixpath.py:52(normcase)
     6363    0.003    0.000    0.003    0.000 __init__.py:358(getMessage)
      202    0.001    0.000    0.003    0.000 <string>:2(__init__)
      202    0.001    0.000    0.002    0.000 precision.py:173(val_step_context)
      808    0.001    0.000    0.002    0.000 enums.py:81(__eq__)
     1070    0.002    0.000    0.002    0.000 result.py:294(__setattr__)
      101    0.000    0.000    0.002    0.000 _ops.py:943(_must_dispatch_in_python)
     3636    0.002    0.000    0.002    0.000 interface_sock.py:41(_assign)
        2    0.000    0.000    0.002    0.001 result.py:187(__init__)
      202    0.001    0.000    0.002    0.000 fx_validator.py:191(check_logging_and_get_default_levels)
     1214    0.001    0.000    0.002    0.000 {built-in method builtins.all}
 1313/505    0.001    0.000    0.002    0.000 data.py:42(_extract_batch_size)
      606    0.000    0.000    0.002    0.000 _collections_abc.py:760(get)
      101    0.000    0.000    0.002    0.000 _pytree.py:1181(tree_any)
      101    0.000    0.000    0.002    0.000 contextlib.py:114(__enter__)
     1212    0.001    0.000    0.002    0.000 common.py:32(is_available)
    116/8    0.000    0.000    0.002    0.000 copy.py:128(deepcopy)
      606    0.002    0.000    0.002    0.000 cutlass.py:136(_custom_mask_type)
      202    0.001    0.000    0.002    0.000 memory.py:24(recursive_detach)
      404    0.000    0.000    0.002    0.000 trainer.py:1381(training)
      500    0.001    0.000    0.002    0.000 apply_func.py:17(is_dataclass_instance)
      606    0.001    0.000    0.002    0.000 os.py:674(__getitem__)
        4    0.000    0.000    0.002    0.000 metric.py:196(add_state)
      404    0.002    0.000    0.002    0.000 result.py:90(_generate_sync_fn)
    19089    0.002    0.000    0.002    0.000 {built-in method posix.fspath}
        4    0.000    0.000    0.002    0.000 _tensor.py:83(__deepcopy__)
     6363    0.002    0.000    0.002    0.000 threading.py:1093(name)
     9999    0.002    0.000    0.002    0.000 {method 'write' of '_io.TextIOWrapper' objects}
  707/202    0.001    0.000    0.002    0.000 _pytree.py:874(tree_iter)
    13033    0.002    0.000    0.002    0.000 {method 'get' of 'dict' objects}
     2424    0.001    0.000    0.002    0.000 __init__.py:834(device_count)
      202    0.000    0.000    0.002    0.000 result.py:57(__post_init__)
     6363    0.002    0.000    0.002    0.000 {built-in method sys._getframe}
      101    0.002    0.000    0.002    0.000 {method 'float' of 'torch._C.TensorBase' objects}
     4246    0.002    0.000    0.002    0.000 {method 'stride' of 'torch._C.TensorBase' objects}
     3636    0.002    0.000    0.002    0.000 {built-in method utcnow}
     3636    0.002    0.000    0.002    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
    12726    0.002    0.000    0.002    0.000 {built-in method _thread.get_ident}
      404    0.002    0.000    0.002    0.000 {method 'detach' of 'torch._C.TensorBase' objects}
     6363    0.002    0.000    0.002    0.000 {method 'find' of 'str' objects}
      606    0.001    0.000    0.002    0.000 attn_bias.py:656(to)
     1212    0.001    0.000    0.001    0.000 common.py:331(shape_not_supported_reasons)
        4    0.000    0.000    0.001    0.000 storage.py:907(_deepcopy)
     3636    0.001    0.000    0.001    0.000 {built-in method _struct.pack}
     3030    0.001    0.000    0.001    0.000 __init__.py:106(_is_compiled)
        4    0.000    0.000    0.001    0.000 storage.py:140(__deepcopy__)
     1818    0.001    0.000    0.001    0.000 __init__.py:284(_lazy_init)
        4    0.000    0.000    0.001    0.000 storage.py:156(clone)
     6363    0.001    0.000    0.001    0.000 {built-in method time.time}
      909    0.001    0.000    0.001    0.000 _VF.py:26(__getattr__)
    12727    0.001    0.000    0.001    0.000 {method 'release' of '_thread.RLock' objects}
        4    0.001    0.000    0.001    0.000 {method 'copy_' of 'torch._C.StorageBase' objects}
      210    0.001    0.000    0.001    0.000 grad_mode.py:184(__init__)
      202    0.000    0.000    0.001    0.000 result.py:122(__post_init__)
      606    0.001    0.000    0.001    0.000 common.py:96(normalize_bmhk)
      101    0.001    0.000    0.001    0.000 precision.py:68(forward_context)
      202    0.000    0.000    0.001    0.000 contextlib.py:123(__exit__)
      202    0.001    0.000    0.001    0.000 result.py:127(_parse_reduce_fx)
     6363    0.001    0.000    0.001    0.000 process.py:189(name)
      101    0.000    0.000    0.001    0.000 contextlib.py:261(helper)
     8593    0.001    0.000    0.001    0.000 {built-in method builtins.len}
      101    0.000    0.000    0.001    0.000 module.py:262(current_epoch)
     1212    0.000    0.000    0.001    0.000 cutlass.py:87(_get_tensor_bias)
     1818    0.001    0.000    0.001    0.000 __init__.py:237(is_initialized)
      101    0.001    0.000    0.001    0.000 {built-in method torch._ops.profiler.}
      606    0.001    0.000    0.001    0.000 cutlass.py:66(_get_seqlen_info)
      200    0.001    0.000    0.001    0.000 <string>:2(__eq__)
     2424    0.001    0.000    0.001    0.000 common.py:192(<genexpr>)
     1616    0.001    0.000    0.001    0.000 types.py:171(__get__)
      202    0.001    0.000    0.001    0.000 {method 'squeeze' of 'torch._C.TensorBase' objects}
     6363    0.001    0.000    0.001    0.000 process.py:37(current_process)
     2424    0.001    0.000    0.001    0.000 {built-in method builtins.max}
      202    0.000    0.000    0.001    0.000 trainer.py:1554(_evaluation_loop)
      606    0.000    0.000    0.001    0.000 cutlass.py:98(_check_bias_alignment)
      202    0.000    0.000    0.001    0.000 memory.py:40(detach_and_move)
     2424    0.001    0.000    0.001    0.000 common.py:131(<genexpr>)
     2424    0.001    0.000    0.001    0.000 __init__.py:461(<genexpr>)
      202    0.001    0.000    0.001    0.000 fx_validator.py:151(check_logging)
      404    0.000    0.000    0.001    0.000 _pytree.py:656(_is_leaf)
     2424    0.001    0.000    0.001    0.000 common.py:71(device)
      707    0.000    0.000    0.001    0.000 _pytree.py:649(_get_node_type)
     3636    0.001    0.000    0.001    0.000 {method '__exit__' of '_thread.lock' objects}
     2424    0.001    0.000    0.001    0.000 common.py:122(<genexpr>)
     3636    0.001    0.000    0.001    0.000 {built-in method time.monotonic}
      500    0.000    0.000    0.001    0.000 dataclasses.py:1047(is_dataclass)
      101    0.000    0.000    0.001    0.000 profiler.py:676(__init__)
      202    0.000    0.000    0.001    0.000 result.py:148(sync)
      101    0.001    0.000    0.001    0.000 contextlib.py:86(__init__)
      101    0.001    0.000    0.001    0.000 trainer.py:1467(current_epoch)
      606    0.000    0.000    0.001    0.000 os.py:754(encode)
      202    0.000    0.000    0.001    0.000 trainer.py:1429(sanity_checking)
     2424    0.001    0.000    0.001    0.000 common.py:152(<genexpr>)
      606    0.000    0.000    0.001    0.000 flash.py:511(_check_needs_no_topleft)
      606    0.000    0.000    0.000    0.000 os.py:758(decode)
     1620    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}
      101    0.000    0.000    0.000    0.000 container.py:317(__iter__)
      202    0.000    0.000    0.000    0.000 logger_connector.py:213(should_reset_tensors)
      707    0.000    0.000    0.000    0.000 _pytree.py:638(_is_namedtuple_instance)
     3636    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
      101    0.000    0.000    0.000    0.000 _reduction.py:7(get_enum)
     1212    0.000    0.000    0.000    0.000 attn_bias.py:316(to)
     2855    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
      202    0.000    0.000    0.000    0.000 fx_validator.py:166(get_default_logging_levels)
     1818    0.000    0.000    0.000    0.000 flash.py:533(_check_strides_for_bmghk)
      614    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}
      101    0.000    0.000    0.000    0.000 contextlib.py:688(__init__)
      202    0.000    0.000    0.000    0.000 fx_validator.py:177(check_logging_levels)
      202    0.000    0.000    0.000    0.000 result.py:74(op)
      202    0.000    0.000    0.000    0.000 {built-in method torch.is_floating_point}
     3636    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
     1818    0.000    0.000    0.000    0.000 {built-in method torch._C._cuda_isInBadFork}
        2    0.000    0.000    0.000    0.000 metric.py:101(__init__)
      204    0.000    0.000    0.000    0.000 result.py:536(_get_default_dtype)
      202    0.000    0.000    0.000    0.000 grad_mode.py:196(__exit__)
      200    0.000    0.000    0.000    0.000 trainer.py:1425(evaluating)
     1923    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
     1616    0.000    0.000    0.000    0.000 enum.py:792(value)
     1212    0.000    0.000    0.000    0.000 {built-in method builtins.min}
      500    0.000    0.000    0.000    0.000 apply_func.py:11(is_namedtuple)
     1818    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      101    0.000    0.000    0.000    0.000 _ops.py:945(<lambda>)
      408    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C.TensorBase' objects}
      202    0.000    0.000    0.000    0.000 {built-in method torch.numel}
        4    0.000    0.000    0.000    0.000 apply_func.py:69(<genexpr>)
      101    0.000    0.000    0.000    0.000 _pytree.py:423(_dict_flatten)
        4    0.000    0.000    0.000    0.000 {method 'new_empty' of 'torch._C.TensorBase' objects}
      6/4    0.000    0.000    0.000    0.000 copy.py:258(_reconstruct)
        8    0.000    0.000    0.000    0.000 apply_func.py:71(move_data_to_device)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      303    0.000    0.000    0.000    0.000 {method '__format__' of 'int' objects}
      606    0.000    0.000    0.000    0.000 {method 'decode' of 'bytes' objects}
      606    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      202    0.000    0.000    0.000    0.000 grad_mode.py:193(__enter__)
        8    0.000    0.000    0.000    0.000 apply_func.py:91(batch_to)
     10/8    0.000    0.000    0.000    0.000 copy.py:226(_deepcopy_dict)
      303    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      606    0.000    0.000    0.000    0.000 dispatch.py:27(_get_use_fa3)
        2    0.000    0.000    0.000    0.000 inspect.py:3111(signature)
        8    0.000    0.000    0.000    0.000 {method 'to' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 inspect.py:2859(from_callable)
      4/2    0.000    0.000    0.000    0.000 inspect.py:2246(_signature_from_callable)
      214    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}
      305    0.000    0.000    0.000    0.000 result.py:163(is_mean_reduction)
      606    0.000    0.000    0.000    0.000 cutlass.py:41(_uses_tensorcores)
      101    0.000    0.000    0.000    0.000 module.py:215(trainer)
      200    0.000    0.000    0.000    0.000 states.py:63(evaluating)
      206    0.000    0.000    0.000    0.000 {built-in method torch.get_default_dtype}
      103    0.000    0.000    0.000    0.000 typing.py:271(inner)
      606    0.000    0.000    0.000    0.000 result.py:70(op)
      202    0.000    0.000    0.000    0.000 strategy.py:351(model)
      202    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      303    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C.TensorBase' objects}
      404    0.000    0.000    0.000    0.000 result.py:60(should)
      202    0.000    0.000    0.000    0.000 _pytree.py:395(_tuple_flatten)
      101    0.000    0.000    0.000    0.000 __init__.py:129(annotate)
        2    0.000    0.000    0.000    0.000 inspect.py:2152(_signature_from_function)
      404    0.000    0.000    0.000    0.000 result.py:80(group)
      559    0.000    0.000    0.000    0.000 _collections_abc.py:409(__subclasshook__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.iter}
      101    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_variadic}
      101    0.000    0.000    0.000    0.000 strategy.py:93(precision_plugin)
        2    0.000    0.000    0.000    0.000 module.py:429(__init__)
      209    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
      4/2    0.000    0.000    0.000    0.000 copy.py:209(_deepcopy_tuple)
      202    0.000    0.000    0.000    0.000 typing.py:1375(cast)
        6    0.000    0.000    0.000    0.000 {method '__reduce_ex__' of 'object' objects}
        4    0.000    0.000    0.000    0.000 _tensor.py:242(_typed_storage)
        4    0.000    0.000    0.000    0.000 dataclasses.py:1024(fields)
      4/2    0.000    0.000    0.000    0.000 copy.py:210(<listcomp>)
        2    0.000    0.000    0.000    0.000 metric.py:475(_wrap_update)
        2    0.000    0.000    0.000    0.000 result.py:273(_wrap_compute)
      101    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}
        4    0.000    0.000    0.000    0.000 functools.py:35(update_wrapper)
       58    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}
       26    0.000    0.000    0.000    0.000 copy.py:242(_keep_alive)
        4    0.000    0.000    0.000    0.000 grad_mode.py:80(__enter__)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      101    0.000    0.000    0.000    0.000 contextlib.py:691(__enter__)
      101    0.000    0.000    0.000    0.000 contextlib.py:694(__exit__)
      101    0.000    0.000    0.000    0.000 {method '__exit__' of 'torch._C.DisableTorchFunctionSubclass' objects}
        2    0.000    0.000    0.000    0.000 inspect.py:1840(_signature_bound_method)
      214    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        4    0.000    0.000    0.000    0.000 inspect.py:2781(__init__)
        6    0.000    0.000    0.000    0.000 inspect.py:2498(__init__)
        4    0.000    0.000    0.000    0.000 storage.py:712(_new_wrapped_storage)
      186    0.000    0.000    0.000    0.000 _collections_abc.py:315(__subclasshook__)
       12    0.000    0.000    0.000    0.000 copy.py:263(<genexpr>)
       38    0.000    0.000    0.000    0.000 dataclasses.py:1039(<genexpr>)
        4    0.000    0.000    0.000    0.000 {method 'set_' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 copyreg.py:103(_slotnames)
        8    0.000    0.000    0.000    0.000 storage.py:629(__init__)
        4    0.000    0.000    0.000    0.000 _contextlib.py:154(__new__)
        2    0.000    0.000    0.000    0.000 inspect.py:494(unwrap)
        2    0.000    0.000    0.000    0.000 inspect.py:2873(replace)
        4    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}
        4    0.000    0.000    0.000    0.000 grad_mode.py:84(__exit__)
        8    0.000    0.000    0.000    0.000 storage.py:557(__new__)
        4    0.000    0.000    0.000    0.000 grad_mode.py:75(__init__)
       10    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 {method 'untyped_storage' of 'torch._C.TensorBase' objects}
        4    0.000    0.000    0.000    0.000 copyreg.py:94(__newobj__)
        6    0.000    0.000    0.000    0.000 enum.py:358(__call__)
        2    0.000    0.000    0.000    0.000 result.py:171(is_max_reduction)
       48    0.000    0.000    0.000    0.000 copy.py:182(_deepcopy_atomic)
        4    0.000    0.000    0.000    0.000 {method 'contiguous' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 __init__.py:218(_acquireLock)
        8    0.000    0.000    0.000    0.000 inspect.py:2830(<genexpr>)
        4    0.000    0.000    0.000    0.000 {built-in method torch._C._has_storage}
        1    0.000    0.000    0.000    0.000 __init__.py:1276(disable)
       16    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f88d25377c0}
       28    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
        4    0.000    0.000    0.000    0.000 {method 'nbytes' of 'torch._C.StorageBase' objects}
       22    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 result.py:502(reset)
        4    0.000    0.000    0.000    0.000 functools.py:65(wraps)
        4    0.000    0.000    0.000    0.000 inspect.py:159(isfunction)
        2    0.000    0.000    0.000    0.000 inspect.py:514(_is_wrapper)
        2    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}
        6    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}
        6    0.000    0.000    0.000    0.000 enum.py:670(__new__)
       14    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 __init__.py:227(_releaseLock)
       10    0.000    0.000    0.000    0.000 inspect.py:2548(name)
        4    0.000    0.000    0.000    0.000 {method 'is_conj' of 'torch._C.TensorBase' objects}
        4    0.000    0.000    0.000    0.000 {method 'storage_offset' of 'torch._C.TensorBase' objects}
        8    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
        4    0.000    0.000    0.000    0.000 {method 'is_neg' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 result.py:175(is_min_reduction)
        8    0.000    0.000    0.000    0.000 inspect.py:2560(kind)
        1    0.000    0.000    0.000    0.000 __init__.py:1675(getEffectiveLevel)
        4    0.000    0.000    0.000    0.000 storage.py:30(__init__)
        2    0.000    0.000    0.000    0.000 {method 'get' of 'mappingproxy' objects}
        2    0.000    0.000    0.000    0.000 {method '__setstate__' of 'functools.partial' objects}
        4    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 {built-in method sys.getrecursionlimit}
        4    0.000    0.000    0.000    0.000 inspect.py:2552(default)
        2    0.000    0.000    0.000    0.000 inspect.py:2865(parameters)



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_validation_batch_end
         1010 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:152(on_validation_batch_end)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_validation_batch_end
         41946 function calls in 0.035 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.001    0.000    0.034    0.000 tqdm_progress.py:309(on_validation_batch_end)
      101    0.000    0.000    0.033    0.000 tqdm_progress.py:451(_update_n)
      101    0.000    0.000    0.033    0.000 std.py:1325(refresh)
      101    0.000    0.000    0.032    0.000 std.py:1464(display)
      602    0.000    0.000    0.018    0.000 utils.py:194(inner)
      301    0.000    0.000    0.015    0.000 redirect.py:644(write)
      301    0.000    0.000    0.014    0.000 wandb_run.py:2304(<lambda>)
      301    0.000    0.000    0.014    0.000 wandb_run.py:390(wrapper_fn)
      200    0.001    0.000    0.014    0.000 std.py:1441(moveto)
      301    0.000    0.000    0.014    0.000 wandb_run.py:1429(_console_raw_callback)
      301    0.003    0.000    0.013    0.000 interface.py:749(publish_output_raw)
      101    0.000    0.000    0.009    0.000 std.py:457(print_status)
      101    0.000    0.000    0.008    0.000 std.py:1150(__str__)
      301    0.000    0.000    0.007    0.000 interface_shared.py:76(_publish_output_raw)
      301    0.000    0.000    0.007    0.000 interface_sock.py:45(_publish)
      301    0.000    0.000    0.006    0.000 sock_client.py:219(send_record_publish)
      301    0.000    0.000    0.006    0.000 sock_client.py:153(send_server_request)
      301    0.001    0.000    0.006    0.000 sock_client.py:145(_send_message)
      101    0.000    0.000    0.005    0.000 std.py:451(fp_write)
      101    0.002    0.000    0.005    0.000 std.py:464(format_meter)
      301    0.000    0.000    0.004    0.000 sock_client.py:121(_sendall_with_error_handle)
      301    0.004    0.000    0.004    0.000 {method 'send' of '_socket.socket' objects}
      101    0.000    0.000    0.003    0.000 utils.py:378(disp_len)
      301    0.003    0.000    0.003    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      101    0.000    0.000    0.003    0.000 utils.py:374(_text_width)
      101    0.001    0.000    0.003    0.000 {built-in method builtins.sum}
      101    0.000    0.000    0.003    0.000 std.py:1446(format_dict)
     7176    0.002    0.000    0.002    0.000 utils.py:375(<genexpr>)
      101    0.001    0.000    0.002    0.000 utils.py:333(_screen_shape_linux)
      301    0.000    0.000    0.002    0.000 well_known_types.py:172(GetCurrentTime)
      301    0.001    0.000    0.002    0.000 well_known_types.py:242(FromDatetime)
      202    0.001    0.000    0.002    0.000 utils.py:273(_is_ascii)
      202    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}
      101    0.001    0.000    0.001    0.000 {built-in method fcntl.ioctl}
     7075    0.001    0.000    0.001    0.000 {built-in method unicodedata.east_asian_width}
      301    0.001    0.000    0.001    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
      301    0.001    0.000    0.001    0.000 enum_type_wrapper.py:92(__getattr__)
      301    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
     8987    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
      202    0.000    0.000    0.000    0.000 std.py:400(format_interval)
      101    0.000    0.000    0.000    0.000 utils.py:347(<listcomp>)
      101    0.000    0.000    0.000    0.000 std.py:102(acquire)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 tqdm_progress.py:425(_should_update)
      301    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 os.py:674(__getitem__)
      602    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 std.py:186(__format__)
      101    0.000    0.000    0.000    0.000 std.py:106(release)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      301    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
      101    0.000    0.000    0.000    0.000 tqdm_progress.py:169(is_enabled)
      301    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
      903    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      101    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
      101    0.000    0.000    0.000    0.000 std.py:153(__init__)
      301    0.000    0.000    0.000    0.000 {built-in method utcnow}
      301    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
      301    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
      101    0.000    0.000    0.000    0.000 {built-in method now}
      101    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
      101    0.000    0.000    0.000    0.000 os.py:754(encode)
      301    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      101    0.000    0.000    0.000    0.000 std.py:231(__call__)
      200    0.000    0.000    0.000    0.000 utils.py:370(_term_move_up)
      703    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      202    0.000    0.000    0.000    0.000 tqdm_progress.py:131(val_progress_bar)
      101    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      505    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
      202    0.000    0.000    0.000    0.000 tqdm_progress.py:161(refresh_rate)
      202    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      301    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      101    0.000    0.000    0.000    0.000 utils.py:108(__init__)
      301    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      101    0.000    0.000    0.000    0.000 {built-in method builtins.max}
      101    0.000    0.000    0.000    0.000 utils.py:112(__format__)
      202    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      101    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      301    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
      101    0.000    0.000    0.000    0.000 std.py:167(colour)
      303    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      101    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      101    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      101    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {built-in method time.time}
      101    0.000    0.000    0.000    0.000 std.py:163(colour)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      101    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}



Profile stats for: [Callback]ModelSummary.on_validation_batch_end
         1010 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 callback.py:152(on_validation_batch_end)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_end
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:152(on_validation_batch_end)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_batch_end
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 hooks.py:103(on_validation_batch_end)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_validation_epoch_end
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 callback.py:127(on_validation_epoch_end)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_validation_epoch_end
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      101    0.000    0.000    0.000    0.000 callback.py:127(on_validation_epoch_end)



Profile stats for: [Callback]ModelSummary.on_validation_epoch_end
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 callback.py:127(on_validation_epoch_end)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_end
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:127(on_validation_epoch_end)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_epoch_end
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 hooks.py:244(on_validation_epoch_end)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_validation_end
         1010 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 early_stopping.py:192(on_validation_end)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)



Profile stats for: [Callback]TQDMProgressBar.on_validation_end
         75542 function calls (75528 primitive calls) in 0.057 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.001    0.000    0.057    0.001 tqdm_progress.py:323(on_validation_end)
      201    0.000    0.000    0.031    0.000 std.py:1464(display)
      101    0.001    0.000    0.024    0.000 std.py:1265(close)
      904    0.000    0.000    0.021    0.000 utils.py:194(inner)
      101    0.001    0.000    0.021    0.000 std.py:1402(set_postfix)
      503    0.000    0.000    0.018    0.000 redirect.py:644(write)
      101    0.000    0.000    0.018    0.000 std.py:1325(refresh)
      503    0.000    0.000    0.018    0.000 wandb_run.py:2304(<lambda>)
      503    0.000    0.000    0.018    0.000 wandb_run.py:390(wrapper_fn)
      503    0.001    0.000    0.017    0.000 wandb_run.py:1429(_console_raw_callback)
      503    0.002    0.000    0.016    0.000 interface.py:749(publish_output_raw)
      201    0.000    0.000    0.014    0.000 std.py:457(print_status)
      503    0.001    0.000    0.011    0.000 interface_shared.py:76(_publish_output_raw)
      101    0.000    0.000    0.010    0.000 progress_bar.py:177(get_metrics)
      503    0.000    0.000    0.010    0.000 interface_sock.py:45(_publish)
      201    0.000    0.000    0.009    0.000 std.py:451(fp_write)
      503    0.001    0.000    0.009    0.000 sock_client.py:219(send_record_publish)
      200    0.001    0.000    0.009    0.000 std.py:1441(moveto)
      503    0.000    0.000    0.008    0.000 sock_client.py:153(send_server_request)
      503    0.001    0.000    0.008    0.000 sock_client.py:145(_send_message)
      100    0.000    0.000    0.008    0.000 std.py:1150(__str__)
      503    0.001    0.000    0.006    0.000 sock_client.py:121(_sendall_with_error_handle)
      101    0.001    0.000    0.006    0.000 trainer.py:1632(progress_bar_metrics)
      101    0.000    0.000    0.005    0.000 logger_connector.py:250(progress_bar_metrics)
      100    0.002    0.000    0.005    0.000 std.py:464(format_meter)
      503    0.005    0.000    0.005    0.000 {method 'send' of '_socket.socket' objects}
      101    0.001    0.000    0.005    0.000 std.py:686(_decr_instances)
      102    0.000    0.000    0.004    0.000 std.py:1286(fp_write)
      101    0.002    0.000    0.004    0.000 progress_bar.py:210(get_standard_metrics)
      201    0.000    0.000    0.004    0.000 utils.py:378(disp_len)
      101    0.000    0.000    0.004    0.000 logger_connector.py:229(metrics)
      201    0.000    0.000    0.004    0.000 utils.py:374(_text_width)
      201    0.001    0.000    0.003    0.000 {built-in method builtins.sum}
      303    0.000    0.000    0.003    0.000 trainer.py:1642(_results)
      303    0.000    0.000    0.003    0.000 trainer.py:1564(_active_loop)
      503    0.000    0.000    0.003    0.000 well_known_types.py:172(GetCurrentTime)
      401    0.003    0.000    0.003    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
     9663    0.002    0.000    0.003    0.000 utils.py:375(<genexpr>)
      100    0.001    0.000    0.002    0.000 std.py:1446(format_dict)
      503    0.001    0.000    0.002    0.000 well_known_types.py:242(FromDatetime)
     1010    0.001    0.000    0.002    0.000 enums.py:81(__eq__)
      101    0.000    0.000    0.002    0.000 utilities.py:25(_version)
      101    0.000    0.000    0.002    0.000 result.py:476(metrics)
      101    0.000    0.000    0.002    0.000 wandb.py:572(version)
      100    0.001    0.000    0.002    0.000 utils.py:333(_screen_shape_linux)
      200    0.001    0.000    0.002    0.000 utils.py:273(_is_ascii)
      201    0.001    0.000    0.001    0.000 _weakrefset.py:63(__iter__)
      101    0.001    0.000    0.001    0.000 wandb_run.py:357(wrapper)
      101    0.000    0.000    0.001    0.000 _weakrefset.py:111(remove)
      200    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}
       99    0.000    0.000    0.001    0.000 tqdm_progress.py:46(format_num)
     2413    0.000    0.000    0.001    0.000 {built-in method builtins.isinstance}
      202    0.000    0.000    0.001    0.000 {method 'remove' of 'set' objects}
      303    0.000    0.000    0.001    0.000 trainer.py:1429(sanity_checking)
      303    0.000    0.000    0.001    0.000 trainer.py:1381(training)
      100    0.001    0.000    0.001    0.000 {built-in method fcntl.ioctl}
      202    0.000    0.000    0.001    0.000 result.py:430(_get_cache)
      303    0.000    0.000    0.001    0.000 trainer.py:1554(_evaluation_loop)
     9462    0.001    0.000    0.001    0.000 {built-in method unicodedata.east_asian_width}
      503    0.001    0.000    0.001    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
     1919    0.001    0.000    0.001    0.000 types.py:171(__get__)
      503    0.001    0.000    0.001    0.000 calendar.py:655(timegm)
      503    0.001    0.000    0.001    0.000 enum_type_wrapper.py:92(__getattr__)
      101    0.001    0.000    0.001    0.000 wandb_run.py:881(id)
      302    0.001    0.000    0.001    0.000 std.py:106(release)
      202    0.001    0.000    0.001    0.000 {method 'detach' of 'torch._C.TensorBase' objects}
      202    0.000    0.000    0.001    0.000 std.py:113(__exit__)
       99    0.000    0.000    0.001    0.000 std.py:419(format_num)
      302    0.000    0.000    0.001    0.000 std.py:102(acquire)
      101    0.000    0.000    0.001    0.000 utils.py:125(__eq__)
      200    0.000    0.000    0.001    0.000 abc.py:117(__instancecheck__)
      202    0.000    0.000    0.001    0.000 std.py:110(__enter__)
     9103    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
      200    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}
      503    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
      503    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
     1006    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
      100    0.000    0.000    0.000    0.000 utils.py:347(<listcomp>)
      200    0.000    0.000    0.000    0.000 std.py:400(format_interval)
      101    0.000    0.000    0.000    0.000 _weakrefset.py:27(__exit__)
      101    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
     1811    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      604    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
      100    0.000    0.000    0.000    0.000 os.py:674(__getitem__)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      503    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
      202    0.000    0.000    0.000    0.000 result.py:465(_forked_name)
      100    0.000    0.000    0.000    0.000 std.py:186(__format__)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      503    0.000    0.000    0.000    0.000 {built-in method utcnow}
     1919    0.000    0.000    0.000    0.000 enum.py:792(value)
      300    0.000    0.000    0.000    0.000 trainer.py:1425(evaluating)
      101    0.000    0.000    0.000    0.000 _weakrefset.py:53(_commit_removals)
     2020    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      301    0.000    0.000    0.000    0.000 std.py:1428(<genexpr>)
      503    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
      100    0.000    0.000    0.000    0.000 std.py:153(__init__)
      303    0.000    0.000    0.000    0.000 result.py:463(<genexpr>)
      202    0.000    0.000    0.000    0.000 std.py:1153(_comparable)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 _weakrefset.py:21(__enter__)
     1504    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      201    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
      101    0.000    0.000    0.000    0.000 _weakrefset.py:17(__init__)
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      302    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
      503    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      401    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      100    0.000    0.000    0.000    0.000 {built-in method now}
      100    0.000    0.000    0.000    0.000 std.py:708(<lambda>)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 result.py:461(valid_items)
      202    0.000    0.000    0.000    0.000 result.py:158(forked_name)
      100    0.000    0.000    0.000    0.000 os.py:754(encode)
      101    0.000    0.000    0.000    0.000 std.py:1157(__hash__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}
      503    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      503    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      300    0.000    0.000    0.000    0.000 states.py:63(evaluating)
      201    0.000    0.000    0.000    0.000 {built-in method builtins.max}
      100    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
      198    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}
      301    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      403    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      200    0.000    0.000    0.000    0.000 utils.py:370(_term_move_up)
      302    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      302    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
      500    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
      100    0.000    0.000    0.000    0.000 std.py:231(__call__)
      503    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 utils.py:112(__format__)
      101    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
      101    0.000    0.000    0.000    0.000 progress_bar.py:150(reset_dataloader_idx_tracker)
      302    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      101    0.000    0.000    0.000    0.000 {built-in method builtins.id}
      101    0.000    0.000    0.000    0.000 tqdm_progress.py:121(train_progress_bar)
      303    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}
      101    0.000    0.000    0.000    0.000 tqdm_progress.py:131(val_progress_bar)
      200    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}
      202    0.000    0.000    0.000    0.000 result.py:154(forked)
      100    0.000    0.000    0.000    0.000 utils.py:108(__init__)
      100    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      100    0.000    0.000    0.000    0.000 std.py:167(colour)
      202    0.000    0.000    0.000    0.000 trainer.py:1590(loggers)
      202    0.000    0.000    0.000    0.000 {method 'keys' of 'collections.OrderedDict' objects}
      8/1    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)
      100    0.000    0.000    0.000    0.000 {built-in method time.time}
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      8/1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 std.py:163(colour)
      101    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}



Profile stats for: [Callback]ModelSummary.on_validation_end
         1010 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.001    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 callback.py:214(on_validation_end)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_end
         4820 function calls in 0.006 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.006    0.000 model_checkpoint.py:326(on_validation_end)
      101    0.001    0.000    0.004    0.000 model_checkpoint.py:413(_should_skip_saving_checkpoint)
      100    0.001    0.000    0.002    0.000 trainer.py:1458(global_step)
      100    0.001    0.000    0.002    0.000 training_epoch_loop.py:99(global_step)
      100    0.001    0.000    0.002    0.000 model_checkpoint.py:423(_should_save_on_train_epoch_end)
      301    0.000    0.000    0.001    0.000 enums.py:81(__eq__)
      100    0.001    0.000    0.001    0.000 progress.py:274(optimizer_steps)
      200    0.000    0.000    0.001    0.000 trainer.py:1535(num_val_batches)
      101    0.000    0.000    0.000    0.000 trainer.py:1429(sanity_checking)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      602    0.000    0.000    0.000    0.000 types.py:171(__get__)
      201    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      401    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      602    0.000    0.000    0.000    0.000 enum.py:792(value)
      602    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      201    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.sum}
      100    0.000    0.000    0.000    0.000 module.py:295(automatic_optimization)
      201    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_end
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 hooks.py:53(on_validation_end)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Strategy]SingleDeviceStrategy.on_validation_end
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 strategy.py:565(on_validation_end)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_sanity_check_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:73(on_sanity_check_end)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_sanity_check_end
         14 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:254(on_sanity_check_end)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        2    0.000    0.000    0.000    0.000 std.py:1265(close)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:131(val_progress_bar)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:121(train_progress_bar)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_sanity_check_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:73(on_sanity_check_end)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:73(on_sanity_check_end)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningDataModule]PMTfiedDataModule.train_dataloader
         478 function calls (473 primitive calls) in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 PMTfiedDataModule.py:67(train_dataloader)
      2/1    0.000    0.000    0.000    0.000 data.py:287(wrapper)
        1    0.000    0.000    0.000    0.000 dataloader.py:227(__init__)
        2    0.000    0.000    0.000    0.000 inspect.py:3111(signature)
        2    0.000    0.000    0.000    0.000 inspect.py:2859(from_callable)
        2    0.000    0.000    0.000    0.000 inspect.py:2246(_signature_from_callable)
        2    0.000    0.000    0.000    0.000 inspect.py:2152(_signature_from_function)
    23/22    0.000    0.000    0.000    0.000 data.py:334(wrapper)
       21    0.000    0.000    0.000    0.000 inspect.py:2498(__init__)
       21    0.000    0.000    0.000    0.000 data.py:295(<genexpr>)
       37    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
    20/19    0.000    0.000    0.000    0.000 dataloader.py:418(__setattr__)
        2    0.000    0.000    0.000    0.000 inspect.py:2781(__init__)
       21    0.000    0.000    0.000    0.000 enum.py:358(__call__)
        3    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)
       27    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 dataloader.py:487(check_worker_number_rationality)
        3    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}
        1    0.000    0.000    0.000    0.000 sampler.py:262(__init__)
       23    0.000    0.000    0.000    0.000 inspect.py:2830(<genexpr>)
       50    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        1    0.000    0.000    0.000    0.000 sampler.py:133(__init__)
        2    0.000    0.000    0.000    0.000 inspect.py:494(unwrap)
        1    0.000    0.000    0.000    0.000 {built-in method posix.sched_getaffinity}
       61    0.000    0.000    0.000    0.000 inspect.py:2548(name)
        2    0.000    0.000    0.000    0.000 data.py:303(<dictcomp>)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        2    0.000    0.000    0.000    0.000 sampler.py:146(num_samples)
       21    0.000    0.000    0.000    0.000 enum.py:670(__new__)
        1    0.000    0.000    0.000    0.000 {built-in method torch.set_vital}
       21    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}
        1    0.000    0.000    0.000    0.000 dataloader.py:394(multiprocessing_context)
      8/6    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
       21    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}
        2    0.000    0.000    0.000    0.000 inspect.py:514(_is_wrapper)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
       19    0.000    0.000    0.000    0.000 inspect.py:2560(kind)
       19    0.000    0.000    0.000    0.000 inspect.py:2552(default)
        5    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        4    0.000    0.000    0.000    0.000 inspect.py:159(isfunction)
        2    0.000    0.000    0.000    0.000 dataset.py:422(__len__)
        2    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}
        1    0.000    0.000    0.000    0.000 dataloader.py:442(_auto_collation)
        2    0.000    0.000    0.000    0.000 inspect.py:2865(parameters)
        2    0.000    0.000    0.000    0.000 {built-in method sys.getrecursionlimit}
        2    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        2    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
        2    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'index' of 'tuple' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_train_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:205(on_train_start)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_train_start
         266 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.001    0.001 tqdm_progress.py:259(on_train_start)
        1    0.000    0.000    0.001    0.001 tqdm_progress.py:197(init_train_tqdm)
        1    0.000    0.000    0.001    0.001 tqdm_progress.py:40(__init__)
        1    0.000    0.000    0.001    0.001 std.py:952(__init__)
        1    0.000    0.000    0.001    0.001 std.py:1325(refresh)
        1    0.000    0.000    0.001    0.001 std.py:1464(display)
        1    0.000    0.000    0.000    0.000 std.py:457(print_status)
        1    0.000    0.000    0.000    0.000 std.py:451(fp_write)
        2    0.000    0.000    0.000    0.000 utils.py:194(inner)
        1    0.000    0.000    0.000    0.000 redirect.py:644(write)
        1    0.000    0.000    0.000    0.000 wandb_run.py:2304(<lambda>)
        1    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
        1    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
        1    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
        1    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
        1    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
        1    0.000    0.000    0.000    0.000 std.py:1150(__str__)
        2    0.000    0.000    0.000    0.000 utils.py:333(_screen_shape_linux)
        1    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
        1    0.000    0.000    0.000    0.000 std.py:663(__new__)
        1    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        1    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
        1    0.000    0.000    0.000    0.000 std.py:464(format_meter)
        1    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
        1    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
        1    0.000    0.000    0.000    0.000 utils.py:378(disp_len)
        1    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
        1    0.000    0.000    0.000    0.000 utils.py:374(_text_width)
        1    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.sum}
        2    0.000    0.000    0.000    0.000 utils.py:347(<listcomp>)
        3    0.000    0.000    0.000    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
       44    0.000    0.000    0.000    0.000 utils.py:375(<genexpr>)
        2    0.000    0.000    0.000    0.000 {built-in method fcntl.ioctl}
        1    0.000    0.000    0.000    0.000 utils.py:213(__init__)
        1    0.000    0.000    0.000    0.000 std.py:1446(format_dict)
        1    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        2    0.000    0.000    0.000    0.000 std.py:110(__enter__)
        2    0.000    0.000    0.000    0.000 os.py:674(__getitem__)
        3    0.000    0.000    0.000    0.000 std.py:102(acquire)
        1    0.000    0.000    0.000    0.000 _weakrefset.py:86(add)
        1    0.000    0.000    0.000    0.000 std.py:438(status_printer)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:173(is_disabled)
        2    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}
        1    0.000    0.000    0.000    0.000 functools.py:392(__get__)
        1    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        1    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
        1    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
        1    0.000    0.000    0.000    0.000 {built-in method fromtimestamp}
       13    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        2    0.000    0.000    0.000    0.000 utils.py:266(_supports_unicode)
        1    0.000    0.000    0.000    0.000 std.py:760(get_lock)
        2    0.000    0.000    0.000    0.000 utils.py:187(disable_on_exception)
        2    0.000    0.000    0.000    0.000 std.py:113(__exit__)
        3    0.000    0.000    0.000    0.000 std.py:226(__init__)
        1    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
        5    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        1    0.000    0.000    0.000    0.000 utils.py:273(_is_ascii)
        1    0.000    0.000    0.000    0.000 utils.py:156(__init__)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:169(is_enabled)
        2    0.000    0.000    0.000    0.000 os.py:754(encode)
        1    0.000    0.000    0.000    0.000 _monitor.py:94(report)
        3    0.000    0.000    0.000    0.000 std.py:106(release)
        1    0.000    0.000    0.000    0.000 std.py:400(format_interval)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:127(train_progress_bar)
       43    0.000    0.000    0.000    0.000 {built-in method unicodedata.east_asian_width}
        4    0.000    0.000    0.000    0.000 utils.py:222(__eq__)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 std.py:153(__init__)
        3    0.000    0.000    0.000    0.000 utils.py:152(wrapper_setattr)
        1    0.000    0.000    0.000    0.000 std.py:186(__format__)
        3    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        1    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:165(process_position)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        3    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
        2    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
        1    0.000    0.000    0.000    0.000 std.py:1157(__hash__)
        3    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
        1    0.000    0.000    0.000    0.000 std.py:1147(__del__)
        3    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
        1    0.000    0.000    0.000    0.000 {built-in method utcnow}
        1    0.000    0.000    0.000    0.000 utils.py:252(_is_utf)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        2    0.000    0.000    0.000    0.000 {built-in method _weakref.proxy}
        1    0.000    0.000    0.000    0.000 utils.py:282(_screen_shape_wrapper)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        1    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        1    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
        3    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
        1    0.000    0.000    0.000    0.000 utils.py:108(__init__)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        1    0.000    0.000    0.000    0.000 std.py:231(__call__)
        1    0.000    0.000    0.000    0.000 utils.py:112(__format__)
        3    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.max}
        1    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
        3    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
        1    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f88d25377c0}
        1    0.000    0.000    0.000    0.000 std.py:167(colour)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        1    0.000    0.000    0.000    0.000 threading.py:536(is_set)
        1    0.000    0.000    0.000    0.000 {built-in method time.time}
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 progress_bar.py:64(train_description)
        1    0.000    0.000    0.000    0.000 std.py:1265(close)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:161(refresh_rate)
        1    0.000    0.000    0.000    0.000 std.py:163(colour)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)



Profile stats for: [Callback]ModelSummary.on_train_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:205(on_train_start)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_start
         11 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 model_checkpoint.py:280(on_train_start)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_train_start
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 hooks.py:44(on_train_start)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Strategy]SingleDeviceStrategy.on_train_start
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 strategy.py:545(on_train_start)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_train_epoch_start
         1000 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 callback.py:92(on_train_epoch_start)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_train_epoch_start
         72288 function calls in 0.043 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000    0.043    0.000 tqdm_progress.py:263(on_train_epoch_start)
      200    0.000    0.000    0.041    0.000 std.py:1325(refresh)
      200    0.000    0.000    0.040    0.000 std.py:1464(display)
      100    0.000    0.000    0.027    0.000 std.py:1360(reset)
      200    0.001    0.000    0.022    0.000 std.py:1150(__str__)
      200    0.001    0.000    0.018    0.000 std.py:457(print_status)
      200    0.011    0.000    0.017    0.000 std.py:464(format_meter)
      100    0.000    0.000    0.015    0.000 std.py:1382(set_description)
      200    0.000    0.000    0.011    0.000 std.py:451(fp_write)
      400    0.000    0.000    0.011    0.000 utils.py:194(inner)
      200    0.000    0.000    0.009    0.000 redirect.py:644(write)
      200    0.000    0.000    0.009    0.000 wandb_run.py:2304(<lambda>)
      200    0.000    0.000    0.009    0.000 wandb_run.py:390(wrapper_fn)
      200    0.000    0.000    0.008    0.000 wandb_run.py:1429(_console_raw_callback)
      200    0.001    0.000    0.008    0.000 interface.py:749(publish_output_raw)
      200    0.000    0.000    0.006    0.000 utils.py:378(disp_len)
      200    0.000    0.000    0.006    0.000 utils.py:374(_text_width)
      200    0.001    0.000    0.006    0.000 {built-in method builtins.sum}
      200    0.000    0.000    0.005    0.000 interface_shared.py:76(_publish_output_raw)
      200    0.000    0.000    0.005    0.000 interface_sock.py:45(_publish)
      200    0.000    0.000    0.004    0.000 sock_client.py:219(send_record_publish)
      200    0.001    0.000    0.004    0.000 std.py:1446(format_dict)
    17444    0.003    0.000    0.004    0.000 utils.py:375(<genexpr>)
      200    0.000    0.000    0.004    0.000 sock_client.py:153(send_server_request)
      200    0.001    0.000    0.004    0.000 sock_client.py:145(_send_message)
      200    0.001    0.000    0.003    0.000 utils.py:333(_screen_shape_linux)
      200    0.000    0.000    0.003    0.000 sock_client.py:121(_sendall_with_error_handle)
      400    0.002    0.000    0.003    0.000 utils.py:273(_is_ascii)
      200    0.003    0.000    0.003    0.000 {method 'send' of '_socket.socket' objects}
      200    0.002    0.000    0.002    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      200    0.001    0.000    0.002    0.000 {built-in method fcntl.ioctl}
    17244    0.001    0.000    0.001    0.000 {built-in method unicodedata.east_asian_width}
      400    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}
      200    0.000    0.000    0.001    0.000 well_known_types.py:172(GetCurrentTime)
      200    0.000    0.000    0.001    0.000 well_known_types.py:242(FromDatetime)
    17400    0.001    0.000    0.001    0.000 {built-in method builtins.ord}
      200    0.000    0.000    0.001    0.000 utils.py:347(<listcomp>)
      200    0.000    0.000    0.001    0.000 os.py:674(__getitem__)
      200    0.000    0.000    0.001    0.000 std.py:186(__format__)
      200    0.000    0.000    0.000    0.000 std.py:400(format_interval)
      200    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
      200    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
      200    0.000    0.000    0.000    0.000 std.py:102(acquire)
      200    0.000    0.000    0.000    0.000 {built-in method fromtimestamp}
      200    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
      100    0.000    0.000    0.000    0.000 tqdm_progress.py:440(convert_inf)
      200    0.000    0.000    0.000    0.000 std.py:106(release)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      200    0.000    0.000    0.000    0.000 std.py:153(__init__)
      100    0.000    0.000    0.000    0.000 progress_bar.py:80(total_train_batches)
      200    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      200    0.000    0.000    0.000    0.000 os.py:754(encode)
      400    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
      200    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
      200    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
      200    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      200    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
      400    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      200    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
      300    0.000    0.000    0.000    0.000 std.py:226(__init__)
      600    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      200    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      200    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
      100    0.000    0.000    0.000    0.000 {built-in method math.isinf}
      200    0.000    0.000    0.000    0.000 {built-in method utcnow}
      200    0.000    0.000    0.000    0.000 std.py:231(__call__)
      300    0.000    0.000    0.000    0.000 tqdm_progress.py:121(train_progress_bar)
      600    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
      100    0.000    0.000    0.000    0.000 progress_bar.py:54(trainer)
      600    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      200    0.000    0.000    0.000    0.000 utils.py:112(__format__)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.max}
      400    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      200    0.000    0.000    0.000    0.000 utils.py:108(__init__)
      600    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      200    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      300    0.000    0.000    0.000    0.000 {built-in method time.time}
      200    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      200    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1467(current_epoch)
      200    0.000    0.000    0.000    0.000 std.py:167(colour)
      200    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      200    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
      200    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      100    0.000    0.000    0.000    0.000 trainer.py:1523(num_training_batches)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      200    0.000    0.000    0.000    0.000 std.py:163(colour)
      100    0.000    0.000    0.000    0.000 {built-in method math.isnan}
      200    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      200    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_train_epoch_start
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:92(on_train_epoch_start)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_start
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:92(on_train_epoch_start)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_train_epoch_start
         14300 function calls in 0.014 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.014    0.000 FlavourClassificationTransformerEncoder.py:133(on_train_epoch_start)
      200    0.001    0.000    0.013    0.000 __init__.py:1436(info)
      200    0.000    0.000    0.013    0.000 __init__.py:1565(_log)
      200    0.000    0.000    0.009    0.000 __init__.py:1591(handle)
      200    0.000    0.000    0.008    0.000 __init__.py:1645(callHandlers)
      200    0.000    0.000    0.008    0.000 __init__.py:939(handle)
      200    0.000    0.000    0.008    0.000 __init__.py:1178(emit)
      200    0.000    0.000    0.007    0.000 __init__.py:1071(emit)
      200    0.000    0.000    0.005    0.000 __init__.py:1060(flush)
      200    0.004    0.000    0.004    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      200    0.000    0.000    0.003    0.000 __init__.py:1550(makeRecord)
      200    0.001    0.000    0.003    0.000 __init__.py:282(__init__)
      200    0.000    0.000    0.002    0.000 __init__.py:916(format)
      200    0.000    0.000    0.002    0.000 __init__.py:650(format)
      200    0.000    0.000    0.001    0.000 __init__.py:582(formatTime)
      200    0.000    0.000    0.001    0.000 __init__.py:1514(findCaller)
      200    0.000    0.000    0.000    0.000 {built-in method time.strftime}
      200    0.000    0.000    0.000    0.000 posixpath.py:117(splitext)
      200    0.000    0.000    0.000    0.000 posixpath.py:140(basename)
      200    0.000    0.000    0.000    0.000 __init__.py:634(formatMessage)
      200    0.000    0.000    0.000    0.000 module.py:262(current_epoch)
      200    0.000    0.000    0.000    0.000 {built-in method time.localtime}
      200    0.000    0.000    0.000    0.000 genericpath.py:121(_splitext)
      200    0.000    0.000    0.000    0.000 __init__.py:432(format)
      400    0.000    0.000    0.000    0.000 __init__.py:896(acquire)
      200    0.000    0.000    0.000    0.000 __init__.py:628(usesTime)
      200    0.000    0.000    0.000    0.000 __init__.py:429(_format)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      400    0.000    0.000    0.000    0.000 __init__.py:903(release)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      200    0.000    0.000    0.000    0.000 __init__.py:421(usesTime)
      200    0.000    0.000    0.000    0.000 __init__.py:160(<lambda>)
      600    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      200    0.000    0.000    0.000    0.000 threading.py:1358(current_thread)
      600    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      400    0.000    0.000    0.000    0.000 __init__.py:791(filter)
      200    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
      200    0.000    0.000    0.000    0.000 trainer.py:1467(current_epoch)
      200    0.000    0.000    0.000    0.000 __init__.py:119(getLevelName)
      200    0.000    0.000    0.000    0.000 posixpath.py:52(normcase)
      500    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      400    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      200    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)
      200    0.000    0.000    0.000    0.000 __init__.py:358(getMessage)
      200    0.000    0.000    0.000    0.000 module.py:215(trainer)
      200    0.000    0.000    0.000    0.000 threading.py:1093(name)
      200    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
      600    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
      400    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      200    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}
      200    0.000    0.000    0.000    0.000 {built-in method sys._getframe}
      400    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
      200    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      400    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      200    0.000    0.000    0.000    0.000 process.py:189(name)
      200    0.000    0.000    0.000    0.000 {built-in method time.time}
      200    0.000    0.000    0.000    0.000 process.py:37(current_process)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: run_training_epoch
         460920 function calls (459373 primitive calls) in 54.514 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000   54.514    0.545 training_epoch_loop.py:135(run)
    11584    0.012    0.000   36.301    0.003 {built-in method builtins.next}
      100    0.002    0.000   36.170    0.362 training_epoch_loop.py:192(advance)
      100    0.000    0.000   36.161    0.362 fetchers.py:119(__next__)
      100    0.000    0.000   36.161    0.362 fetchers.py:55(__next__)
      100    0.002    0.000   36.161    0.362 training_epoch_loop.py:186(_on_before_fetch)
      100    0.002    0.000   36.159    0.362 advanced.py:65(start)
      100   36.157    0.362   36.157    0.362 {method 'enable' of '_lsprof.Profiler' objects}
      100    0.002    0.000   18.328    0.183 training_epoch_loop.py:176(on_run_start)
   594/99    0.004    0.000   18.326    0.185 {built-in method builtins.iter}
       99    0.000    0.000   18.326    0.185 fetchers.py:102(__iter__)
       99    0.002    0.000   18.326    0.185 fetchers.py:49(__iter__)
       99    0.033    0.000   18.313    0.185 combined_loader.py:347(__iter__)
       99    0.002    0.000   14.587    0.147 combined_loader.py:90(__iter__)
       99    0.002    0.000   14.585    0.147 combined_loader.py:41(__iter__)
       99    0.000    0.000   14.583    0.147 combined_loader.py:43(<listcomp>)
       99    0.000    0.000   14.582    0.147 dataloader.py:427(__iter__)
       99    0.001    0.000   14.582    0.147 dataloader.py:383(_get_iterator)
       99    0.071    0.001   14.580    0.147 dataloader.py:989(__init__)
      792    0.034    0.000   13.756    0.017 process.py:110(start)
      792    0.008    0.000   13.653    0.017 context.py:222(_Popen)
      792    0.014    0.000   13.645    0.017 context.py:274(_Popen)
      792    0.011    0.000   13.631    0.017 popen_fork.py:15(__init__)
      792    0.049    0.000   13.606    0.017 popen_fork.py:62(_launch)
      792   13.487    0.017   13.503    0.017 {built-in method posix.fork}
       99    0.000    0.000    3.680    0.037 dataloader.py:1476(__del__)
       99    0.008    0.000    3.680    0.037 dataloader.py:1399(_shutdown_workers)
      792    0.003    0.000    3.421    0.004 process.py:142(join)
      792    0.003    0.000    3.418    0.004 popen_fork.py:36(wait)
      792    0.003    0.000    3.397    0.004 connection.py:921(wait)
      792    0.003    0.000    3.386    0.004 selectors.py:403(select)
      792    3.375    0.004    3.381    0.004 {method 'poll' of 'select.poll' objects}
      891    0.049    0.000    0.542    0.001 context.py:100(Queue)
      891    0.040    0.000    0.481    0.001 queues.py:37(__init__)
     3168    0.170    0.000    0.344    0.000 synchronize.py:50(__init__)
     1881    0.015    0.000    0.305    0.000 context.py:65(Lock)
     1881    0.012    0.000    0.288    0.000 synchronize.py:161(__init__)
      990    0.005    0.000    0.225    0.000 threading.py:880(start)
      990    0.005    0.000    0.223    0.000 queues.py:86(put)
      891    0.013    0.000    0.207    0.000 queues.py:161(_start_thread)
     6039    0.186    0.000    0.186    0.000 {method 'acquire' of '_thread.lock' objects}
      990    0.003    0.000    0.177    0.000 threading.py:563(wait)
      990    0.005    0.000    0.172    0.000 threading.py:280(wait)
      792    0.001    0.000    0.163    0.000 dataloader.py:1373(_mark_worker_as_unavailable)
     3168    0.015    0.000    0.125    0.000 synchronize.py:114(_make_name)
     3168    0.028    0.000    0.100    0.000 tempfile.py:149(__next__)
      792    0.038    0.000    0.065    0.000 process.py:61(_cleanup)
     3168    0.011    0.000    0.063    0.000 tempfile.py:152(<listcomp>)
       99    0.003    0.000    0.062    0.001 dataloader.py:1085(_reset)
      891    0.006    0.000    0.059    0.000 context.py:85(BoundedSemaphore)
     1584    0.001    0.000    0.054    0.000 dataloader.py:1346(_try_put_index)
    25344    0.029    0.000    0.052    0.000 random.py:343(choice)
      891    0.001    0.000    0.052    0.000 synchronize.py:144(__init__)
     4059    0.008    0.000    0.048    0.000 util.py:171(register_after_fork)
      891    0.016    0.000    0.045    0.000 connection.py:520(Pipe)
    17028    0.011    0.000    0.044    0.000 popen_fork.py:24(poll)
      990    0.043    0.000    0.043    0.000 {built-in method _thread.start_new_thread}
     1782    0.039    0.000    0.042    0.000 util.py:186(__init__)
     4059    0.021    0.000    0.038    0.000 weakref.py:165(__setitem__)
      891    0.024    0.000    0.032    0.000 queues.py:71(_reset)
    16236    0.031    0.000    0.031    0.000 {built-in method posix.waitpid}
      792    0.018    0.000    0.029    0.000 process.py:80(__init__)
     1584    0.000    0.000    0.028    0.000 dataloader.py:619(_next_index)
      297    0.002    0.000    0.027    0.000 sampler.py:275(__iter__)
      990    0.010    0.000    0.026    0.000 threading.py:802(__init__)
     2475    0.026    0.000    0.026    0.000 {built-in method posix.pipe}
     2475    0.010    0.000    0.025    0.000 sampler.py:153(__iter__)
       99    0.000    0.000    0.024    0.000 context.py:90(Event)
       99    0.000    0.000    0.023    0.000 synchronize.py:323(__init__)
       99    0.000    0.000    0.023    0.000 threading.py:1028(join)
       99    0.001    0.000    0.022    0.000 threading.py:1066(_wait_for_tstate_lock)
    25344    0.015    0.000    0.021    0.000 random.py:237(_randbelow_with_getrandbits)
     1683    0.005    0.000    0.019    0.000 util.py:205(__call__)
     2277    0.018    0.000    0.018    0.000 threading.py:228(__init__)
      396    0.001    0.000    0.018    0.000 context.py:80(Semaphore)
      396    0.000    0.000    0.017    0.000 synchronize.py:125(__init__)
      792    0.011    0.000    0.015    0.000 __init__.py:227(_releaseLock)
       99    0.000    0.000    0.014    0.000 context.py:75(Condition)
     5940    0.011    0.000    0.014    0.000 <frozen importlib._bootstrap>:398(parent)
     3168    0.014    0.000    0.014    0.000 {built-in method posix.close}
       99    0.000    0.000    0.014    0.000 synchronize.py:212(__init__)
      792    0.008    0.000    0.013    0.000 util.py:433(_flush_std_streams)
      100    0.000    0.000    0.013    0.000 training_epoch_loop.py:116(done)
     1089    0.003    0.000    0.013    0.000 threading.py:528(__init__)
      891    0.001    0.000    0.013    0.000 queues.py:140(close)
      100    0.002    0.000    0.012    0.000 training_epoch_loop.py:106(_is_training_done)
     1782    0.011    0.000    0.011    0.000 connection.py:121(__init__)
     4059    0.006    0.000    0.010    0.000 weakref.py:348(__new__)
       99    0.001    0.000    0.010    0.000 fetchers.py:139(reset)
27324/26433    0.005    0.000    0.010    0.000 {built-in method builtins.len}
  637/575    0.001    0.000    0.010    0.000 signal_handling.py:64(handler)
       99    0.003    0.000    0.009    0.000 fetchers.py:71(reset)
      637    0.009    0.000    0.009    0.000 {built-in method torch._C._error_if_any_worker_fails}
      100    0.004    0.000    0.009    0.000 training_epoch_loop.py:99(global_step)
     4077    0.005    0.000    0.008    0.000 {built-in method builtins.isinstance}
     3168    0.005    0.000    0.008    0.000 tempfile.py:138(rng)
      891    0.002    0.000    0.007    0.000 queues.py:204(_finalize_close)
       99    0.003    0.000    0.007    0.000 dataloader.py:564(__init__)
     4059    0.006    0.000    0.006    0.000 weakref.py:353(__init__)
     1782    0.004    0.000    0.006    0.000 _weakrefset.py:86(add)
   198/99    0.000    0.000    0.006    0.000 data.py:47(sized_len)
       99    0.003    0.000    0.006    0.000 queue.py:34(__init__)
     3960    0.005    0.000    0.006    0.000 {method 'join' of 'str' objects}
    10692    0.006    0.000    0.006    0.000 {built-in method posix.getpid}
     1980    0.002    0.000    0.006    0.000 threading.py:351(notify)
      100    0.002    0.000    0.006    0.000 loop.py:32(restarting)
     1584    0.006    0.000    0.006    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      198    0.006    0.000    0.006    0.000 {built-in method torch.empty}
     2574    0.005    0.000    0.005    0.000 {method 'add' of 'set' objects}
      792    0.001    0.000    0.005    0.000 util.py:461(close_fds)
       99    0.001    0.000    0.005    0.000 dataloader.py:609(_reset)
       99    0.000    0.000    0.005    0.000 combined_loader.py:355(__len__)
     2970    0.002    0.000    0.005    0.000 threading.py:256(__enter__)
       99    0.001    0.000    0.005    0.000 combined_loader.py:96(__len__)
     4851    0.005    0.000    0.005    0.000 {built-in method __new__ of type object at 0x7f88d25377c0}
       99    0.001    0.000    0.005    0.000 synchronize.py:334(set)
      792    0.004    0.000    0.004    0.000 {method 'release' of '_thread.RLock' objects}
      198    0.004    0.000    0.004    0.000 {built-in method torch.randperm}
     1683    0.003    0.000    0.004    0.000 queues.py:153(cancel_join_thread)
      792    0.001    0.000    0.004    0.000 selectors.py:352(register)
    43683    0.004    0.000    0.004    0.000 {method 'getrandbits' of '_random.Random' objects}
      198    0.004    0.000    0.004    0.000 {method 'random_' of 'torch._C.TensorBase' objects}
      792    0.001    0.000    0.004    0.000 synchronize.py:327(is_set)
      891    0.001    0.000    0.004    0.000 dataloader.py:1080(<genexpr>)
     5940    0.003    0.000    0.003    0.000 {method 'rpartition' of 'str' objects}
       99    0.000    0.000    0.003    0.000 combined_loader.py:404(_get_iterables_lengths)
      496    0.000    0.000    0.003    0.000 abc.py:117(__instancecheck__)
     2178    0.003    0.000    0.003    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
     2970    0.003    0.000    0.003    0.000 {method '__enter__' of '_thread.lock' objects}
       99    0.000    0.000    0.003    0.000 combined_loader.py:405(<listcomp>)
     3168    0.003    0.000    0.003    0.000 synchronize.py:90(_make_methods)
       99    0.001    0.000    0.003    0.000 __init__.py:876(current_device)
     2871    0.003    0.000    0.003    0.000 {method 'append' of 'collections.deque' objects}
      990    0.003    0.000    0.003    0.000 threading.py:1229(_make_invoke_excepthook)
      792    0.001    0.000    0.003    0.000 selectors.py:235(register)
       99    0.001    0.000    0.003    0.000 dataloader.py:458(__len__)
      792    0.003    0.000    0.003    0.000 {method 'copy' of 'dict' objects}
     3069    0.003    0.000    0.003    0.000 {built-in method _thread.allocate_lock}
       99    0.003    0.000    0.003    0.000 {built-in method torch._C._set_worker_pids}
     2970    0.001    0.000    0.003    0.000 threading.py:259(__exit__)
      792    0.003    0.000    0.003    0.000 process.py:234(ident)
     2475    0.002    0.000    0.003    0.000 weakref.py:106(remove)
      100    0.002    0.000    0.003    0.000 trainer.py:1178(lightning_module)
      496    0.003    0.000    0.003    0.000 {built-in method _abc._abc_instancecheck}
      100    0.001    0.000    0.002    0.000 training_epoch_loop.py:147(reset)
       99    0.001    0.000    0.002    0.000 synchronize.py:296(notify_all)
     2079    0.002    0.000    0.002    0.000 {method 'release' of '_thread.lock' objects}
     1782    0.002    0.000    0.002    0.000 context.py:233(get_context)
      891    0.001    0.000    0.002    0.000 synchronize.py:229(__enter__)
       99    0.002    0.000    0.002    0.000 {built-in method torch._C._remove_worker_pids}
      792    0.001    0.000    0.002    0.000 selectors.py:348(__init__)
      100    0.001    0.000    0.002    0.000 training_epoch_loop.py:331(_num_ready_batches_reached)
    25344    0.002    0.000    0.002    0.000 {method 'bit_length' of 'int' objects}
     2970    0.001    0.000    0.002    0.000 threading.py:271(_is_owned)
     1188    0.001    0.000    0.002    0.000 threading.py:1358(current_thread)
     2970    0.002    0.000    0.002    0.000 {method '__exit__' of '_thread.lock' objects}
      891    0.000    0.000    0.001    0.000 synchronize.py:94(__enter__)
      792    0.001    0.000    0.001    0.000 process.py:153(is_alive)
     3168    0.001    0.000    0.001    0.000 {built-in method builtins.getattr}
     2376    0.001    0.000    0.001    0.000 {method 'append' of 'list' objects}
     8415    0.001    0.000    0.001    0.000 util.py:48(debug)
      792    0.000    0.000    0.001    0.000 selectors.py:203(__exit__)
       99    0.001    0.000    0.001    0.000 synchronize.py:270(notify)
      990    0.001    0.000    0.001    0.000 threading.py:1162(daemon)
       99    0.001    0.000    0.001    0.000 {built-in method torch._C._cuda_getDevice}
      198    0.001    0.000    0.001    0.000 {method 'item' of 'torch._C.TensorBase' objects}
     2475    0.001    0.000    0.001    0.000 {built-in method _weakref._remove_dead_weakref}
      990    0.001    0.000    0.001    0.000 _weakrefset.py:39(_remove)
       99    0.001    0.000    0.001    0.000 {built-in method torch._C._get_privateuse1_backend_name}
       99    0.001    0.000    0.001    0.000 dataloader.py:487(check_worker_number_rationality)
     1584    0.001    0.000    0.001    0.000 process.py:94(<genexpr>)
     4257    0.001    0.000    0.001    0.000 {built-in method builtins.id}
      100    0.001    0.000    0.001    0.000 progress.py:274(optimizer_steps)
      990    0.001    0.000    0.001    0.000 threading.py:268(_acquire_restore)
      891    0.001    0.000    0.001    0.000 {method '__enter__' of '_multiprocessing.SemLock' objects}
      891    0.001    0.000    0.001    0.000 synchronize.py:232(__exit__)
      100    0.000    0.000    0.001    0.000 progress.py:204(reset_on_run)
       99    0.000    0.000    0.001    0.000 sampler.py:298(__len__)
       99    0.000    0.000    0.001    0.000 __init__.py:115(is_available)
      400    0.000    0.000    0.001    0.000 progress.py:172(reset_on_run)
       99    0.000    0.000    0.001    0.000 __init__.py:284(_lazy_init)
      792    0.001    0.000    0.001    0.000 selectors.py:210(__init__)
       99    0.000    0.000    0.001    0.000 threading.py:542(set)
      792    0.000    0.000    0.001    0.000 selectors.py:216(_fileobj_lookup)
      792    0.001    0.000    0.001    0.000 selectors.py:269(close)
       99    0.001    0.000    0.001    0.000 threading.py:757(_newname)
      396    0.001    0.000    0.001    0.000 dataset.py:422(__len__)
     2574    0.001    0.000    0.001    0.000 {method 'discard' of 'set' objects}
       99    0.001    0.000    0.001    0.000 signal_handling.py:48(_set_SIGCHLD_handler)
     1089    0.001    0.000    0.001    0.000 threading.py:1147(daemon)
      100    0.001    0.000    0.001    0.000 trainer.py:1523(num_training_batches)
       99    0.001    0.000    0.001    0.000 queue.py:206(_init)
       99    0.000    0.000    0.001    0.000 combined_loader.py:68(__init__)
      792    0.000    0.000    0.001    0.000 __init__.py:218(_acquireLock)
     3168    0.001    0.000    0.001    0.000 context.py:197(get_start_method)
      297    0.000    0.000    0.001    0.000 sampler.py:146(num_samples)
       99    0.000    0.000    0.001    0.000 __init__.py:111(_nvml_based_avail)
     3267    0.001    0.000    0.001    0.000 context.py:187(get_context)
      792    0.000    0.000    0.001    0.000 <string>:1(<lambda>)
      100    0.001    0.000    0.001    0.000 module.py:295(automatic_optimization)
      198    0.001    0.000    0.001    0.000 {method 'tolist' of 'torch._C.TensorBase' objects}
       99    0.001    0.000    0.001    0.000 combined_loader.py:29(__init__)
       99    0.000    0.000    0.001    0.000 os.py:771(getenv)
      792    0.000    0.000    0.001    0.000 selectors.py:21(_fileobj_to_fd)
     3168    0.000    0.000    0.000    0.000 process.py:99(_check_closed)
      792    0.000    0.000    0.000    0.000 selectors.py:276(_key_from_fd)
       99    0.000    0.000    0.000    0.000 {built-in method posix.sched_getaffinity}
     3168    0.000    0.000    0.000    0.000 process.py:37(current_process)
       99    0.000    0.000    0.000    0.000 sampler.py:171(__len__)
      990    0.000    0.000    0.000    0.000 threading.py:265(_release_save)
       99    0.000    0.000    0.000    0.000 dataloader.py:94(_get_distributed_settings)
      891    0.000    0.000    0.000    0.000 synchronize.py:97(__exit__)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
       99    0.000    0.000    0.000    0.000 _collections_abc.py:760(get)
      200    0.000    0.000    0.000    0.000 progress.py:114(reset)
      100    0.000    0.000    0.000    0.000 progress.py:282(reset_on_run)
       99    0.000    0.000    0.000    0.000 __init__.py:237(is_initialized)
       99    0.000    0.000    0.000    0.000 {method 'manual_seed' of 'torch._C.Generator' objects}
       99    0.000    0.000    0.000    0.000 os.py:674(__getitem__)
       99    0.000    0.000    0.000    0.000 {built-in method builtins.max}
       99    0.000    0.000    0.000    0.000 util.py:229(cancel)
      299    0.000    0.000    0.000    0.000 loop.py:27(restarting)
       99    0.000    0.000    0.000    0.000 threading.py:992(_stop)
      300    0.000    0.000    0.000    0.000 progress.py:87(reset)
      792    0.000    0.000    0.000    0.000 {built-in method math.ceil}
      100    0.000    0.000    0.000    0.000 progress.py:249(reset_on_run)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
     2079    0.000    0.000    0.000    0.000 threading.py:536(is_set)
      792    0.000    0.000    0.000    0.000 connection.py:937(<listcomp>)
      792    0.000    0.000    0.000    0.000 {built-in method select.poll}
       99    0.000    0.000    0.000    0.000 threading.py:381(notify_all)
      792    0.000    0.000    0.000    0.000 process.py:205(daemon)
     1683    0.000    0.000    0.000    0.000 util.py:44(sub_debug)
      792    0.000    0.000    0.000    0.000 selectors.py:64(__init__)
     1188    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
      792    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      198    0.000    0.000    0.000    0.000 dataloader.py:446(_index_sampler)
      792    0.000    0.000    0.000    0.000 {method 'register' of 'select.poll' objects}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.vars}
      792    0.000    0.000    0.000    0.000 process.py:189(name)
      792    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
       99    0.000    0.000    0.000    0.000 dataloader.py:1101(<listcomp>)
      891    0.000    0.000    0.000    0.000 {method 'clear' of 'collections.deque' objects}
      495    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      500    0.000    0.000    0.000    0.000 progress.py:56(reset)
      198    0.000    0.000    0.000    0.000 combined_loader.py:100(<genexpr>)
       99    0.000    0.000    0.000    0.000 distributed_c10d.py:996(is_initialized)
      792    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      990    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}
      891    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
      792    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}
      792    0.000    0.000    0.000    0.000 {built-in method posix.waitstatus_to_exitcode}
      990    0.000    0.000    0.000    0.000 {method 'remove' of 'collections.deque' objects}
      891    0.000    0.000    0.000    0.000 {method '__exit__' of '_multiprocessing.SemLock' objects}
      198    0.000    0.000    0.000    0.000 __init__.py:106(_is_compiled)
      198    0.000    0.000    0.000    0.000 fetchers.py:38(combined_loader)
       99    0.000    0.000    0.000    0.000 os.py:754(encode)
      297    0.000    0.000    0.000    0.000 dataloader.py:442(_auto_collation)
       99    0.000    0.000    0.000    0.000 distributed_c10d.py:608(WORLD)
       99    0.000    0.000    0.000    0.000 __init__.py:834(device_count)
      792    0.000    0.000    0.000    0.000 selectors.py:200(__enter__)
      198    0.000    0.000    0.000    0.000 connection.py:134(__del__)
       99    0.000    0.000    0.000    0.000 __init__.py:10(is_available)
       99    0.000    0.000    0.000    0.000 {built-in method torch._C._cuda_isInBadFork}
       99    0.000    0.000    0.000    0.000 os.py:758(decode)
       99    0.000    0.000    0.000    0.000 synchronize.py:235(_make_methods)
      100    0.000    0.000    0.000    0.000 utilities.py:113(_is_max_limit_reached)
       99    0.000    0.000    0.000    0.000 {built-in method builtins.min}
      100    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
       99    0.000    0.000    0.000    0.000 {method 'locked' of '_thread.lock' objects}
       99    0.000    0.000    0.000    0.000 dataloader.py:390(multiprocessing_context)
       99    0.000    0.000    0.000    0.000 {method 'decode' of 'bytes' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1467(current_epoch)
       99    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
       99    0.000    0.000    0.000    0.000 distributed_c10d.py:480(default_pg)
       99    0.000    0.000    0.000    0.000 {method '_is_mine' of '_multiprocessing.SemLock' objects}
       99    0.000    0.000    0.000    0.000 combined_loader.py:308(flattened)



Profile stats for: [_TrainingEpochLoop].train_dataloader_next
         10017 function calls (9217 primitive calls) in 0.110 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
  400/100    0.000    0.000    0.109    0.001 {built-in method builtins.next}
      100    0.001    0.000    0.109    0.001 combined_loader.py:339(__next__)
      100    0.000    0.000    0.105    0.001 combined_loader.py:72(__next__)
      100    0.004    0.000    0.105    0.001 dataloader.py:625(__next__)
      100    0.003    0.000    0.049    0.000 profiler.py:687(__enter__)
      100    0.001    0.000    0.046    0.000 _ops.py:1047(__call__)
      100    0.045    0.000    0.045    0.000 {built-in method torch._ops.profiler._record_function_enter_new}
      100    0.001    0.000    0.029    0.000 dataloader.py:1297(_next_data)
      100    0.001    0.000    0.028    0.000 dataloader.py:1264(_get_data)
      100    0.000    0.000    0.026    0.000 dataloader.py:1118(_try_get_data)
      100    0.001    0.000    0.026    0.000 queue.py:154(get)
      204    0.025    0.000    0.025    0.000 {method 'acquire' of '_thread.lock' objects}
        1    0.000    0.000    0.025    0.025 threading.py:280(wait)
      100    0.004    0.000    0.017    0.000 profiler.py:693(__exit__)
      100    0.001    0.000    0.014    0.000 _ops.py:887(__call__)
      100    0.001    0.000    0.010    0.000 _ops.py:943(_must_dispatch_in_python)
      100    0.001    0.000    0.008    0.000 _pytree.py:1181(tree_any)
      100    0.000    0.000    0.007    0.000 {built-in method builtins.any}
  700/200    0.003    0.000    0.006    0.000 _pytree.py:874(tree_iter)
      100    0.003    0.000    0.005    0.000 profiler.py:676(__init__)
      100    0.003    0.000    0.003    0.000 {built-in method torch._ops.profiler.}
      400    0.001    0.000    0.002    0.000 _pytree.py:656(_is_leaf)
      700    0.001    0.000    0.002    0.000 _pytree.py:649(_get_node_type)
      100    0.002    0.000    0.002    0.000 _ops.py:945(<lambda>)
      100    0.001    0.000    0.002    0.000 _pytree.py:862(tree_unflatten)
      700    0.001    0.000    0.001    0.000 _pytree.py:638(_is_namedtuple_instance)
      100    0.001    0.000    0.001    0.000 typing.py:271(inner)
      500    0.000    0.000    0.001    0.000 {built-in method builtins.isinstance}
      100    0.001    0.000    0.001    0.000 threading.py:1133(is_alive)
      100    0.000    0.000    0.001    0.000 abc.py:117(__instancecheck__)
      100    0.001    0.000    0.001    0.000 {built-in method _abc._abc_instancecheck}
      100    0.001    0.000    0.001    0.000 _pytree.py:785(unflatten)
      100    0.000    0.000    0.001    0.000 training_epoch_loop.py:189(_on_after_fetch)
      100    0.000    0.000    0.001    0.000 dataloader.py:1366(_process_data)
      100    0.000    0.000    0.000    0.000 dataloader.py:1346(_try_put_index)
      100    0.000    0.000    0.000    0.000 _pytree.py:423(_dict_flatten)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 threading.py:1066(_wait_for_tstate_lock)
      100    0.000    0.000    0.000    0.000 threading.py:351(notify)
     1201    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      100    0.000    0.000    0.000    0.000 threading.py:256(__enter__)
      200    0.000    0.000    0.000    0.000 _pytree.py:395(_tuple_flatten)
      100    0.000    0.000    0.000    0.000 queue.py:217(_get)
      101    0.000    0.000    0.000    0.000 queue.py:209(_qsize)
      100    0.000    0.000    0.000    0.000 dataloader.py:619(_next_index)
      100    0.000    0.000    0.000    0.000 threading.py:536(is_set)
      101    0.000    0.000    0.000    0.000 threading.py:271(_is_owned)
      100    0.000    0.000    0.000    0.000 threading.py:259(__exit__)
      100    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}
      100    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
      100    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 _pytree.py:701(is_leaf)
      100    0.000    0.000    0.000    0.000 __init__.py:129(annotate)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)
      101    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}
      100    0.000    0.000    0.000    0.000 {method '__exit__' of 'torch._C.DisableTorchFunctionSubclass' objects}
      100    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 _collections_abc.py:283(__subclasshook__)
        1    0.000    0.000    0.000    0.000 threading.py:268(_acquire_restore)
        1    0.000    0.000    0.000    0.000 threading.py:265(_release_save)
        1    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}
        1    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}
        1    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_train_batch_start
         1000 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.001    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 callback.py:76(on_train_batch_start)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_train_batch_start
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:76(on_train_batch_start)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_train_batch_start
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:76(on_train_batch_start)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_start
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:76(on_train_batch_start)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_train_batch_start
         700 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 hooks.py:68(on_train_batch_start)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Strategy]SingleDeviceStrategy.on_train_batch_start
         700 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 strategy.py:577(on_train_batch_start)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: run_training_batch
         5900 function calls (5800 primitive calls) in 11.554 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000   11.554    0.116 automatic.py:161(run)
      100    0.000    0.000   11.547    0.115 automatic.py:243(_optimizer_step)
      100    0.000    0.000   11.546    0.115 call.py:137(_call_lightning_module_hook)
      100    0.000    0.000   11.545    0.115 contextlib.py:114(__enter__)
      100    0.000    0.000   11.545    0.115 {built-in method builtins.next}
      100    0.000    0.000   11.545    0.115 profiler.py:55(profile)
      100    0.001    0.000   11.545    0.115 advanced.py:65(start)
      100   11.545    0.115   11.545    0.115 {method 'enable' of '_lsprof.Profiler' objects}
      100    0.001    0.000    0.005    0.000 automatic.py:197(_make_closure)
      100    0.001    0.000    0.002    0.000 automatic.py:115(__init__)
      100    0.001    0.000    0.001    0.000 automatic.py:228(_make_backward_fn)
      100    0.001    0.000    0.001    0.000 closure.py:41(__init__)
      100    0.001    0.000    0.001    0.000 automatic.py:209(_make_zero_grad_fn)
      200    0.000    0.000    0.001    0.000 fit_loop.py:427(_should_accumulate)
      100    0.000    0.000    0.000    0.000 module.py:1731(__setattr__)
      200    0.000    0.000    0.000    0.000 training_epoch_loop.py:336(_should_accumulate)
      100    0.000    0.000    0.000    0.000 automatic.py:205(_make_step_fn)
      100    0.000    0.000    0.000    0.000 trainer.py:1183(optimizers)
      100    0.000    0.000    0.000    0.000 strategy.py:101(optimizers)
  300/200    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      200    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 contextlib.py:261(helper)
      100    0.000    0.000    0.000    0.000 parameter.py:8(__instancecheck__)
      200    0.000    0.000    0.000    0.000 training_epoch_loop.py:331(_num_ready_batches_reached)
      100    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)
      700    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 progress.py:142(increment_ready)
      200    0.000    0.000    0.000    0.000 training_epoch_loop.py:327(_accumulated_batches_reached)
      100    0.000    0.000    0.000    0.000 __init__.py:1424(debug)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      100    0.000    0.000    0.000    0.000 trainer.py:1467(current_epoch)
      100    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
      200    0.000    0.000    0.000    0.000 trainer.py:1523(num_training_batches)
      300    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      200    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      300    0.000    0.000    0.000    0.000 strategy.py:468(handles_gradient_accumulation)
      100    0.000    0.000    0.000    0.000 module.py:295(automatic_optimization)
      100    0.000    0.000    0.000    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f88c4c64040}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.callable}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.optimizer_step
         9300 function calls (9200 primitive calls) in 11.542 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000   11.542    0.115 module.py:1277(optimizer_step)
      100    0.001    0.000   11.541    0.115 optimizer.py:84(step)
      100    0.001    0.000   11.540    0.115 strategy.py:219(optimizer_step)
      100    0.001    0.000   11.539    0.115 precision.py:112(optimizer_step)
      100    0.005    0.000   11.538    0.115 optimizer.py:464(wrapper)
      100    0.004    0.000   11.530    0.115 optimizer.py:70(_use_grad)
      100    0.002    0.000   11.525    0.115 adam.py:192(step)
      100    0.000    0.000   11.511    0.115 precision.py:95(_wrap_closure)
      100    0.000    0.000   11.511    0.115 automatic.py:142(__call__)
      100    0.000    0.000   11.511    0.115 _contextlib.py:113(decorate_context)
      100    0.000    0.000   11.511    0.115 automatic.py:126(closure)
      100    0.000    0.000   11.511    0.115 automatic.py:305(_training_step)
      100    0.000    0.000   11.510    0.115 call.py:294(_call_strategy_hook)
      100    0.000    0.000   11.509    0.115 contextlib.py:114(__enter__)
      100    0.000    0.000   11.509    0.115 {built-in method builtins.next}
      100    0.000    0.000   11.509    0.115 profiler.py:55(profile)
      100    0.000    0.000   11.509    0.115 advanced.py:65(start)
      100   11.509    0.115   11.509    0.115 {method 'enable' of '_lsprof.Profiler' objects}
      100    0.003    0.000    0.011    0.000 optimizer.py:410(_cuda_graph_capture_health_check)
      100    0.001    0.000    0.005    0.000 __init__.py:115(is_available)
      100    0.001    0.000    0.004    0.000 __init__.py:111(_nvml_based_avail)
      100    0.001    0.000    0.003    0.000 os.py:771(getenv)
      100    0.000    0.000    0.002    0.000 _collections_abc.py:760(get)
      100    0.000    0.000    0.002    0.000 profiler.py:687(__enter__)
      100    0.000    0.000    0.002    0.000 _ops.py:1047(__call__)
      100    0.002    0.000    0.002    0.000 {built-in method torch._ops.profiler._record_function_enter_new}
      100    0.001    0.000    0.002    0.000 os.py:674(__getitem__)
      100    0.001    0.000    0.001    0.000 _utils.py:856(is_compiling)
      100    0.001    0.000    0.001    0.000 grad_mode.py:184(__init__)
      200    0.001    0.000    0.001    0.000 _contextlib.py:154(__new__)
      100    0.001    0.000    0.001    0.000 __init__.py:34(is_built)
      100    0.000    0.000    0.000    0.000 graphs.py:24(is_current_stream_capturing)
      100    0.000    0.000    0.000    0.000 module.py:1731(__setattr__)
      100    0.000    0.000    0.000    0.000 {built-in method torch._C._cuda_isCurrentStreamCapturing}
      300    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      100    0.000    0.000    0.000    0.000 profiler.py:676(__init__)
      100    0.000    0.000    0.000    0.000 os.py:758(decode)
      200    0.000    0.000    0.000    0.000 grad_mode.py:135(__enter__)
      100    0.000    0.000    0.000    0.000 os.py:754(encode)
  500/400    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      300    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}
      100    0.000    0.000    0.000    0.000 __init__.py:213(is_compiling)
      100    0.000    0.000    0.000    0.000 contextlib.py:261(helper)
      200    0.000    0.000    0.000    0.000 __init__.py:106(_is_compiled)
      400    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}
      100    0.000    0.000    0.000    0.000 parameter.py:8(__instancecheck__)
      100    0.000    0.000    0.000    0.000 {method 'decode' of 'bytes' objects}
      100    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)
      100    0.000    0.000    0.000    0.000 _contextlib.py:146(clone)
      100    0.000    0.000    0.000    0.000 __init__.py:834(device_count)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 typing.py:271(inner)
      100    0.000    0.000    0.000    0.000 __init__.py:1424(debug)
      300    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}
      300    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      200    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f88d25377c0}
      200    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      100    0.000    0.000    0.000    0.000 decorators.py:140(graph_break)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
      100    0.000    0.000    0.000    0.000 typing.py:1375(cast)
      100    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
      200    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      300    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f88c4c64040}
      100    0.000    0.000    0.000    0.000 strategy.py:93(precision_plugin)
      100    0.000    0.000    0.000    0.000 optimizer.py:34(do_nothing_closure)
      100    0.000    0.000    0.000    0.000 __init__.py:129(annotate)
      100    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)



Profile stats for: [Strategy]SingleDeviceStrategy.training_step
         3446881 function calls (3435721 primitive calls) in 5.392 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.002    0.000    5.413    0.054 strategy.py:379(training_step)
      100    0.005    0.000    5.407    0.054 FlavourClassificationTransformerEncoder.py:81(training_step)
 4800/100    0.008    0.000    5.311    0.053 module.py:1549(_wrapped_call_impl)
 4800/100    0.030    0.000    5.310    0.053 module.py:1555(_call_impl)
      100    0.006    0.000    5.309    0.053 FlavourClassificationTransformerEncoder.py:55(forward)
      300    0.032    0.000    5.231    0.017 EncoderBlock.py:57(forward)
      300    0.516    0.002    4.087    0.014 XFormersAttention.py:31(forward)
     6600    0.014    0.000    1.855    0.000 __init__.py:189(memory_efficient_attention)
     6600    0.035    0.000    1.836    0.000 __init__.py:457(_memory_efficient_attention)
     6600    0.022    0.000    1.741    0.000 function.py:558(apply)
     6600    0.099    0.000    1.667    0.000 {built-in method apply}
     6600    0.049    0.000    1.567    0.000 __init__.py:76(forward)
    13200    0.025    0.000    0.943    0.000 dispatch.py:55(_run_priority_list)
     6600    0.018    0.000    0.930    0.000 __init__.py:489(_memory_efficient_attention_forward_requires_grad)
      600    0.378    0.001    0.702    0.001 LayerNormalisation.py:17(forward)
     8200    0.015    0.000    0.699    0.000 {built-in method builtins.print}
    16400    0.018    0.000    0.684    0.000 redirect.py:644(write)
    16400    0.010    0.000    0.663    0.000 wandb_run.py:2304(<lambda>)
    16400    0.016    0.000    0.653    0.000 wandb_run.py:390(wrapper_fn)
    16400    0.028    0.000    0.634    0.000 wandb_run.py:1429(_console_raw_callback)
    16400    0.073    0.000    0.594    0.000 interface.py:749(publish_output_raw)
    26400    0.221    0.000    0.520    0.000 common.py:348(not_supported_reasons)
     6600    0.013    0.000    0.513    0.000 dispatch.py:146(_dispatch_bw)
     6600    0.006    0.000    0.464    0.000 dispatch.py:126(_dispatch_fw)
     6300    0.010    0.000    0.413    0.000 __init__.py:1436(info)
     6300    0.010    0.000    0.400    0.000 __init__.py:1565(_log)
    16400    0.025    0.000    0.387    0.000 interface_shared.py:76(_publish_output_raw)
    46200    0.043    0.000    0.387    0.000 __init__.py:438(get_device_capability)
    16400    0.020    0.000    0.356    0.000 interface_sock.py:45(_publish)
    46200    0.042    0.000    0.344    0.000 __init__.py:455(get_device_properties)
    16400    0.020    0.000    0.323    0.000 sock_client.py:219(send_record_publish)
     6600    0.018    0.000    0.305    0.000 cutlass.py:211(apply)
     6600    0.014    0.000    0.301    0.000 attn_bias.py:707(from_seqlens)
    16400    0.007    0.000    0.294    0.000 sock_client.py:153(send_server_request)
     6600    0.038    0.000    0.288    0.000 cutlass.py:275(apply_bmhk)
    16400    0.040    0.000    0.287    0.000 sock_client.py:145(_send_message)
     6300    0.005    0.000    0.277    0.000 __init__.py:1591(handle)
     6300    0.010    0.000    0.271    0.000 __init__.py:1645(callHandlers)
     6300    0.007    0.000    0.261    0.000 __init__.py:939(handle)
    13200    0.012    0.000    0.260    0.000 common.py:450(not_supported_reasons)
     6600    0.028    0.000    0.258    0.000 flash.py:752(not_supported_reasons)
     6300    0.004    0.000    0.246    0.000 __init__.py:1178(emit)
      300    0.088    0.000    0.243    0.001 FFN.py:16(forward)
     6300    0.007    0.000    0.242    0.000 __init__.py:1071(emit)
     6600    0.013    0.000    0.228    0.000 attn_bias.py:350(from_seqlens)
     6600    0.015    0.000    0.227    0.000 cutlass.py:377(not_supported_reasons)
    19800    0.222    0.000    0.222    0.000 {method 'reshape' of 'torch._C.TensorBase' objects}
     6700    0.009    0.000    0.220    0.000 _ops.py:1047(__call__)
     6600    0.013    0.000    0.216    0.000 cutlass.py:326(not_supported_reasons)
     6600    0.017    0.000    0.215    0.000 flash.py:629(not_supported_reasons)
     6600    0.014    0.000    0.210    0.000 attn_bias.py:329(_get_seqstart)
     6600    0.210    0.000    0.210    0.000 {built-in method torch._ops.aten._efficient_attention_forward}
    16400    0.021    0.000    0.209    0.000 sock_client.py:121(_sendall_with_error_handle)
    46200    0.091    0.000    0.204    0.000 _utils.py:9(_get_device_index)
     6602    0.190    0.000    0.190    0.000 {built-in method torch.tensor}
    16400    0.183    0.000    0.183    0.000 {method 'send' of '_socket.socket' objects}
     6300    0.008    0.000    0.157    0.000 __init__.py:1060(flush)
    13200    0.044    0.000    0.152    0.000 cutlass.py:49(_minimum_gemm_alignment)
     6300    0.143    0.000    0.143    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
     2000    0.005    0.000    0.137    0.000 linear.py:116(forward)
     2000    0.130    0.000    0.130    0.000 {built-in method torch._C._nn.linear}
     7700    0.122    0.000    0.122    0.000 {method 'item' of 'torch._C.TensorBase' objects}
    16400    0.015    0.000    0.105    0.000 well_known_types.py:172(GetCurrentTime)
     6600    0.053    0.000    0.101    0.000 common.py:120(validate_inputs)
     7200    0.095    0.000    0.095    0.000 {method 'sum' of 'torch._C.TensorBase' objects}
    46200    0.082    0.000    0.092    0.000 _utils.py:764(_get_device_index)
     7200    0.092    0.000    0.092    0.000 {built-in method torch.zeros}
     6300    0.006    0.000    0.092    0.000 __init__.py:1550(makeRecord)
     6300    0.040    0.000    0.086    0.000 __init__.py:282(__init__)
    16400    0.028    0.000    0.081    0.000 well_known_types.py:242(FromDatetime)
      900    0.002    0.000    0.077    0.000 dropout.py:58(forward)
     6300    0.003    0.000    0.076    0.000 __init__.py:916(format)
      900    0.004    0.000    0.075    0.000 functional.py:1279(dropout)
     6300    0.009    0.000    0.074    0.000 __init__.py:650(format)
591279/590251    0.068    0.000    0.073    0.000 {built-in method builtins.isinstance}
     4600    0.072    0.000    0.072    0.000 {method 'any' of 'torch._C.TensorBase' objects}
      900    0.070    0.000    0.070    0.000 {built-in method torch.dropout}
    46200    0.037    0.000    0.069    0.000 common.py:482(check_lastdim_alignment_stride1)
     4600    0.064    0.000    0.064    0.000 {built-in method torch.isnan}
     6600    0.062    0.000    0.062    0.000 __init__.py:503(_detect_lse_packed_or_raise)
      100    0.007    0.000    0.059    0.001 module.py:382(log)
    13200    0.020    0.000    0.058    0.000 attn_bias.py:90(_get_default_bias_device)
    19800    0.057    0.000    0.057    0.000 {method 'unsqueeze' of 'torch._C.TensorBase' objects}
     6300    0.011    0.000    0.045    0.000 __init__.py:582(formatTime)
     6600    0.012    0.000    0.044    0.000 utils.py:31(unwrap_dead_wrappers)
    46200    0.038    0.000    0.038    0.000 {built-in method torch.cuda._get_device_properties}
     6600    0.005    0.000    0.038    0.000 __init__.py:115(is_available)
    19900    0.013    0.000    0.037    0.000 {built-in method builtins.any}
    52800    0.014    0.000    0.036    0.000 __init__.py:834(device_count)
    99002    0.034    0.000    0.034    0.000 {method 'stride' of 'torch._C.TensorBase' objects}
     6600    0.033    0.000    0.033    0.000 {method 'transpose' of 'torch._C.TensorBase' objects}
    59400    0.016    0.000    0.033    0.000 utils.py:33(<genexpr>)
    13200    0.029    0.000    0.029    0.000 common.py:96(normalize_bmhk)
    26400    0.017    0.000    0.029    0.000 common.py:331(shape_not_supported_reasons)
    16400    0.029    0.000    0.029    0.000 enum_type_wrapper.py:92(__getattr__)
    46200    0.012    0.000    0.029    0.000 __init__.py:284(_lazy_init)
    16400    0.028    0.000    0.028    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
     6600    0.003    0.000    0.025    0.000 __init__.py:111(_nvml_based_avail)
    59400    0.016    0.000    0.025    0.000 __init__.py:106(_is_compiled)
    16400    0.023    0.000    0.024    0.000 calendar.py:655(timegm)
     6300    0.023    0.000    0.023    0.000 {built-in method time.strftime}
     6300    0.013    0.000    0.022    0.000 __init__.py:1514(findCaller)
     6600    0.003    0.000    0.022    0.000 os.py:771(getenv)
    26400    0.011    0.000    0.019    0.000 common.py:32(is_available)
     6600    0.003    0.000    0.019    0.000 _collections_abc.py:760(get)
      100    0.004    0.000    0.018    0.000 result.py:355(log)
    46200    0.011    0.000    0.017    0.000 __init__.py:237(is_initialized)
    26400    0.009    0.000    0.017    0.000 cutlass.py:87(_get_tensor_bias)
  405/401    0.001    0.000    0.017    0.000 apply_func.py:23(apply_to_collection)
    66000    0.017    0.000    0.017    0.000 common.py:71(device)
    16400    0.016    0.000    0.016    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
    32800    0.016    0.000    0.016    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
     6300    0.006    0.000    0.016    0.000 posixpath.py:117(splitext)
     6600    0.006    0.000    0.016    0.000 os.py:674(__getitem__)
    13201    0.006    0.000    0.015    0.000 {built-in method builtins.all}
    52800    0.014    0.000    0.014    0.000 {built-in method builtins.max}
    22700    0.014    0.000    0.014    0.000 {built-in method posix.getpid}
      700    0.014    0.000    0.014    0.000 {method 'mean' of 'torch._C.TensorBase' objects}
    13200    0.004    0.000    0.014    0.000 cutlass.py:98(_check_bias_alignment)
    60642    0.014    0.000    0.014    0.000 {built-in method builtins.getattr}
    16400    0.013    0.000    0.013    0.000 interface_sock.py:41(_assign)
     6600    0.008    0.000    0.013    0.000 attn_bias.py:656(to)
     6600    0.012    0.000    0.013    0.000 dispatch.py:79(_dispatch_fw_priority_list)
      300    0.001    0.000    0.013    0.000 _tensor.py:982(__format__)
    78652    0.012    0.000    0.012    0.000 {built-in method builtins.hasattr}
     6600    0.009    0.000    0.012    0.000 cutlass.py:136(_custom_mask_type)
     6300    0.006    0.000    0.012    0.000 posixpath.py:140(basename)
      300    0.011    0.000    0.011    0.000 {built-in method torch.stack}
     6300    0.011    0.000    0.011    0.000 {built-in method time.localtime}
      600    0.010    0.000    0.010    0.000 {method 'var' of 'torch._C.TensorBase' objects}
     6300    0.003    0.000    0.010    0.000 __init__.py:634(formatMessage)
  270/201    0.001    0.000    0.010    0.000 apply_func.py:84(_apply_to_collection_slow)
    19800    0.010    0.000    0.010    0.000 {built-in method torch._C._functorch.unwrap_if_dead}
    13200    0.006    0.000    0.009    0.000 flash.py:511(_check_needs_no_topleft)
    16400    0.009    0.000    0.009    0.000 {built-in method utcnow}
      100    0.002    0.000    0.009    0.000 result.py:502(reset)
      300    0.001    0.000    0.009    0.000 activation.py:103(forward)
     6300    0.007    0.000    0.009    0.000 genericpath.py:121(_splitext)
     9700    0.008    0.000    0.008    0.000 module.py:1716(__getattr__)
     6600    0.008    0.000    0.008    0.000 cutlass.py:66(_get_seqlen_info)
    26400    0.008    0.000    0.008    0.000 common.py:131(<genexpr>)
      100    0.002    0.000    0.008    0.000 functional.py:3014(cross_entropy)
    16400    0.008    0.000    0.008    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
      300    0.001    0.000    0.008    0.000 functional.py:1489(relu)
      600    0.008    0.000    0.008    0.000 {built-in method torch.sqrt}
    16400    0.008    0.000    0.008    0.000 {built-in method _struct.pack}
     6300    0.002    0.000    0.007    0.000 __init__.py:432(format)
    26400    0.007    0.000    0.007    0.000 common.py:192(<genexpr>)
    12600    0.004    0.000    0.007    0.000 __init__.py:896(acquire)
       99    0.002    0.000    0.007    0.000 result.py:256(reset)
      300    0.007    0.000    0.007    0.000 {built-in method torch.relu}
    46200    0.007    0.000    0.007    0.000 {built-in method torch._C._cuda_isInBadFork}
    26400    0.006    0.000    0.006    0.000 common.py:122(<genexpr>)
     6300    0.002    0.000    0.006    0.000 __init__.py:628(usesTime)
    26400    0.006    0.000    0.006    0.000 common.py:152(<genexpr>)
      398    0.006    0.000    0.006    0.000 {method 'clone' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.006    0.000 result.py:424(update_metrics)
     6600    0.006    0.000    0.006    0.000 {built-in method torch._C._are_functorch_transforms_active}
      100    0.000    0.000    0.006    0.000 signature_utils.py:18(is_param_in_hook_signature)
      100    0.000    0.000    0.006    0.000 result.py:264(forward)
     6600    0.003    0.000    0.006    0.000 os.py:754(encode)
     6300    0.005    0.000    0.005    0.000 __init__.py:429(_format)
      100    0.002    0.000    0.005    0.000 module.py:654(__to_tensor)
      300    0.005    0.000    0.005    0.000 {built-in method torch.all}
      100    0.001    0.000    0.005    0.000 profiler.py:693(__exit__)
      100    0.001    0.000    0.005    0.000 metric.py:476(wrapped_func)
    46704    0.005    0.000    0.005    0.000 {built-in method builtins.len}
      100    0.001    0.000    0.005    0.000 inspect.py:1129(getfullargspec)
      100    0.005    0.000    0.005    0.000 {built-in method torch._C._nn.cross_entropy_loss}
       99    0.002    0.000    0.005    0.000 metric.py:689(reset)
    46302    0.005    0.000    0.005    0.000 _jit_internal.py:1130(is_scripting)
      900    0.005    0.000    0.005    0.000 {method 'view' of 'torch._C.TensorBase' objects}
    12600    0.004    0.000    0.005    0.000 __init__.py:903(release)
    40964    0.005    0.000    0.005    0.000 {method 'append' of 'list' objects}
    26400    0.005    0.000    0.005    0.000 {built-in method builtins.min}
    13200    0.005    0.000    0.005    0.000 __init__.py:461(<genexpr>)
     6300    0.003    0.000    0.005    0.000 __init__.py:160(<lambda>)
    22700    0.005    0.000    0.005    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      100    0.000    0.000    0.004    0.000 _ops.py:887(__call__)
    18900    0.004    0.000    0.004    0.000 {method 'rfind' of 'str' objects}
      514    0.002    0.000    0.004    0.000 typing.py:719(__instancecheck__)
     4800    0.004    0.000    0.004    0.000 {built-in method torch._C._get_tracing_state}
     6300    0.003    0.000    0.004    0.000 __init__.py:421(usesTime)
    13200    0.004    0.000    0.004    0.000 attn_bias.py:316(to)
     6600    0.002    0.000    0.004    0.000 __init__.py:1148(are_deterministic_algorithms_enabled)
    19800    0.004    0.000    0.004    0.000 flash.py:533(_check_strides_for_bmghk)
    13200    0.003    0.000    0.004    0.000 __init__.py:69(_unserialize_op)
  202/101    0.001    0.000    0.004    0.000 inspect.py:2246(_signature_from_callable)
      100    0.001    0.000    0.003    0.000 result.py:207(update)
     6600    0.002    0.000    0.003    0.000 os.py:758(decode)
    12600    0.003    0.000    0.003    0.000 __init__.py:791(filter)
      100    0.000    0.000    0.003    0.000 _ops.py:943(_must_dispatch_in_python)
     6300    0.002    0.000    0.003    0.000 __init__.py:119(getLevelName)
     6300    0.003    0.000    0.003    0.000 threading.py:1358(current_thread)
      100    0.003    0.000    0.003    0.000 {built-in method torch.argmax}
    12600    0.003    0.000    0.003    0.000 {method 'acquire' of '_thread.RLock' objects}
    16400    0.003    0.000    0.003    0.000 {method '__exit__' of '_thread.lock' objects}
      100    0.000    0.000    0.003    0.000 _pytree.py:1181(tree_any)
    16400    0.003    0.000    0.003    0.000 {built-in method time.monotonic}
     6300    0.003    0.000    0.003    0.000 __init__.py:1689(isEnabledFor)
     6300    0.002    0.000    0.003    0.000 posixpath.py:41(_get_sep)
      300    0.000    0.000    0.003    0.000 {built-in method builtins.next}
     6300    0.002    0.000    0.003    0.000 posixpath.py:52(normcase)
      101    0.001    0.000    0.003    0.000 inspect.py:2152(_signature_from_function)
     6300    0.003    0.000    0.003    0.000 __init__.py:358(getMessage)
      514    0.001    0.000    0.002    0.000 typing.py:848(__subclasscheck__)
      200    0.001    0.000    0.002    0.000 precision.py:167(train_step_context)
      100    0.000    0.000    0.002    0.000 contextlib.py:114(__enter__)
    13200    0.002    0.000    0.002    0.000 cutlass.py:41(_uses_tensorcores)
  700/200    0.001    0.000    0.002    0.000 _pytree.py:874(tree_iter)
     6600    0.002    0.000    0.002    0.000 function.py:34(save_for_backward)
     6300    0.002    0.000    0.002    0.000 threading.py:1093(name)
      100    0.001    0.000    0.002    0.000 <string>:2(__init__)
    18900    0.002    0.000    0.002    0.000 {built-in method posix.fspath}
    16400    0.002    0.000    0.002    0.000 {method 'toordinal' of 'datetime.date' objects}
    13203    0.002    0.000    0.002    0.000 {method 'get' of 'dict' objects}
     6600    0.002    0.000    0.002    0.000 function.py:591(_is_setup_context_defined)
      100    0.001    0.000    0.002    0.000 fx_validator.py:191(check_logging_and_get_default_levels)
     6300    0.002    0.000    0.002    0.000 {built-in method sys._getframe}
      248    0.001    0.000    0.002    0.000 apply_func.py:17(is_dataclass_instance)
      100    0.000    0.000    0.002    0.000 profiler.py:687(__enter__)
     6600    0.002    0.000    0.002    0.000 {method 'encode' of 'str' objects}
    12600    0.002    0.000    0.002    0.000 {built-in method _thread.get_ident}
     6600    0.002    0.000    0.002    0.000 {built-in method torch._C._get_deterministic_algorithms}
      100    0.001    0.000    0.002    0.000 memory.py:24(recursive_detach)
     6300    0.002    0.000    0.002    0.000 {method 'find' of 'str' objects}
      300    0.000    0.000    0.002    0.000 trainer.py:1381(training)
      100    0.000    0.000    0.002    0.000 trainer.py:1642(_results)
      372    0.000    0.000    0.002    0.000 abc.py:117(__instancecheck__)
     6600    0.002    0.000    0.002    0.000 {method 'decode' of 'bytes' objects}
    16400    0.002    0.000    0.002    0.000 {built-in method builtins.ord}
      100    0.001    0.000    0.001    0.000 {built-in method torch._ops.profiler._record_function_enter_new}
      200    0.001    0.000    0.001    0.000 result.py:90(_generate_sync_fn)
    13200    0.001    0.000    0.001    0.000 __init__.py:63(_serialize_op)
      372    0.001    0.000    0.001    0.000 {built-in method _abc._abc_instancecheck}
     6300    0.001    0.000    0.001    0.000 {built-in method time.time}
      100    0.000    0.000    0.001    0.000 result.py:57(__post_init__)
      100    0.001    0.000    0.001    0.000 precision.py:68(forward_context)
        1    0.000    0.000    0.001    0.001 result.py:415(register_key)
      900    0.001    0.000    0.001    0.000 _VF.py:26(__getattr__)
    12600    0.001    0.000    0.001    0.000 {method 'release' of '_thread.RLock' objects}
      300    0.001    0.000    0.001    0.000 enums.py:81(__eq__)
      398    0.001    0.000    0.001    0.000 {method 'detach' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.001    0.000 module.py:262(current_epoch)
      100    0.001    0.000    0.001    0.000 container.py:317(__iter__)
      200    0.000    0.000    0.001    0.000 contextlib.py:123(__exit__)
     6300    0.001    0.000    0.001    0.000 process.py:189(name)
      100    0.001    0.000    0.001    0.000 {built-in method torch._ops.profiler.}
      521    0.000    0.000    0.001    0.000 {built-in method builtins.issubclass}
      100    0.000    0.000    0.001    0.000 trainer.py:1564(_active_loop)
      100    0.000    0.000    0.001    0.000 result.py:122(__post_init__)
      303    0.001    0.000    0.001    0.000 inspect.py:2498(__init__)
      100    0.001    0.000    0.001    0.000 result.py:127(_parse_reduce_fx)
        1    0.000    0.000    0.001    0.001 result.py:306(to)
      400    0.000    0.000    0.001    0.000 _pytree.py:656(_is_leaf)
      514    0.000    0.000    0.001    0.000 abc.py:121(__subclasscheck__)
     6600    0.001    0.000    0.001    0.000 dispatch.py:142(_is_cutlassB_faster_than_flash)
     6300    0.001    0.000    0.001    0.000 process.py:37(current_process)
      100    0.000    0.000    0.001    0.000 contextlib.py:261(helper)
      700    0.000    0.000    0.001    0.000 _pytree.py:649(_get_node_type)
     6600    0.001    0.000    0.001    0.000 dispatch.py:27(_get_use_fa3)
     1322    0.001    0.000    0.001    0.000 result.py:294(__setattr__)
      102    0.000    0.000    0.001    0.000 inspect.py:2781(__init__)
      100    0.001    0.000    0.001    0.000 contextlib.py:688(__init__)
      100    0.000    0.000    0.001    0.000 contextlib.py:86(__init__)
      514    0.001    0.000    0.001    0.000 {built-in method _abc._abc_subclasscheck}
      100    0.001    0.000    0.001    0.000 fx_validator.py:177(check_logging_levels)
      100    0.001    0.000    0.001    0.000 trainer.py:1467(current_epoch)
      100    0.001    0.000    0.001    0.000 {method 'squeeze' of 'torch._C.TensorBase' objects}
      700    0.000    0.000    0.000    0.000 _pytree.py:638(_is_namedtuple_instance)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 result.py:187(__init__)
     1502    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}
      100    0.000    0.000    0.000    0.000 _reduction.py:7(get_enum)
      200    0.000    0.000    0.000    0.000 strategy.py:351(model)
      100    0.000    0.000    0.000    0.000 fit_loop.py:140(_results)
      600    0.000    0.000    0.000    0.000 types.py:171(__get__)
      100    0.000    0.000    0.000    0.000 fx_validator.py:151(check_logging)
      100    0.000    0.000    0.000    0.000 logger_connector.py:213(should_reset_tensors)
      202    0.000    0.000    0.000    0.000 {method 'to' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.000    0.000 memory.py:40(detach_and_move)
      104    0.000    0.000    0.000    0.000 grad_mode.py:184(__init__)
      100    0.000    0.000    0.000    0.000 profiler.py:676(__init__)
     58/4    0.000    0.000    0.000    0.000 copy.py:128(deepcopy)
      100    0.000    0.000    0.000    0.000 result.py:148(sync)
       99    0.000    0.000    0.000    0.000 <string>:2(__eq__)
      248    0.000    0.000    0.000    0.000 dataclasses.py:1047(is_dataclass)
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      303    0.000    0.000    0.000    0.000 enum.py:358(__call__)
      100    0.000    0.000    0.000    0.000 _ops.py:945(<lambda>)
      227    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}
      100    0.000    0.000    0.000    0.000 _pytree.py:423(_dict_flatten)
        2    0.000    0.000    0.000    0.000 metric.py:196(add_state)
      404    0.000    0.000    0.000    0.000 inspect.py:2830(<genexpr>)
      304    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 result.py:536(_get_default_dtype)
        2    0.000    0.000    0.000    0.000 _tensor.py:83(__deepcopy__)
        1    0.000    0.000    0.000    0.000 metric.py:101(__init__)
      300    0.000    0.000    0.000    0.000 {method '__format__' of 'int' objects}
      100    0.000    0.000    0.000    0.000 result.py:74(op)
      100    0.000    0.000    0.000    0.000 module.py:215(trainer)
      100    0.000    0.000    0.000    0.000 grad_mode.py:196(__exit__)
      100    0.000    0.000    0.000    0.000 <string>:1(<lambda>)
      248    0.000    0.000    0.000    0.000 apply_func.py:11(is_namedtuple)
      100    0.000    0.000    0.000    0.000 {built-in method torch.numel}
      300    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      600    0.000    0.000    0.000    0.000 enum.py:792(value)
      100    0.000    0.000    0.000    0.000 {built-in method torch.is_floating_point}
      3/2    0.000    0.000    0.000    0.000 copy.py:258(_reconstruct)
      100    0.000    0.000    0.000    0.000 grad_mode.py:193(__enter__)
      100    0.000    0.000    0.000    0.000 fx_validator.py:166(get_default_logging_levels)
      700    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      100    0.000    0.000    0.000    0.000 result.py:340(_extract_batch_size)
      202    0.000    0.000    0.000    0.000 inspect.py:159(isfunction)
      300    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C.TensorBase' objects}
      303    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}
        4    0.000    0.000    0.000    0.000 apply_func.py:71(move_data_to_device)
        2    0.000    0.000    0.000    0.000 storage.py:907(_deepcopy)
      303    0.000    0.000    0.000    0.000 enum.py:670(__new__)
      5/4    0.000    0.000    0.000    0.000 copy.py:226(_deepcopy_dict)
      100    0.000    0.000    0.000    0.000 strategy.py:93(precision_plugin)
      101    0.000    0.000    0.000    0.000 typing.py:271(inner)
      101    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}
      200    0.000    0.000    0.000    0.000 _pytree.py:395(_tuple_flatten)
        1    0.000    0.000    0.000    0.000 inspect.py:3111(signature)
      605    0.000    0.000    0.000    0.000 inspect.py:2548(name)
      102    0.000    0.000    0.000    0.000 {built-in method torch.get_default_dtype}
        4    0.000    0.000    0.000    0.000 apply_func.py:91(batch_to)
        1    0.000    0.000    0.000    0.000 inspect.py:2859(from_callable)
      300    0.000    0.000    0.000    0.000 result.py:70(op)
      203    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 storage.py:140(__deepcopy__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.iter}
      106    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}
      108    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f88d25377c0}
      100    0.000    0.000    0.000    0.000 contextlib.py:691(__enter__)
        2    0.000    0.000    0.000    0.000 storage.py:156(clone)
      200    0.000    0.000    0.000    0.000 result.py:80(group)
      200    0.000    0.000    0.000    0.000 result.py:60(should)
      304    0.000    0.000    0.000    0.000 inspect.py:2560(kind)
      100    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_variadic}
      100    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 __init__.py:129(annotate)
      100    0.000    0.000    0.000    0.000 result.py:143(sync)
      100    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}
      300    0.000    0.000    0.000    0.000 inspect.py:2556(annotation)
      302    0.000    0.000    0.000    0.000 inspect.py:2552(default)
      100    0.000    0.000    0.000    0.000 contextlib.py:694(__exit__)
      100    0.000    0.000    0.000    0.000 result.py:97(__call__)
        1    0.000    0.000    0.000    0.000 module.py:429(__init__)
        1    0.000    0.000    0.000    0.000 metric.py:475(_wrap_update)
        2    0.000    0.000    0.000    0.000 dataclasses.py:1024(fields)
        2    0.000    0.000    0.000    0.000 {method 'copy_' of 'torch._C.StorageBase' objects}
      101    0.000    0.000    0.000    0.000 inspect.py:2865(parameters)
      100    0.000    0.000    0.000    0.000 typing.py:1375(cast)
      204    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
      100    0.000    0.000    0.000    0.000 {method '__exit__' of 'torch._C.DisableTorchFunctionSubclass' objects}
      2/1    0.000    0.000    0.000    0.000 copy.py:209(_deepcopy_tuple)
      100    0.000    0.000    0.000    0.000 inspect.py:2869(return_annotation)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        2    0.000    0.000    0.000    0.000 _tensor.py:242(_typed_storage)
        2    0.000    0.000    0.000    0.000 functools.py:35(update_wrapper)
      110    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 storage.py:712(_new_wrapped_storage)
      2/1    0.000    0.000    0.000    0.000 copy.py:210(<listcomp>)
        1    0.000    0.000    0.000    0.000 inspect.py:1840(_signature_bound_method)
       19    0.000    0.000    0.000    0.000 dataclasses.py:1039(<genexpr>)
        1    0.000    0.000    0.000    0.000 result.py:273(_wrap_compute)
       13    0.000    0.000    0.000    0.000 copy.py:242(_keep_alive)
        3    0.000    0.000    0.000    0.000 {method '__reduce_ex__' of 'object' objects}
      100    0.000    0.000    0.000    0.000 result.py:101(no_op)
      107    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        2    0.000    0.000    0.000    0.000 grad_mode.py:80(__enter__)
        6    0.000    0.000    0.000    0.000 copy.py:263(<genexpr>)
        4    0.000    0.000    0.000    0.000 storage.py:557(__new__)
        2    0.000    0.000    0.000    0.000 {method 'set_' of 'torch._C.TensorBase' objects}
        4    0.000    0.000    0.000    0.000 storage.py:629(__init__)
        2    0.000    0.000    0.000    0.000 {method 'new_empty' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 inspect.py:494(unwrap)
        1    0.000    0.000    0.000    0.000 result.py:171(is_max_reduction)
        2    0.000    0.000    0.000    0.000 grad_mode.py:84(__exit__)
        5    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 inspect.py:2873(replace)
        2    0.000    0.000    0.000    0.000 apply_func.py:69(<genexpr>)
        2    0.000    0.000    0.000    0.000 grad_mode.py:75(__init__)
        2    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}
        2    0.000    0.000    0.000    0.000 _contextlib.py:154(__new__)
        2    0.000    0.000    0.000    0.000 {method 'untyped_storage' of 'torch._C.TensorBase' objects}
       24    0.000    0.000    0.000    0.000 copy.py:182(_deepcopy_atomic)
        2    0.000    0.000    0.000    0.000 copyreg.py:94(__newobj__)
        2    0.000    0.000    0.000    0.000 {built-in method torch._C._has_storage}
       14    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
        2    0.000    0.000    0.000    0.000 functools.py:65(wraps)
        2    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 inspect.py:514(_is_wrapper)
        2    0.000    0.000    0.000    0.000 {method 'contiguous' of 'torch._C.TensorBase' objects}
        3    0.000    0.000    0.000    0.000 _collections_abc.py:315(__subclasshook__)
        2    0.000    0.000    0.000    0.000 {method 'is_conj' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 {method 'nbytes' of 'torch._C.StorageBase' objects}
        2    0.000    0.000    0.000    0.000 storage.py:30(__init__)
        1    0.000    0.000    0.000    0.000 result.py:175(is_min_reduction)
        2    0.000    0.000    0.000    0.000 {method 'storage_offset' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 {method 'is_neg' of 'torch._C.TensorBase' objects}
        7    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method '__setstate__' of 'functools.partial' objects}
        1    0.000    0.000    0.000    0.000 result.py:163(is_mean_reduction)
        2    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {built-in method sys.getrecursionlimit}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_before_zero_grad
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 callback.py:285(on_before_zero_grad)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_before_zero_grad
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:285(on_before_zero_grad)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_before_zero_grad
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 callback.py:285(on_before_zero_grad)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_zero_grad
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:285(on_before_zero_grad)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_before_zero_grad
         700 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 hooks.py:259(on_before_zero_grad)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.optimizer_zero_grad
         6934 function calls (6434 primitive calls) in 0.010 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000    0.010    0.000 module.py:1310(optimizer_zero_grad)
      100    0.001    0.000    0.009    0.000 _compile.py:21(inner)
      100    0.001    0.000    0.008    0.000 eval_frame.py:596(_fn)
      100    0.002    0.000    0.008    0.000 optimizer.py:911(zero_grad)
      100    0.000    0.000    0.003    0.000 profiler.py:693(__exit__)
      100    0.000    0.000    0.002    0.000 _ops.py:887(__call__)
      100    0.000    0.000    0.002    0.000 profiler.py:687(__enter__)
      100    0.000    0.000    0.002    0.000 _ops.py:1047(__call__)
      100    0.002    0.000    0.002    0.000 {built-in method torch._ops.profiler._record_function_enter_new}
      100    0.000    0.000    0.002    0.000 _ops.py:943(_must_dispatch_in_python)
      100    0.000    0.000    0.002    0.000 _pytree.py:1181(tree_any)
      100    0.000    0.000    0.002    0.000 {built-in method builtins.any}
  700/200    0.001    0.000    0.001    0.000 _pytree.py:874(tree_iter)
      700    0.000    0.000    0.001    0.000 _pytree.py:649(_get_node_type)
      400    0.000    0.000    0.001    0.000 _pytree.py:656(_is_leaf)
      100    0.000    0.000    0.000    0.000 profiler.py:676(__init__)
      100    0.000    0.000    0.000    0.000 {built-in method torch._ops.profiler.}
      700    0.000    0.000    0.000    0.000 _pytree.py:638(_is_namedtuple_instance)
      103    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      107    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 _pytree.py:423(_dict_flatten)
      100    0.000    0.000    0.000    0.000 typing.py:271(inner)
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 _ops.py:945(<lambda>)
      200    0.000    0.000    0.000    0.000 {built-in method torch._C._dynamo.eval_frame.set_eval_frame}
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      300    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 decorators.py:38(disable)
      700    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      200    0.000    0.000    0.000    0.000 _pytree.py:395(_tuple_flatten)
      102    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        1    0.000    0.000    0.000    0.000 eval_frame.py:562(__init__)
        1    0.000    0.000    0.000    0.000 eval_frame.py:565(__call__)
      100    0.000    0.000    0.000    0.000 __init__.py:129(annotate)
      100    0.000    0.000    0.000    0.000 {method '__exit__' of 'torch._C.DisableTorchFunctionSubclass' objects}
        1    0.000    0.000    0.000    0.000 eval_frame.py:265(__init__)
      100    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 functools.py:35(update_wrapper)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
        3    0.000    0.000    0.000    0.000 eval_frame.py:240(innermost_fn)
        5    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}
        1    0.000    0.000    0.000    0.000 functools.py:65(wraps)
        1    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 inspect.py:73(isclass)
        1    0.000    0.000    0.000    0.000 eval_frame.py:232(nothing)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
        1    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.id}



Profile stats for: [Strategy]SingleDeviceStrategy.backward
         3500 function calls (3400 primitive calls) in 5.191 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000    5.191    0.052 strategy.py:191(backward)
      100    0.000    0.000    5.190    0.052 precision.py:45(pre_backward)
      100    0.000    0.000    5.190    0.052 call.py:185(_call_callback_hooks)
      100    0.000    0.000    5.188    0.052 contextlib.py:114(__enter__)
      100    0.000    0.000    5.188    0.052 {built-in method builtins.next}
      100    0.000    0.000    5.188    0.052 profiler.py:55(profile)
      100    0.000    0.000    5.188    0.052 advanced.py:65(start)
      100    5.188    0.052    5.188    0.052 {method 'enable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 early_stopping.py:130(state_key)
      100    0.000    0.000    0.000    0.000 module.py:1731(__setattr__)
      100    0.000    0.000    0.000    0.000 contextlib.py:261(helper)
  300/200    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      100    0.000    0.000    0.000    0.000 callback.py:48(_generate_state_key)
      100    0.000    0.000    0.000    0.000 parameter.py:8(__instancecheck__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.repr}
      100    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)
      100    0.000    0.000    0.000    0.000 __init__.py:1424(debug)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      100    0.000    0.000    0.000    0.000 module.py:215(trainer)
      100    0.000    0.000    0.000    0.000 strategy.py:93(precision_plugin)
      300    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      300    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
      100    0.000    0.000    0.000    0.000 strategy.py:345(pre_backward)
      100    0.000    0.000    0.000    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f88c4c64040}
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.callable}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_before_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 callback.py:274(on_before_backward)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_before_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:274(on_before_backward)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_before_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:274(on_before_backward)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 callback.py:274(on_before_backward)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_before_backward
         700 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 hooks.py:280(on_before_backward)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_after_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:277(on_after_backward)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)



Profile stats for: [Callback]TQDMProgressBar.on_after_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:277(on_after_backward)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_after_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:277(on_after_backward)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_after_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:277(on_after_backward)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_after_backward
         700 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 hooks.py:289(on_after_backward)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_before_optimizer_step
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 callback.py:280(on_before_optimizer_step)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_before_optimizer_step
         1000 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.001    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:280(on_before_optimizer_step)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_before_optimizer_step
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:280(on_before_optimizer_step)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_optimizer_step
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:280(on_before_optimizer_step)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_before_optimizer_step
         700 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 hooks.py:298(on_before_optimizer_step)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.configure_gradient_clipping
         27500 function calls in 0.643 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.643    0.006 module.py:1218(configure_gradient_clipping)
      100    0.002    0.000    0.642    0.006 module.py:1150(clip_gradients)
      100    0.001    0.000    0.637    0.006 precision.py:143(clip_gradients)
      100    0.003    0.000    0.635    0.006 precision.py:162(clip_grad_by_norm)
      100    0.004    0.000    0.632    0.006 clip_grad.py:19(_no_grad_wrapper)
      100    0.008    0.000    0.627    0.006 clip_grad.py:25(clip_grad_norm_)
      100    0.418    0.004    0.418    0.004 {built-in method torch._foreach_norm}
      100    0.088    0.001    0.088    0.001 {built-in method torch._foreach_mul_}
      100    0.066    0.001    0.066    0.001 {built-in method torch._C._linalg.linalg_vector_norm}
      100    0.000    0.000    0.019    0.000 _tensor.py:35(wrapped)
      100    0.002    0.000    0.019    0.000 _tensor.py:964(__rdiv__)
      100    0.017    0.000    0.017    0.000 {method 'reciprocal' of 'torch._C.TensorBase' objects}
      200    0.001    0.000    0.008    0.000 _foreach_utils.py:43(_has_foreach_support)
      200    0.006    0.000    0.006    0.000 _foreach_utils.py:39(_device_has_foreach_support)
      100    0.005    0.000    0.005    0.000 {built-in method torch.stack}
      100    0.004    0.000    0.004    0.000 clip_grad.py:53(<listcomp>)
      100    0.000    0.000    0.004    0.000 _contextlib.py:113(decorate_context)
      100    0.001    0.000    0.003    0.000 enums.py:34(supported_type)
      100    0.001    0.000    0.003    0.000 clip_grad.py:74(<listcomp>)
      100    0.003    0.000    0.003    0.000 {built-in method torch.clamp}
      100    0.001    0.000    0.002    0.000 _foreach_utils.py:32(_group_tensors_by_device_and_dtype)
     4100    0.002    0.000    0.002    0.000 {method 'to' of 'torch._C.TensorBase' objects}
      200    0.001    0.000    0.002    0.000 {built-in method builtins.all}
      100    0.002    0.000    0.002    0.000 {built-in method torch._C._group_tensors_by_device_and_dtype}
      100    0.000    0.000    0.001    0.000 {built-in method builtins.any}
     8200    0.001    0.000    0.001    0.000 _foreach_utils.py:44(<genexpr>)
      300    0.000    0.000    0.001    0.000 enums.py:36(<genexpr>)
      100    0.001    0.000    0.001    0.000 enum.py:434(__iter__)
      600    0.000    0.000    0.001    0.000 types.py:171(__get__)
      200    0.000    0.000    0.001    0.000 grad_mode.py:80(__enter__)
      400    0.000    0.000    0.001    0.000 grad_mode.py:184(__init__)
      200    0.000    0.000    0.001    0.000 enums.py:81(__eq__)
      200    0.000    0.000    0.000    0.000 grad_mode.py:84(__exit__)
      600    0.000    0.000    0.000    0.000 enum.py:792(value)
      300    0.000    0.000    0.000    0.000 enum.py:438(<genexpr>)
     4100    0.000    0.000    0.000    0.000 precision.py:126(main_params)
      200    0.000    0.000    0.000    0.000 grad_mode.py:75(__init__)
      100    0.000    0.000    0.000    0.000 _contextlib.py:146(clone)
      200    0.000    0.000    0.000    0.000 _foreach_utils.py:8(_get_foreach_kernels_supported_devices)
      100    0.000    0.000    0.000    0.000 enum.py:358(__call__)
      200    0.000    0.000    0.000    0.000 _contextlib.py:154(__new__)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      400    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}
      100    0.000    0.000    0.000    0.000 enum.py:670(__new__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      200    0.000    0.000    0.000    0.000 {built-in method torch._C._get_privateuse1_backend_name}
      600    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}
      400    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      100    0.000    0.000    0.000    0.000 trainer.py:1129(precision_plugin)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      400    0.000    0.000    0.000    0.000 module.py:215(trainer)
      500    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      400    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
      200    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f88d25377c0}
      100    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}
      200    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function}
      100    0.000    0.000    0.000    0.000 module.py:230(fabric)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:93(precision_plugin)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_train_batch_end
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 callback.py:81(on_train_batch_end)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_train_batch_end
         87559 function calls (87555 primitive calls) in 0.086 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.003    0.000    0.086    0.001 tqdm_progress.py:269(on_train_batch_end)
      200    0.001    0.000    0.062    0.000 std.py:1325(refresh)
      200    0.001    0.000    0.059    0.000 std.py:1464(display)
      100    0.002    0.000    0.048    0.000 tqdm_progress.py:451(_update_n)
      200    0.002    0.000    0.029    0.000 std.py:1150(__str__)
      200    0.001    0.000    0.029    0.000 std.py:457(print_status)
      100    0.002    0.000    0.021    0.000 std.py:1402(set_postfix)
      200    0.000    0.000    0.020    0.000 std.py:451(fp_write)
      400    0.001    0.000    0.019    0.000 utils.py:194(inner)
      200    0.007    0.000    0.014    0.000 std.py:464(format_meter)
      100    0.000    0.000    0.014    0.000 progress_bar.py:177(get_metrics)
      200    0.003    0.000    0.013    0.000 std.py:1446(format_dict)
      200    0.000    0.000    0.011    0.000 redirect.py:644(write)
      200    0.000    0.000    0.011    0.000 wandb_run.py:2304(<lambda>)
      200    0.000    0.000    0.011    0.000 wandb_run.py:390(wrapper_fn)
      200    0.000    0.000    0.011    0.000 wandb_run.py:1429(_console_raw_callback)
      200    0.001    0.000    0.010    0.000 interface.py:749(publish_output_raw)
      200    0.005    0.000    0.009    0.000 utils.py:333(_screen_shape_linux)
      200    0.001    0.000    0.008    0.000 utils.py:378(disp_len)
      200    0.000    0.000    0.007    0.000 utils.py:374(_text_width)
      200    0.007    0.000    0.007    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      200    0.002    0.000    0.007    0.000 {built-in method builtins.sum}
      100    0.002    0.000    0.007    0.000 progress_bar.py:210(get_standard_metrics)
      200    0.000    0.000    0.007    0.000 interface_shared.py:76(_publish_output_raw)
      100    0.000    0.000    0.007    0.000 trainer.py:1632(progress_bar_metrics)
      100    0.001    0.000    0.007    0.000 logger_connector.py:250(progress_bar_metrics)
      200    0.000    0.000    0.006    0.000 interface_sock.py:45(_publish)
      200    0.000    0.000    0.006    0.000 sock_client.py:219(send_record_publish)
    19084    0.004    0.000    0.006    0.000 utils.py:375(<genexpr>)
      200    0.000    0.000    0.005    0.000 sock_client.py:153(send_server_request)
      200    0.001    0.000    0.005    0.000 sock_client.py:145(_send_message)
      100    0.000    0.000    0.005    0.000 logger_connector.py:229(metrics)
      100    0.001    0.000    0.005    0.000 utilities.py:25(_version)
      100    0.001    0.000    0.004    0.000 wandb.py:572(version)
      200    0.000    0.000    0.004    0.000 sock_client.py:121(_sendall_with_error_handle)
      100    0.001    0.000    0.004    0.000 result.py:476(metrics)
      400    0.002    0.000    0.003    0.000 utils.py:273(_is_ascii)
      200    0.003    0.000    0.003    0.000 {method 'send' of '_socket.socket' objects}
      200    0.003    0.000    0.003    0.000 {built-in method fcntl.ioctl}
      100    0.001    0.000    0.003    0.000 wandb_run.py:357(wrapper)
      300    0.000    0.000    0.002    0.000 trainer.py:1642(_results)
      400    0.001    0.000    0.002    0.000 {method 'format' of 'str' objects}
      600    0.000    0.000    0.002    0.000 trainer.py:1381(training)
    18884    0.002    0.000    0.002    0.000 {built-in method unicodedata.east_asian_width}
      200    0.000    0.000    0.002    0.000 well_known_types.py:172(GetCurrentTime)
      100    0.002    0.000    0.002    0.000 wandb_run.py:881(id)
      200    0.001    0.000    0.001    0.000 std.py:102(acquire)
      600    0.001    0.000    0.001    0.000 enums.py:81(__eq__)
      200    0.000    0.000    0.001    0.000 well_known_types.py:242(FromDatetime)
      300    0.000    0.000    0.001    0.000 trainer.py:1564(_active_loop)
     2096    0.000    0.000    0.001    0.000 {built-in method builtins.isinstance}
      200    0.000    0.000    0.001    0.000 utils.py:347(<listcomp>)
    17400    0.001    0.000    0.001    0.000 {built-in method builtins.ord}
      100    0.001    0.000    0.001    0.000 tqdm_progress.py:425(_should_update)
      400    0.001    0.000    0.001    0.000 std.py:400(format_interval)
       99    0.000    0.000    0.001    0.000 tqdm_progress.py:46(format_num)
      300    0.000    0.000    0.001    0.000 fit_loop.py:140(_results)
      100    0.001    0.000    0.001    0.000 result.py:461(valid_items)
      199    0.000    0.000    0.001    0.000 abc.py:117(__instancecheck__)
      200    0.001    0.000    0.001    0.000 os.py:674(__getitem__)
      100    0.000    0.000    0.001    0.000 result.py:430(_get_cache)
      199    0.001    0.000    0.001    0.000 {built-in method _abc._abc_instancecheck}
      100    0.001    0.000    0.001    0.000 {method 'detach' of 'torch._C.TensorBase' objects}
      200    0.001    0.000    0.001    0.000 std.py:153(__init__)
      200    0.001    0.000    0.001    0.000 result.py:463(<genexpr>)
      100    0.000    0.000    0.001    0.000 result.py:465(_forked_name)
     1200    0.000    0.000    0.001    0.000 types.py:171(__get__)
       99    0.000    0.000    0.001    0.000 std.py:419(format_num)
      200    0.001    0.000    0.001    0.000 enum_type_wrapper.py:92(__getattr__)
      200    0.001    0.000    0.001    0.000 std.py:231(__call__)
      200    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
      200    0.000    0.000    0.000    0.000 std.py:186(__format__)
      100    0.000    0.000    0.000    0.000 result.py:158(forked_name)
      200    0.000    0.000    0.000    0.000 std.py:106(release)
      200    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
      500    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      200    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
      200    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      200    0.000    0.000    0.000    0.000 {built-in method now}
      100    0.000    0.000    0.000    0.000 tqdm_progress.py:169(is_enabled)
      100    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
      200    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
      200    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      400    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
      200    0.000    0.000    0.000    0.000 os.py:754(encode)
      300    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
      200    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
      800    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      299    0.000    0.000    0.000    0.000 std.py:1428(<genexpr>)
      200    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
     1200    0.000    0.000    0.000    0.000 enum.py:792(value)
      300    0.000    0.000    0.000    0.000 tqdm_progress.py:121(train_progress_bar)
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      200    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      200    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
      997    0.000    0.000    0.000    0.000 {built-in method builtins.len}
     1000    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
      200    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
     1200    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.max}
      200    0.000    0.000    0.000    0.000 {built-in method utcnow}
      400    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}
      200    0.000    0.000    0.000    0.000 trainer.py:1590(loggers)
      198    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}
      200    0.000    0.000    0.000    0.000 tqdm_progress.py:161(refresh_rate)
      200    0.000    0.000    0.000    0.000 utils.py:108(__init__)
      200    0.000    0.000    0.000    0.000 utils.py:112(__format__)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      200    0.000    0.000    0.000    0.000 std.py:167(colour)
      200    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      200    0.000    0.000    0.000    0.000 {built-in method time.time}
      3/1    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      199    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}
      3/1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}
      200    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      200    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      200    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
      100    0.000    0.000    0.000    0.000 result.py:154(forked)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      200    0.000    0.000    0.000    0.000 std.py:163(colour)
      300    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      200    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      200    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
      200    0.000    0.000    0.000    0.000 {method 'keys' of 'collections.OrderedDict' objects}
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}



Profile stats for: [Callback]ModelSummary.on_train_batch_end
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 callback.py:81(on_train_batch_end)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_end
         2800 function calls in 0.003 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.003    0.000 model_checkpoint.py:284(on_train_batch_end)
      100    0.002    0.000    0.003    0.000 model_checkpoint.py:413(_should_skip_saving_checkpoint)
      100    0.000    0.000    0.000    0.000 trainer.py:1429(sanity_checking)
      100    0.000    0.000    0.000    0.000 trainer.py:1458(global_step)
      100    0.000    0.000    0.000    0.000 enums.py:81(__eq__)
      100    0.000    0.000    0.000    0.000 training_epoch_loop.py:99(global_step)
      200    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      200    0.000    0.000    0.000    0.000 types.py:171(__get__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 progress.py:274(optimizer_steps)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      200    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      200    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      200    0.000    0.000    0.000    0.000 enum.py:792(value)
      100    0.000    0.000    0.000    0.000 module.py:295(automatic_optimization)
      200    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_train_batch_end
         700 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 hooks.py:79(on_train_batch_end)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_model_zero_grad
         75900 function calls (56400 primitive calls) in 0.035 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000    0.034    0.000 hooks.py:158(on_validation_model_zero_grad)
      100    0.009    0.000    0.033    0.000 module.py:2500(zero_grad)
     4100    0.002    0.000    0.024    0.000 module.py:2233(parameters)
     4100    0.001    0.000    0.022    0.000 module.py:2258(named_parameters)
     4100    0.005    0.000    0.021    0.000 module.py:2219(_named_members)
24200/4700    0.010    0.000    0.011    0.000 module.py:2395(named_modules)
     8600    0.002    0.000    0.002    0.000 {method 'add' of 'set' objects}
     4600    0.002    0.000    0.002    0.000 module.py:2286(<lambda>)
     8000    0.001    0.000    0.002    0.000 _tensor.py:1055(__hash__)
     9200    0.001    0.000    0.001    0.000 {method 'items' of 'collections.OrderedDict' objects}
     8000    0.001    0.000    0.001    0.000 {built-in method builtins.id}
      100    0.000    0.000    0.001    0.000 {built-in method builtins.getattr}
      100    0.000    0.000    0.000    0.000 module.py:1716(__getattr__)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_train_epoch_end
         49709 function calls in 0.026 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.025    0.000 tqdm_progress.py:278(on_train_epoch_end)
      100    0.001    0.000    0.019    0.000 std.py:1402(set_postfix)
      100    0.000    0.000    0.017    0.000 std.py:1325(refresh)
      100    0.000    0.000    0.017    0.000 std.py:1464(display)
      100    0.000    0.000    0.010    0.000 std.py:457(print_status)
      100    0.000    0.000    0.007    0.000 std.py:1150(__str__)
      100    0.000    0.000    0.006    0.000 std.py:451(fp_write)
      200    0.000    0.000    0.006    0.000 utils.py:194(inner)
      100    0.000    0.000    0.006    0.000 progress_bar.py:177(get_metrics)
      100    0.000    0.000    0.005    0.000 redirect.py:644(write)
      100    0.000    0.000    0.005    0.000 wandb_run.py:2304(<lambda>)
      100    0.000    0.000    0.005    0.000 wandb_run.py:390(wrapper_fn)
      100    0.000    0.000    0.004    0.000 wandb_run.py:1429(_console_raw_callback)
      100    0.002    0.000    0.004    0.000 std.py:464(format_meter)
      100    0.000    0.000    0.004    0.000 interface.py:749(publish_output_raw)
      100    0.000    0.000    0.004    0.000 trainer.py:1632(progress_bar_metrics)
      100    0.000    0.000    0.004    0.000 logger_connector.py:250(progress_bar_metrics)
      100    0.000    0.000    0.003    0.000 utils.py:378(disp_len)
      100    0.000    0.000    0.003    0.000 utils.py:374(_text_width)
      100    0.001    0.000    0.003    0.000 {built-in method builtins.sum}
      100    0.000    0.000    0.003    0.000 interface_shared.py:76(_publish_output_raw)
      100    0.000    0.000    0.003    0.000 interface_sock.py:45(_publish)
      100    0.000    0.000    0.002    0.000 logger_connector.py:229(metrics)
      100    0.000    0.000    0.002    0.000 sock_client.py:219(send_record_publish)
     9562    0.002    0.000    0.002    0.000 utils.py:375(<genexpr>)
      300    0.000    0.000    0.002    0.000 trainer.py:1642(_results)
      100    0.000    0.000    0.002    0.000 sock_client.py:153(send_server_request)
      100    0.000    0.000    0.002    0.000 sock_client.py:145(_send_message)
      100    0.000    0.000    0.002    0.000 std.py:1446(format_dict)
      600    0.000    0.000    0.002    0.000 trainer.py:1381(training)
      100    0.000    0.000    0.002    0.000 sock_client.py:121(_sendall_with_error_handle)
      100    0.000    0.000    0.002    0.000 utils.py:333(_screen_shape_linux)
      100    0.000    0.000    0.001    0.000 progress_bar.py:210(get_standard_metrics)
      100    0.001    0.000    0.001    0.000 {method 'send' of '_socket.socket' objects}
      600    0.001    0.000    0.001    0.000 enums.py:81(__eq__)
      200    0.001    0.000    0.001    0.000 utils.py:273(_is_ascii)
      100    0.000    0.000    0.001    0.000 result.py:476(metrics)
      300    0.000    0.000    0.001    0.000 trainer.py:1564(_active_loop)
      100    0.000    0.000    0.001    0.000 utilities.py:25(_version)
      100    0.001    0.000    0.001    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      300    0.000    0.000    0.001    0.000 fit_loop.py:140(_results)
      100    0.000    0.000    0.001    0.000 wandb.py:572(version)
     9462    0.001    0.000    0.001    0.000 {built-in method unicodedata.east_asian_width}
      200    0.001    0.000    0.001    0.000 result.py:463(<genexpr>)
      100    0.001    0.000    0.001    0.000 {built-in method fcntl.ioctl}
       99    0.000    0.000    0.001    0.000 tqdm_progress.py:46(format_num)
      200    0.000    0.000    0.001    0.000 {method 'format' of 'str' objects}
      100    0.000    0.000    0.001    0.000 well_known_types.py:172(GetCurrentTime)
      100    0.000    0.000    0.001    0.000 well_known_types.py:242(FromDatetime)
     1200    0.000    0.000    0.000    0.000 types.py:171(__get__)
       99    0.000    0.000    0.000    0.000 std.py:419(format_num)
     8700    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
     1696    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      100    0.000    0.000    0.000    0.000 wandb_run.py:357(wrapper)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 utils.py:347(<listcomp>)
      200    0.000    0.000    0.000    0.000 std.py:400(format_interval)
      100    0.000    0.000    0.000    0.000 os.py:674(__getitem__)
      100    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 std.py:186(__format__)
      100    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
      100    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
      100    0.000    0.000    0.000    0.000 std.py:102(acquire)
      100    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      199    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)
      100    0.000    0.000    0.000    0.000 std.py:106(release)
      200    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
     1200    0.000    0.000    0.000    0.000 enum.py:792(value)
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      299    0.000    0.000    0.000    0.000 std.py:1428(<genexpr>)
      100    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
      100    0.000    0.000    0.000    0.000 {built-in method now}
      100    0.000    0.000    0.000    0.000 std.py:153(__init__)
     1200    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      500    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      697    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      200    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
      199    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}
      100    0.000    0.000    0.000    0.000 os.py:754(encode)
      100    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
      200    0.000    0.000    0.000    0.000 tqdm_progress.py:121(train_progress_bar)
      100    0.000    0.000    0.000    0.000 result.py:461(valid_items)
      100    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
      300    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      500    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
      100    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
      100    0.000    0.000    0.000    0.000 wandb_run.py:881(id)
      100    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      100    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
      100    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 std.py:231(__call__)
      100    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
      200    0.000    0.000    0.000    0.000 trainer.py:1590(loggers)
      100    0.000    0.000    0.000    0.000 result.py:430(_get_cache)
      100    0.000    0.000    0.000    0.000 {built-in method utcnow}
      100    0.000    0.000    0.000    0.000 utils.py:112(__format__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.max}
      200    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      100    0.000    0.000    0.000    0.000 utils.py:108(__init__)
      198    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}
      300    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      100    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      200    0.000    0.000    0.000    0.000 {method 'keys' of 'collections.OrderedDict' objects}
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 std.py:167(colour)
      100    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      199    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}
      100    0.000    0.000    0.000    0.000 {built-in method time.time}
      100    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      100    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      100    0.000    0.000    0.000    0.000 std.py:163(colour)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      100    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
      100    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}



Profile stats for: [Callback]ModelSummary.on_train_epoch_end
         1000 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 callback.py:95(on_train_epoch_end)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_train_epoch_end
         34381 function calls (33121 primitive calls) in 0.052 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.002    0.000    0.051    0.001 FlavourClassificationTransformerEncoder.py:137(on_train_epoch_end)
      100    0.001    0.000    0.031    0.000 module.py:382(log)
      100    0.001    0.000    0.010    0.000 result.py:355(log)
      100    0.000    0.000    0.008    0.000 __init__.py:1436(info)
      100    0.000    0.000    0.008    0.000 __init__.py:1565(_log)
  405/401    0.001    0.000    0.007    0.000 apply_func.py:23(apply_to_collection)
      100    0.000    0.000    0.006    0.000 result.py:502(reset)
      100    0.006    0.000    0.006    0.000 {built-in method torch.stack}
       99    0.001    0.000    0.006    0.000 result.py:256(reset)
      100    0.000    0.000    0.006    0.000 signature_utils.py:18(is_param_in_hook_signature)
      100    0.000    0.000    0.005    0.000 __init__.py:1591(handle)
      100    0.000    0.000    0.005    0.000 result.py:424(update_metrics)
      100    0.000    0.000    0.005    0.000 __init__.py:1645(callHandlers)
      100    0.000    0.000    0.005    0.000 result.py:264(forward)
      100    0.000    0.000    0.005    0.000 metric.py:476(wrapped_func)
      100    0.001    0.000    0.005    0.000 inspect.py:1129(getfullargspec)
      100    0.000    0.000    0.005    0.000 __init__.py:939(handle)
       99    0.001    0.000    0.005    0.000 metric.py:689(reset)
      100    0.000    0.000    0.005    0.000 __init__.py:1178(emit)
      100    0.000    0.000    0.005    0.000 __init__.py:1071(emit)
      100    0.004    0.000    0.004    0.000 result.py:207(update)
      298    0.004    0.000    0.004    0.000 {method 'clone' of 'torch._C.TensorBase' objects}
  202/101    0.001    0.000    0.004    0.000 inspect.py:2246(_signature_from_callable)
      100    0.001    0.000    0.003    0.000 module.py:654(__to_tensor)
      100    0.000    0.000    0.003    0.000 __init__.py:1060(flush)
      101    0.001    0.000    0.003    0.000 inspect.py:2152(_signature_from_function)
  270/201    0.000    0.000    0.003    0.000 apply_func.py:84(_apply_to_collection_slow)
      100    0.003    0.000    0.003    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
4579/3551    0.001    0.000    0.002    0.000 {built-in method builtins.isinstance}
      100    0.000    0.000    0.002    0.000 __init__.py:1550(makeRecord)
      100    0.001    0.000    0.002    0.000 __init__.py:282(__init__)
      100    0.002    0.000    0.002    0.000 {method 'item' of 'torch._C.TensorBase' objects}
      100    0.002    0.000    0.002    0.000 {method 'mean' of 'torch._C.TensorBase' objects}
      398    0.001    0.000    0.001    0.000 {method 'detach' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.001    0.000 __init__.py:916(format)
        1    0.000    0.000    0.001    0.001 result.py:415(register_key)
      100    0.000    0.000    0.001    0.000 __init__.py:650(format)
      300    0.000    0.000    0.001    0.000 trainer.py:1381(training)
      514    0.000    0.000    0.001    0.000 typing.py:719(__instancecheck__)
        1    0.000    0.000    0.001    0.001 result.py:306(to)
      100    0.000    0.000    0.001    0.000 trainer.py:1642(_results)
      514    0.000    0.000    0.001    0.000 typing.py:848(__subclasscheck__)
      300    0.000    0.000    0.001    0.000 enums.py:81(__eq__)
      303    0.000    0.000    0.001    0.000 inspect.py:2498(__init__)
      100    0.000    0.000    0.001    0.000 __init__.py:582(formatTime)
      100    0.000    0.000    0.001    0.000 contextlib.py:123(__exit__)
     1322    0.001    0.000    0.001    0.000 result.py:294(__setattr__)
      102    0.000    0.000    0.001    0.000 inspect.py:2781(__init__)
      100    0.000    0.000    0.001    0.000 fx_validator.py:191(check_logging_and_get_default_levels)
      100    0.000    0.000    0.001    0.000 memory.py:24(recursive_detach)
      100    0.000    0.000    0.001    0.000 __init__.py:1514(findCaller)
      100    0.000    0.000    0.000    0.000 trainer.py:1564(_active_loop)
      100    0.000    0.000    0.000    0.000 module.py:262(current_epoch)
      521    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}
      100    0.000    0.000    0.000    0.000 {method 'clear' of 'list' objects}
        1    0.000    0.000    0.000    0.000 result.py:187(__init__)
      227    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 <string>:2(__init__)
      200    0.000    0.000    0.000    0.000 result.py:90(_generate_sync_fn)
      248    0.000    0.000    0.000    0.000 apply_func.py:17(is_dataclass_instance)
     58/4    0.000    0.000    0.000    0.000 copy.py:128(deepcopy)
      100    0.000    0.000    0.000    0.000 fx_validator.py:177(check_logging_levels)
      100    0.000    0.000    0.000    0.000 {method 'squeeze' of 'torch._C.TensorBase' objects}
      202    0.000    0.000    0.000    0.000 {method 'to' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.000    0.000 fit_loop.py:140(_results)
      100    0.000    0.000    0.000    0.000 memory.py:40(detach_and_move)
      652    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      514    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)
      100    0.000    0.000    0.000    0.000 {built-in method time.strftime}
      600    0.000    0.000    0.000    0.000 types.py:171(__get__)
      100    0.000    0.000    0.000    0.000 result.py:148(sync)
      100    0.000    0.000    0.000    0.000 posixpath.py:117(splitext)
      100    0.000    0.000    0.000    0.000 result.py:57(__post_init__)
      100    0.000    0.000    0.000    0.000 posixpath.py:140(basename)
      372    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)
      303    0.000    0.000    0.000    0.000 enum.py:358(__call__)
      248    0.000    0.000    0.000    0.000 dataclasses.py:1047(is_dataclass)
        2    0.000    0.000    0.000    0.000 metric.py:196(add_state)
      100    0.000    0.000    0.000    0.000 result.py:122(__post_init__)
       99    0.000    0.000    0.000    0.000 <string>:2(__eq__)
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        2    0.000    0.000    0.000    0.000 _tensor.py:83(__deepcopy__)
      104    0.000    0.000    0.000    0.000 grad_mode.py:184(__init__)
      404    0.000    0.000    0.000    0.000 inspect.py:2830(<genexpr>)
      100    0.000    0.000    0.000    0.000 {built-in method time.localtime}
      514    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}
      100    0.000    0.000    0.000    0.000 result.py:127(_parse_reduce_fx)
      100    0.000    0.000    0.000    0.000 __init__.py:634(formatMessage)
      100    0.000    0.000    0.000    0.000 result.py:74(op)
      372    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      304    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}
      100    0.000    0.000    0.000    0.000 genericpath.py:121(_splitext)
      100    0.000    0.000    0.000    0.000 __init__.py:160(<lambda>)
        1    0.000    0.000    0.000    0.000 metric.py:101(__init__)
      100    0.000    0.000    0.000    0.000 __init__.py:432(format)
      200    0.000    0.000    0.000    0.000 __init__.py:896(acquire)
      803    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 result.py:536(_get_default_dtype)
      100    0.000    0.000    0.000    0.000 __init__.py:628(usesTime)
      442    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      100    0.000    0.000    0.000    0.000 <string>:1(<lambda>)
      100    0.000    0.000    0.000    0.000 __init__.py:119(getLevelName)
      248    0.000    0.000    0.000    0.000 apply_func.py:11(is_namedtuple)
      100    0.000    0.000    0.000    0.000 grad_mode.py:196(__exit__)
      3/2    0.000    0.000    0.000    0.000 copy.py:258(_reconstruct)
      100    0.000    0.000    0.000    0.000 __init__.py:429(_format)
      100    0.000    0.000    0.000    0.000 module.py:215(trainer)
      200    0.000    0.000    0.000    0.000 __init__.py:903(release)
        4    0.000    0.000    0.000    0.000 apply_func.py:71(move_data_to_device)
      100    0.000    0.000    0.000    0.000 posixpath.py:52(normcase)
      5/4    0.000    0.000    0.000    0.000 copy.py:226(_deepcopy_dict)
      100    0.000    0.000    0.000    0.000 {built-in method torch.numel}
        2    0.000    0.000    0.000    0.000 storage.py:907(_deepcopy)
      202    0.000    0.000    0.000    0.000 inspect.py:159(isfunction)
      300    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1467(current_epoch)
      303    0.000    0.000    0.000    0.000 enum.py:670(__new__)
      700    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      100    0.000    0.000    0.000    0.000 __init__.py:421(usesTime)
        4    0.000    0.000    0.000    0.000 apply_func.py:91(batch_to)
      303    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}
      600    0.000    0.000    0.000    0.000 enum.py:792(value)
      100    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
      605    0.000    0.000    0.000    0.000 inspect.py:2548(name)
      100    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
      100    0.000    0.000    0.000    0.000 grad_mode.py:193(__enter__)
      100    0.000    0.000    0.000    0.000 threading.py:1358(current_thread)
      100    0.000    0.000    0.000    0.000 {built-in method torch.is_floating_point}
      664    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
      100    0.000    0.000    0.000    0.000 logger_connector.py:213(should_reset_tensors)
      200    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      200    0.000    0.000    0.000    0.000 __init__.py:791(filter)
      100    0.000    0.000    0.000    0.000 fx_validator.py:151(check_logging)
      100    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)
      108    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f88d25377c0}
      100    0.000    0.000    0.000    0.000 fx_validator.py:166(get_default_logging_levels)
      101    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}
        2    0.000    0.000    0.000    0.000 storage.py:140(__deepcopy__)
      100    0.000    0.000    0.000    0.000 {built-in method sys._getframe}
      100    0.000    0.000    0.000    0.000 __init__.py:358(getMessage)
      100    0.000    0.000    0.000    0.000 threading.py:1093(name)
      300    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        1    0.000    0.000    0.000    0.000 inspect.py:3111(signature)
        1    0.000    0.000    0.000    0.000 inspect.py:2859(from_callable)
        2    0.000    0.000    0.000    0.000 storage.py:156(clone)
      100    0.000    0.000    0.000    0.000 result.py:340(_extract_batch_size)
      102    0.000    0.000    0.000    0.000 {built-in method torch.get_default_dtype}
      304    0.000    0.000    0.000    0.000 inspect.py:2560(kind)
      101    0.000    0.000    0.000    0.000 result.py:163(is_mean_reduction)
      300    0.000    0.000    0.000    0.000 result.py:70(op)
      100    0.000    0.000    0.000    0.000 {built-in method time.time}
      106    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      200    0.000    0.000    0.000    0.000 result.py:60(should)
      100    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}
      200    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
      300    0.000    0.000    0.000    0.000 inspect.py:2556(annotation)
      302    0.000    0.000    0.000    0.000 inspect.py:2552(default)
      2/1    0.000    0.000    0.000    0.000 copy.py:209(_deepcopy_tuple)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      204    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
      100    0.000    0.000    0.000    0.000 process.py:189(name)
        2    0.000    0.000    0.000    0.000 dataclasses.py:1024(fields)
      200    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      200    0.000    0.000    0.000    0.000 result.py:80(group)
      100    0.000    0.000    0.000    0.000 process.py:37(current_process)
        2    0.000    0.000    0.000    0.000 {method 'copy_' of 'torch._C.StorageBase' objects}
        2    0.000    0.000    0.000    0.000 _tensor.py:242(_typed_storage)
        1    0.000    0.000    0.000    0.000 metric.py:475(_wrap_update)
      101    0.000    0.000    0.000    0.000 inspect.py:2865(parameters)
      100    0.000    0.000    0.000    0.000 inspect.py:2869(return_annotation)
      103    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 module.py:429(__init__)
        2    0.000    0.000    0.000    0.000 {built-in method torch.tensor}
      2/1    0.000    0.000    0.000    0.000 copy.py:210(<listcomp>)
        1    0.000    0.000    0.000    0.000 result.py:273(_wrap_compute)
      100    0.000    0.000    0.000    0.000 typing.py:1375(cast)
      110    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 functools.py:35(update_wrapper)
       19    0.000    0.000    0.000    0.000 dataclasses.py:1039(<genexpr>)
       13    0.000    0.000    0.000    0.000 copy.py:242(_keep_alive)
      107    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        2    0.000    0.000    0.000    0.000 storage.py:712(_new_wrapped_storage)
        4    0.000    0.000    0.000    0.000 storage.py:629(__init__)
        3    0.000    0.000    0.000    0.000 {method '__reduce_ex__' of 'object' objects}
        1    0.000    0.000    0.000    0.000 inspect.py:1840(_signature_bound_method)
        6    0.000    0.000    0.000    0.000 copy.py:263(<genexpr>)
        2    0.000    0.000    0.000    0.000 {method 'set_' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.all}
        2    0.000    0.000    0.000    0.000 {method 'new_empty' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 grad_mode.py:80(__enter__)
        1    0.000    0.000    0.000    0.000 inspect.py:494(unwrap)
        1    0.000    0.000    0.000    0.000 typing.py:271(inner)
        5    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 apply_func.py:69(<genexpr>)
        4    0.000    0.000    0.000    0.000 storage.py:557(__new__)
        2    0.000    0.000    0.000    0.000 grad_mode.py:84(__exit__)
        1    0.000    0.000    0.000    0.000 inspect.py:2873(replace)
        2    0.000    0.000    0.000    0.000 _contextlib.py:154(__new__)
        2    0.000    0.000    0.000    0.000 grad_mode.py:75(__init__)
        1    0.000    0.000    0.000    0.000 result.py:171(is_max_reduction)
        3    0.000    0.000    0.000    0.000 _collections_abc.py:315(__subclasshook__)
        2    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}
        2    0.000    0.000    0.000    0.000 {method 'untyped_storage' of 'torch._C.TensorBase' objects}
       24    0.000    0.000    0.000    0.000 copy.py:182(_deepcopy_atomic)
        2    0.000    0.000    0.000    0.000 copyreg.py:94(__newobj__)
        2    0.000    0.000    0.000    0.000 {built-in method torch._C._has_storage}
       14    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
        2    0.000    0.000    0.000    0.000 functools.py:65(wraps)
        2    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 inspect.py:514(_is_wrapper)
        2    0.000    0.000    0.000    0.000 {method 'contiguous' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 {method 'stride' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 {method 'is_conj' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 {method 'nbytes' of 'torch._C.StorageBase' objects}
        2    0.000    0.000    0.000    0.000 {method 'storage_offset' of 'torch._C.TensorBase' objects}
        7    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 storage.py:30(__init__)
        2    0.000    0.000    0.000    0.000 {method 'is_neg' of 'torch._C.TensorBase' objects}
        4    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        1    0.000    0.000    0.000    0.000 result.py:175(is_min_reduction)
        2    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}
        1    0.000    0.000    0.000    0.000 {method '__setstate__' of 'functools.partial' objects}
        2    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
        1    0.000    0.000    0.000    0.000 {built-in method sys.getrecursionlimit}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_train_epoch_end
         15325 function calls in 0.073 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.072    0.001 early_stopping.py:186(on_train_epoch_end)
      100    0.002    0.000    0.071    0.001 early_stopping.py:198(_run_early_stopping_check)
      100    0.006    0.000    0.056    0.001 early_stopping.py:218(_evaluate_stopping_criteria)
      102    0.047    0.000    0.047    0.000 {built-in method torch.isfinite}
      100    0.000    0.000    0.012    0.000 trainer.py:1606(callback_metrics)
      100    0.000    0.000    0.012    0.000 logger_connector.py:236(callback_metrics)
      100    0.000    0.000    0.010    0.000 logger_connector.py:229(metrics)
      100    0.001    0.000    0.009    0.000 result.py:476(metrics)
      200    0.000    0.000    0.006    0.000 result.py:430(_get_cache)
      100    0.000    0.000    0.004    0.000 result.py:276(wrapped_func)
      100    0.002    0.000    0.004    0.000 result.py:246(compute)
      300    0.000    0.000    0.002    0.000 trainer.py:1642(_results)
      100    0.002    0.000    0.002    0.000 {method 'clone' of 'torch._C.TensorBase' objects}
      700    0.001    0.000    0.002    0.000 enums.py:81(__eq__)
      100    0.002    0.000    0.002    0.000 {built-in method torch.lt}
      100    0.000    0.000    0.002    0.000 apply_func.py:113(convert_tensors_to_scalars)
      600    0.000    0.000    0.002    0.000 trainer.py:1381(training)
      100    0.000    0.000    0.002    0.000 apply_func.py:23(apply_to_collection)
      100    0.000    0.000    0.001    0.000 apply_func.py:122(to_item)
      100    0.000    0.000    0.001    0.000 early_stopping.py:181(_should_skip_check)
      103    0.001    0.000    0.001    0.000 {method 'item' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.001    0.000 trainer.py:1429(sanity_checking)
      300    0.000    0.000    0.001    0.000 trainer.py:1564(_active_loop)
      300    0.000    0.000    0.001    0.000 fit_loop.py:140(_results)
      100    0.001    0.000    0.001    0.000 early_stopping.py:142(_validate_condition_metric)
     1400    0.000    0.000    0.001    0.000 types.py:171(__get__)
      100    0.000    0.000    0.000    0.000 distributed.py:392(_distributed_is_initialized)
        2    0.000    0.000    0.000    0.000 early_stopping.py:268(_log_info)
        2    0.000    0.000    0.000    0.000 __init__.py:1436(info)
      100    0.000    0.000    0.000    0.000 {method 'squeeze' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.000    0.000 early_stopping.py:161(monitor_op)
        2    0.000    0.000    0.000    0.000 __init__.py:1565(_log)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 result.py:64(should)
      100    0.000    0.000    0.000    0.000 result.py:90(_generate_sync_fn)
      100    0.000    0.000    0.000    0.000 {method 'detach' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        2    0.000    0.000    0.000    0.000 __init__.py:1591(handle)
        2    0.000    0.000    0.000    0.000 __init__.py:1645(callHandlers)
        2    0.000    0.000    0.000    0.000 __init__.py:939(handle)
        2    0.000    0.000    0.000    0.000 __init__.py:1071(emit)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
     1104    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        2    0.000    0.000    0.000    0.000 redirect.py:644(write)
      100    0.000    0.000    0.000    0.000 {method 'to' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
     1400    0.000    0.000    0.000    0.000 enum.py:792(value)
        2    0.000    0.000    0.000    0.000 early_stopping.py:257(_improvement_message)
        2    0.000    0.000    0.000    0.000 wandb_run.py:2310(<lambda>)
     1400    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
      100    0.000    0.000    0.000    0.000 distributed_c10d.py:996(is_initialized)
        2    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
        2    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
      100    0.000    0.000    0.000    0.000 result.py:461(valid_items)
      100    0.000    0.000    0.000    0.000 result.py:465(_forked_name)
      300    0.000    0.000    0.000    0.000 result.py:463(<genexpr>)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 imports.py:162(__bool__)
        2    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
      202    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 typing.py:271(inner)
        2    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
      400    0.000    0.000    0.000    0.000 result.py:143(sync)
      100    0.000    0.000    0.000    0.000 __init__.py:10(is_available)
        2    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
      100    0.000    0.000    0.000    0.000 distributed_c10d.py:608(WORLD)
      100    0.000    0.000    0.000    0.000 result.py:294(__setattr__)
        2    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        2    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
      100    0.000    0.000    0.000    0.000 {method 'numel' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.000    0.000 result.py:158(forked_name)
        2    0.000    0.000    0.000    0.000 __init__.py:1550(makeRecord)
      100    0.000    0.000    0.000    0.000 imports.py:154(_check_available)
        3    0.000    0.000    0.000    0.000 _tensor.py:982(__format__)
        2    0.000    0.000    0.000    0.000 __init__.py:282(__init__)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      206    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      100    0.000    0.000    0.000    0.000 result.py:163(is_mean_reduction)
        2    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
      204    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      200    0.000    0.000    0.000    0.000 result.py:97(__call__)
      200    0.000    0.000    0.000    0.000 result.py:60(should)
        2    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
      100    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      100    0.000    0.000    0.000    0.000 metric.py:181(update_called)
        2    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
        2    0.000    0.000    0.000    0.000 __init__.py:1514(findCaller)
      100    0.000    0.000    0.000    0.000 strategy.py:341(reduce_boolean_decision)
        2    0.000    0.000    0.000    0.000 __init__.py:916(format)
      100    0.000    0.000    0.000    0.000 distributed_c10d.py:480(default_pg)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        2    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
      200    0.000    0.000    0.000    0.000 result.py:101(no_op)
        2    0.000    0.000    0.000    0.000 __init__.py:650(format)
      100    0.000    0.000    0.000    0.000 result.py:70(op)
        2    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      100    0.000    0.000    0.000    0.000 result.py:80(group)
      100    0.000    0.000    0.000    0.000 result.py:154(forked)
      100    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
        4    0.000    0.000    0.000    0.000 __init__.py:791(filter)
        2    0.000    0.000    0.000    0.000 posixpath.py:140(basename)
        2    0.000    0.000    0.000    0.000 __init__.py:628(usesTime)
        4    0.000    0.000    0.000    0.000 __init__.py:896(acquire)
        2    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        2    0.000    0.000    0.000    0.000 __init__.py:1060(flush)
        5    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
        2    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
        2    0.000    0.000    0.000    0.000 posixpath.py:117(splitext)
        2    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        2    0.000    0.000    0.000    0.000 __init__.py:634(formatMessage)
        2    0.000    0.000    0.000    0.000 trainer.py:1147(world_size)
        3    0.000    0.000    0.000    0.000 {method '__format__' of 'float' objects}
        2    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        2    0.000    0.000    0.000    0.000 __init__.py:160(<lambda>)
        2    0.000    0.000    0.000    0.000 __init__.py:432(format)
        2    0.000    0.000    0.000    0.000 {built-in method utcnow}
        2    0.000    0.000    0.000    0.000 genericpath.py:121(_splitext)
        2    0.000    0.000    0.000    0.000 posixpath.py:52(normcase)
        2    0.000    0.000    0.000    0.000 __init__.py:421(usesTime)
        4    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        4    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
        2    0.000    0.000    0.000    0.000 __init__.py:429(_format)
        2    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
        1    0.000    0.000    0.000    0.000 __init__.py:218(_acquireLock)
        6    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}
        6    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        2    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        4    0.000    0.000    0.000    0.000 __init__.py:903(release)
        2    0.000    0.000    0.000    0.000 threading.py:1358(current_thread)
        2    0.000    0.000    0.000    0.000 __init__.py:119(getLevelName)
        2    0.000    0.000    0.000    0.000 threading.py:1093(name)
        2    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)
        1    0.000    0.000    0.000    0.000 __init__.py:227(_releaseLock)
        2    0.000    0.000    0.000    0.000 __init__.py:358(getMessage)
        1    0.000    0.000    0.000    0.000 __init__.py:1675(getEffectiveLevel)
        2    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        2    0.000    0.000    0.000    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
        6    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        2    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {built-in method sys._getframe}
        3    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}
        4    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        5    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
        2    0.000    0.000    0.000    0.000 process.py:37(current_process)
        3    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 rank_zero.py:91(rank_prefixed_message)
        4    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
        2    0.000    0.000    0.000    0.000 {built-in method time.time}
        2    0.000    0.000    0.000    0.000 process.py:189(name)
        1    0.000    0.000    0.000    0.000 __init__.py:1276(disable)
        2    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        2    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        2    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
        2    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_end
         68937 function calls (65787 primitive calls) in 0.255 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000    0.254    0.003 model_checkpoint.py:317(on_train_epoch_end)
      100    0.000    0.000    0.205    0.002 model_checkpoint.py:368(_save_topk_checkpoint)
      100    0.000    0.000    0.205    0.002 model_checkpoint.py:698(_save_monitor_checkpoint)
        8    0.000    0.000    0.200    0.025 model_checkpoint.py:718(_update_best_and_save)
        8    0.000    0.000    0.194    0.024 model_checkpoint.py:387(_save_checkpoint)
        8    0.000    0.000    0.194    0.024 trainer.py:1346(save_checkpoint)
        8    0.000    0.000    0.194    0.024 checkpoint_connector.py:404(dump_checkpoint)
      101    0.000    0.000    0.184    0.002 {built-in method builtins.next}
      100    0.000    0.000    0.184    0.002 profiler.py:55(profile)
        8    0.000    0.000    0.184    0.023 call.py:167(_call_lightning_datamodule_hook)
        8    0.000    0.000    0.183    0.023 contextlib.py:114(__enter__)
        8    0.000    0.000    0.183    0.023 advanced.py:65(start)
        8    0.183    0.023    0.183    0.023 {method 'enable' of '_lsprof.Profiler' objects}
      100    0.001    0.000    0.045    0.000 model_checkpoint.py:667(_monitor_candidates)
 2260/660    0.005    0.000    0.038    0.000 copy.py:128(deepcopy)
  500/100    0.001    0.000    0.037    0.000 copy.py:226(_deepcopy_dict)
      400    0.008    0.000    0.033    0.000 _tensor.py:83(__deepcopy__)
      400    0.001    0.000    0.014    0.000 storage.py:907(_deepcopy)
      400    0.001    0.000    0.010    0.000 storage.py:140(__deepcopy__)
      400    0.002    0.000    0.009    0.000 storage.py:156(clone)
      400    0.006    0.000    0.006    0.000 {method 'copy_' of 'torch._C.StorageBase' objects}
      100    0.000    0.000    0.005    0.000 trainer.py:1606(callback_metrics)
        8    0.000    0.000    0.005    0.001 checkpoint_connector.py:499(_get_loops_state_dict)
      100    0.000    0.000    0.005    0.000 logger_connector.py:236(callback_metrics)
    64/32    0.001    0.000    0.005    0.000 loop.py:52(state_dict)
      100    0.003    0.000    0.005    0.000 model_checkpoint.py:512(check_monitor_top_k)
        8    0.000    0.000    0.004    0.001 model_checkpoint.py:654(_get_metric_interpolated_filepath_name)
      100    0.000    0.000    0.004    0.000 logger_connector.py:229(metrics)
       72    0.000    0.000    0.003    0.000 progress.py:24(state_dict)
       72    0.000    0.000    0.003    0.000 dataclasses.py:1054(asdict)
   816/72    0.001    0.000    0.003    0.000 dataclasses.py:1078(_asdict_inner)
      100    0.001    0.000    0.003    0.000 result.py:476(metrics)
        8    0.000    0.000    0.003    0.000 checkpoint_connector.py:496(_get_lightning_module_state_dict)
        8    0.000    0.000    0.003    0.000 strategy.py:473(lightning_module_state_dict)
    368/8    0.001    0.000    0.003    0.000 module.py:1865(state_dict)
        8    0.000    0.000    0.003    0.000 model_checkpoint.py:770(file_exists)
        8    0.000    0.000    0.003    0.000 spec.py:633(exists)
        8    0.000    0.000    0.003    0.000 local.py:71(info)
      400    0.001    0.000    0.002    0.000 _tensor.py:242(_typed_storage)
        8    0.002    0.000    0.002    0.000 {built-in method posix.stat}
      900    0.001    0.000    0.002    0.000 enums.py:81(__eq__)
      300    0.000    0.000    0.002    0.000 trainer.py:1642(_results)
6786/6782    0.001    0.000    0.002    0.000 {built-in method builtins.isinstance}
        8    0.000    0.000    0.002    0.000 model_checkpoint.py:567(format_checkpoint_name)
      368    0.001    0.000    0.002    0.000 module.py:1826(_save_to_state_dict)
      100    0.000    0.000    0.002    0.000 apply_func.py:113(convert_tensors_to_scalars)
      200    0.002    0.000    0.002    0.000 {built-in method torch.tensor}
      400    0.002    0.000    0.002    0.000 {method 'new_empty' of 'torch._C.TensorBase' objects}
        8    0.000    0.000    0.001    0.000 model_checkpoint.py:531(_format_checkpoint_name)
      400    0.001    0.000    0.001    0.000 storage.py:712(_new_wrapped_storage)
      400    0.001    0.000    0.001    0.000 {method 'set_' of 'torch._C.TensorBase' objects}
      600    0.000    0.000    0.001    0.000 trainer.py:1381(training)
        8    0.000    0.000    0.001    0.000 strategy.py:173(optimizer_state)
      100    0.000    0.000    0.001    0.000 apply_func.py:23(apply_to_collection)
       97    0.001    0.000    0.001    0.000 {built-in method torch.lt}
      100    0.000    0.000    0.001    0.000 apply_func.py:122(to_item)
      420    0.001    0.000    0.001    0.000 {method 'detach' of 'torch._C.TensorBase' objects}
      116    0.001    0.000    0.001    0.000 {method 'item' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.001    0.000 model_checkpoint.py:413(_should_skip_saving_checkpoint)
        8    0.000    0.000    0.001    0.000 _compile.py:21(inner)
        8    0.000    0.000    0.001    0.000 re.py:233(findall)
      800    0.001    0.000    0.001    0.000 storage.py:629(__init__)
     1300    0.001    0.000    0.001    0.000 copy.py:242(_keep_alive)
        8    0.000    0.000    0.001    0.000 eval_frame.py:596(_fn)
     2247    0.001    0.000    0.001    0.000 {built-in method builtins.getattr}
        8    0.000    0.000    0.001    0.000 optimizer.py:631(state_dict)
      800    0.000    0.000    0.001    0.000 grad_mode.py:184(__init__)
        8    0.000    0.000    0.001    0.000 fit_loop.py:415(on_save_checkpoint)
      400    0.000    0.000    0.001    0.000 grad_mode.py:80(__enter__)
      100    0.000    0.000    0.001    0.000 model_checkpoint.py:423(_should_save_on_train_epoch_end)
      300    0.000    0.000    0.001    0.000 trainer.py:1564(_active_loop)
      300    0.000    0.000    0.001    0.000 fit_loop.py:140(_results)
      400    0.000    0.000    0.001    0.000 grad_mode.py:84(__exit__)
        8    0.000    0.000    0.001    0.000 combined_loader.py:378(_state_dicts)
      800    0.001    0.000    0.001    0.000 storage.py:557(__new__)
        8    0.000    0.000    0.001    0.000 combined_loader.py:380(<listcomp>)
      208    0.000    0.000    0.001    0.000 trainer.py:1458(global_step)
     1800    0.001    0.000    0.001    0.000 types.py:171(__get__)
        8    0.001    0.000    0.001    0.000 {method 'findall' of 're.Pattern' objects}
        8    0.000    0.000    0.001    0.000 typing.py:1141(__instancecheck__)
      200    0.000    0.000    0.001    0.000 trainer.py:1535(num_val_batches)
     6241    0.001    0.000    0.001    0.000 {built-in method builtins.id}
      400    0.001    0.000    0.001    0.000 _contextlib.py:154(__new__)
        6    0.001    0.000    0.001    0.000 {built-in method builtins.max}
       92    0.000    0.000    0.001    0.000 contextlib.py:123(__exit__)
      208    0.000    0.000    0.001    0.000 training_epoch_loop.py:99(global_step)
      256    0.000    0.000    0.001    0.000 dataclasses.py:1024(fields)
      400    0.001    0.000    0.001    0.000 grad_mode.py:75(__init__)
     4916    0.001    0.000    0.001    0.000 {method 'get' of 'dict' objects}
      888    0.000    0.000    0.001    0.000 dataclasses.py:1042(_is_dataclass_instance)
        8    0.000    0.000    0.001    0.000 optimizer.py:703(<listcomp>)
        8    0.000    0.000    0.001    0.000 optimizer.py:689(pack_group)
      200    0.000    0.000    0.000    0.000 result.py:430(_get_cache)
      100    0.000    0.000    0.000    0.000 trainer.py:1429(sanity_checking)
      400    0.000    0.000    0.000    0.000 {built-in method torch._C._has_storage}
       18    0.000    0.000    0.000    0.000 typing.py:1065(_get_protocol_attrs)
        9    0.000    0.000    0.000    0.000 typing.py:1082(_is_callable_members_only)
        8    0.000    0.000    0.000    0.000 re.py:289(_compile)
      800    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}
      316    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
     1291    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      400    0.000    0.000    0.000    0.000 {method 'untyped_storage' of 'torch._C.TensorBase' objects}
       15    0.000    0.000    0.000    0.000 {built-in method builtins.min}
        1    0.000    0.000    0.000    0.000 sre_compile.py:783(compile)
     1000    0.000    0.000    0.000    0.000 dataclasses.py:1039(<genexpr>)
        8    0.000    0.000    0.000    0.000 optimizer.py:691(<dictcomp>)
     1200    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f88d25377c0}
        8    0.000    0.000    0.000    0.000 optimizer.py:705(<dictcomp>)
     1800    0.000    0.000    0.000    0.000 enum.py:792(value)
     1200    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}
        8    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}
     2001    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
      808    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}
      400    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C.TensorBase' objects}
        8    0.000    0.000    0.000    0.000 lr_scheduler.py:1383(state_dict)
     1800    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      116    0.000    0.000    0.000    0.000 typing.py:271(inner)
       16    0.000    0.000    0.000    0.000 _tensor.py:982(__format__)
        8    0.000    0.000    0.000    0.000 call.py:217(_call_callbacks_state_dict)
1345/1342    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        8    0.000    0.000    0.000    0.000 optimizer.py:693(<dictcomp>)
        1    0.000    0.000    0.000    0.000 sre_parse.py:944(parse)
      100    0.000    0.000    0.000    0.000 result.py:465(_forked_name)
        8    0.000    0.000    0.000    0.000 lr_scheduler.py:1384(<dictcomp>)
      300    0.000    0.000    0.000    0.000 result.py:463(<genexpr>)
      469    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      208    0.000    0.000    0.000    0.000 trainer.py:1467(current_epoch)
      400    0.000    0.000    0.000    0.000 {method 'is_conj' of 'torch._C.TensorBase' objects}
      400    0.000    0.000    0.000    0.000 {method 'stride' of 'torch._C.TensorBase' objects}
       92    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      2/1    0.000    0.000    0.000    0.000 sre_parse.py:436(_parse_sub)
        8    0.000    0.000    0.000    0.000 local.py:230(_strip_protocol)
      2/1    0.000    0.000    0.000    0.000 sre_parse.py:494(_parse)
      400    0.000    0.000    0.000    0.000 {method 'storage_offset' of 'torch._C.TensorBase' objects}
      400    0.000    0.000    0.000    0.000 {method 'nbytes' of 'torch._C.StorageBase' objects}
        8    0.000    0.000    0.000    0.000 {built-in method torch.isnan}
      208    0.000    0.000    0.000    0.000 progress.py:274(optimizer_steps)
      400    0.000    0.000    0.000    0.000 {method 'is_neg' of 'torch._C.TensorBase' objects}
      109    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 result.py:461(valid_items)
      960    0.000    0.000    0.000    0.000 copy.py:182(_deepcopy_atomic)
     1104    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
      689    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 sre_compile.py:622(_code)
      416    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}
        8    0.000    0.000    0.000    0.000 optimizer.py:699(<listcomp>)
      400    0.000    0.000    0.000    0.000 storage.py:30(__init__)
      752    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}
        8    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)
        1    0.000    0.000    0.000    0.000 decorators.py:38(disable)
       16    0.000    0.000    0.000    0.000 callback.py:48(_generate_state_key)
      100    0.000    0.000    0.000    0.000 result.py:158(forked_name)
      332    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        8    0.000    0.000    0.000    0.000 early_stopping.py:130(state_key)
      401    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}
        8    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}
      400    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
       16    0.000    0.000    0.000    0.000 hparams_mixin.py:152(hparams)
      100    0.000    0.000    0.000    0.000 {method 'numel' of 'torch._C.TensorBase' objects}
        8    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}
      208    0.000    0.000    0.000    0.000 module.py:295(automatic_optimization)
       92    0.000    0.000    0.000    0.000 model_checkpoint.py:677(_save_last_checkpoint)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.sum}
        8    0.000    0.000    0.000    0.000 posixpath.py:71(join)
      3/1    0.000    0.000    0.000    0.000 sre_compile.py:87(_compile)
      300    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}
       17    0.000    0.000    0.000    0.000 {built-in method builtins.all}
        1    0.000    0.000    0.000    0.000 typing.py:1200(_proto_hook)
        8    0.000    0.000    0.000    0.000 __init__.py:1424(debug)
        1    0.000    0.000    0.000    0.000 sre_compile.py:560(_compile_info)
        1    0.000    0.000    0.000    0.000 eval_frame.py:565(__call__)
        8    0.000    0.000    0.000    0.000 model_checkpoint.py:256(state_key)
       16    0.000    0.000    0.000    0.000 {built-in method builtins.repr}
      256    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
       27    0.000    0.000    0.000    0.000 typing.py:1084(<genexpr>)
       92    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        8    0.000    0.000    0.000    0.000 trainer.py:1203(model)
        8    0.000    0.000    0.000    0.000 contextlib.py:261(helper)
        1    0.000    0.000    0.000    0.000 eval_frame.py:562(__init__)
      100    0.000    0.000    0.000    0.000 result.py:154(forked)
        8    0.000    0.000    0.000    0.000 early_stopping.py:165(state_dict)
       97    0.000    0.000    0.000    0.000 strategy.py:341(reduce_boolean_decision)
       32    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}
        1    0.000    0.000    0.000    0.000 eval_frame.py:265(__init__)
        8    0.000    0.000    0.000    0.000 local.py:280(make_path_posix)
        1    0.000    0.000    0.000    0.000 functools.py:35(update_wrapper)
        8    0.000    0.000    0.000    0.000 trainer.py:1183(optimizers)
        8    0.000    0.000    0.000    0.000 trainer.py:1129(precision_plugin)
        8    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)
      2/1    0.000    0.000    0.000    0.000 sre_compile.py:485(_get_literal_prefix)
        1    0.000    0.000    0.000    0.000 sre_compile.py:292(_optimize_charset)
        1    0.000    0.000    0.000    0.000 sre_parse.py:97(closegroup)
        8    0.000    0.000    0.000    0.000 model_checkpoint.py:335(state_dict)
      4/2    0.000    0.000    0.000    0.000 sre_parse.py:175(getwidth)
        8    0.000    0.000    0.000    0.000 {method '__format__' of 'float' objects}
        1    0.000    0.000    0.000    0.000 sre_parse.py:356(_escape)
       11    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)
        8    0.000    0.000    0.000    0.000 training_epoch_loop.py:317(on_save_checkpoint)
       64    0.000    0.000    0.000    0.000 loop.py:40(on_save_checkpoint)
       16    0.000    0.000    0.000    0.000 typing.py:1149(<genexpr>)
        8    0.000    0.000    0.000    0.000 {method '__format__' of 'int' objects}
        1    0.000    0.000    0.000    0.000 sre_parse.py:225(__init__)
        8    0.000    0.000    0.000    0.000 trainer.py:1191(lr_scheduler_configs)
       14    0.000    0.000    0.000    0.000 sre_parse.py:165(__getitem__)
       16    0.000    0.000    0.000    0.000 {built-in method torch._C._dynamo.eval_frame.set_eval_frame}
        1    0.000    0.000    0.000    0.000 enum.py:977(__and__)
        8    0.000    0.000    0.000    0.000 utils.py:327(stringify_path)
        1    0.000    0.000    0.000    0.000 sre_parse.py:85(opengroup)
       16    0.000    0.000    0.000    0.000 model_checkpoint.py:547(<lambda>)
        8    0.000    0.000    0.000    0.000 sre_parse.py:255(get)
        1    0.000    0.000    0.000    0.000 sre_parse.py:76(__init__)
       18    0.000    0.000    0.000    0.000 {method 'keys' of 'mappingproxy' objects}
       11    0.000    0.000    0.000    0.000 sre_parse.py:234(__next)
        8    0.000    0.000    0.000    0.000 strategy.py:351(model)
       11    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}
       36    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
        8    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)
        8    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
       16    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C.TensorBase' objects}
       29    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
        8    0.000    0.000    0.000    0.000 strategy.py:93(precision_plugin)
        2    0.000    0.000    0.000    0.000 enum.py:358(__call__)
        8    0.000    0.000    0.000    0.000 combined_loader.py:308(flattened)
        2    0.000    0.000    0.000    0.000 sre_compile.py:619(isstring)
        8    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}
       16    0.000    0.000    0.000    0.000 callback.py:232(state_dict)
        1    0.000    0.000    0.000    0.000 sre_compile.py:265(_compile_charset)
        8    0.000    0.000    0.000    0.000 strategy.py:101(optimizers)
        1    0.000    0.000    0.000    0.000 sre_parse.py:433(_uniq)
        3    0.000    0.000    0.000    0.000 eval_frame.py:240(innermost_fn)
        8    0.000    0.000    0.000    0.000 hooks.py:692(on_save_checkpoint)
        5    0.000    0.000    0.000    0.000 sre_parse.py:287(tell)
        1    0.000    0.000    0.000    0.000 sre_parse.py:296(_class_escape)
        3    0.000    0.000    0.000    0.000 sre_parse.py:112(__init__)
        8    0.000    0.000    0.000    0.000 sre_parse.py:250(match)
        8    0.000    0.000    0.000    0.000 precision.py:138(state_dict)
        8    0.000    0.000    0.000    0.000 single_device.py:90(broadcast)
        8    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}
       18    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
        5    0.000    0.000    0.000    0.000 sre_parse.py:161(__len__)
        1    0.000    0.000    0.000    0.000 sre_compile.py:447(_simple)
        4    0.000    0.000    0.000    0.000 sre_parse.py:82(groups)
        8    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        4    0.000    0.000    0.000    0.000 sre_parse.py:173(append)
        5    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}
        5    0.000    0.000    0.000    0.000 {method 'find' of 'bytearray' objects}
        2    0.000    0.000    0.000    0.000 enum.py:670(__new__)
        1    0.000    0.000    0.000    0.000 sre_compile.py:456(_generate_overlap_table)
        1    0.000    0.000    0.000    0.000 sre_parse.py:928(fix_flags)
        1    0.000    0.000    0.000    0.000 {built-in method _sre.compile}
        1    0.000    0.000    0.000    0.000 functools.py:65(wraps)
        2    0.000    0.000    0.000    0.000 sre_compile.py:477(_get_iscased)
        2    0.000    0.000    0.000    0.000 sre_compile.py:81(_combine_flags)
        5    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}
        1    0.000    0.000    0.000    0.000 {built-in method fromkeys}
        1    0.000    0.000    0.000    0.000 inspect.py:73(isclass)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
        3    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'get' of 'mappingproxy' objects}
        1    0.000    0.000    0.000    0.000 sre_parse.py:169(__setitem__)
        1    0.000    0.000    0.000    0.000 eval_frame.py:232(nothing)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.iter}



Profile stats for: [LightningDataModule]PMTfiedDataModule.state_dict
         56 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        8    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        8    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        8    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        8    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        8    0.000    0.000    0.000    0.000 datamodule.py:150(state_dict)
        8    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        8    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_save_checkpoint
         80 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        8    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        8    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        8    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        8    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        8    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        8    0.000    0.000    0.000    0.000 callback.py:250(on_save_checkpoint)
        8    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        8    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        8    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        8    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_save_checkpoint
         80 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        8    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        8    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        8    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        8    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        8    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        8    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        8    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        8    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        8    0.000    0.000    0.000    0.000 callback.py:250(on_save_checkpoint)
        8    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_save_checkpoint
         80 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        8    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        8    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        8    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        8    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        8    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        8    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        8    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        8    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        8    0.000    0.000    0.000    0.000 callback.py:250(on_save_checkpoint)
        8    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_save_checkpoint
         80 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        8    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        8    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        8    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        8    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        8    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        8    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        8    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        8    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        8    0.000    0.000    0.000    0.000 callback.py:250(on_save_checkpoint)
        8    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_save_checkpoint
         56 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        8    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        8    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        8    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        8    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        8    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        8    0.000    0.000    0.000    0.000 hooks.py:692(on_save_checkpoint)
        8    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.lr_scheduler_step
         1130 function calls in 0.007 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000    0.007    0.000 module.py:1249(lr_scheduler_step)
      100    0.005    0.000    0.006    0.000 lr_scheduler.py:1316(step)
      100    0.001    0.000    0.001    0.000 lr_scheduler.py:1353(is_better)
      100    0.000    0.000    0.000    0.000 lr_scheduler.py:1349(in_cooldown)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
       15    0.000    0.000    0.000    0.000 lr_scheduler.py:1342(_reduce_lr)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 lr_scheduler.py:1340(<listcomp>)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
       15    0.000    0.000    0.000    0.000 {built-in method builtins.max}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_train_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:208(on_train_end)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)



Profile stats for: [Callback]TQDMProgressBar.on_train_end
         484 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:283(on_train_end)
        1    0.000    0.000    0.000    0.000 std.py:1265(close)
        1    0.000    0.000    0.000    0.000 std.py:1464(display)
        1    0.000    0.000    0.000    0.000 std.py:1150(__str__)
        1    0.000    0.000    0.000    0.000 std.py:464(format_meter)
        4    0.000    0.000    0.000    0.000 utils.py:194(inner)
        3    0.000    0.000    0.000    0.000 redirect.py:644(write)
        3    0.000    0.000    0.000    0.000 wandb_run.py:2304(<lambda>)
        3    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
        3    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
        3    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
        1    0.000    0.000    0.000    0.000 std.py:457(print_status)
        2    0.000    0.000    0.000    0.000 std.py:1286(fp_write)
        3    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
        3    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
        3    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
        3    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        3    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
        1    0.000    0.000    0.000    0.000 std.py:451(fp_write)
        1    0.000    0.000    0.000    0.000 utils.py:378(disp_len)
        3    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
        1    0.000    0.000    0.000    0.000 utils.py:374(_text_width)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.sum}
        3    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
       96    0.000    0.000    0.000    0.000 utils.py:375(<genexpr>)
        1    0.000    0.000    0.000    0.000 std.py:686(_decr_instances)
        1    0.000    0.000    0.000    0.000 std.py:1446(format_dict)
        3    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
        1    0.000    0.000    0.000    0.000 utils.py:333(_screen_shape_linux)
        3    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
        2    0.000    0.000    0.000    0.000 utils.py:273(_is_ascii)
       95    0.000    0.000    0.000    0.000 {built-in method unicodedata.east_asian_width}
        1    0.000    0.000    0.000    0.000 {built-in method fcntl.ioctl}
        2    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}
        1    0.000    0.000    0.000    0.000 _weakrefset.py:63(__iter__)
        1    0.000    0.000    0.000    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
        1    0.000    0.000    0.000    0.000 _weakrefset.py:111(remove)
        3    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        2    0.000    0.000    0.000    0.000 std.py:110(__enter__)
        1    0.000    0.000    0.000    0.000 utils.py:347(<listcomp>)
       89    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
        2    0.000    0.000    0.000    0.000 std.py:102(acquire)
        3    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
        2    0.000    0.000    0.000    0.000 {method 'remove' of 'set' objects}
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        3    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        2    0.000    0.000    0.000    0.000 std.py:400(format_interval)
        2    0.000    0.000    0.000    0.000 std.py:113(__exit__)
        1    0.000    0.000    0.000    0.000 os.py:674(__getitem__)
        2    0.000    0.000    0.000    0.000 std.py:106(release)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 _weakrefset.py:27(__exit__)
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        6    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        3    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 std.py:186(__format__)
        1    0.000    0.000    0.000    0.000 os.py:754(encode)
        1    0.000    0.000    0.000    0.000 utils.py:125(__eq__)
        3    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
        1    0.000    0.000    0.000    0.000 _weakrefset.py:53(_commit_removals)
        1    0.000    0.000    0.000    0.000 _weakrefset.py:21(__enter__)
        3    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
        2    0.000    0.000    0.000    0.000 std.py:1153(_comparable)
        3    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 std.py:153(__init__)
       10    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        3    0.000    0.000    0.000    0.000 {built-in method utcnow}
        3    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        1    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
        1    0.000    0.000    0.000    0.000 {built-in method now}
        1    0.000    0.000    0.000    0.000 std.py:1157(__hash__)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        3    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
        1    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
        7    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        2    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
        5    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
        3    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        2    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        3    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.max}
        1    0.000    0.000    0.000    0.000 _weakrefset.py:17(__init__)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        1    0.000    0.000    0.000    0.000 utils.py:112(__format__)
        3    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
        2    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
        2    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
        1    0.000    0.000    0.000    0.000 utils.py:108(__init__)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        1    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
        3    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
        1    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}
        1    0.000    0.000    0.000    0.000 std.py:167(colour)
        2    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:121(train_progress_bar)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {built-in method time.time}
        1    0.000    0.000    0.000    0.000 std.py:163(colour)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 std.py:1301(<lambda>)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)



Profile stats for: [Callback]ModelSummary.on_train_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 callback.py:208(on_train_end)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:208(on_train_end)



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_train_end
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 hooks.py:47(on_train_end)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Strategy]SingleDeviceStrategy.on_train_end
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 strategy.py:561(on_train_end)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_fit_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:67(on_fit_end)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_fit_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 callback.py:67(on_fit_end)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_fit_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:67(on_fit_end)



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 callback.py:67(on_fit_end)



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_fit_end
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 hooks.py:37(on_fit_end)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningDataModule]PMTfiedDataModule.teardown
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 hooks.py:447(teardown)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.teardown
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:61(teardown)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.teardown
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:61(teardown)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.teardown
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:61(teardown)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.teardown
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:61(teardown)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.teardown
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 hooks.py:447(teardown)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: epoch_avg_train_loss ‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:           train_loss ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:              val_acc ‚ñÜ‚ñÅ‚ñÅ‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:             val_loss ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                epoch 99
wandb: epoch_avg_train_loss 1.02238
wandb:           train_loss 1.02238
wandb:  trainer/global_step 99
wandb:              val_acc 0.33333
wandb:             val_loss 1.03869
wandb: 
wandb: üöÄ View run flowing-bird-1 at: https://wandb.ai/cyans-k-benhavns-universitet/%5B20250204_163459%5D%20Flavour%20Classification/runs/3nidyxib
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/cyans-k-benhavns-universitet/%5B20250204_163459%5D%20Flavour%20Classification
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250204_163508-3nidyxib/logs

| Parameter       | Value               |
|-----------------|---------------------|
| attention       | XFormers       |
| d_model         | 128            |
| n_heads         | 8              |
| d_f             | 128            |
| num_layers      | 3              |
| d_input         | 32             |
| num_classes     | 3              |
| dropout         | 0.1            |
| learning_rate   | 0.001          |
| epochs          | 100            |
| batch_size      | 32             |

######## Execution time: 00:01:03 ########
