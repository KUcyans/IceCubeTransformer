2025-02-04 16:59:27,684 - INFO - Starting training with the following parameters:
2025-02-04 16:59:27,685 - INFO - | Parameter       | Value               |
|-----------------|---------------------|
| attention       | Scaled Dot-Product|
| d_model         | 128            |
| n_heads         | 8              |
| d_f             | 128            |
| num_layers      | 3              |
| d_input         | 32             |
| num_classes     | 3              |
| dropout         | 0.1            |
| learning_rate   | 0.01           |
| epochs          | 500            |
| batch_size      | 32             |

2025-02-04 16:59:28,482 - INFO - Epoch 0: val_loss=1.2012, val_acc=33.33%
2025-02-04 16:59:28,636 - INFO - #################### Training epoch 0 ####################
2025-02-04 16:59:28,636 - INFO - Current Learning Rate: 1.000000e-02
2025-02-04 16:59:29,009 - INFO - Epoch 0: train_loss=1.1754
2025-02-04 16:59:29,703 - INFO - Epoch 0: val_loss=5.8121, val_acc=0.00%
2025-02-04 16:59:29,707 - INFO - Epoch 0: EPOCH_AVG_TRAIN_LOSS=1.1754
2025-02-04 16:59:29,736 - INFO - #################### Training epoch 1 ####################
2025-02-04 16:59:29,736 - INFO - Current Learning Rate: 1.000000e-02
2025-02-04 16:59:30,196 - INFO - Epoch 1: train_loss=3.3755
2025-02-04 16:59:30,953 - INFO - Epoch 1: val_loss=3.6450, val_acc=33.33%
2025-02-04 16:59:30,958 - INFO - Epoch 1: EPOCH_AVG_TRAIN_LOSS=3.3755
2025-02-04 16:59:30,992 - INFO - #################### Training epoch 2 ####################
2025-02-04 16:59:30,992 - INFO - Current Learning Rate: 1.000000e-02
2025-02-04 16:59:31,457 - INFO - Epoch 2: train_loss=3.3610
2025-02-04 16:59:32,203 - INFO - Epoch 2: val_loss=3.9709, val_acc=33.33%
2025-02-04 16:59:32,208 - INFO - Epoch 2: EPOCH_AVG_TRAIN_LOSS=3.3610
2025-02-04 16:59:32,251 - INFO - #################### Training epoch 3 ####################
2025-02-04 16:59:32,251 - INFO - Current Learning Rate: 1.000000e-02
2025-02-04 16:59:32,665 - INFO - Epoch 3: train_loss=3.3366
2025-02-04 16:59:33,456 - INFO - Epoch 3: val_loss=4.0971, val_acc=33.33%
2025-02-04 16:59:33,465 - INFO - Epoch 3: EPOCH_AVG_TRAIN_LOSS=3.3366
2025-02-04 16:59:33,541 - INFO - #################### Training epoch 4 ####################
2025-02-04 16:59:33,541 - INFO - Current Learning Rate: 1.000000e-02
2025-02-04 16:59:33,959 - INFO - Epoch 4: train_loss=3.2904
2025-02-04 16:59:34,774 - INFO - Epoch 4: val_loss=2.4398, val_acc=66.67%
2025-02-04 16:59:34,783 - INFO - Epoch 4: EPOCH_AVG_TRAIN_LOSS=3.2904
2025-02-04 16:59:34,847 - INFO - #################### Training epoch 5 ####################
2025-02-04 16:59:34,847 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:59:35,260 - INFO - Epoch 5: train_loss=3.8234
2025-02-04 16:59:36,031 - INFO - Epoch 5: val_loss=2.4579, val_acc=66.67%
2025-02-04 16:59:36,036 - INFO - Epoch 5: EPOCH_AVG_TRAIN_LOSS=3.8234
2025-02-04 16:59:36,100 - INFO - #################### Training epoch 6 ####################
2025-02-04 16:59:36,100 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:59:36,509 - INFO - Epoch 6: train_loss=4.0325
2025-02-04 16:59:37,266 - INFO - Epoch 6: val_loss=2.4233, val_acc=66.67%
2025-02-04 16:59:37,271 - INFO - Epoch 6: EPOCH_AVG_TRAIN_LOSS=4.0325
2025-02-04 16:59:37,318 - INFO - #################### Training epoch 7 ####################
2025-02-04 16:59:37,318 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:59:37,729 - INFO - Epoch 7: train_loss=4.0862
2025-02-04 16:59:38,486 - INFO - Epoch 7: val_loss=2.3937, val_acc=66.67%
2025-02-04 16:59:38,491 - INFO - Epoch 7: EPOCH_AVG_TRAIN_LOSS=4.0862
2025-02-04 16:59:38,560 - INFO - #################### Training epoch 8 ####################
2025-02-04 16:59:38,560 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:59:38,975 - INFO - Epoch 8: train_loss=4.1060
2025-02-04 16:59:39,791 - INFO - Epoch 8: val_loss=2.3683, val_acc=66.67%
2025-02-04 16:59:39,796 - INFO - Epoch 8: EPOCH_AVG_TRAIN_LOSS=4.1060
2025-02-04 16:59:39,843 - INFO - #################### Training epoch 9 ####################
2025-02-04 16:59:39,843 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:59:40,270 - INFO - Epoch 9: train_loss=4.1022
2025-02-04 16:59:41,114 - INFO - Epoch 9: val_loss=2.3495, val_acc=66.67%
2025-02-04 16:59:41,119 - INFO - Epoch 9: EPOCH_AVG_TRAIN_LOSS=4.1022
2025-02-04 16:59:41,166 - INFO - #################### Training epoch 10 ####################
2025-02-04 16:59:41,166 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:59:41,582 - INFO - Epoch 10: train_loss=4.0822
2025-02-04 16:59:42,364 - INFO - Epoch 10: val_loss=2.3135, val_acc=66.67%
2025-02-04 16:59:42,373 - INFO - Epoch 10: EPOCH_AVG_TRAIN_LOSS=4.0822
2025-02-04 16:59:42,439 - INFO - #################### Training epoch 11 ####################
2025-02-04 16:59:42,439 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:59:42,853 - INFO - Epoch 11: train_loss=4.0323
2025-02-04 16:59:43,644 - INFO - Epoch 11: val_loss=1.9391, val_acc=66.67%
2025-02-04 16:59:43,653 - INFO - Epoch 11: EPOCH_AVG_TRAIN_LOSS=4.0323
2025-02-04 16:59:43,721 - INFO - #################### Training epoch 12 ####################
2025-02-04 16:59:43,721 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:59:44,133 - INFO - Epoch 12: train_loss=3.3613
2025-02-04 16:59:44,942 - INFO - Epoch 12: val_loss=1.5146, val_acc=66.67%
2025-02-04 16:59:44,946 - INFO - Epoch 12: EPOCH_AVG_TRAIN_LOSS=3.3613
2025-02-04 16:59:44,993 - INFO - #################### Training epoch 13 ####################
2025-02-04 16:59:44,993 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:59:45,408 - INFO - Epoch 13: train_loss=2.6486
2025-02-04 16:59:46,214 - INFO - Epoch 13: val_loss=1.3198, val_acc=66.67%
2025-02-04 16:59:46,219 - INFO - Epoch 13: EPOCH_AVG_TRAIN_LOSS=2.6486
2025-02-04 16:59:46,266 - INFO - #################### Training epoch 14 ####################
2025-02-04 16:59:46,266 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:59:46,681 - INFO - Epoch 14: train_loss=2.3245
2025-02-04 16:59:47,525 - INFO - Epoch 14: val_loss=1.1577, val_acc=66.67%
2025-02-04 16:59:47,530 - INFO - Epoch 14: EPOCH_AVG_TRAIN_LOSS=2.3245
2025-02-04 16:59:47,588 - INFO - #################### Training epoch 15 ####################
2025-02-04 16:59:47,588 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:59:48,012 - INFO - Epoch 15: train_loss=2.0436
2025-02-04 16:59:48,887 - INFO - Epoch 15: val_loss=0.8871, val_acc=66.67%
2025-02-04 16:59:48,892 - INFO - Epoch 15: EPOCH_AVG_TRAIN_LOSS=2.0436
2025-02-04 16:59:48,930 - INFO - #################### Training epoch 16 ####################
2025-02-04 16:59:48,930 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:59:49,323 - INFO - Epoch 16: train_loss=1.4838
2025-02-04 16:59:50,129 - INFO - Epoch 16: val_loss=0.8611, val_acc=66.67%
2025-02-04 16:59:50,138 - INFO - Epoch 16: EPOCH_AVG_TRAIN_LOSS=1.4838
2025-02-04 16:59:50,205 - INFO - #################### Training epoch 17 ####################
2025-02-04 16:59:50,205 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:59:50,604 - INFO - Epoch 17: train_loss=1.1710
2025-02-04 16:59:51,477 - INFO - Epoch 17: val_loss=0.9461, val_acc=66.67%
2025-02-04 16:59:51,486 - INFO - Epoch 17: EPOCH_AVG_TRAIN_LOSS=1.1710
2025-02-04 16:59:51,532 - INFO - #################### Training epoch 18 ####################
2025-02-04 16:59:51,532 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:59:51,921 - INFO - Epoch 18: train_loss=1.1246
2025-02-04 16:59:52,750 - INFO - Epoch 18: val_loss=1.1610, val_acc=33.33%
2025-02-04 16:59:52,755 - INFO - Epoch 18: EPOCH_AVG_TRAIN_LOSS=1.1246
2025-02-04 16:59:52,765 - INFO - #################### Training epoch 19 ####################
2025-02-04 16:59:52,765 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:59:53,182 - INFO - Epoch 19: train_loss=1.1089
2025-02-04 16:59:53,959 - INFO - Epoch 19: val_loss=1.4006, val_acc=33.33%
2025-02-04 16:59:53,963 - INFO - Epoch 19: EPOCH_AVG_TRAIN_LOSS=1.1089
2025-02-04 16:59:53,974 - INFO - #################### Training epoch 20 ####################
2025-02-04 16:59:53,974 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:59:54,380 - INFO - Epoch 20: train_loss=1.1446
2025-02-04 16:59:55,065 - INFO - Epoch 20: val_loss=1.6307, val_acc=33.33%
2025-02-04 16:59:55,070 - INFO - Epoch 20: EPOCH_AVG_TRAIN_LOSS=1.1446
2025-02-04 16:59:55,080 - INFO - #################### Training epoch 21 ####################
2025-02-04 16:59:55,080 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:59:55,464 - INFO - Epoch 21: train_loss=1.2017
2025-02-04 16:59:56,109 - INFO - Epoch 21: val_loss=1.8370, val_acc=0.00%
2025-02-04 16:59:56,113 - INFO - Epoch 21: EPOCH_AVG_TRAIN_LOSS=1.2017
2025-02-04 16:59:56,116 - INFO - #################### Training epoch 22 ####################
2025-02-04 16:59:56,116 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:59:56,611 - INFO - Epoch 22: train_loss=1.2644
2025-02-04 16:59:57,286 - INFO - Epoch 22: val_loss=1.7409, val_acc=0.00%
2025-02-04 16:59:57,291 - INFO - Epoch 22: EPOCH_AVG_TRAIN_LOSS=1.2644
2025-02-04 16:59:57,301 - INFO - #################### Training epoch 23 ####################
2025-02-04 16:59:57,301 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:59:57,733 - INFO - Epoch 23: train_loss=1.2250
2025-02-04 16:59:58,523 - INFO - Epoch 23: val_loss=1.5193, val_acc=0.00%
2025-02-04 16:59:58,527 - INFO - Epoch 23: EPOCH_AVG_TRAIN_LOSS=1.2250
2025-02-04 16:59:58,537 - INFO - #################### Training epoch 24 ####################
2025-02-04 16:59:58,537 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:59:59,019 - INFO - Epoch 24: train_loss=1.1603
2025-02-04 16:59:59,781 - INFO - Epoch 24: val_loss=1.4264, val_acc=0.00%
2025-02-04 16:59:59,786 - INFO - Epoch 24: EPOCH_AVG_TRAIN_LOSS=1.1603
2025-02-04 16:59:59,796 - INFO - #################### Training epoch 25 ####################
2025-02-04 16:59:59,796 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 17:00:00,202 - INFO - Epoch 25: train_loss=1.1371
2025-02-04 17:00:00,974 - INFO - Epoch 25: val_loss=1.3467, val_acc=0.00%
2025-02-04 17:00:00,983 - INFO - Epoch 25: EPOCH_AVG_TRAIN_LOSS=1.1371
2025-02-04 17:00:01,002 - INFO - #################### Training epoch 26 ####################
2025-02-04 17:00:01,002 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 17:00:01,408 - INFO - Epoch 26: train_loss=1.1225
2025-02-04 17:00:02,092 - INFO - Epoch 26: val_loss=1.2824, val_acc=0.00%
2025-02-04 17:00:02,101 - INFO - Epoch 26: EPOCH_AVG_TRAIN_LOSS=1.1225
2025-02-04 17:00:02,120 - INFO - #################### Training epoch 27 ####################
2025-02-04 17:00:02,120 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 17:00:02,491 - INFO - Epoch 27: train_loss=1.1115
2025-02-04 17:00:03,116 - INFO - Epoch 27: val_loss=1.1870, val_acc=0.00%
2025-02-04 17:00:03,120 - INFO - Epoch 27: EPOCH_AVG_TRAIN_LOSS=1.1115
2025-02-04 17:00:03,131 - INFO - #################### Training epoch 28 ####################
2025-02-04 17:00:03,131 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 17:00:03,644 - INFO - Epoch 28: train_loss=1.1009
2025-02-04 17:00:04,290 - INFO - Epoch 28: val_loss=1.1466, val_acc=0.00%
2025-02-04 17:00:04,295 - INFO - Epoch 28: EPOCH_AVG_TRAIN_LOSS=1.1009
2025-02-04 17:00:04,306 - INFO - #################### Training epoch 29 ####################
2025-02-04 17:00:04,306 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 17:00:04,807 - INFO - Epoch 29: train_loss=1.0980
2025-02-04 17:00:05,517 - INFO - Epoch 29: val_loss=1.1133, val_acc=0.00%
2025-02-04 17:00:05,522 - INFO - Epoch 29: EPOCH_AVG_TRAIN_LOSS=1.0980
2025-02-04 17:00:05,532 - INFO - #################### Training epoch 30 ####################
2025-02-04 17:00:05,532 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 17:00:05,960 - INFO - Epoch 30: train_loss=1.0987
2025-02-04 17:00:06,769 - INFO - Epoch 30: val_loss=1.0841, val_acc=66.67%
2025-02-04 17:00:06,773 - INFO - Epoch 30: EPOCH_AVG_TRAIN_LOSS=1.0987
2025-02-04 17:00:06,783 - INFO - #################### Training epoch 31 ####################
2025-02-04 17:00:06,783 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 17:00:07,218 - INFO - Epoch 31: train_loss=1.1002
2025-02-04 17:00:07,976 - INFO - Epoch 31: val_loss=1.0585, val_acc=66.67%
2025-02-04 17:00:07,981 - INFO - Epoch 31: EPOCH_AVG_TRAIN_LOSS=1.1002
2025-02-04 17:00:07,993 - INFO - #################### Training epoch 32 ####################
2025-02-04 17:00:07,993 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 17:00:08,400 - INFO - Epoch 32: train_loss=1.1005
2025-02-04 17:00:09,086 - INFO - Epoch 32: val_loss=1.0472, val_acc=66.67%
2025-02-04 17:00:09,095 - INFO - Epoch 32: EPOCH_AVG_TRAIN_LOSS=1.1005
2025-02-04 17:00:09,114 - INFO - #################### Training epoch 33 ####################
2025-02-04 17:00:09,114 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 17:00:09,486 - INFO - Epoch 33: train_loss=1.1007
2025-02-04 17:00:10,102 - INFO - Epoch 33: val_loss=1.0488, val_acc=66.67%
2025-02-04 17:00:10,107 - INFO - Epoch 33: EPOCH_AVG_TRAIN_LOSS=1.1007
2025-02-04 17:00:10,117 - INFO - #################### Training epoch 34 ####################
2025-02-04 17:00:10,117 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 17:00:10,638 - INFO - Epoch 34: train_loss=1.0999
2025-02-04 17:00:11,288 - INFO - Epoch 34: val_loss=1.0497, val_acc=66.67%
2025-02-04 17:00:11,293 - INFO - Epoch 34: EPOCH_AVG_TRAIN_LOSS=1.0999
2025-02-04 17:00:11,303 - INFO - #################### Training epoch 35 ####################
2025-02-04 17:00:11,303 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 17:00:11,805 - INFO - Epoch 35: train_loss=1.1001
2025-02-04 17:00:12,522 - INFO - Epoch 35: val_loss=1.0512, val_acc=66.67%
2025-02-04 17:00:12,527 - INFO - Epoch 35: EPOCH_AVG_TRAIN_LOSS=1.1001
2025-02-04 17:00:12,538 - INFO - #################### Training epoch 36 ####################
2025-02-04 17:00:12,538 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 17:00:12,964 - INFO - Epoch 36: train_loss=1.0998
2025-02-04 17:00:13,764 - INFO - Epoch 36: val_loss=1.0524, val_acc=33.33%
2025-02-04 17:00:13,769 - INFO - Epoch 36: EPOCH_AVG_TRAIN_LOSS=1.0998
2025-02-04 17:00:13,779 - INFO - #################### Training epoch 37 ####################
2025-02-04 17:00:13,779 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 17:00:14,236 - INFO - Epoch 37: train_loss=1.1014
2025-02-04 17:00:14,984 - INFO - Epoch 37: val_loss=1.0534, val_acc=33.33%
2025-02-04 17:00:14,989 - INFO - Epoch 37: EPOCH_AVG_TRAIN_LOSS=1.1014
2025-02-04 17:00:15,000 - INFO - #################### Training epoch 38 ####################
2025-02-04 17:00:15,000 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 17:00:15,407 - INFO - Epoch 38: train_loss=1.0995
2025-02-04 17:00:16,111 - INFO - Epoch 38: val_loss=1.0545, val_acc=33.33%
2025-02-04 17:00:16,120 - INFO - Epoch 38: EPOCH_AVG_TRAIN_LOSS=1.0995
2025-02-04 17:00:16,140 - INFO - #################### Training epoch 39 ####################
2025-02-04 17:00:16,140 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 17:00:16,516 - INFO - Epoch 39: train_loss=1.1001
2025-02-04 17:00:17,118 - INFO - Epoch 39: val_loss=1.0575, val_acc=33.33%
2025-02-04 17:00:17,123 - INFO - Epoch 39: EPOCH_AVG_TRAIN_LOSS=1.1001
2025-02-04 17:00:17,134 - INFO - #################### Training epoch 40 ####################
2025-02-04 17:00:17,134 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 17:00:17,655 - INFO - Epoch 40: train_loss=1.1012
2025-02-04 17:00:18,288 - INFO - Epoch 40: val_loss=1.0627, val_acc=33.33%
2025-02-04 17:00:18,293 - INFO - Epoch 40: EPOCH_AVG_TRAIN_LOSS=1.1012
2025-02-04 17:00:18,304 - INFO - #################### Training epoch 41 ####################
2025-02-04 17:00:18,304 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 17:00:18,806 - INFO - Epoch 41: train_loss=1.0991
2025-02-04 17:00:19,533 - INFO - Epoch 41: val_loss=1.0670, val_acc=33.33%
2025-02-04 17:00:19,538 - INFO - Epoch 41: EPOCH_AVG_TRAIN_LOSS=1.0991
2025-02-04 17:00:19,549 - INFO - #################### Training epoch 42 ####################
2025-02-04 17:00:19,549 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 17:00:19,960 - INFO - Epoch 42: train_loss=1.0983
2025-02-04 17:00:20,762 - INFO - Epoch 42: val_loss=1.0693, val_acc=33.33%
2025-02-04 17:00:20,767 - INFO - Epoch 42: EPOCH_AVG_TRAIN_LOSS=1.0983
2025-02-04 17:00:20,777 - INFO - #################### Training epoch 43 ####################
2025-02-04 17:00:20,777 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 17:00:21,205 - INFO - Epoch 43: train_loss=1.0975
2025-02-04 17:00:22,039 - INFO - Epoch 43: val_loss=1.0708, val_acc=33.33%
2025-02-04 17:00:22,044 - INFO - Epoch 43: EPOCH_AVG_TRAIN_LOSS=1.0975
2025-02-04 17:00:22,054 - INFO - #################### Training epoch 44 ####################
2025-02-04 17:00:22,054 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 17:00:22,501 - INFO - Epoch 44: train_loss=1.0983
2025-02-04 17:00:23,259 - INFO - Epoch 44: val_loss=1.0739, val_acc=33.33%
2025-02-04 17:00:23,263 - INFO - Epoch 44: EPOCH_AVG_TRAIN_LOSS=1.0983
2025-02-04 17:00:23,274 - INFO - #################### Training epoch 45 ####################
2025-02-04 17:00:23,274 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 17:00:23,685 - INFO - Epoch 45: train_loss=1.0972
2025-02-04 17:00:24,470 - INFO - Epoch 45: val_loss=1.0763, val_acc=33.33%
2025-02-04 17:00:24,479 - INFO - Epoch 45: EPOCH_AVG_TRAIN_LOSS=1.0972
2025-02-04 17:00:24,498 - INFO - #################### Training epoch 46 ####################
2025-02-04 17:00:24,498 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 17:00:24,909 - INFO - Epoch 46: train_loss=1.0992
2025-02-04 17:00:25,608 - INFO - Epoch 46: val_loss=1.0788, val_acc=33.33%
2025-02-04 17:00:25,618 - INFO - Epoch 46: EPOCH_AVG_TRAIN_LOSS=1.0992
2025-02-04 17:00:25,637 - INFO - #################### Training epoch 47 ####################
2025-02-04 17:00:25,637 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 17:00:26,019 - INFO - Epoch 47: train_loss=1.0973
2025-02-04 17:00:26,711 - INFO - Epoch 47: val_loss=1.0805, val_acc=33.33%
2025-02-04 17:00:26,715 - INFO - Epoch 47: EPOCH_AVG_TRAIN_LOSS=1.0973
2025-02-04 17:00:26,725 - INFO - #################### Training epoch 48 ####################
2025-02-04 17:00:26,725 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 17:00:27,131 - INFO - Epoch 48: train_loss=1.0979
2025-02-04 17:00:27,773 - INFO - Epoch 48: val_loss=1.0823, val_acc=33.33%
2025-02-04 17:00:27,777 - INFO - Epoch 48: EPOCH_AVG_TRAIN_LOSS=1.0979
2025-02-04 17:00:27,788 - INFO - #################### Training epoch 49 ####################
2025-02-04 17:00:27,788 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 17:00:28,288 - INFO - Epoch 49: train_loss=1.0979
2025-02-04 17:00:28,965 - INFO - Epoch 49: val_loss=1.0841, val_acc=33.33%
2025-02-04 17:00:28,970 - INFO - Epoch 49: EPOCH_AVG_TRAIN_LOSS=1.0979
2025-02-04 17:00:28,981 - INFO - #################### Training epoch 50 ####################
2025-02-04 17:00:28,981 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 17:00:29,412 - INFO - Epoch 50: train_loss=1.0971
2025-02-04 17:00:30,202 - INFO - Epoch 50: val_loss=1.0848, val_acc=33.33%
2025-02-04 17:00:30,207 - INFO - Epoch 50: EPOCH_AVG_TRAIN_LOSS=1.0971
2025-02-04 17:00:30,217 - INFO - #################### Training epoch 51 ####################
2025-02-04 17:00:30,217 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 17:00:30,645 - INFO - Epoch 51: train_loss=1.0977
2025-02-04 17:00:31,500 - INFO - Epoch 51: val_loss=1.0854, val_acc=33.33%
2025-02-04 17:00:31,505 - INFO - Epoch 51: EPOCH_AVG_TRAIN_LOSS=1.0977
2025-02-04 17:00:31,515 - INFO - #################### Training epoch 52 ####################
2025-02-04 17:00:31,516 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 17:00:31,928 - INFO - Epoch 52: train_loss=1.0976
2025-02-04 17:00:32,717 - INFO - Epoch 52: val_loss=1.0863, val_acc=33.33%
2025-02-04 17:00:32,726 - INFO - Epoch 52: EPOCH_AVG_TRAIN_LOSS=1.0976
2025-02-04 17:00:32,745 - INFO - #################### Training epoch 53 ####################
2025-02-04 17:00:32,746 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 17:00:33,157 - INFO - Epoch 53: train_loss=1.0980
2025-02-04 17:00:33,852 - INFO - Epoch 53: val_loss=1.0867, val_acc=33.33%
2025-02-04 17:00:33,861 - INFO - Epoch 53: EPOCH_AVG_TRAIN_LOSS=1.0980
2025-02-04 17:00:33,880 - INFO - #################### Training epoch 54 ####################
2025-02-04 17:00:33,880 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 17:00:34,258 - INFO - Epoch 54: train_loss=1.0992
2025-02-04 17:00:34,947 - INFO - Epoch 54: val_loss=1.0870, val_acc=33.33%
2025-02-04 17:00:34,952 - INFO - Epoch 54: EPOCH_AVG_TRAIN_LOSS=1.0992
2025-02-04 17:00:34,962 - INFO - #################### Training epoch 55 ####################
2025-02-04 17:00:34,962 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 17:00:35,377 - INFO - Epoch 55: train_loss=1.0970
2025-02-04 17:00:36,014 - INFO - Epoch 55: val_loss=1.0876, val_acc=33.33%
2025-02-04 17:00:36,018 - INFO - Epoch 55: EPOCH_AVG_TRAIN_LOSS=1.0970
2025-02-04 17:00:36,029 - INFO - #################### Training epoch 56 ####################
2025-02-04 17:00:36,029 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 17:00:36,531 - INFO - Epoch 56: train_loss=1.0954
2025-02-04 17:00:37,220 - INFO - Epoch 56: val_loss=1.0887, val_acc=33.33%
2025-02-04 17:00:37,224 - INFO - Epoch 56: EPOCH_AVG_TRAIN_LOSS=1.0954
2025-02-04 17:00:37,235 - INFO - #################### Training epoch 57 ####################
2025-02-04 17:00:37,235 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 17:00:37,666 - INFO - Epoch 57: train_loss=1.0972
2025-02-04 17:00:38,456 - INFO - Epoch 57: val_loss=1.0879, val_acc=33.33%
2025-02-04 17:00:38,461 - INFO - Epoch 57: EPOCH_AVG_TRAIN_LOSS=1.0972
2025-02-04 17:00:38,471 - INFO - #################### Training epoch 58 ####################
2025-02-04 17:00:38,471 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 17:00:38,901 - INFO - Epoch 58: train_loss=1.0963
2025-02-04 17:00:39,747 - INFO - Epoch 58: val_loss=1.0883, val_acc=33.33%
2025-02-04 17:00:39,752 - INFO - Epoch 58: EPOCH_AVG_TRAIN_LOSS=1.0963
2025-02-04 17:00:39,763 - INFO - #################### Training epoch 59 ####################
2025-02-04 17:00:39,763 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 17:00:40,171 - INFO - Epoch 59: train_loss=1.0965
2025-02-04 17:00:40,955 - INFO - Epoch 59: val_loss=1.0887, val_acc=33.33%
2025-02-04 17:00:40,964 - INFO - Epoch 59: EPOCH_AVG_TRAIN_LOSS=1.0965
2025-02-04 17:00:40,983 - INFO - #################### Training epoch 60 ####################
2025-02-04 17:00:40,983 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 17:00:41,392 - INFO - Epoch 60: train_loss=1.0973
2025-02-04 17:00:42,081 - INFO - Epoch 60: val_loss=1.0891, val_acc=33.33%
2025-02-04 17:00:42,090 - INFO - Epoch 60: EPOCH_AVG_TRAIN_LOSS=1.0973
2025-02-04 17:00:42,109 - INFO - #################### Training epoch 61 ####################
2025-02-04 17:00:42,109 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 17:00:42,476 - INFO - Epoch 61: train_loss=1.0977
2025-02-04 17:00:43,104 - INFO - Epoch 61: val_loss=1.0894, val_acc=33.33%
2025-02-04 17:00:43,109 - INFO - Epoch 61: EPOCH_AVG_TRAIN_LOSS=1.0977
2025-02-04 17:00:43,119 - INFO - #################### Training epoch 62 ####################
2025-02-04 17:00:43,119 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 17:00:43,631 - INFO - Epoch 62: train_loss=1.0968
2025-02-04 17:00:44,277 - INFO - Epoch 62: val_loss=1.0897, val_acc=33.33%
2025-02-04 17:00:44,282 - INFO - Epoch 62: EPOCH_AVG_TRAIN_LOSS=1.0968
2025-02-04 17:00:44,293 - INFO - #################### Training epoch 63 ####################
2025-02-04 17:00:44,293 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 17:00:44,796 - INFO - Epoch 63: train_loss=1.0968
2025-02-04 17:00:45,512 - INFO - Epoch 63: val_loss=1.0903, val_acc=33.33%
2025-02-04 17:00:45,517 - INFO - Epoch 63: EPOCH_AVG_TRAIN_LOSS=1.0968
2025-02-04 17:00:45,528 - INFO - #################### Training epoch 64 ####################
2025-02-04 17:00:45,528 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 17:00:45,958 - INFO - Epoch 64: train_loss=1.0975
2025-02-04 17:00:46,763 - INFO - Epoch 64: val_loss=1.0902, val_acc=33.33%
2025-02-04 17:00:46,767 - INFO - Epoch 64: EPOCH_AVG_TRAIN_LOSS=1.0975
2025-02-04 17:00:46,778 - INFO - #################### Training epoch 65 ####################
2025-02-04 17:00:46,778 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 17:00:47,218 - INFO - Epoch 65: train_loss=1.0971
2025-02-04 17:00:47,968 - INFO - Epoch 65: val_loss=1.0904, val_acc=33.33%
2025-02-04 17:00:47,973 - INFO - Epoch 65: EPOCH_AVG_TRAIN_LOSS=1.0971
2025-02-04 17:00:47,984 - INFO - #################### Training epoch 66 ####################
2025-02-04 17:00:47,984 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 17:00:48,382 - INFO - Epoch 66: train_loss=1.0963
2025-02-04 17:00:49,070 - INFO - Epoch 66: val_loss=1.0906, val_acc=33.33%
2025-02-04 17:00:49,075 - INFO - Epoch 66: EPOCH_AVG_TRAIN_LOSS=1.0963
2025-02-04 17:00:49,088 - INFO - #################### Training epoch 67 ####################
2025-02-04 17:00:49,088 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 17:00:49,472 - INFO - Epoch 67: train_loss=1.0958
2025-02-04 17:00:50,084 - INFO - Epoch 67: val_loss=1.0906, val_acc=33.33%
2025-02-04 17:00:50,089 - INFO - Epoch 67: EPOCH_AVG_TRAIN_LOSS=1.0958
2025-02-04 17:00:50,100 - INFO - #################### Training epoch 68 ####################
2025-02-04 17:00:50,100 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 17:00:50,628 - INFO - Epoch 68: train_loss=1.0981
2025-02-04 17:00:51,287 - INFO - Epoch 68: val_loss=1.0911, val_acc=33.33%
2025-02-04 17:00:51,292 - INFO - Epoch 68: EPOCH_AVG_TRAIN_LOSS=1.0981
2025-02-04 17:00:51,303 - INFO - #################### Training epoch 69 ####################
2025-02-04 17:00:51,303 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 17:00:51,797 - INFO - Epoch 69: train_loss=1.0977
2025-02-04 17:00:52,511 - INFO - Epoch 69: val_loss=1.0913, val_acc=33.33%
2025-02-04 17:00:52,516 - INFO - Epoch 69: EPOCH_AVG_TRAIN_LOSS=1.0977
2025-02-04 17:00:52,526 - INFO - #################### Training epoch 70 ####################
2025-02-04 17:00:52,526 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 17:00:52,949 - INFO - Epoch 70: train_loss=1.0966
2025-02-04 17:00:53,750 - INFO - Epoch 70: val_loss=1.0910, val_acc=33.33%
2025-02-04 17:00:53,755 - INFO - Epoch 70: EPOCH_AVG_TRAIN_LOSS=1.0966
2025-02-04 17:00:53,765 - INFO - #################### Training epoch 71 ####################
2025-02-04 17:00:53,765 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 17:00:54,206 - INFO - Epoch 71: train_loss=1.0970
2025-02-04 17:00:54,958 - INFO - Epoch 71: val_loss=1.0911, val_acc=33.33%
2025-02-04 17:00:54,963 - INFO - Epoch 71: EPOCH_AVG_TRAIN_LOSS=1.0970
2025-02-04 17:00:54,974 - INFO - #################### Training epoch 72 ####################
2025-02-04 17:00:54,974 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 17:00:55,377 - INFO - Epoch 72: train_loss=1.0971
2025-02-04 17:00:56,070 - INFO - Epoch 72: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:00:56,075 - INFO - Epoch 72: EPOCH_AVG_TRAIN_LOSS=1.0971
2025-02-04 17:00:56,090 - INFO - #################### Training epoch 73 ####################
2025-02-04 17:00:56,090 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 17:00:56,467 - INFO - Epoch 73: train_loss=1.0968
2025-02-04 17:00:57,080 - INFO - Epoch 73: val_loss=1.0911, val_acc=33.33%
2025-02-04 17:00:57,084 - INFO - Epoch 73: EPOCH_AVG_TRAIN_LOSS=1.0968
2025-02-04 17:00:57,095 - INFO - #################### Training epoch 74 ####################
2025-02-04 17:00:57,095 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 17:00:57,621 - INFO - Epoch 74: train_loss=1.0980
2025-02-04 17:00:58,243 - INFO - Epoch 74: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:00:58,249 - INFO - Epoch 74: EPOCH_AVG_TRAIN_LOSS=1.0980
2025-02-04 17:00:58,260 - INFO - #################### Training epoch 75 ####################
2025-02-04 17:00:58,260 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 17:00:58,799 - INFO - Epoch 75: train_loss=1.0982
2025-02-04 17:00:59,536 - INFO - Epoch 75: val_loss=1.0912, val_acc=33.33%
2025-02-04 17:00:59,541 - INFO - Epoch 75: EPOCH_AVG_TRAIN_LOSS=1.0982
2025-02-04 17:00:59,552 - INFO - #################### Training epoch 76 ####################
2025-02-04 17:00:59,552 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 17:01:00,044 - INFO - Epoch 76: train_loss=1.0978
2025-02-04 17:01:00,755 - INFO - Epoch 76: val_loss=1.0924, val_acc=33.33%
2025-02-04 17:01:00,759 - INFO - Epoch 76: EPOCH_AVG_TRAIN_LOSS=1.0978
2025-02-04 17:01:00,769 - INFO - #################### Training epoch 77 ####################
2025-02-04 17:01:00,769 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 17:01:01,203 - INFO - Epoch 77: train_loss=1.0966
2025-02-04 17:01:01,896 - INFO - Epoch 77: val_loss=1.0913, val_acc=33.33%
2025-02-04 17:01:01,901 - INFO - Epoch 77: EPOCH_AVG_TRAIN_LOSS=1.0966
2025-02-04 17:01:01,911 - INFO - #################### Training epoch 78 ####################
2025-02-04 17:01:01,911 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 17:01:02,344 - INFO - Epoch 78: train_loss=1.0964
2025-02-04 17:01:03,050 - INFO - Epoch 78: val_loss=1.0913, val_acc=33.33%
2025-02-04 17:01:03,055 - INFO - Epoch 78: EPOCH_AVG_TRAIN_LOSS=1.0964
2025-02-04 17:01:03,065 - INFO - #################### Training epoch 79 ####################
2025-02-04 17:01:03,065 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 17:01:03,506 - INFO - Epoch 79: train_loss=1.0973
2025-02-04 17:01:04,209 - INFO - Epoch 79: val_loss=1.0927, val_acc=33.33%
2025-02-04 17:01:04,214 - INFO - Epoch 79: EPOCH_AVG_TRAIN_LOSS=1.0973
2025-02-04 17:01:04,224 - INFO - #################### Training epoch 80 ####################
2025-02-04 17:01:04,224 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 17:01:04,677 - INFO - Epoch 80: train_loss=1.0977
2025-02-04 17:01:05,387 - INFO - Epoch 80: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:01:05,391 - INFO - Epoch 80: EPOCH_AVG_TRAIN_LOSS=1.0977
2025-02-04 17:01:05,402 - INFO - #################### Training epoch 81 ####################
2025-02-04 17:01:05,402 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 17:01:05,833 - INFO - Epoch 81: train_loss=1.0990
2025-02-04 17:01:06,630 - INFO - Epoch 81: val_loss=1.0914, val_acc=33.33%
2025-02-04 17:01:06,635 - INFO - Epoch 81: EPOCH_AVG_TRAIN_LOSS=1.0990
2025-02-04 17:01:06,646 - INFO - #################### Training epoch 82 ####################
2025-02-04 17:01:06,646 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 17:01:07,098 - INFO - Epoch 82: train_loss=1.0954
2025-02-04 17:01:07,828 - INFO - Epoch 82: val_loss=1.0917, val_acc=33.33%
2025-02-04 17:01:07,833 - INFO - Epoch 82: EPOCH_AVG_TRAIN_LOSS=1.0954
2025-02-04 17:01:07,843 - INFO - #################### Training epoch 83 ####################
2025-02-04 17:01:07,843 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 17:01:08,242 - INFO - Epoch 83: train_loss=1.0968
2025-02-04 17:01:08,927 - INFO - Epoch 83: val_loss=1.0917, val_acc=33.33%
2025-02-04 17:01:08,932 - INFO - Epoch 83: EPOCH_AVG_TRAIN_LOSS=1.0968
2025-02-04 17:01:08,943 - INFO - #################### Training epoch 84 ####################
2025-02-04 17:01:08,943 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 17:01:09,331 - INFO - Epoch 84: train_loss=1.0969
2025-02-04 17:01:09,953 - INFO - Epoch 84: val_loss=1.0914, val_acc=33.33%
2025-02-04 17:01:09,957 - INFO - Epoch 84: EPOCH_AVG_TRAIN_LOSS=1.0969
2025-02-04 17:01:09,962 - INFO - #################### Training epoch 85 ####################
2025-02-04 17:01:09,962 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 17:01:10,539 - INFO - Epoch 85: train_loss=1.0977
2025-02-04 17:01:11,157 - INFO - Epoch 85: val_loss=1.0914, val_acc=33.33%
2025-02-04 17:01:11,163 - INFO - Epoch 85: EPOCH_AVG_TRAIN_LOSS=1.0977
2025-02-04 17:01:11,173 - INFO - #################### Training epoch 86 ####################
2025-02-04 17:01:11,173 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 17:01:11,717 - INFO - Epoch 86: train_loss=1.0981
2025-02-04 17:01:12,442 - INFO - Epoch 86: val_loss=1.0914, val_acc=33.33%
2025-02-04 17:01:12,448 - INFO - Epoch 86: EPOCH_AVG_TRAIN_LOSS=1.0981
2025-02-04 17:01:12,459 - INFO - #################### Training epoch 87 ####################
2025-02-04 17:01:12,459 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 17:01:12,929 - INFO - Epoch 87: train_loss=1.0982
2025-02-04 17:01:13,633 - INFO - Epoch 87: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:01:13,638 - INFO - Epoch 87: EPOCH_AVG_TRAIN_LOSS=1.0982
2025-02-04 17:01:13,648 - INFO - #################### Training epoch 88 ####################
2025-02-04 17:01:13,648 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 17:01:14,079 - INFO - Epoch 88: train_loss=1.0970
2025-02-04 17:01:14,789 - INFO - Epoch 88: val_loss=1.0914, val_acc=33.33%
2025-02-04 17:01:14,794 - INFO - Epoch 88: EPOCH_AVG_TRAIN_LOSS=1.0970
2025-02-04 17:01:14,804 - INFO - #################### Training epoch 89 ####################
2025-02-04 17:01:14,804 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:15,229 - INFO - Epoch 89: train_loss=1.0975
2025-02-04 17:01:16,017 - INFO - Epoch 89: val_loss=1.0917, val_acc=33.33%
2025-02-04 17:01:16,022 - INFO - Epoch 89: EPOCH_AVG_TRAIN_LOSS=1.0975
2025-02-04 17:01:16,032 - INFO - #################### Training epoch 90 ####################
2025-02-04 17:01:16,033 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:16,488 - INFO - Epoch 90: train_loss=1.0975
2025-02-04 17:01:17,212 - INFO - Epoch 90: val_loss=1.0918, val_acc=33.33%
2025-02-04 17:01:17,217 - INFO - Epoch 90: EPOCH_AVG_TRAIN_LOSS=1.0975
2025-02-04 17:01:17,228 - INFO - #################### Training epoch 91 ####################
2025-02-04 17:01:17,228 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:17,632 - INFO - Epoch 91: train_loss=1.0975
2025-02-04 17:01:18,322 - INFO - Epoch 91: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:01:18,327 - INFO - Epoch 91: EPOCH_AVG_TRAIN_LOSS=1.0975
2025-02-04 17:01:18,338 - INFO - #################### Training epoch 92 ####################
2025-02-04 17:01:18,338 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:18,723 - INFO - Epoch 92: train_loss=1.0971
2025-02-04 17:01:19,350 - INFO - Epoch 92: val_loss=1.0918, val_acc=33.33%
2025-02-04 17:01:19,355 - INFO - Epoch 92: EPOCH_AVG_TRAIN_LOSS=1.0971
2025-02-04 17:01:19,366 - INFO - #################### Training epoch 93 ####################
2025-02-04 17:01:19,366 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:19,893 - INFO - Epoch 93: train_loss=1.0972
2025-02-04 17:01:20,529 - INFO - Epoch 93: val_loss=1.0918, val_acc=33.33%
2025-02-04 17:01:20,533 - INFO - Epoch 93: EPOCH_AVG_TRAIN_LOSS=1.0972
2025-02-04 17:01:20,544 - INFO - #################### Training epoch 94 ####################
2025-02-04 17:01:20,544 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:21,087 - INFO - Epoch 94: train_loss=1.0960
2025-02-04 17:01:21,820 - INFO - Epoch 94: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:01:21,825 - INFO - Epoch 94: EPOCH_AVG_TRAIN_LOSS=1.0960
2025-02-04 17:01:21,836 - INFO - #################### Training epoch 95 ####################
2025-02-04 17:01:21,836 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:22,327 - INFO - Epoch 95: train_loss=1.0973
2025-02-04 17:01:23,052 - INFO - Epoch 95: val_loss=1.0914, val_acc=33.33%
2025-02-04 17:01:23,057 - INFO - Epoch 95: EPOCH_AVG_TRAIN_LOSS=1.0973
2025-02-04 17:01:23,067 - INFO - #################### Training epoch 96 ####################
2025-02-04 17:01:23,067 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:23,500 - INFO - Epoch 96: train_loss=1.0981
2025-02-04 17:01:24,196 - INFO - Epoch 96: val_loss=1.0917, val_acc=33.33%
2025-02-04 17:01:24,200 - INFO - Epoch 96: EPOCH_AVG_TRAIN_LOSS=1.0981
2025-02-04 17:01:24,210 - INFO - #################### Training epoch 97 ####################
2025-02-04 17:01:24,211 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:24,648 - INFO - Epoch 97: train_loss=1.0980
2025-02-04 17:01:25,376 - INFO - Epoch 97: val_loss=1.0914, val_acc=33.33%
2025-02-04 17:01:25,380 - INFO - Epoch 97: EPOCH_AVG_TRAIN_LOSS=1.0980
2025-02-04 17:01:25,383 - INFO - #################### Training epoch 98 ####################
2025-02-04 17:01:25,383 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:25,986 - INFO - Epoch 98: train_loss=1.0988
2025-02-04 17:01:26,629 - INFO - Epoch 98: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:01:26,634 - INFO - Epoch 98: EPOCH_AVG_TRAIN_LOSS=1.0988
2025-02-04 17:01:26,645 - INFO - #################### Training epoch 99 ####################
2025-02-04 17:01:26,645 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:27,137 - INFO - Epoch 99: train_loss=1.0960
2025-02-04 17:01:27,853 - INFO - Epoch 99: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:01:27,858 - INFO - Epoch 99: EPOCH_AVG_TRAIN_LOSS=1.0960
2025-02-04 17:01:27,868 - INFO - #################### Training epoch 100 ####################
2025-02-04 17:01:27,868 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:28,285 - INFO - Epoch 100: train_loss=1.0968
2025-02-04 17:01:28,967 - INFO - Epoch 100: val_loss=1.0914, val_acc=33.33%
2025-02-04 17:01:28,971 - INFO - Epoch 100: EPOCH_AVG_TRAIN_LOSS=1.0968
2025-02-04 17:01:28,982 - INFO - #################### Training epoch 101 ####################
2025-02-04 17:01:28,982 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:29,409 - INFO - Epoch 101: train_loss=1.0967
2025-02-04 17:01:30,043 - INFO - Epoch 101: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:01:30,047 - INFO - Epoch 101: EPOCH_AVG_TRAIN_LOSS=1.0967
2025-02-04 17:01:30,050 - INFO - #################### Training epoch 102 ####################
2025-02-04 17:01:30,050 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:30,631 - INFO - Epoch 102: train_loss=1.0968
2025-02-04 17:01:31,271 - INFO - Epoch 102: val_loss=1.0914, val_acc=33.33%
2025-02-04 17:01:31,276 - INFO - Epoch 102: EPOCH_AVG_TRAIN_LOSS=1.0968
2025-02-04 17:01:31,286 - INFO - #################### Training epoch 103 ####################
2025-02-04 17:01:31,286 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:31,850 - INFO - Epoch 103: train_loss=1.0963
2025-02-04 17:01:32,587 - INFO - Epoch 103: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:01:32,592 - INFO - Epoch 103: EPOCH_AVG_TRAIN_LOSS=1.0963
2025-02-04 17:01:32,601 - INFO - #################### Training epoch 104 ####################
2025-02-04 17:01:32,601 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:33,089 - INFO - Epoch 104: train_loss=1.0968
2025-02-04 17:01:33,797 - INFO - Epoch 104: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:01:33,801 - INFO - Epoch 104: EPOCH_AVG_TRAIN_LOSS=1.0968
2025-02-04 17:01:33,812 - INFO - #################### Training epoch 105 ####################
2025-02-04 17:01:33,812 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:34,240 - INFO - Epoch 105: train_loss=1.0954
2025-02-04 17:01:34,950 - INFO - Epoch 105: val_loss=1.0914, val_acc=33.33%
2025-02-04 17:01:34,955 - INFO - Epoch 105: EPOCH_AVG_TRAIN_LOSS=1.0954
2025-02-04 17:01:34,965 - INFO - #################### Training epoch 106 ####################
2025-02-04 17:01:34,965 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:35,395 - INFO - Epoch 106: train_loss=1.0969
2025-02-04 17:01:36,181 - INFO - Epoch 106: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:01:36,186 - INFO - Epoch 106: EPOCH_AVG_TRAIN_LOSS=1.0969
2025-02-04 17:01:36,196 - INFO - #################### Training epoch 107 ####################
2025-02-04 17:01:36,196 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:36,643 - INFO - Epoch 107: train_loss=1.0970
2025-02-04 17:01:37,377 - INFO - Epoch 107: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:01:37,381 - INFO - Epoch 107: EPOCH_AVG_TRAIN_LOSS=1.0970
2025-02-04 17:01:37,393 - INFO - #################### Training epoch 108 ####################
2025-02-04 17:01:37,393 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:37,784 - INFO - Epoch 108: train_loss=1.0981
2025-02-04 17:01:38,457 - INFO - Epoch 108: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:01:38,461 - INFO - Epoch 108: EPOCH_AVG_TRAIN_LOSS=1.0981
2025-02-04 17:01:38,472 - INFO - #################### Training epoch 109 ####################
2025-02-04 17:01:38,472 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:38,877 - INFO - Epoch 109: train_loss=1.0959
2025-02-04 17:01:39,510 - INFO - Epoch 109: val_loss=1.0918, val_acc=33.33%
2025-02-04 17:01:39,514 - INFO - Epoch 109: EPOCH_AVG_TRAIN_LOSS=1.0959
2025-02-04 17:01:39,520 - INFO - #################### Training epoch 110 ####################
2025-02-04 17:01:39,520 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:40,095 - INFO - Epoch 110: train_loss=1.0963
2025-02-04 17:01:40,718 - INFO - Epoch 110: val_loss=1.0917, val_acc=33.33%
2025-02-04 17:01:40,722 - INFO - Epoch 110: EPOCH_AVG_TRAIN_LOSS=1.0963
2025-02-04 17:01:40,733 - INFO - #################### Training epoch 111 ####################
2025-02-04 17:01:40,733 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:41,277 - INFO - Epoch 111: train_loss=1.0989
2025-02-04 17:01:42,015 - INFO - Epoch 111: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:01:42,020 - INFO - Epoch 111: EPOCH_AVG_TRAIN_LOSS=1.0989
2025-02-04 17:01:42,023 - INFO - #################### Training epoch 112 ####################
2025-02-04 17:01:42,023 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:42,509 - INFO - Epoch 112: train_loss=1.0955
2025-02-04 17:01:43,229 - INFO - Epoch 112: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:01:43,234 - INFO - Epoch 112: EPOCH_AVG_TRAIN_LOSS=1.0955
2025-02-04 17:01:43,244 - INFO - #################### Training epoch 113 ####################
2025-02-04 17:01:43,244 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:43,679 - INFO - Epoch 113: train_loss=1.0962
2025-02-04 17:01:44,390 - INFO - Epoch 113: val_loss=1.0929, val_acc=33.33%
2025-02-04 17:01:44,394 - INFO - Epoch 113: EPOCH_AVG_TRAIN_LOSS=1.0962
2025-02-04 17:01:44,404 - INFO - #################### Training epoch 114 ####################
2025-02-04 17:01:44,404 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:44,850 - INFO - Epoch 114: train_loss=1.0972
2025-02-04 17:01:45,567 - INFO - Epoch 114: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:01:45,573 - INFO - Epoch 114: EPOCH_AVG_TRAIN_LOSS=1.0972
2025-02-04 17:01:45,584 - INFO - #################### Training epoch 115 ####################
2025-02-04 17:01:45,584 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:46,140 - INFO - Epoch 115: train_loss=1.0974
2025-02-04 17:01:46,813 - INFO - Epoch 115: val_loss=1.0918, val_acc=33.33%
2025-02-04 17:01:46,818 - INFO - Epoch 115: EPOCH_AVG_TRAIN_LOSS=1.0974
2025-02-04 17:01:46,829 - INFO - #################### Training epoch 116 ####################
2025-02-04 17:01:46,829 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:47,276 - INFO - Epoch 116: train_loss=1.0974
2025-02-04 17:01:47,997 - INFO - Epoch 116: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:01:48,002 - INFO - Epoch 116: EPOCH_AVG_TRAIN_LOSS=1.0974
2025-02-04 17:01:48,013 - INFO - #################### Training epoch 117 ####################
2025-02-04 17:01:48,013 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:48,399 - INFO - Epoch 117: train_loss=1.0967
2025-02-04 17:01:49,071 - INFO - Epoch 117: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:01:49,076 - INFO - Epoch 117: EPOCH_AVG_TRAIN_LOSS=1.0967
2025-02-04 17:01:49,087 - INFO - #################### Training epoch 118 ####################
2025-02-04 17:01:49,087 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:49,499 - INFO - Epoch 118: train_loss=1.0980
2025-02-04 17:01:50,133 - INFO - Epoch 118: val_loss=1.0918, val_acc=33.33%
2025-02-04 17:01:50,137 - INFO - Epoch 118: EPOCH_AVG_TRAIN_LOSS=1.0980
2025-02-04 17:01:50,141 - INFO - #################### Training epoch 119 ####################
2025-02-04 17:01:50,141 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:50,718 - INFO - Epoch 119: train_loss=1.0968
2025-02-04 17:01:51,340 - INFO - Epoch 119: val_loss=1.0919, val_acc=33.33%
2025-02-04 17:01:51,344 - INFO - Epoch 119: EPOCH_AVG_TRAIN_LOSS=1.0968
2025-02-04 17:01:51,352 - INFO - #################### Training epoch 120 ####################
2025-02-04 17:01:51,352 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:51,908 - INFO - Epoch 120: train_loss=1.0962
2025-02-04 17:01:52,649 - INFO - Epoch 120: val_loss=1.0917, val_acc=33.33%
2025-02-04 17:01:52,653 - INFO - Epoch 120: EPOCH_AVG_TRAIN_LOSS=1.0962
2025-02-04 17:01:52,657 - INFO - #################### Training epoch 121 ####################
2025-02-04 17:01:52,657 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:53,143 - INFO - Epoch 121: train_loss=1.0975
2025-02-04 17:01:53,855 - INFO - Epoch 121: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:01:53,859 - INFO - Epoch 121: EPOCH_AVG_TRAIN_LOSS=1.0975
2025-02-04 17:01:53,869 - INFO - #################### Training epoch 122 ####################
2025-02-04 17:01:53,869 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:54,300 - INFO - Epoch 122: train_loss=1.0964
2025-02-04 17:01:55,098 - INFO - Epoch 122: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:01:55,103 - INFO - Epoch 122: EPOCH_AVG_TRAIN_LOSS=1.0964
2025-02-04 17:01:55,114 - INFO - #################### Training epoch 123 ####################
2025-02-04 17:01:55,114 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:55,563 - INFO - Epoch 123: train_loss=1.0978
2025-02-04 17:01:56,290 - INFO - Epoch 123: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:01:56,295 - INFO - Epoch 123: EPOCH_AVG_TRAIN_LOSS=1.0978
2025-02-04 17:01:56,306 - INFO - #################### Training epoch 124 ####################
2025-02-04 17:01:56,306 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:56,705 - INFO - Epoch 124: train_loss=1.0969
2025-02-04 17:01:57,383 - INFO - Epoch 124: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:01:57,388 - INFO - Epoch 124: EPOCH_AVG_TRAIN_LOSS=1.0969
2025-02-04 17:01:57,399 - INFO - #################### Training epoch 125 ####################
2025-02-04 17:01:57,399 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:57,796 - INFO - Epoch 125: train_loss=1.0967
2025-02-04 17:01:58,413 - INFO - Epoch 125: val_loss=1.0918, val_acc=33.33%
2025-02-04 17:01:58,418 - INFO - Epoch 125: EPOCH_AVG_TRAIN_LOSS=1.0967
2025-02-04 17:01:58,428 - INFO - #################### Training epoch 126 ####################
2025-02-04 17:01:58,429 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:01:58,961 - INFO - Epoch 126: train_loss=1.0966
2025-02-04 17:01:59,587 - INFO - Epoch 126: val_loss=1.0919, val_acc=33.33%
2025-02-04 17:01:59,593 - INFO - Epoch 126: EPOCH_AVG_TRAIN_LOSS=1.0966
2025-02-04 17:01:59,604 - INFO - #################### Training epoch 127 ####################
2025-02-04 17:01:59,604 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:00,144 - INFO - Epoch 127: train_loss=1.0963
2025-02-04 17:02:00,881 - INFO - Epoch 127: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:02:00,886 - INFO - Epoch 127: EPOCH_AVG_TRAIN_LOSS=1.0963
2025-02-04 17:02:00,896 - INFO - #################### Training epoch 128 ####################
2025-02-04 17:02:00,896 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:01,384 - INFO - Epoch 128: train_loss=1.0975
2025-02-04 17:02:02,098 - INFO - Epoch 128: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:02:02,103 - INFO - Epoch 128: EPOCH_AVG_TRAIN_LOSS=1.0975
2025-02-04 17:02:02,113 - INFO - #################### Training epoch 129 ####################
2025-02-04 17:02:02,113 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:02,546 - INFO - Epoch 129: train_loss=1.0966
2025-02-04 17:02:03,254 - INFO - Epoch 129: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:02:03,259 - INFO - Epoch 129: EPOCH_AVG_TRAIN_LOSS=1.0966
2025-02-04 17:02:03,269 - INFO - #################### Training epoch 130 ####################
2025-02-04 17:02:03,269 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:03,694 - INFO - Epoch 130: train_loss=1.0950
2025-02-04 17:02:04,472 - INFO - Epoch 130: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:02:04,477 - INFO - Epoch 130: EPOCH_AVG_TRAIN_LOSS=1.0950
2025-02-04 17:02:04,488 - INFO - #################### Training epoch 131 ####################
2025-02-04 17:02:04,488 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:04,953 - INFO - Epoch 131: train_loss=1.0969
2025-02-04 17:02:05,680 - INFO - Epoch 131: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:02:05,685 - INFO - Epoch 131: EPOCH_AVG_TRAIN_LOSS=1.0969
2025-02-04 17:02:05,695 - INFO - #################### Training epoch 132 ####################
2025-02-04 17:02:05,695 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:06,098 - INFO - Epoch 132: train_loss=1.0962
2025-02-04 17:02:06,785 - INFO - Epoch 132: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:02:06,791 - INFO - Epoch 132: EPOCH_AVG_TRAIN_LOSS=1.0962
2025-02-04 17:02:06,801 - INFO - #################### Training epoch 133 ####################
2025-02-04 17:02:06,801 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:07,188 - INFO - Epoch 133: train_loss=1.0958
2025-02-04 17:02:07,893 - INFO - Epoch 133: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:02:07,898 - INFO - Epoch 133: EPOCH_AVG_TRAIN_LOSS=1.0958
2025-02-04 17:02:07,900 - INFO - #################### Training epoch 134 ####################
2025-02-04 17:02:07,900 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:08,366 - INFO - Epoch 134: train_loss=1.0967
2025-02-04 17:02:09,000 - INFO - Epoch 134: val_loss=1.0920, val_acc=33.33%
2025-02-04 17:02:09,005 - INFO - Epoch 134: EPOCH_AVG_TRAIN_LOSS=1.0967
2025-02-04 17:02:09,015 - INFO - #################### Training epoch 135 ####################
2025-02-04 17:02:09,015 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:09,515 - INFO - Epoch 135: train_loss=1.0972
2025-02-04 17:02:10,278 - INFO - Epoch 135: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:02:10,283 - INFO - Epoch 135: EPOCH_AVG_TRAIN_LOSS=1.0972
2025-02-04 17:02:10,293 - INFO - #################### Training epoch 136 ####################
2025-02-04 17:02:10,293 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:10,716 - INFO - Epoch 136: train_loss=1.0960
2025-02-04 17:02:11,515 - INFO - Epoch 136: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:02:11,520 - INFO - Epoch 136: EPOCH_AVG_TRAIN_LOSS=1.0960
2025-02-04 17:02:11,530 - INFO - #################### Training epoch 137 ####################
2025-02-04 17:02:11,530 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:11,961 - INFO - Epoch 137: train_loss=1.0953
2025-02-04 17:02:12,826 - INFO - Epoch 137: val_loss=1.0915, val_acc=33.33%
2025-02-04 17:02:12,831 - INFO - Epoch 137: EPOCH_AVG_TRAIN_LOSS=1.0953
2025-02-04 17:02:12,841 - INFO - #################### Training epoch 138 ####################
2025-02-04 17:02:12,841 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:13,256 - INFO - Epoch 138: train_loss=1.0963
2025-02-04 17:02:14,048 - INFO - Epoch 138: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:02:14,057 - INFO - Epoch 138: EPOCH_AVG_TRAIN_LOSS=1.0963
2025-02-04 17:02:14,076 - INFO - #################### Training epoch 139 ####################
2025-02-04 17:02:14,076 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:14,486 - INFO - Epoch 139: train_loss=1.0964
2025-02-04 17:02:15,192 - INFO - Epoch 139: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:02:15,201 - INFO - Epoch 139: EPOCH_AVG_TRAIN_LOSS=1.0964
2025-02-04 17:02:15,220 - INFO - #################### Training epoch 140 ####################
2025-02-04 17:02:15,220 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:15,617 - INFO - Epoch 140: train_loss=1.0961
2025-02-04 17:02:16,228 - INFO - Epoch 140: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:02:16,233 - INFO - Epoch 140: EPOCH_AVG_TRAIN_LOSS=1.0961
2025-02-04 17:02:16,244 - INFO - #################### Training epoch 141 ####################
2025-02-04 17:02:16,244 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:16,724 - INFO - Epoch 141: train_loss=1.0962
2025-02-04 17:02:17,357 - INFO - Epoch 141: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:02:17,362 - INFO - Epoch 141: EPOCH_AVG_TRAIN_LOSS=1.0962
2025-02-04 17:02:17,372 - INFO - #################### Training epoch 142 ####################
2025-02-04 17:02:17,372 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:17,883 - INFO - Epoch 142: train_loss=1.0991
2025-02-04 17:02:18,540 - INFO - Epoch 142: val_loss=1.0927, val_acc=33.33%
2025-02-04 17:02:18,545 - INFO - Epoch 142: EPOCH_AVG_TRAIN_LOSS=1.0991
2025-02-04 17:02:18,555 - INFO - #################### Training epoch 143 ####################
2025-02-04 17:02:18,555 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:19,024 - INFO - Epoch 143: train_loss=1.0972
2025-02-04 17:02:19,802 - INFO - Epoch 143: val_loss=1.0917, val_acc=33.33%
2025-02-04 17:02:19,806 - INFO - Epoch 143: EPOCH_AVG_TRAIN_LOSS=1.0972
2025-02-04 17:02:19,816 - INFO - #################### Training epoch 144 ####################
2025-02-04 17:02:19,816 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:20,294 - INFO - Epoch 144: train_loss=1.0958
2025-02-04 17:02:21,047 - INFO - Epoch 144: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:02:21,052 - INFO - Epoch 144: EPOCH_AVG_TRAIN_LOSS=1.0958
2025-02-04 17:02:21,063 - INFO - #################### Training epoch 145 ####################
2025-02-04 17:02:21,063 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:21,467 - INFO - Epoch 145: train_loss=1.0974
2025-02-04 17:02:22,247 - INFO - Epoch 145: val_loss=1.0916, val_acc=33.33%
2025-02-04 17:02:22,256 - INFO - Epoch 145: EPOCH_AVG_TRAIN_LOSS=1.0974
2025-02-04 17:02:22,275 - INFO - #################### Training epoch 146 ####################
2025-02-04 17:02:22,275 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:22,684 - INFO - Epoch 146: train_loss=1.0966
2025-02-04 17:02:23,379 - INFO - Epoch 146: val_loss=1.0920, val_acc=33.33%
2025-02-04 17:02:23,388 - INFO - Epoch 146: EPOCH_AVG_TRAIN_LOSS=1.0966
2025-02-04 17:02:23,407 - INFO - #################### Training epoch 147 ####################
2025-02-04 17:02:23,407 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 17:02:23,784 - INFO - Epoch 147: train_loss=1.0963
