2025-02-04 16:54:50,254 - INFO - Starting training with the following parameters:
2025-02-04 16:54:50,255 - INFO - | Parameter       | Value               |
|-----------------|---------------------|
| attention       | XFormers       |
| d_model         | 128            |
| n_heads         | 8              |
| d_f             | 128            |
| num_layers      | 3              |
| d_input         | 32             |
| num_classes     | 3              |
| dropout         | 0.1            |
| learning_rate   | 0.005          |
| epochs          | 200            |
| batch_size      | 32             |

2025-02-04 16:54:50,797 - INFO - Epoch 0: val_loss=1.0680, val_acc=66.67%
2025-02-04 16:54:50,915 - INFO - #################### Training epoch 0 ####################
2025-02-04 16:54:50,916 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:54:50,994 - INFO - Epoch 0: train_loss=1.1443
2025-02-04 16:54:51,299 - INFO - Epoch 0: val_loss=3.9021, val_acc=33.33%
2025-02-04 16:54:51,303 - INFO - Epoch 0: EPOCH_AVG_TRAIN_LOSS=1.1443
2025-02-04 16:54:51,330 - INFO - #################### Training epoch 1 ####################
2025-02-04 16:54:51,330 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:54:51,529 - INFO - Epoch 1: train_loss=3.1829
2025-02-04 16:54:51,786 - INFO - Epoch 1: val_loss=3.5077, val_acc=33.33%
2025-02-04 16:54:51,789 - INFO - Epoch 1: EPOCH_AVG_TRAIN_LOSS=3.1829
2025-02-04 16:54:51,814 - INFO - #################### Training epoch 2 ####################
2025-02-04 16:54:51,814 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:54:52,017 - INFO - Epoch 2: train_loss=2.2749
2025-02-04 16:54:52,281 - INFO - Epoch 2: val_loss=3.4869, val_acc=0.00%
2025-02-04 16:54:52,285 - INFO - Epoch 2: EPOCH_AVG_TRAIN_LOSS=2.2749
2025-02-04 16:54:52,309 - INFO - #################### Training epoch 3 ####################
2025-02-04 16:54:52,309 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:54:52,511 - INFO - Epoch 3: train_loss=2.0548
2025-02-04 16:54:52,771 - INFO - Epoch 3: val_loss=2.3982, val_acc=0.00%
2025-02-04 16:54:52,775 - INFO - Epoch 3: EPOCH_AVG_TRAIN_LOSS=2.0548
2025-02-04 16:54:52,801 - INFO - #################### Training epoch 4 ####################
2025-02-04 16:54:52,801 - INFO - Current Learning Rate: 5.000000e-03
2025-02-04 16:54:53,007 - INFO - Epoch 4: train_loss=1.5033
2025-02-04 16:54:53,267 - INFO - Epoch 4: val_loss=1.7122, val_acc=0.00%
2025-02-04 16:54:53,271 - INFO - Epoch 4: EPOCH_AVG_TRAIN_LOSS=1.5033
2025-02-04 16:54:53,298 - INFO - #################### Training epoch 5 ####################
2025-02-04 16:54:53,298 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:54:53,499 - INFO - Epoch 5: train_loss=1.2350
2025-02-04 16:54:53,760 - INFO - Epoch 5: val_loss=1.2886, val_acc=33.33%
2025-02-04 16:54:53,764 - INFO - Epoch 5: EPOCH_AVG_TRAIN_LOSS=1.2350
2025-02-04 16:54:53,791 - INFO - #################### Training epoch 6 ####################
2025-02-04 16:54:53,791 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:54:53,996 - INFO - Epoch 6: train_loss=1.1279
2025-02-04 16:54:54,255 - INFO - Epoch 6: val_loss=0.9993, val_acc=66.67%
2025-02-04 16:54:54,259 - INFO - Epoch 6: EPOCH_AVG_TRAIN_LOSS=1.1279
2025-02-04 16:54:54,285 - INFO - #################### Training epoch 7 ####################
2025-02-04 16:54:54,285 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:54:54,490 - INFO - Epoch 7: train_loss=1.1206
2025-02-04 16:54:54,750 - INFO - Epoch 7: val_loss=0.9108, val_acc=66.67%
2025-02-04 16:54:54,753 - INFO - Epoch 7: EPOCH_AVG_TRAIN_LOSS=1.1206
2025-02-04 16:54:54,780 - INFO - #################### Training epoch 8 ####################
2025-02-04 16:54:54,780 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:54:54,983 - INFO - Epoch 8: train_loss=1.1247
2025-02-04 16:54:55,242 - INFO - Epoch 8: val_loss=0.9635, val_acc=66.67%
2025-02-04 16:54:55,246 - INFO - Epoch 8: EPOCH_AVG_TRAIN_LOSS=1.1247
2025-02-04 16:54:55,273 - INFO - #################### Training epoch 9 ####################
2025-02-04 16:54:55,273 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:54:55,479 - INFO - Epoch 9: train_loss=1.0994
2025-02-04 16:54:55,737 - INFO - Epoch 9: val_loss=1.1029, val_acc=0.00%
2025-02-04 16:54:55,741 - INFO - Epoch 9: EPOCH_AVG_TRAIN_LOSS=1.0994
2025-02-04 16:54:55,744 - INFO - #################### Training epoch 10 ####################
2025-02-04 16:54:55,744 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:54:55,949 - INFO - Epoch 10: train_loss=1.0885
2025-02-04 16:54:56,209 - INFO - Epoch 10: val_loss=1.1752, val_acc=0.00%
2025-02-04 16:54:56,213 - INFO - Epoch 10: EPOCH_AVG_TRAIN_LOSS=1.0885
2025-02-04 16:54:56,216 - INFO - #################### Training epoch 11 ####################
2025-02-04 16:54:56,216 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:54:56,423 - INFO - Epoch 11: train_loss=1.0941
2025-02-04 16:54:56,686 - INFO - Epoch 11: val_loss=1.1344, val_acc=0.00%
2025-02-04 16:54:56,690 - INFO - Epoch 11: EPOCH_AVG_TRAIN_LOSS=1.0941
2025-02-04 16:54:56,693 - INFO - #################### Training epoch 12 ####################
2025-02-04 16:54:56,693 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:54:56,898 - INFO - Epoch 12: train_loss=1.0872
2025-02-04 16:54:57,158 - INFO - Epoch 12: val_loss=1.0553, val_acc=66.67%
2025-02-04 16:54:57,162 - INFO - Epoch 12: EPOCH_AVG_TRAIN_LOSS=1.0872
2025-02-04 16:54:57,165 - INFO - #################### Training epoch 13 ####################
2025-02-04 16:54:57,165 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:54:57,372 - INFO - Epoch 13: train_loss=1.0868
2025-02-04 16:54:57,634 - INFO - Epoch 13: val_loss=0.9975, val_acc=66.67%
2025-02-04 16:54:57,637 - INFO - Epoch 13: EPOCH_AVG_TRAIN_LOSS=1.0868
2025-02-04 16:54:57,664 - INFO - #################### Training epoch 14 ####################
2025-02-04 16:54:57,664 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:54:57,873 - INFO - Epoch 14: train_loss=1.0860
2025-02-04 16:54:58,134 - INFO - Epoch 14: val_loss=0.9859, val_acc=66.67%
2025-02-04 16:54:58,138 - INFO - Epoch 14: EPOCH_AVG_TRAIN_LOSS=1.0860
2025-02-04 16:54:58,165 - INFO - #################### Training epoch 15 ####################
2025-02-04 16:54:58,165 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:54:58,372 - INFO - Epoch 15: train_loss=1.0798
2025-02-04 16:54:58,634 - INFO - Epoch 15: val_loss=1.0160, val_acc=66.67%
2025-02-04 16:54:58,638 - INFO - Epoch 15: EPOCH_AVG_TRAIN_LOSS=1.0798
2025-02-04 16:54:58,641 - INFO - #################### Training epoch 16 ####################
2025-02-04 16:54:58,641 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:54:58,847 - INFO - Epoch 16: train_loss=1.0755
2025-02-04 16:54:59,109 - INFO - Epoch 16: val_loss=1.0571, val_acc=33.33%
2025-02-04 16:54:59,113 - INFO - Epoch 16: EPOCH_AVG_TRAIN_LOSS=1.0755
2025-02-04 16:54:59,116 - INFO - #################### Training epoch 17 ####################
2025-02-04 16:54:59,116 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:54:59,321 - INFO - Epoch 17: train_loss=1.0767
2025-02-04 16:54:59,582 - INFO - Epoch 17: val_loss=1.0522, val_acc=33.33%
2025-02-04 16:54:59,585 - INFO - Epoch 17: EPOCH_AVG_TRAIN_LOSS=1.0767
2025-02-04 16:54:59,589 - INFO - #################### Training epoch 18 ####################
2025-02-04 16:54:59,589 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:54:59,793 - INFO - Epoch 18: train_loss=1.0729
2025-02-04 16:55:00,056 - INFO - Epoch 18: val_loss=1.0572, val_acc=33.33%
2025-02-04 16:55:00,060 - INFO - Epoch 18: EPOCH_AVG_TRAIN_LOSS=1.0729
2025-02-04 16:55:00,063 - INFO - #################### Training epoch 19 ####################
2025-02-04 16:55:00,063 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:00,274 - INFO - Epoch 19: train_loss=1.0616
2025-02-04 16:55:00,535 - INFO - Epoch 19: val_loss=1.1076, val_acc=33.33%
2025-02-04 16:55:00,538 - INFO - Epoch 19: EPOCH_AVG_TRAIN_LOSS=1.0616
2025-02-04 16:55:00,542 - INFO - #################### Training epoch 20 ####################
2025-02-04 16:55:00,542 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:00,747 - INFO - Epoch 20: train_loss=1.0625
2025-02-04 16:55:01,010 - INFO - Epoch 20: val_loss=1.0409, val_acc=66.67%
2025-02-04 16:55:01,014 - INFO - Epoch 20: EPOCH_AVG_TRAIN_LOSS=1.0625
2025-02-04 16:55:01,017 - INFO - #################### Training epoch 21 ####################
2025-02-04 16:55:01,017 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:01,221 - INFO - Epoch 21: train_loss=1.0408
2025-02-04 16:55:01,484 - INFO - Epoch 21: val_loss=1.0159, val_acc=66.67%
2025-02-04 16:55:01,488 - INFO - Epoch 21: EPOCH_AVG_TRAIN_LOSS=1.0408
2025-02-04 16:55:01,491 - INFO - #################### Training epoch 22 ####################
2025-02-04 16:55:01,491 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:01,697 - INFO - Epoch 22: train_loss=1.0500
2025-02-04 16:55:01,957 - INFO - Epoch 22: val_loss=1.0718, val_acc=66.67%
2025-02-04 16:55:01,961 - INFO - Epoch 22: EPOCH_AVG_TRAIN_LOSS=1.0500
2025-02-04 16:55:01,964 - INFO - #################### Training epoch 23 ####################
2025-02-04 16:55:01,964 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:02,170 - INFO - Epoch 23: train_loss=1.0228
2025-02-04 16:55:02,430 - INFO - Epoch 23: val_loss=1.0882, val_acc=66.67%
2025-02-04 16:55:02,434 - INFO - Epoch 23: EPOCH_AVG_TRAIN_LOSS=1.0228
2025-02-04 16:55:02,437 - INFO - #################### Training epoch 24 ####################
2025-02-04 16:55:02,437 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:02,642 - INFO - Epoch 24: train_loss=1.0127
2025-02-04 16:55:02,903 - INFO - Epoch 24: val_loss=1.0261, val_acc=33.33%
2025-02-04 16:55:02,906 - INFO - Epoch 24: EPOCH_AVG_TRAIN_LOSS=1.0127
2025-02-04 16:55:02,909 - INFO - #################### Training epoch 25 ####################
2025-02-04 16:55:02,910 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:03,116 - INFO - Epoch 25: train_loss=1.0012
2025-02-04 16:55:03,377 - INFO - Epoch 25: val_loss=1.0020, val_acc=66.67%
2025-02-04 16:55:03,380 - INFO - Epoch 25: EPOCH_AVG_TRAIN_LOSS=1.0012
2025-02-04 16:55:03,384 - INFO - #################### Training epoch 26 ####################
2025-02-04 16:55:03,384 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:03,591 - INFO - Epoch 26: train_loss=0.9899
2025-02-04 16:55:03,852 - INFO - Epoch 26: val_loss=1.0967, val_acc=0.00%
2025-02-04 16:55:03,856 - INFO - Epoch 26: EPOCH_AVG_TRAIN_LOSS=0.9899
2025-02-04 16:55:03,859 - INFO - #################### Training epoch 27 ####################
2025-02-04 16:55:03,859 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:04,065 - INFO - Epoch 27: train_loss=0.9959
2025-02-04 16:55:04,326 - INFO - Epoch 27: val_loss=0.9940, val_acc=66.67%
2025-02-04 16:55:04,330 - INFO - Epoch 27: EPOCH_AVG_TRAIN_LOSS=0.9959
2025-02-04 16:55:04,333 - INFO - #################### Training epoch 28 ####################
2025-02-04 16:55:04,333 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:04,537 - INFO - Epoch 28: train_loss=0.9630
2025-02-04 16:55:04,800 - INFO - Epoch 28: val_loss=0.9891, val_acc=66.67%
2025-02-04 16:55:04,804 - INFO - Epoch 28: EPOCH_AVG_TRAIN_LOSS=0.9630
2025-02-04 16:55:04,807 - INFO - #################### Training epoch 29 ####################
2025-02-04 16:55:04,807 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:05,014 - INFO - Epoch 29: train_loss=1.0841
2025-02-04 16:55:05,277 - INFO - Epoch 29: val_loss=1.0199, val_acc=66.67%
2025-02-04 16:55:05,280 - INFO - Epoch 29: EPOCH_AVG_TRAIN_LOSS=1.0841
2025-02-04 16:55:05,284 - INFO - #################### Training epoch 30 ####################
2025-02-04 16:55:05,284 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:05,490 - INFO - Epoch 30: train_loss=0.9582
2025-02-04 16:55:05,750 - INFO - Epoch 30: val_loss=1.1189, val_acc=33.33%
2025-02-04 16:55:05,755 - INFO - Epoch 30: EPOCH_AVG_TRAIN_LOSS=0.9582
2025-02-04 16:55:05,759 - INFO - #################### Training epoch 31 ####################
2025-02-04 16:55:05,759 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:05,964 - INFO - Epoch 31: train_loss=0.9712
2025-02-04 16:55:06,226 - INFO - Epoch 31: val_loss=1.0485, val_acc=33.33%
2025-02-04 16:55:06,229 - INFO - Epoch 31: EPOCH_AVG_TRAIN_LOSS=0.9712
2025-02-04 16:55:06,233 - INFO - #################### Training epoch 32 ####################
2025-02-04 16:55:06,233 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:06,436 - INFO - Epoch 32: train_loss=0.9540
2025-02-04 16:55:06,696 - INFO - Epoch 32: val_loss=1.0345, val_acc=66.67%
2025-02-04 16:55:06,700 - INFO - Epoch 32: EPOCH_AVG_TRAIN_LOSS=0.9540
2025-02-04 16:55:06,703 - INFO - #################### Training epoch 33 ####################
2025-02-04 16:55:06,703 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:06,905 - INFO - Epoch 33: train_loss=0.9485
2025-02-04 16:55:07,164 - INFO - Epoch 33: val_loss=1.0042, val_acc=100.00%
2025-02-04 16:55:07,168 - INFO - Epoch 33: EPOCH_AVG_TRAIN_LOSS=0.9485
2025-02-04 16:55:07,171 - INFO - #################### Training epoch 34 ####################
2025-02-04 16:55:07,171 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:07,377 - INFO - Epoch 34: train_loss=0.9422
2025-02-04 16:55:07,637 - INFO - Epoch 34: val_loss=0.9981, val_acc=66.67%
2025-02-04 16:55:07,640 - INFO - Epoch 34: EPOCH_AVG_TRAIN_LOSS=0.9422
2025-02-04 16:55:07,643 - INFO - #################### Training epoch 35 ####################
2025-02-04 16:55:07,643 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:07,848 - INFO - Epoch 35: train_loss=0.9392
2025-02-04 16:55:08,110 - INFO - Epoch 35: val_loss=1.0406, val_acc=0.00%
2025-02-04 16:55:08,114 - INFO - Epoch 35: EPOCH_AVG_TRAIN_LOSS=0.9392
2025-02-04 16:55:08,117 - INFO - #################### Training epoch 36 ####################
2025-02-04 16:55:08,117 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:08,320 - INFO - Epoch 36: train_loss=0.9329
2025-02-04 16:55:08,579 - INFO - Epoch 36: val_loss=1.1217, val_acc=0.00%
2025-02-04 16:55:08,583 - INFO - Epoch 36: EPOCH_AVG_TRAIN_LOSS=0.9329
2025-02-04 16:55:08,586 - INFO - #################### Training epoch 37 ####################
2025-02-04 16:55:08,586 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:08,791 - INFO - Epoch 37: train_loss=0.9369
2025-02-04 16:55:09,051 - INFO - Epoch 37: val_loss=1.0529, val_acc=0.00%
2025-02-04 16:55:09,055 - INFO - Epoch 37: EPOCH_AVG_TRAIN_LOSS=0.9369
2025-02-04 16:55:09,058 - INFO - #################### Training epoch 38 ####################
2025-02-04 16:55:09,058 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:09,264 - INFO - Epoch 38: train_loss=0.9289
2025-02-04 16:55:09,525 - INFO - Epoch 38: val_loss=0.9954, val_acc=33.33%
2025-02-04 16:55:09,528 - INFO - Epoch 38: EPOCH_AVG_TRAIN_LOSS=0.9289
2025-02-04 16:55:09,531 - INFO - #################### Training epoch 39 ####################
2025-02-04 16:55:09,532 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:09,737 - INFO - Epoch 39: train_loss=0.9239
2025-02-04 16:55:09,997 - INFO - Epoch 39: val_loss=0.9796, val_acc=33.33%
2025-02-04 16:55:10,000 - INFO - Epoch 39: EPOCH_AVG_TRAIN_LOSS=0.9239
2025-02-04 16:55:10,025 - INFO - #################### Training epoch 40 ####################
2025-02-04 16:55:10,025 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:10,232 - INFO - Epoch 40: train_loss=0.9607
2025-02-04 16:55:10,494 - INFO - Epoch 40: val_loss=1.0202, val_acc=33.33%
2025-02-04 16:55:10,498 - INFO - Epoch 40: EPOCH_AVG_TRAIN_LOSS=0.9607
2025-02-04 16:55:10,501 - INFO - #################### Training epoch 41 ####################
2025-02-04 16:55:10,501 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:10,706 - INFO - Epoch 41: train_loss=0.9483
2025-02-04 16:55:10,968 - INFO - Epoch 41: val_loss=1.0529, val_acc=33.33%
2025-02-04 16:55:10,972 - INFO - Epoch 41: EPOCH_AVG_TRAIN_LOSS=0.9483
2025-02-04 16:55:10,975 - INFO - #################### Training epoch 42 ####################
2025-02-04 16:55:10,975 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:11,182 - INFO - Epoch 42: train_loss=0.9447
2025-02-04 16:55:11,439 - INFO - Epoch 42: val_loss=1.1747, val_acc=0.00%
2025-02-04 16:55:11,443 - INFO - Epoch 42: EPOCH_AVG_TRAIN_LOSS=0.9447
2025-02-04 16:55:11,446 - INFO - #################### Training epoch 43 ####################
2025-02-04 16:55:11,446 - INFO - Current Learning Rate: 2.500000e-03
2025-02-04 16:55:11,653 - INFO - Epoch 43: train_loss=0.9442
2025-02-04 16:55:11,916 - INFO - Epoch 43: val_loss=1.2323, val_acc=0.00%
2025-02-04 16:55:11,919 - INFO - Epoch 43: EPOCH_AVG_TRAIN_LOSS=0.9442
2025-02-04 16:55:11,923 - INFO - #################### Training epoch 44 ####################
2025-02-04 16:55:11,923 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:55:12,132 - INFO - Epoch 44: train_loss=0.9331
2025-02-04 16:55:12,396 - INFO - Epoch 44: val_loss=1.3959, val_acc=0.00%
2025-02-04 16:55:12,400 - INFO - Epoch 44: EPOCH_AVG_TRAIN_LOSS=0.9331
2025-02-04 16:55:12,403 - INFO - #################### Training epoch 45 ####################
2025-02-04 16:55:12,403 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:55:12,609 - INFO - Epoch 45: train_loss=0.9761
2025-02-04 16:55:12,869 - INFO - Epoch 45: val_loss=1.3055, val_acc=0.00%
2025-02-04 16:55:12,873 - INFO - Epoch 45: EPOCH_AVG_TRAIN_LOSS=0.9761
2025-02-04 16:55:12,876 - INFO - #################### Training epoch 46 ####################
2025-02-04 16:55:12,876 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:55:13,082 - INFO - Epoch 46: train_loss=0.9142
2025-02-04 16:55:13,344 - INFO - Epoch 46: val_loss=0.9610, val_acc=66.67%
2025-02-04 16:55:13,348 - INFO - Epoch 46: EPOCH_AVG_TRAIN_LOSS=0.9142
2025-02-04 16:55:13,375 - INFO - #################### Training epoch 47 ####################
2025-02-04 16:55:13,375 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:55:13,581 - INFO - Epoch 47: train_loss=0.9274
2025-02-04 16:55:13,843 - INFO - Epoch 47: val_loss=0.9555, val_acc=66.67%
2025-02-04 16:55:13,847 - INFO - Epoch 47: EPOCH_AVG_TRAIN_LOSS=0.9274
2025-02-04 16:55:13,874 - INFO - #################### Training epoch 48 ####################
2025-02-04 16:55:13,874 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:55:14,081 - INFO - Epoch 48: train_loss=0.9413
2025-02-04 16:55:14,343 - INFO - Epoch 48: val_loss=0.9726, val_acc=33.33%
2025-02-04 16:55:14,346 - INFO - Epoch 48: EPOCH_AVG_TRAIN_LOSS=0.9413
2025-02-04 16:55:14,350 - INFO - #################### Training epoch 49 ####################
2025-02-04 16:55:14,350 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:55:14,557 - INFO - Epoch 49: train_loss=0.9248
2025-02-04 16:55:14,819 - INFO - Epoch 49: val_loss=1.0397, val_acc=33.33%
2025-02-04 16:55:14,822 - INFO - Epoch 49: EPOCH_AVG_TRAIN_LOSS=0.9248
2025-02-04 16:55:14,826 - INFO - #################### Training epoch 50 ####################
2025-02-04 16:55:14,826 - INFO - Current Learning Rate: 1.250000e-03
2025-02-04 16:55:15,032 - INFO - Epoch 50: train_loss=0.9225
2025-02-04 16:55:15,293 - INFO - Epoch 50: val_loss=1.0847, val_acc=33.33%
2025-02-04 16:55:15,297 - INFO - Epoch 50: EPOCH_AVG_TRAIN_LOSS=0.9225
2025-02-04 16:55:15,300 - INFO - #################### Training epoch 51 ####################
2025-02-04 16:55:15,300 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:55:15,507 - INFO - Epoch 51: train_loss=0.9216
2025-02-04 16:55:15,766 - INFO - Epoch 51: val_loss=1.0914, val_acc=33.33%
2025-02-04 16:55:15,770 - INFO - Epoch 51: EPOCH_AVG_TRAIN_LOSS=0.9216
2025-02-04 16:55:15,773 - INFO - #################### Training epoch 52 ####################
2025-02-04 16:55:15,773 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:55:15,980 - INFO - Epoch 52: train_loss=0.9219
2025-02-04 16:55:16,242 - INFO - Epoch 52: val_loss=1.0852, val_acc=33.33%
2025-02-04 16:55:16,245 - INFO - Epoch 52: EPOCH_AVG_TRAIN_LOSS=0.9219
2025-02-04 16:55:16,249 - INFO - #################### Training epoch 53 ####################
2025-02-04 16:55:16,249 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:55:16,455 - INFO - Epoch 53: train_loss=0.9184
2025-02-04 16:55:16,717 - INFO - Epoch 53: val_loss=1.0736, val_acc=33.33%
2025-02-04 16:55:16,720 - INFO - Epoch 53: EPOCH_AVG_TRAIN_LOSS=0.9184
2025-02-04 16:55:16,724 - INFO - #################### Training epoch 54 ####################
2025-02-04 16:55:16,724 - INFO - Current Learning Rate: 6.250000e-04
2025-02-04 16:55:16,932 - INFO - Epoch 54: train_loss=0.9184
2025-02-04 16:55:17,193 - INFO - Epoch 54: val_loss=1.0663, val_acc=33.33%
2025-02-04 16:55:17,197 - INFO - Epoch 54: EPOCH_AVG_TRAIN_LOSS=0.9184
2025-02-04 16:55:17,200 - INFO - #################### Training epoch 55 ####################
2025-02-04 16:55:17,200 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:55:17,411 - INFO - Epoch 55: train_loss=0.9133
2025-02-04 16:55:17,675 - INFO - Epoch 55: val_loss=1.0811, val_acc=33.33%
2025-02-04 16:55:17,678 - INFO - Epoch 55: EPOCH_AVG_TRAIN_LOSS=0.9133
2025-02-04 16:55:17,681 - INFO - #################### Training epoch 56 ####################
2025-02-04 16:55:17,681 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:55:17,886 - INFO - Epoch 56: train_loss=0.9141
2025-02-04 16:55:18,150 - INFO - Epoch 56: val_loss=1.1058, val_acc=0.00%
2025-02-04 16:55:18,153 - INFO - Epoch 56: EPOCH_AVG_TRAIN_LOSS=0.9141
2025-02-04 16:55:18,157 - INFO - #################### Training epoch 57 ####################
2025-02-04 16:55:18,157 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:55:18,363 - INFO - Epoch 57: train_loss=0.9119
2025-02-04 16:55:18,628 - INFO - Epoch 57: val_loss=1.1437, val_acc=0.00%
2025-02-04 16:55:18,632 - INFO - Epoch 57: EPOCH_AVG_TRAIN_LOSS=0.9119
2025-02-04 16:55:18,635 - INFO - #################### Training epoch 58 ####################
2025-02-04 16:55:18,635 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:55:18,843 - INFO - Epoch 58: train_loss=0.9101
2025-02-04 16:55:19,106 - INFO - Epoch 58: val_loss=1.1773, val_acc=0.00%
2025-02-04 16:55:19,110 - INFO - Epoch 58: EPOCH_AVG_TRAIN_LOSS=0.9101
2025-02-04 16:55:19,113 - INFO - #################### Training epoch 59 ####################
2025-02-04 16:55:19,113 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:55:19,320 - INFO - Epoch 59: train_loss=0.9036
2025-02-04 16:55:19,580 - INFO - Epoch 59: val_loss=1.1901, val_acc=0.00%
2025-02-04 16:55:19,584 - INFO - Epoch 59: EPOCH_AVG_TRAIN_LOSS=0.9036
2025-02-04 16:55:19,587 - INFO - #################### Training epoch 60 ####################
2025-02-04 16:55:19,587 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:55:19,794 - INFO - Epoch 60: train_loss=0.8865
2025-02-04 16:55:20,058 - INFO - Epoch 60: val_loss=1.1928, val_acc=0.00%
2025-02-04 16:55:20,062 - INFO - Epoch 60: EPOCH_AVG_TRAIN_LOSS=0.8865
2025-02-04 16:55:20,065 - INFO - #################### Training epoch 61 ####################
2025-02-04 16:55:20,065 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:55:20,273 - INFO - Epoch 61: train_loss=0.8795
2025-02-04 16:55:20,536 - INFO - Epoch 61: val_loss=1.1914, val_acc=0.00%
2025-02-04 16:55:20,540 - INFO - Epoch 61: EPOCH_AVG_TRAIN_LOSS=0.8795
2025-02-04 16:55:20,543 - INFO - #################### Training epoch 62 ####################
2025-02-04 16:55:20,543 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:55:20,751 - INFO - Epoch 62: train_loss=0.8650
2025-02-04 16:55:21,016 - INFO - Epoch 62: val_loss=1.1912, val_acc=0.00%
2025-02-04 16:55:21,020 - INFO - Epoch 62: EPOCH_AVG_TRAIN_LOSS=0.8650
2025-02-04 16:55:21,023 - INFO - #################### Training epoch 63 ####################
2025-02-04 16:55:21,023 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:55:21,230 - INFO - Epoch 63: train_loss=0.8707
2025-02-04 16:55:21,495 - INFO - Epoch 63: val_loss=1.1958, val_acc=0.00%
2025-02-04 16:55:21,499 - INFO - Epoch 63: EPOCH_AVG_TRAIN_LOSS=0.8707
2025-02-04 16:55:21,502 - INFO - #################### Training epoch 64 ####################
2025-02-04 16:55:21,502 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:55:21,709 - INFO - Epoch 64: train_loss=0.8750
2025-02-04 16:55:21,971 - INFO - Epoch 64: val_loss=1.2302, val_acc=0.00%
2025-02-04 16:55:21,975 - INFO - Epoch 64: EPOCH_AVG_TRAIN_LOSS=0.8750
2025-02-04 16:55:21,978 - INFO - #################### Training epoch 65 ####################
2025-02-04 16:55:21,978 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:55:22,184 - INFO - Epoch 65: train_loss=0.8646
2025-02-04 16:55:22,447 - INFO - Epoch 65: val_loss=1.2808, val_acc=33.33%
2025-02-04 16:55:22,451 - INFO - Epoch 65: EPOCH_AVG_TRAIN_LOSS=0.8646
2025-02-04 16:55:22,454 - INFO - #################### Training epoch 66 ####################
2025-02-04 16:55:22,454 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:55:22,660 - INFO - Epoch 66: train_loss=0.8747
2025-02-04 16:55:22,922 - INFO - Epoch 66: val_loss=1.3106, val_acc=33.33%
2025-02-04 16:55:22,925 - INFO - Epoch 66: EPOCH_AVG_TRAIN_LOSS=0.8747
2025-02-04 16:55:22,929 - INFO - #################### Training epoch 67 ####################
2025-02-04 16:55:22,929 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:55:23,134 - INFO - Epoch 67: train_loss=0.9025
2025-02-04 16:55:23,395 - INFO - Epoch 67: val_loss=1.3105, val_acc=33.33%
2025-02-04 16:55:23,399 - INFO - Epoch 67: EPOCH_AVG_TRAIN_LOSS=0.9025
2025-02-04 16:55:23,402 - INFO - #################### Training epoch 68 ####################
2025-02-04 16:55:23,402 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:55:23,607 - INFO - Epoch 68: train_loss=0.8901
2025-02-04 16:55:23,869 - INFO - Epoch 68: val_loss=1.3280, val_acc=33.33%
2025-02-04 16:55:23,872 - INFO - Epoch 68: EPOCH_AVG_TRAIN_LOSS=0.8901
2025-02-04 16:55:23,876 - INFO - #################### Training epoch 69 ####################
2025-02-04 16:55:23,876 - INFO - Current Learning Rate: 3.125000e-04
2025-02-04 16:55:24,082 - INFO - Epoch 69: train_loss=0.9083
2025-02-04 16:55:24,346 - INFO - Epoch 69: val_loss=1.3456, val_acc=33.33%
2025-02-04 16:55:24,349 - INFO - Epoch 69: EPOCH_AVG_TRAIN_LOSS=0.9083
2025-02-04 16:55:24,353 - INFO - #################### Training epoch 70 ####################
2025-02-04 16:55:24,353 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 16:55:24,560 - INFO - Epoch 70: train_loss=0.9022
2025-02-04 16:55:24,822 - INFO - Epoch 70: val_loss=1.3446, val_acc=33.33%
2025-02-04 16:55:24,825 - INFO - Epoch 70: EPOCH_AVG_TRAIN_LOSS=0.9022
2025-02-04 16:55:24,829 - INFO - #################### Training epoch 71 ####################
2025-02-04 16:55:24,829 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 16:55:25,032 - INFO - Epoch 71: train_loss=0.9229
2025-02-04 16:55:25,296 - INFO - Epoch 71: val_loss=1.3436, val_acc=33.33%
2025-02-04 16:55:25,299 - INFO - Epoch 71: EPOCH_AVG_TRAIN_LOSS=0.9229
2025-02-04 16:55:25,303 - INFO - #################### Training epoch 72 ####################
2025-02-04 16:55:25,303 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 16:55:25,508 - INFO - Epoch 72: train_loss=0.9231
2025-02-04 16:55:25,770 - INFO - Epoch 72: val_loss=1.3428, val_acc=33.33%
2025-02-04 16:55:25,773 - INFO - Epoch 72: EPOCH_AVG_TRAIN_LOSS=0.9231
2025-02-04 16:55:25,777 - INFO - #################### Training epoch 73 ####################
2025-02-04 16:55:25,777 - INFO - Current Learning Rate: 1.562500e-04
2025-02-04 16:55:25,980 - INFO - Epoch 73: train_loss=0.9270
2025-02-04 16:55:26,244 - INFO - Epoch 73: val_loss=1.3421, val_acc=33.33%
2025-02-04 16:55:26,247 - INFO - Epoch 73: EPOCH_AVG_TRAIN_LOSS=0.9270
2025-02-04 16:55:26,250 - INFO - #################### Training epoch 74 ####################
2025-02-04 16:55:26,250 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 16:55:26,455 - INFO - Epoch 74: train_loss=0.9621
2025-02-04 16:55:26,718 - INFO - Epoch 74: val_loss=1.3417, val_acc=33.33%
2025-02-04 16:55:26,722 - INFO - Epoch 74: EPOCH_AVG_TRAIN_LOSS=0.9621
2025-02-04 16:55:26,725 - INFO - #################### Training epoch 75 ####################
2025-02-04 16:55:26,725 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 16:55:26,932 - INFO - Epoch 75: train_loss=0.9554
2025-02-04 16:55:27,193 - INFO - Epoch 75: val_loss=1.3414, val_acc=33.33%
2025-02-04 16:55:27,197 - INFO - Epoch 75: EPOCH_AVG_TRAIN_LOSS=0.9554
2025-02-04 16:55:27,200 - INFO - #################### Training epoch 76 ####################
2025-02-04 16:55:27,200 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 16:55:27,406 - INFO - Epoch 76: train_loss=0.9880
2025-02-04 16:55:27,668 - INFO - Epoch 76: val_loss=1.3411, val_acc=33.33%
2025-02-04 16:55:27,672 - INFO - Epoch 76: EPOCH_AVG_TRAIN_LOSS=0.9880
2025-02-04 16:55:27,675 - INFO - #################### Training epoch 77 ####################
2025-02-04 16:55:27,675 - INFO - Current Learning Rate: 7.812500e-05
2025-02-04 16:55:27,879 - INFO - Epoch 77: train_loss=0.9607
2025-02-04 16:55:28,143 - INFO - Epoch 77: val_loss=1.3409, val_acc=33.33%
2025-02-04 16:55:28,146 - INFO - Epoch 77: EPOCH_AVG_TRAIN_LOSS=0.9607
2025-02-04 16:55:28,149 - INFO - #################### Training epoch 78 ####################
2025-02-04 16:55:28,150 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 16:55:28,354 - INFO - Epoch 78: train_loss=0.9445
2025-02-04 16:55:28,617 - INFO - Epoch 78: val_loss=1.3408, val_acc=33.33%
2025-02-04 16:55:28,620 - INFO - Epoch 78: EPOCH_AVG_TRAIN_LOSS=0.9445
2025-02-04 16:55:28,624 - INFO - #################### Training epoch 79 ####################
2025-02-04 16:55:28,624 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 16:55:28,831 - INFO - Epoch 79: train_loss=0.9691
2025-02-04 16:55:29,091 - INFO - Epoch 79: val_loss=1.3406, val_acc=33.33%
2025-02-04 16:55:29,095 - INFO - Epoch 79: EPOCH_AVG_TRAIN_LOSS=0.9691
2025-02-04 16:55:29,098 - INFO - #################### Training epoch 80 ####################
2025-02-04 16:55:29,098 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 16:55:29,305 - INFO - Epoch 80: train_loss=0.9703
2025-02-04 16:55:29,566 - INFO - Epoch 80: val_loss=1.3406, val_acc=33.33%
2025-02-04 16:55:29,569 - INFO - Epoch 80: EPOCH_AVG_TRAIN_LOSS=0.9703
2025-02-04 16:55:29,573 - INFO - #################### Training epoch 81 ####################
2025-02-04 16:55:29,573 - INFO - Current Learning Rate: 3.906250e-05
2025-02-04 16:55:29,779 - INFO - Epoch 81: train_loss=0.9674
2025-02-04 16:55:30,042 - INFO - Epoch 81: val_loss=1.3405, val_acc=33.33%
2025-02-04 16:55:30,046 - INFO - Epoch 81: EPOCH_AVG_TRAIN_LOSS=0.9674
2025-02-04 16:55:30,049 - INFO - #################### Training epoch 82 ####################
2025-02-04 16:55:30,049 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 16:55:30,256 - INFO - Epoch 82: train_loss=0.9902
2025-02-04 16:55:30,519 - INFO - Epoch 82: val_loss=1.3405, val_acc=33.33%
2025-02-04 16:55:30,523 - INFO - Epoch 82: EPOCH_AVG_TRAIN_LOSS=0.9902
2025-02-04 16:55:30,526 - INFO - #################### Training epoch 83 ####################
2025-02-04 16:55:30,526 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 16:55:30,730 - INFO - Epoch 83: train_loss=0.9476
2025-02-04 16:55:30,993 - INFO - Epoch 83: val_loss=1.3404, val_acc=33.33%
2025-02-04 16:55:30,996 - INFO - Epoch 83: EPOCH_AVG_TRAIN_LOSS=0.9476
2025-02-04 16:55:30,999 - INFO - #################### Training epoch 84 ####################
2025-02-04 16:55:31,000 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 16:55:31,206 - INFO - Epoch 84: train_loss=0.9704
2025-02-04 16:55:31,471 - INFO - Epoch 84: val_loss=1.3404, val_acc=33.33%
2025-02-04 16:55:31,475 - INFO - Epoch 84: EPOCH_AVG_TRAIN_LOSS=0.9704
2025-02-04 16:55:31,478 - INFO - #################### Training epoch 85 ####################
2025-02-04 16:55:31,478 - INFO - Current Learning Rate: 1.953125e-05
2025-02-04 16:55:31,684 - INFO - Epoch 85: train_loss=0.9359
2025-02-04 16:55:31,945 - INFO - Epoch 85: val_loss=1.3404, val_acc=33.33%
2025-02-04 16:55:31,949 - INFO - Epoch 85: EPOCH_AVG_TRAIN_LOSS=0.9359
2025-02-04 16:55:31,952 - INFO - #################### Training epoch 86 ####################
2025-02-04 16:55:31,952 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 16:55:32,160 - INFO - Epoch 86: train_loss=0.9173
2025-02-04 16:55:32,422 - INFO - Epoch 86: val_loss=1.3404, val_acc=33.33%
2025-02-04 16:55:32,426 - INFO - Epoch 86: EPOCH_AVG_TRAIN_LOSS=0.9173
2025-02-04 16:55:32,429 - INFO - #################### Training epoch 87 ####################
2025-02-04 16:55:32,429 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 16:55:32,634 - INFO - Epoch 87: train_loss=0.9424
2025-02-04 16:55:32,896 - INFO - Epoch 87: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:32,900 - INFO - Epoch 87: EPOCH_AVG_TRAIN_LOSS=0.9424
2025-02-04 16:55:32,903 - INFO - #################### Training epoch 88 ####################
2025-02-04 16:55:32,903 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 16:55:33,110 - INFO - Epoch 88: train_loss=0.9265
2025-02-04 16:55:33,370 - INFO - Epoch 88: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:33,374 - INFO - Epoch 88: EPOCH_AVG_TRAIN_LOSS=0.9265
2025-02-04 16:55:33,377 - INFO - #################### Training epoch 89 ####################
2025-02-04 16:55:33,377 - INFO - Current Learning Rate: 9.765625e-06
2025-02-04 16:55:33,582 - INFO - Epoch 89: train_loss=0.9653
2025-02-04 16:55:33,846 - INFO - Epoch 89: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:33,849 - INFO - Epoch 89: EPOCH_AVG_TRAIN_LOSS=0.9653
2025-02-04 16:55:33,853 - INFO - #################### Training epoch 90 ####################
2025-02-04 16:55:33,853 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 16:55:34,058 - INFO - Epoch 90: train_loss=0.9822
2025-02-04 16:55:34,320 - INFO - Epoch 90: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:34,324 - INFO - Epoch 90: EPOCH_AVG_TRAIN_LOSS=0.9822
2025-02-04 16:55:34,327 - INFO - #################### Training epoch 91 ####################
2025-02-04 16:55:34,327 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 16:55:34,533 - INFO - Epoch 91: train_loss=0.9394
2025-02-04 16:55:34,796 - INFO - Epoch 91: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:34,800 - INFO - Epoch 91: EPOCH_AVG_TRAIN_LOSS=0.9394
2025-02-04 16:55:34,803 - INFO - #################### Training epoch 92 ####################
2025-02-04 16:55:34,803 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 16:55:35,009 - INFO - Epoch 92: train_loss=0.9657
2025-02-04 16:55:35,275 - INFO - Epoch 92: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:35,279 - INFO - Epoch 92: EPOCH_AVG_TRAIN_LOSS=0.9657
2025-02-04 16:55:35,282 - INFO - #################### Training epoch 93 ####################
2025-02-04 16:55:35,282 - INFO - Current Learning Rate: 4.882813e-06
2025-02-04 16:55:35,489 - INFO - Epoch 93: train_loss=0.9532
2025-02-04 16:55:35,750 - INFO - Epoch 93: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:35,754 - INFO - Epoch 93: EPOCH_AVG_TRAIN_LOSS=0.9532
2025-02-04 16:55:35,757 - INFO - #################### Training epoch 94 ####################
2025-02-04 16:55:35,757 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 16:55:35,965 - INFO - Epoch 94: train_loss=0.9139
2025-02-04 16:55:36,228 - INFO - Epoch 94: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:36,231 - INFO - Epoch 94: EPOCH_AVG_TRAIN_LOSS=0.9139
2025-02-04 16:55:36,235 - INFO - #################### Training epoch 95 ####################
2025-02-04 16:55:36,235 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 16:55:36,442 - INFO - Epoch 95: train_loss=0.9696
2025-02-04 16:55:36,705 - INFO - Epoch 95: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:36,709 - INFO - Epoch 95: EPOCH_AVG_TRAIN_LOSS=0.9696
2025-02-04 16:55:36,712 - INFO - #################### Training epoch 96 ####################
2025-02-04 16:55:36,712 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 16:55:36,919 - INFO - Epoch 96: train_loss=0.9625
2025-02-04 16:55:37,181 - INFO - Epoch 96: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:37,186 - INFO - Epoch 96: EPOCH_AVG_TRAIN_LOSS=0.9625
2025-02-04 16:55:37,189 - INFO - #################### Training epoch 97 ####################
2025-02-04 16:55:37,189 - INFO - Current Learning Rate: 2.441406e-06
2025-02-04 16:55:37,396 - INFO - Epoch 97: train_loss=0.9829
2025-02-04 16:55:37,658 - INFO - Epoch 97: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:37,662 - INFO - Epoch 97: EPOCH_AVG_TRAIN_LOSS=0.9829
2025-02-04 16:55:37,665 - INFO - #################### Training epoch 98 ####################
2025-02-04 16:55:37,665 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 16:55:37,872 - INFO - Epoch 98: train_loss=0.9253
2025-02-04 16:55:38,134 - INFO - Epoch 98: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:38,138 - INFO - Epoch 98: EPOCH_AVG_TRAIN_LOSS=0.9253
2025-02-04 16:55:38,141 - INFO - #################### Training epoch 99 ####################
2025-02-04 16:55:38,141 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 16:55:38,347 - INFO - Epoch 99: train_loss=0.9779
2025-02-04 16:55:38,612 - INFO - Epoch 99: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:38,615 - INFO - Epoch 99: EPOCH_AVG_TRAIN_LOSS=0.9779
2025-02-04 16:55:38,619 - INFO - #################### Training epoch 100 ####################
2025-02-04 16:55:38,619 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 16:55:38,825 - INFO - Epoch 100: train_loss=0.9342
2025-02-04 16:55:39,087 - INFO - Epoch 100: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:39,091 - INFO - Epoch 100: EPOCH_AVG_TRAIN_LOSS=0.9342
2025-02-04 16:55:39,094 - INFO - #################### Training epoch 101 ####################
2025-02-04 16:55:39,094 - INFO - Current Learning Rate: 1.220703e-06
2025-02-04 16:55:39,301 - INFO - Epoch 101: train_loss=0.9481
2025-02-04 16:55:39,564 - INFO - Epoch 101: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:39,567 - INFO - Epoch 101: EPOCH_AVG_TRAIN_LOSS=0.9481
2025-02-04 16:55:39,571 - INFO - #################### Training epoch 102 ####################
2025-02-04 16:55:39,571 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 16:55:39,778 - INFO - Epoch 102: train_loss=0.9637
2025-02-04 16:55:40,041 - INFO - Epoch 102: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:40,044 - INFO - Epoch 102: EPOCH_AVG_TRAIN_LOSS=0.9637
2025-02-04 16:55:40,048 - INFO - #################### Training epoch 103 ####################
2025-02-04 16:55:40,048 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 16:55:40,257 - INFO - Epoch 103: train_loss=0.9695
2025-02-04 16:55:40,520 - INFO - Epoch 103: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:40,524 - INFO - Epoch 103: EPOCH_AVG_TRAIN_LOSS=0.9695
2025-02-04 16:55:40,527 - INFO - #################### Training epoch 104 ####################
2025-02-04 16:55:40,527 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 16:55:40,731 - INFO - Epoch 104: train_loss=0.9578
2025-02-04 16:55:40,992 - INFO - Epoch 104: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:40,996 - INFO - Epoch 104: EPOCH_AVG_TRAIN_LOSS=0.9578
2025-02-04 16:55:40,999 - INFO - #################### Training epoch 105 ####################
2025-02-04 16:55:40,999 - INFO - Current Learning Rate: 6.103516e-07
2025-02-04 16:55:41,205 - INFO - Epoch 105: train_loss=0.9455
2025-02-04 16:55:41,465 - INFO - Epoch 105: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:41,469 - INFO - Epoch 105: EPOCH_AVG_TRAIN_LOSS=0.9455
2025-02-04 16:55:41,472 - INFO - #################### Training epoch 106 ####################
2025-02-04 16:55:41,472 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 16:55:41,680 - INFO - Epoch 106: train_loss=0.9507
2025-02-04 16:55:41,940 - INFO - Epoch 106: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:41,944 - INFO - Epoch 106: EPOCH_AVG_TRAIN_LOSS=0.9507
2025-02-04 16:55:41,947 - INFO - #################### Training epoch 107 ####################
2025-02-04 16:55:41,947 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 16:55:42,153 - INFO - Epoch 107: train_loss=0.9447
2025-02-04 16:55:42,417 - INFO - Epoch 107: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:42,420 - INFO - Epoch 107: EPOCH_AVG_TRAIN_LOSS=0.9447
2025-02-04 16:55:42,424 - INFO - #################### Training epoch 108 ####################
2025-02-04 16:55:42,424 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 16:55:42,629 - INFO - Epoch 108: train_loss=0.9767
2025-02-04 16:55:42,891 - INFO - Epoch 108: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:42,895 - INFO - Epoch 108: EPOCH_AVG_TRAIN_LOSS=0.9767
2025-02-04 16:55:42,898 - INFO - #################### Training epoch 109 ####################
2025-02-04 16:55:42,898 - INFO - Current Learning Rate: 3.051758e-07
2025-02-04 16:55:43,103 - INFO - Epoch 109: train_loss=0.9159
2025-02-04 16:55:43,366 - INFO - Epoch 109: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:43,370 - INFO - Epoch 109: EPOCH_AVG_TRAIN_LOSS=0.9159
2025-02-04 16:55:43,373 - INFO - #################### Training epoch 110 ####################
2025-02-04 16:55:43,373 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 16:55:43,579 - INFO - Epoch 110: train_loss=0.9799
2025-02-04 16:55:43,840 - INFO - Epoch 110: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:43,844 - INFO - Epoch 110: EPOCH_AVG_TRAIN_LOSS=0.9799
2025-02-04 16:55:43,847 - INFO - #################### Training epoch 111 ####################
2025-02-04 16:55:43,847 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 16:55:44,054 - INFO - Epoch 111: train_loss=0.9712
2025-02-04 16:55:44,315 - INFO - Epoch 111: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:44,319 - INFO - Epoch 111: EPOCH_AVG_TRAIN_LOSS=0.9712
2025-02-04 16:55:44,322 - INFO - #################### Training epoch 112 ####################
2025-02-04 16:55:44,322 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 16:55:44,526 - INFO - Epoch 112: train_loss=0.9583
2025-02-04 16:55:44,789 - INFO - Epoch 112: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:44,793 - INFO - Epoch 112: EPOCH_AVG_TRAIN_LOSS=0.9583
2025-02-04 16:55:44,796 - INFO - #################### Training epoch 113 ####################
2025-02-04 16:55:44,796 - INFO - Current Learning Rate: 1.525879e-07
2025-02-04 16:55:45,004 - INFO - Epoch 113: train_loss=0.9699
2025-02-04 16:55:45,267 - INFO - Epoch 113: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:45,271 - INFO - Epoch 113: EPOCH_AVG_TRAIN_LOSS=0.9699
2025-02-04 16:55:45,274 - INFO - #################### Training epoch 114 ####################
2025-02-04 16:55:45,274 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 16:55:45,480 - INFO - Epoch 114: train_loss=0.9506
2025-02-04 16:55:45,737 - INFO - Epoch 114: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:45,741 - INFO - Epoch 114: EPOCH_AVG_TRAIN_LOSS=0.9506
2025-02-04 16:55:45,744 - INFO - #################### Training epoch 115 ####################
2025-02-04 16:55:45,744 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 16:55:45,949 - INFO - Epoch 115: train_loss=0.9814
2025-02-04 16:55:46,213 - INFO - Epoch 115: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:46,217 - INFO - Epoch 115: EPOCH_AVG_TRAIN_LOSS=0.9814
2025-02-04 16:55:46,220 - INFO - #################### Training epoch 116 ####################
2025-02-04 16:55:46,220 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 16:55:46,426 - INFO - Epoch 116: train_loss=0.9337
2025-02-04 16:55:46,686 - INFO - Epoch 116: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:46,690 - INFO - Epoch 116: EPOCH_AVG_TRAIN_LOSS=0.9337
2025-02-04 16:55:46,693 - INFO - #################### Training epoch 117 ####################
2025-02-04 16:55:46,693 - INFO - Current Learning Rate: 7.629395e-08
2025-02-04 16:55:46,901 - INFO - Epoch 117: train_loss=0.9679
2025-02-04 16:55:47,163 - INFO - Epoch 117: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:47,167 - INFO - Epoch 117: EPOCH_AVG_TRAIN_LOSS=0.9679
2025-02-04 16:55:47,170 - INFO - #################### Training epoch 118 ####################
2025-02-04 16:55:47,170 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 16:55:47,378 - INFO - Epoch 118: train_loss=0.9628
2025-02-04 16:55:47,642 - INFO - Epoch 118: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:47,645 - INFO - Epoch 118: EPOCH_AVG_TRAIN_LOSS=0.9628
2025-02-04 16:55:47,649 - INFO - #################### Training epoch 119 ####################
2025-02-04 16:55:47,649 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 16:55:47,854 - INFO - Epoch 119: train_loss=0.9620
2025-02-04 16:55:48,118 - INFO - Epoch 119: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:48,122 - INFO - Epoch 119: EPOCH_AVG_TRAIN_LOSS=0.9620
2025-02-04 16:55:48,125 - INFO - #################### Training epoch 120 ####################
2025-02-04 16:55:48,125 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 16:55:48,331 - INFO - Epoch 120: train_loss=0.9911
2025-02-04 16:55:48,594 - INFO - Epoch 120: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:48,598 - INFO - Epoch 120: EPOCH_AVG_TRAIN_LOSS=0.9911
2025-02-04 16:55:48,601 - INFO - #################### Training epoch 121 ####################
2025-02-04 16:55:48,601 - INFO - Current Learning Rate: 3.814697e-08
2025-02-04 16:55:48,807 - INFO - Epoch 121: train_loss=0.9250
2025-02-04 16:55:49,069 - INFO - Epoch 121: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:49,073 - INFO - Epoch 121: EPOCH_AVG_TRAIN_LOSS=0.9250
2025-02-04 16:55:49,076 - INFO - #################### Training epoch 122 ####################
2025-02-04 16:55:49,076 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:49,281 - INFO - Epoch 122: train_loss=0.9716
2025-02-04 16:55:49,543 - INFO - Epoch 122: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:49,546 - INFO - Epoch 122: EPOCH_AVG_TRAIN_LOSS=0.9716
2025-02-04 16:55:49,550 - INFO - #################### Training epoch 123 ####################
2025-02-04 16:55:49,550 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:49,756 - INFO - Epoch 123: train_loss=0.9777
2025-02-04 16:55:50,018 - INFO - Epoch 123: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:50,022 - INFO - Epoch 123: EPOCH_AVG_TRAIN_LOSS=0.9777
2025-02-04 16:55:50,025 - INFO - #################### Training epoch 124 ####################
2025-02-04 16:55:50,025 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:50,233 - INFO - Epoch 124: train_loss=0.9677
2025-02-04 16:55:50,496 - INFO - Epoch 124: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:50,500 - INFO - Epoch 124: EPOCH_AVG_TRAIN_LOSS=0.9677
2025-02-04 16:55:50,503 - INFO - #################### Training epoch 125 ####################
2025-02-04 16:55:50,503 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:50,710 - INFO - Epoch 125: train_loss=0.9539
2025-02-04 16:55:50,969 - INFO - Epoch 125: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:50,973 - INFO - Epoch 125: EPOCH_AVG_TRAIN_LOSS=0.9539
2025-02-04 16:55:50,976 - INFO - #################### Training epoch 126 ####################
2025-02-04 16:55:50,976 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:51,183 - INFO - Epoch 126: train_loss=0.9741
2025-02-04 16:55:51,443 - INFO - Epoch 126: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:51,447 - INFO - Epoch 126: EPOCH_AVG_TRAIN_LOSS=0.9741
2025-02-04 16:55:51,450 - INFO - #################### Training epoch 127 ####################
2025-02-04 16:55:51,450 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:51,656 - INFO - Epoch 127: train_loss=0.9577
2025-02-04 16:55:51,919 - INFO - Epoch 127: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:51,922 - INFO - Epoch 127: EPOCH_AVG_TRAIN_LOSS=0.9577
2025-02-04 16:55:51,926 - INFO - #################### Training epoch 128 ####################
2025-02-04 16:55:51,926 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:52,132 - INFO - Epoch 128: train_loss=0.9654
2025-02-04 16:55:52,395 - INFO - Epoch 128: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:52,399 - INFO - Epoch 128: EPOCH_AVG_TRAIN_LOSS=0.9654
2025-02-04 16:55:52,402 - INFO - #################### Training epoch 129 ####################
2025-02-04 16:55:52,402 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:52,609 - INFO - Epoch 129: train_loss=0.9725
2025-02-04 16:55:52,872 - INFO - Epoch 129: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:52,876 - INFO - Epoch 129: EPOCH_AVG_TRAIN_LOSS=0.9725
2025-02-04 16:55:52,879 - INFO - #################### Training epoch 130 ####################
2025-02-04 16:55:52,879 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:53,088 - INFO - Epoch 130: train_loss=0.9567
2025-02-04 16:55:53,350 - INFO - Epoch 130: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:53,353 - INFO - Epoch 130: EPOCH_AVG_TRAIN_LOSS=0.9567
2025-02-04 16:55:53,357 - INFO - #################### Training epoch 131 ####################
2025-02-04 16:55:53,357 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:53,566 - INFO - Epoch 131: train_loss=1.0470
2025-02-04 16:55:53,829 - INFO - Epoch 131: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:53,832 - INFO - Epoch 131: EPOCH_AVG_TRAIN_LOSS=1.0470
2025-02-04 16:55:53,836 - INFO - #################### Training epoch 132 ####################
2025-02-04 16:55:53,836 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:54,042 - INFO - Epoch 132: train_loss=0.9687
2025-02-04 16:55:54,302 - INFO - Epoch 132: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:54,305 - INFO - Epoch 132: EPOCH_AVG_TRAIN_LOSS=0.9687
2025-02-04 16:55:54,309 - INFO - #################### Training epoch 133 ####################
2025-02-04 16:55:54,309 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:54,514 - INFO - Epoch 133: train_loss=0.9281
2025-02-04 16:55:54,775 - INFO - Epoch 133: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:54,778 - INFO - Epoch 133: EPOCH_AVG_TRAIN_LOSS=0.9281
2025-02-04 16:55:54,781 - INFO - #################### Training epoch 134 ####################
2025-02-04 16:55:54,782 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:54,988 - INFO - Epoch 134: train_loss=0.9611
2025-02-04 16:55:55,250 - INFO - Epoch 134: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:55,254 - INFO - Epoch 134: EPOCH_AVG_TRAIN_LOSS=0.9611
2025-02-04 16:55:55,257 - INFO - #################### Training epoch 135 ####################
2025-02-04 16:55:55,257 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:55,464 - INFO - Epoch 135: train_loss=0.9166
2025-02-04 16:55:55,725 - INFO - Epoch 135: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:55,729 - INFO - Epoch 135: EPOCH_AVG_TRAIN_LOSS=0.9166
2025-02-04 16:55:55,732 - INFO - #################### Training epoch 136 ####################
2025-02-04 16:55:55,732 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:55,935 - INFO - Epoch 136: train_loss=0.9852
2025-02-04 16:55:56,199 - INFO - Epoch 136: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:56,202 - INFO - Epoch 136: EPOCH_AVG_TRAIN_LOSS=0.9852
2025-02-04 16:55:56,206 - INFO - #################### Training epoch 137 ####################
2025-02-04 16:55:56,206 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:56,414 - INFO - Epoch 137: train_loss=0.9545
2025-02-04 16:55:56,675 - INFO - Epoch 137: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:56,679 - INFO - Epoch 137: EPOCH_AVG_TRAIN_LOSS=0.9545
2025-02-04 16:55:56,682 - INFO - #################### Training epoch 138 ####################
2025-02-04 16:55:56,682 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:56,889 - INFO - Epoch 138: train_loss=0.9151
2025-02-04 16:55:57,151 - INFO - Epoch 138: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:57,154 - INFO - Epoch 138: EPOCH_AVG_TRAIN_LOSS=0.9151
2025-02-04 16:55:57,158 - INFO - #################### Training epoch 139 ####################
2025-02-04 16:55:57,158 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:57,365 - INFO - Epoch 139: train_loss=0.9817
2025-02-04 16:55:57,625 - INFO - Epoch 139: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:57,629 - INFO - Epoch 139: EPOCH_AVG_TRAIN_LOSS=0.9817
2025-02-04 16:55:57,632 - INFO - #################### Training epoch 140 ####################
2025-02-04 16:55:57,632 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:57,839 - INFO - Epoch 140: train_loss=0.9858
2025-02-04 16:55:58,100 - INFO - Epoch 140: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:58,104 - INFO - Epoch 140: EPOCH_AVG_TRAIN_LOSS=0.9858
2025-02-04 16:55:58,107 - INFO - #################### Training epoch 141 ####################
2025-02-04 16:55:58,107 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:58,313 - INFO - Epoch 141: train_loss=0.9621
2025-02-04 16:55:58,574 - INFO - Epoch 141: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:58,578 - INFO - Epoch 141: EPOCH_AVG_TRAIN_LOSS=0.9621
2025-02-04 16:55:58,581 - INFO - #################### Training epoch 142 ####################
2025-02-04 16:55:58,581 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:58,789 - INFO - Epoch 142: train_loss=0.9647
2025-02-04 16:55:59,051 - INFO - Epoch 142: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:59,054 - INFO - Epoch 142: EPOCH_AVG_TRAIN_LOSS=0.9647
2025-02-04 16:55:59,057 - INFO - #################### Training epoch 143 ####################
2025-02-04 16:55:59,057 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:59,264 - INFO - Epoch 143: train_loss=0.9619
2025-02-04 16:55:59,528 - INFO - Epoch 143: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:55:59,532 - INFO - Epoch 143: EPOCH_AVG_TRAIN_LOSS=0.9619
2025-02-04 16:55:59,536 - INFO - #################### Training epoch 144 ####################
2025-02-04 16:55:59,536 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:55:59,741 - INFO - Epoch 144: train_loss=0.9709
2025-02-04 16:56:00,003 - INFO - Epoch 144: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:00,007 - INFO - Epoch 144: EPOCH_AVG_TRAIN_LOSS=0.9709
2025-02-04 16:56:00,010 - INFO - #################### Training epoch 145 ####################
2025-02-04 16:56:00,010 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:00,219 - INFO - Epoch 145: train_loss=0.9745
2025-02-04 16:56:00,482 - INFO - Epoch 145: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:00,486 - INFO - Epoch 145: EPOCH_AVG_TRAIN_LOSS=0.9745
2025-02-04 16:56:00,489 - INFO - #################### Training epoch 146 ####################
2025-02-04 16:56:00,489 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:00,697 - INFO - Epoch 146: train_loss=0.9231
2025-02-04 16:56:00,957 - INFO - Epoch 146: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:00,961 - INFO - Epoch 146: EPOCH_AVG_TRAIN_LOSS=0.9231
2025-02-04 16:56:00,964 - INFO - #################### Training epoch 147 ####################
2025-02-04 16:56:00,964 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:01,171 - INFO - Epoch 147: train_loss=0.9463
2025-02-04 16:56:01,434 - INFO - Epoch 147: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:01,437 - INFO - Epoch 147: EPOCH_AVG_TRAIN_LOSS=0.9463
2025-02-04 16:56:01,441 - INFO - #################### Training epoch 148 ####################
2025-02-04 16:56:01,441 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:01,647 - INFO - Epoch 148: train_loss=0.9688
2025-02-04 16:56:01,908 - INFO - Epoch 148: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:01,912 - INFO - Epoch 148: EPOCH_AVG_TRAIN_LOSS=0.9688
2025-02-04 16:56:01,915 - INFO - #################### Training epoch 149 ####################
2025-02-04 16:56:01,916 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:02,122 - INFO - Epoch 149: train_loss=0.9553
2025-02-04 16:56:02,383 - INFO - Epoch 149: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:02,387 - INFO - Epoch 149: EPOCH_AVG_TRAIN_LOSS=0.9553
2025-02-04 16:56:02,390 - INFO - #################### Training epoch 150 ####################
2025-02-04 16:56:02,390 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:02,596 - INFO - Epoch 150: train_loss=0.9983
2025-02-04 16:56:02,858 - INFO - Epoch 150: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:02,861 - INFO - Epoch 150: EPOCH_AVG_TRAIN_LOSS=0.9983
2025-02-04 16:56:02,865 - INFO - #################### Training epoch 151 ####################
2025-02-04 16:56:02,865 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:03,073 - INFO - Epoch 151: train_loss=0.9337
2025-02-04 16:56:03,337 - INFO - Epoch 151: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:03,340 - INFO - Epoch 151: EPOCH_AVG_TRAIN_LOSS=0.9337
2025-02-04 16:56:03,344 - INFO - #################### Training epoch 152 ####################
2025-02-04 16:56:03,344 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:03,551 - INFO - Epoch 152: train_loss=0.9464
2025-02-04 16:56:03,813 - INFO - Epoch 152: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:03,817 - INFO - Epoch 152: EPOCH_AVG_TRAIN_LOSS=0.9464
2025-02-04 16:56:03,821 - INFO - #################### Training epoch 153 ####################
2025-02-04 16:56:03,821 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:04,030 - INFO - Epoch 153: train_loss=0.9495
2025-02-04 16:56:04,292 - INFO - Epoch 153: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:04,296 - INFO - Epoch 153: EPOCH_AVG_TRAIN_LOSS=0.9495
2025-02-04 16:56:04,300 - INFO - #################### Training epoch 154 ####################
2025-02-04 16:56:04,300 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:04,508 - INFO - Epoch 154: train_loss=0.9647
2025-02-04 16:56:04,771 - INFO - Epoch 154: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:04,775 - INFO - Epoch 154: EPOCH_AVG_TRAIN_LOSS=0.9647
2025-02-04 16:56:04,778 - INFO - #################### Training epoch 155 ####################
2025-02-04 16:56:04,778 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:04,983 - INFO - Epoch 155: train_loss=0.9776
2025-02-04 16:56:05,247 - INFO - Epoch 155: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:05,251 - INFO - Epoch 155: EPOCH_AVG_TRAIN_LOSS=0.9776
2025-02-04 16:56:05,254 - INFO - #################### Training epoch 156 ####################
2025-02-04 16:56:05,254 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:05,462 - INFO - Epoch 156: train_loss=0.9430
2025-02-04 16:56:05,725 - INFO - Epoch 156: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:05,729 - INFO - Epoch 156: EPOCH_AVG_TRAIN_LOSS=0.9430
2025-02-04 16:56:05,733 - INFO - #################### Training epoch 157 ####################
2025-02-04 16:56:05,733 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:05,939 - INFO - Epoch 157: train_loss=0.9621
2025-02-04 16:56:06,200 - INFO - Epoch 157: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:06,204 - INFO - Epoch 157: EPOCH_AVG_TRAIN_LOSS=0.9621
2025-02-04 16:56:06,207 - INFO - #################### Training epoch 158 ####################
2025-02-04 16:56:06,208 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:06,415 - INFO - Epoch 158: train_loss=0.9650
2025-02-04 16:56:06,679 - INFO - Epoch 158: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:06,683 - INFO - Epoch 158: EPOCH_AVG_TRAIN_LOSS=0.9650
2025-02-04 16:56:06,686 - INFO - #################### Training epoch 159 ####################
2025-02-04 16:56:06,686 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:06,893 - INFO - Epoch 159: train_loss=0.9649
2025-02-04 16:56:07,154 - INFO - Epoch 159: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:07,158 - INFO - Epoch 159: EPOCH_AVG_TRAIN_LOSS=0.9649
2025-02-04 16:56:07,161 - INFO - #################### Training epoch 160 ####################
2025-02-04 16:56:07,161 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:07,369 - INFO - Epoch 160: train_loss=0.9604
2025-02-04 16:56:07,631 - INFO - Epoch 160: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:07,635 - INFO - Epoch 160: EPOCH_AVG_TRAIN_LOSS=0.9604
2025-02-04 16:56:07,638 - INFO - #################### Training epoch 161 ####################
2025-02-04 16:56:07,638 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:07,844 - INFO - Epoch 161: train_loss=0.9566
2025-02-04 16:56:08,105 - INFO - Epoch 161: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:08,108 - INFO - Epoch 161: EPOCH_AVG_TRAIN_LOSS=0.9566
2025-02-04 16:56:08,112 - INFO - #################### Training epoch 162 ####################
2025-02-04 16:56:08,112 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:08,319 - INFO - Epoch 162: train_loss=0.9098
2025-02-04 16:56:08,581 - INFO - Epoch 162: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:08,584 - INFO - Epoch 162: EPOCH_AVG_TRAIN_LOSS=0.9098
2025-02-04 16:56:08,588 - INFO - #################### Training epoch 163 ####################
2025-02-04 16:56:08,588 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:08,793 - INFO - Epoch 163: train_loss=0.9614
2025-02-04 16:56:09,056 - INFO - Epoch 163: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:09,060 - INFO - Epoch 163: EPOCH_AVG_TRAIN_LOSS=0.9614
2025-02-04 16:56:09,063 - INFO - #################### Training epoch 164 ####################
2025-02-04 16:56:09,064 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:09,421 - INFO - Epoch 164: train_loss=0.9404
2025-02-04 16:56:09,687 - INFO - Epoch 164: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:09,691 - INFO - Epoch 164: EPOCH_AVG_TRAIN_LOSS=0.9404
2025-02-04 16:56:09,694 - INFO - #################### Training epoch 165 ####################
2025-02-04 16:56:09,694 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:09,910 - INFO - Epoch 165: train_loss=0.9694
2025-02-04 16:56:10,175 - INFO - Epoch 165: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:10,179 - INFO - Epoch 165: EPOCH_AVG_TRAIN_LOSS=0.9694
2025-02-04 16:56:10,182 - INFO - #################### Training epoch 166 ####################
2025-02-04 16:56:10,182 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:10,396 - INFO - Epoch 166: train_loss=0.9590
2025-02-04 16:56:10,662 - INFO - Epoch 166: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:10,666 - INFO - Epoch 166: EPOCH_AVG_TRAIN_LOSS=0.9590
2025-02-04 16:56:10,669 - INFO - #################### Training epoch 167 ####################
2025-02-04 16:56:10,669 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:10,882 - INFO - Epoch 167: train_loss=0.9812
2025-02-04 16:56:11,148 - INFO - Epoch 167: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:11,152 - INFO - Epoch 167: EPOCH_AVG_TRAIN_LOSS=0.9812
2025-02-04 16:56:11,155 - INFO - #################### Training epoch 168 ####################
2025-02-04 16:56:11,155 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:11,366 - INFO - Epoch 168: train_loss=0.9593
2025-02-04 16:56:11,632 - INFO - Epoch 168: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:11,636 - INFO - Epoch 168: EPOCH_AVG_TRAIN_LOSS=0.9593
2025-02-04 16:56:11,639 - INFO - #################### Training epoch 169 ####################
2025-02-04 16:56:11,639 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:11,853 - INFO - Epoch 169: train_loss=0.9233
2025-02-04 16:56:12,120 - INFO - Epoch 169: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:12,123 - INFO - Epoch 169: EPOCH_AVG_TRAIN_LOSS=0.9233
2025-02-04 16:56:12,126 - INFO - #################### Training epoch 170 ####################
2025-02-04 16:56:12,126 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:12,339 - INFO - Epoch 170: train_loss=0.9670
2025-02-04 16:56:12,604 - INFO - Epoch 170: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:12,607 - INFO - Epoch 170: EPOCH_AVG_TRAIN_LOSS=0.9670
2025-02-04 16:56:12,611 - INFO - #################### Training epoch 171 ####################
2025-02-04 16:56:12,611 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:12,824 - INFO - Epoch 171: train_loss=0.9664
2025-02-04 16:56:13,087 - INFO - Epoch 171: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:13,091 - INFO - Epoch 171: EPOCH_AVG_TRAIN_LOSS=0.9664
2025-02-04 16:56:13,094 - INFO - #################### Training epoch 172 ####################
2025-02-04 16:56:13,094 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:13,309 - INFO - Epoch 172: train_loss=0.9597
2025-02-04 16:56:13,574 - INFO - Epoch 172: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:13,578 - INFO - Epoch 172: EPOCH_AVG_TRAIN_LOSS=0.9597
2025-02-04 16:56:13,581 - INFO - #################### Training epoch 173 ####################
2025-02-04 16:56:13,581 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:13,794 - INFO - Epoch 173: train_loss=0.9600
2025-02-04 16:56:14,060 - INFO - Epoch 173: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:14,063 - INFO - Epoch 173: EPOCH_AVG_TRAIN_LOSS=0.9600
2025-02-04 16:56:14,067 - INFO - #################### Training epoch 174 ####################
2025-02-04 16:56:14,067 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:14,280 - INFO - Epoch 174: train_loss=0.9209
2025-02-04 16:56:14,549 - INFO - Epoch 174: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:14,553 - INFO - Epoch 174: EPOCH_AVG_TRAIN_LOSS=0.9209
2025-02-04 16:56:14,556 - INFO - #################### Training epoch 175 ####################
2025-02-04 16:56:14,556 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:14,767 - INFO - Epoch 175: train_loss=0.9624
2025-02-04 16:56:15,035 - INFO - Epoch 175: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:15,039 - INFO - Epoch 175: EPOCH_AVG_TRAIN_LOSS=0.9624
2025-02-04 16:56:15,042 - INFO - #################### Training epoch 176 ####################
2025-02-04 16:56:15,042 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:15,254 - INFO - Epoch 176: train_loss=0.9529
2025-02-04 16:56:15,523 - INFO - Epoch 176: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:15,527 - INFO - Epoch 176: EPOCH_AVG_TRAIN_LOSS=0.9529
2025-02-04 16:56:15,530 - INFO - #################### Training epoch 177 ####################
2025-02-04 16:56:15,530 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:15,742 - INFO - Epoch 177: train_loss=0.8995
2025-02-04 16:56:16,009 - INFO - Epoch 177: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:16,013 - INFO - Epoch 177: EPOCH_AVG_TRAIN_LOSS=0.8995
2025-02-04 16:56:16,016 - INFO - #################### Training epoch 178 ####################
2025-02-04 16:56:16,016 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:16,230 - INFO - Epoch 178: train_loss=0.9563
2025-02-04 16:56:16,498 - INFO - Epoch 178: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:16,502 - INFO - Epoch 178: EPOCH_AVG_TRAIN_LOSS=0.9563
2025-02-04 16:56:16,505 - INFO - #################### Training epoch 179 ####################
2025-02-04 16:56:16,505 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:16,717 - INFO - Epoch 179: train_loss=0.9613
2025-02-04 16:56:16,983 - INFO - Epoch 179: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:16,987 - INFO - Epoch 179: EPOCH_AVG_TRAIN_LOSS=0.9613
2025-02-04 16:56:16,990 - INFO - #################### Training epoch 180 ####################
2025-02-04 16:56:16,990 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:17,203 - INFO - Epoch 180: train_loss=0.9374
2025-02-04 16:56:17,473 - INFO - Epoch 180: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:17,477 - INFO - Epoch 180: EPOCH_AVG_TRAIN_LOSS=0.9374
2025-02-04 16:56:17,480 - INFO - #################### Training epoch 181 ####################
2025-02-04 16:56:17,480 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:17,692 - INFO - Epoch 181: train_loss=0.9735
2025-02-04 16:56:17,958 - INFO - Epoch 181: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:17,962 - INFO - Epoch 181: EPOCH_AVG_TRAIN_LOSS=0.9735
2025-02-04 16:56:17,965 - INFO - #################### Training epoch 182 ####################
2025-02-04 16:56:17,965 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:18,177 - INFO - Epoch 182: train_loss=0.9540
2025-02-04 16:56:18,444 - INFO - Epoch 182: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:18,447 - INFO - Epoch 182: EPOCH_AVG_TRAIN_LOSS=0.9540
2025-02-04 16:56:18,451 - INFO - #################### Training epoch 183 ####################
2025-02-04 16:56:18,451 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:18,663 - INFO - Epoch 183: train_loss=0.9741
2025-02-04 16:56:18,933 - INFO - Epoch 183: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:18,936 - INFO - Epoch 183: EPOCH_AVG_TRAIN_LOSS=0.9741
2025-02-04 16:56:18,940 - INFO - #################### Training epoch 184 ####################
2025-02-04 16:56:18,940 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:19,152 - INFO - Epoch 184: train_loss=0.9537
2025-02-04 16:56:19,421 - INFO - Epoch 184: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:19,425 - INFO - Epoch 184: EPOCH_AVG_TRAIN_LOSS=0.9537
2025-02-04 16:56:19,428 - INFO - #################### Training epoch 185 ####################
2025-02-04 16:56:19,428 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:19,640 - INFO - Epoch 185: train_loss=0.9844
2025-02-04 16:56:19,909 - INFO - Epoch 185: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:19,912 - INFO - Epoch 185: EPOCH_AVG_TRAIN_LOSS=0.9844
2025-02-04 16:56:19,915 - INFO - #################### Training epoch 186 ####################
2025-02-04 16:56:19,916 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:20,126 - INFO - Epoch 186: train_loss=0.9614
2025-02-04 16:56:20,393 - INFO - Epoch 186: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:20,397 - INFO - Epoch 186: EPOCH_AVG_TRAIN_LOSS=0.9614
2025-02-04 16:56:20,400 - INFO - #################### Training epoch 187 ####################
2025-02-04 16:56:20,400 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:20,613 - INFO - Epoch 187: train_loss=0.9616
2025-02-04 16:56:20,882 - INFO - Epoch 187: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:20,886 - INFO - Epoch 187: EPOCH_AVG_TRAIN_LOSS=0.9616
2025-02-04 16:56:20,889 - INFO - #################### Training epoch 188 ####################
2025-02-04 16:56:20,889 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:21,102 - INFO - Epoch 188: train_loss=0.9506
2025-02-04 16:56:21,371 - INFO - Epoch 188: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:21,375 - INFO - Epoch 188: EPOCH_AVG_TRAIN_LOSS=0.9506
2025-02-04 16:56:21,378 - INFO - #################### Training epoch 189 ####################
2025-02-04 16:56:21,378 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:21,591 - INFO - Epoch 189: train_loss=0.9601
2025-02-04 16:56:21,858 - INFO - Epoch 189: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:21,862 - INFO - Epoch 189: EPOCH_AVG_TRAIN_LOSS=0.9601
2025-02-04 16:56:21,865 - INFO - #################### Training epoch 190 ####################
2025-02-04 16:56:21,865 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:22,078 - INFO - Epoch 190: train_loss=0.9705
2025-02-04 16:56:22,345 - INFO - Epoch 190: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:22,349 - INFO - Epoch 190: EPOCH_AVG_TRAIN_LOSS=0.9705
2025-02-04 16:56:22,352 - INFO - #################### Training epoch 191 ####################
2025-02-04 16:56:22,353 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:22,565 - INFO - Epoch 191: train_loss=0.9772
2025-02-04 16:56:22,834 - INFO - Epoch 191: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:22,838 - INFO - Epoch 191: EPOCH_AVG_TRAIN_LOSS=0.9772
2025-02-04 16:56:22,841 - INFO - #################### Training epoch 192 ####################
2025-02-04 16:56:22,841 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:23,054 - INFO - Epoch 192: train_loss=0.9713
2025-02-04 16:56:23,318 - INFO - Epoch 192: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:23,322 - INFO - Epoch 192: EPOCH_AVG_TRAIN_LOSS=0.9713
2025-02-04 16:56:23,325 - INFO - #################### Training epoch 193 ####################
2025-02-04 16:56:23,325 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:23,537 - INFO - Epoch 193: train_loss=0.9555
2025-02-04 16:56:23,806 - INFO - Epoch 193: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:23,810 - INFO - Epoch 193: EPOCH_AVG_TRAIN_LOSS=0.9555
2025-02-04 16:56:23,814 - INFO - #################### Training epoch 194 ####################
2025-02-04 16:56:23,814 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:24,026 - INFO - Epoch 194: train_loss=0.9825
2025-02-04 16:56:24,296 - INFO - Epoch 194: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:24,300 - INFO - Epoch 194: EPOCH_AVG_TRAIN_LOSS=0.9825
2025-02-04 16:56:24,303 - INFO - #################### Training epoch 195 ####################
2025-02-04 16:56:24,303 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:24,519 - INFO - Epoch 195: train_loss=0.9075
2025-02-04 16:56:24,795 - INFO - Epoch 195: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:24,799 - INFO - Epoch 195: EPOCH_AVG_TRAIN_LOSS=0.9075
2025-02-04 16:56:24,803 - INFO - #################### Training epoch 196 ####################
2025-02-04 16:56:24,803 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:25,019 - INFO - Epoch 196: train_loss=0.9320
2025-02-04 16:56:25,291 - INFO - Epoch 196: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:25,295 - INFO - Epoch 196: EPOCH_AVG_TRAIN_LOSS=0.9320
2025-02-04 16:56:25,298 - INFO - #################### Training epoch 197 ####################
2025-02-04 16:56:25,298 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:25,513 - INFO - Epoch 197: train_loss=0.9684
2025-02-04 16:56:25,782 - INFO - Epoch 197: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:25,786 - INFO - Epoch 197: EPOCH_AVG_TRAIN_LOSS=0.9684
2025-02-04 16:56:25,790 - INFO - #################### Training epoch 198 ####################
2025-02-04 16:56:25,790 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:26,004 - INFO - Epoch 198: train_loss=0.9711
2025-02-04 16:56:26,272 - INFO - Epoch 198: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:26,275 - INFO - Epoch 198: EPOCH_AVG_TRAIN_LOSS=0.9711
2025-02-04 16:56:26,279 - INFO - #################### Training epoch 199 ####################
2025-02-04 16:56:26,279 - INFO - Current Learning Rate: 1.907349e-08
2025-02-04 16:56:26,495 - INFO - Epoch 199: train_loss=0.9615
2025-02-04 16:56:26,764 - INFO - Epoch 199: val_loss=1.3403, val_acc=33.33%
2025-02-04 16:56:26,768 - INFO - Epoch 199: EPOCH_AVG_TRAIN_LOSS=0.9615
2025-02-04 16:56:26,961 - INFO - Model saved.
