nohup: ignoring input
wandb: Currently logged in as: cyans (cyans-k-benhavns-universitet) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.5
wandb: Run data is saved locally in /lustre/hpc/icecube/cyan/factory/IceCubeTransformer/wandb/run-20250204_163730-p1l9sz4w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-water-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/cyans-k-benhavns-universitet/%5B20250204_163721%5D%20Flavour%20Classification
wandb: üöÄ View run at https://wandb.ai/cyans-k-benhavns-universitet/%5B20250204_163721%5D%20Flavour%20Classification/runs/p1l9sz4w
/groups/icecube/cyan/.local/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.9 TrainingDebuggingYard.py --date 20250204 --time 1 ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/groups/icecube/cyan/.local/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/groups/icecube/cyan/.local/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /lustre/hpc/icecube/cyan/factory/IceCubeTransformer/checkpoints/20250204 exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
/groups/icecube/cyan/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(

  | Name                        | Type       | Params | Mode 
-------------------------------------------------------------------
0 | input_projection            | Linear     | 4.2 K  | train
1 | encoder_blocks              | ModuleList | 297 K  | train
2 | classification_output_layer | Linear     | 387    | train
-------------------------------------------------------------------
301 K     Trainable params
0         Non-trainable params
301 K     Total params
1.207     Total estimated model params size (MB)
Number of GPUs available: 2
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
Number of GPUs available: 2
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
------------- Multi-Flavour Shard (Energy Band: ER_1_PEV_100_PEV, Part: 1, Shard: 1) -------------
------------- Multi-Flavour Shard (Energy Band: ER_1_PEV_100_PEV, Part: 1, Shard: 1) -------------
Config validation passed.
Dataset split into train (24), val (3), and test (3)
Class weights: tensor([0.1250, 0.1250, 0.1250])
Sanity Checking: |          | 0/? [00:00<?, ?it/s]/groups/icecube/cyan/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.96it/s]                                                                           Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]  mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.3210
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.10it/s]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.09it/s, v_num=sz4w]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.34it/s][A
                                                                      [AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.31it/s, v_num=sz4w]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.30it/s, v_num=sz4w]Metric val_acc improved. New best score: 0.333
Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w]        Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 0: train_loss=1.1125
Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.58it/s, v_num=sz4w]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.57it/s, v_num=sz4w, epoch_avg_train_loss=1.320]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.21it/s][A
                                                                      [AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.91it/s, v_num=sz4w, epoch_avg_train_loss=1.320]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.91it/s, v_num=sz4w, epoch_avg_train_loss=1.320]Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.320]        Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.320] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 0: train_loss=1.1471
Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=sz4w, epoch_avg_train_loss=1.320]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=sz4w, epoch_avg_train_loss=1.110]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.13it/s][A
                                                                      [AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.110]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.110]Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.110]        Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.110] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.1514
Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=sz4w, epoch_avg_train_loss=1.110]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.150]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.57it/s][A
                                                                      [AEpoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.150]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.150]Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.150]        Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.150] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.1078
Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=sz4w, epoch_avg_train_loss=1.150]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=sz4w, epoch_avg_train_loss=1.150]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.58it/s][A
                                                                      [AEpoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.150]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.150]Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.150]        Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.150] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 0: train_loss=1.0809
Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=sz4w, epoch_avg_train_loss=1.150]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=sz4w, epoch_avg_train_loss=1.110]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.27it/s][A
                                                                      [AEpoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.110]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.110]Metric val_acc improved by 0.333 >= min_delta = 0.0. New best score: 0.000
Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.110]        Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.110] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 8: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 8: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 8: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 0: train_loss=1.0937
Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=sz4w, epoch_avg_train_loss=1.110]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=sz4w, epoch_avg_train_loss=1.080]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.23it/s][A
                                                                      [AEpoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.080]        Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.080] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 4: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 4: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 4: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 0: train_loss=1.0972
Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=sz4w, epoch_avg_train_loss=1.090]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.12it/s][A
                                                                      [AEpoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.090]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=sz4w, epoch_avg_train_loss=1.090]Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.090]        Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.090] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 0: train_loss=1.0877
Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s, v_num=sz4w, epoch_avg_train_loss=1.090]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=sz4w, epoch_avg_train_loss=1.100]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.02it/s][A
                                                                      [AEpoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.100]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.100]Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.100]        Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.100] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 3: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 3: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 3: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 0: train_loss=1.0784
Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=sz4w, epoch_avg_train_loss=1.100]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=sz4w, epoch_avg_train_loss=1.090]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.81it/s][A
                                                                      [AEpoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=sz4w, epoch_avg_train_loss=1.090]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.090]Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.090]        Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.090] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 15: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 15: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 15: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 0: train_loss=1.0765
Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=sz4w, epoch_avg_train_loss=1.090]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=sz4w, epoch_avg_train_loss=1.080]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 63.48it/s][A
                                                                      [AEpoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.83it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.82it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.080]        Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.080] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 0: train_loss=1.0772
Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=sz4w, epoch_avg_train_loss=1.080]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.05it/s][A
                                                                      [AEpoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.080]        Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.080] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 0: train_loss=1.0781
Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, v_num=sz4w, epoch_avg_train_loss=1.080]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.76it/s][A
                                                                      [AEpoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.080]        Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.080] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 0: train_loss=1.0762
Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.080]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.40it/s][A
                                                                      [AEpoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.080]        Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.080] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 0: train_loss=1.0729
Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=sz4w, epoch_avg_train_loss=1.080]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.01it/s][A
                                                                      [AEpoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.080]        Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.080] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 0: train_loss=1.0694
Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.080]Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=sz4w, epoch_avg_train_loss=1.070]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.90it/s][A
                                                                      [AEpoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=sz4w, epoch_avg_train_loss=1.070]Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=sz4w, epoch_avg_train_loss=1.070]Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.070]        Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.070] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 0: train_loss=1.0677
Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.070]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.070]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.45it/s][A
                                                                      [AEpoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=sz4w, epoch_avg_train_loss=1.070]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=sz4w, epoch_avg_train_loss=1.070]Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.070]        Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.070] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 0: train_loss=1.0657
Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.070]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=sz4w, epoch_avg_train_loss=1.070]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.91it/s][A
                                                                      [AEpoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.070]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=sz4w, epoch_avg_train_loss=1.070]Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.070]        Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.070] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 7: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 7: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 7: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 0: train_loss=1.0643
Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.31it/s, v_num=sz4w, epoch_avg_train_loss=1.070]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.30it/s, v_num=sz4w, epoch_avg_train_loss=1.070]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 63.03it/s][A
                                                                      [AEpoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.83it/s, v_num=sz4w, epoch_avg_train_loss=1.070]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.82it/s, v_num=sz4w, epoch_avg_train_loss=1.070]Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.070]        Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.070] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 0: train_loss=1.0637
Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=sz4w, epoch_avg_train_loss=1.070]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=sz4w, epoch_avg_train_loss=1.060]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.30it/s][A
                                                                      [AEpoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060]        Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 0: train_loss=1.0618
Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=sz4w, epoch_avg_train_loss=1.060]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.25it/s][A
                                                                      [AEpoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060]        Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 0: train_loss=1.0605
Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.36it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.36it/s, v_num=sz4w, epoch_avg_train_loss=1.060]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.95it/s][A
                                                                      [AEpoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.84it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.84it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060]        Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 0: train_loss=1.0604
Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s, v_num=sz4w, epoch_avg_train_loss=1.060]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.00it/s][A
                                                                      [AEpoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060]        Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 0: train_loss=1.0592
Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.36it/s, v_num=sz4w, epoch_avg_train_loss=1.060]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.04it/s][A
                                                                      [AEpoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.84it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.84it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060]        Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 20: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 20: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 20: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 0: train_loss=1.0580
Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=sz4w, epoch_avg_train_loss=1.060]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.28it/s][A
                                                                      [AEpoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060]        Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 20: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 20: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 20: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 0: train_loss=1.0568
Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.060]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.37it/s][A
                                                                      [AEpoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060]        Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 0: train_loss=1.0547
Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=sz4w, epoch_avg_train_loss=1.060]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.76it/s][A
                                                                      [AEpoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060]        Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 6: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 6: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 6: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.0552
Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.59it/s][A
                                                                      [AEpoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 0: train_loss=1.0544
Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=sz4w, epoch_avg_train_loss=1.060]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.40it/s][A
                                                                      [AEpoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060]        Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.060] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 0: train_loss=1.0537
Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=sz4w, epoch_avg_train_loss=1.060]Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.35it/s][A
                                                                      [AEpoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0527
Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.59it/s][A
                                                                      [AEpoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 0: train_loss=1.0521
Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.11it/s][A
                                                                      [AEpoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 0: train_loss=1.0511
Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.01it/s][A
                                                                      [AEpoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 0: train_loss=1.0501
Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 61.18it/s][A
                                                                      [AEpoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 0: train_loss=1.0505
Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.91it/s][A
                                                                      [AEpoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0494
Epoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.41it/s][A
                                                                      [AEpoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 8: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 8: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 8: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 0: train_loss=1.0498
Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.69it/s][A
                                                                      [AEpoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.0488
Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.62it/s][A
                                                                      [AEpoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 0: train_loss=1.0478
Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 64.87it/s][A
                                                                      [AEpoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 3: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 19: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 0: train_loss=1.0484
Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.62it/s][A
                                                                      [AEpoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 3: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 3: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 3: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 0: train_loss=1.0490
Epoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.42it/s][A
                                                                      [AEpoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 5: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 5: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 5: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 6: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 0: train_loss=1.0479
Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.23it/s][A
                                                                      [AEpoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 0: train_loss=1.0470
Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.82it/s][A
                                                                      [AEpoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 0: train_loss=1.0476
Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.01it/s][A
                                                                      [AEpoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 0: train_loss=1.0473
Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.11it/s][A
                                                                      [AEpoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 0: train_loss=1.0472
Epoch 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.68it/s][A
                                                                      [AEpoch 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 4: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 4: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 4: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 0: train_loss=1.0469
Epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.55it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.42it/s][A
                                                                      [AEpoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 0: train_loss=1.0471
Epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.91it/s][A
                                                                      [AEpoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 0: train_loss=1.0462
Epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.78it/s][A
                                                                      [AEpoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 0: train_loss=1.0463
Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.39it/s][A
                                                                      [AEpoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 0: train_loss=1.0466
Epoch 50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.56it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.55it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.15it/s][A
                                                                      [AEpoch 50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 0: train_loss=1.0467
Epoch 51: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 51: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.12it/s][A
                                                                      [AEpoch 51: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 51: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 0: train_loss=1.0463
Epoch 52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.67it/s][A
                                                                      [AEpoch 52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 0: train_loss=1.0466
Epoch 53: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 53: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.03it/s][A
                                                                      [AEpoch 53: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 53: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 5: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 6: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0464
Epoch 54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.13it/s][A
                                                                      [AEpoch 54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 0: train_loss=1.0466
Epoch 55: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 55: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.26it/s][A
                                                                      [AEpoch 55: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 55: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.0467
Epoch 56: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 56: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.81it/s][A
                                                                      [AEpoch 56: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 56: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 0: train_loss=1.0470
Epoch 57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.15it/s][A
                                                                      [AEpoch 57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 0: train_loss=1.0467
Epoch 58: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 58: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.90it/s][A
                                                                      [AEpoch 58: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 58: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 10: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 14: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 0: train_loss=1.0465
Epoch 59: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 59: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.63it/s][A
                                                                      [AEpoch 59: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 59: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 6: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 0: train_loss=1.0467
Epoch 60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.94it/s][A
                                                                      [AEpoch 60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 12: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 0: train_loss=1.0467
Epoch 61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.06it/s][A
                                                                      [AEpoch 61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 0: train_loss=1.0456
Epoch 62: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 62: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.03it/s][A
                                                                      [AEpoch 62: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 62: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 17: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 0: train_loss=1.0459
Epoch 63: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 63: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.87it/s][A
                                                                      [AEpoch 63: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 63: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 19: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 19: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 19: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 0: train_loss=1.0461
Epoch 64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.12it/s][A
                                                                      [AEpoch 64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 0: train_loss=1.0452
Epoch 65: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 65: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.42it/s][A
                                                                      [AEpoch 65: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 65: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 6: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 0: train_loss=1.0458
Epoch 66: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 66: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.63it/s][A
                                                                      [AEpoch 66: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 66: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 7: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 8: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 20: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 0: train_loss=1.0461
Epoch 67: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 67: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.17it/s][A
                                                                      [AEpoch 67: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 67: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 0: train_loss=1.0467
Epoch 68: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 68: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.17it/s][A
                                                                      [AEpoch 68: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 68: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 9: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 22: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 0: train_loss=1.0462
Epoch 69: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 69: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.34it/s][A
                                                                      [AEpoch 69: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 69: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 3: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 4: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 5: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 17: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 0: train_loss=1.0461
Epoch 70: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 70: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.51it/s][A
                                                                      [AEpoch 70: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 70: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 2: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 5: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 0: train_loss=1.0461
Epoch 71: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 71: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.10it/s][A
                                                                      [AEpoch 71: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 71: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 0: train_loss=1.0465
Epoch 72: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 72: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 69.30it/s][A
                                                                      [AEpoch 72: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 72: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 7: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 8: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 9: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 10: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 11: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 0: train_loss=1.0463
Epoch 73: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 73: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.77it/s][A
                                                                      [AEpoch 73: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 73: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 9: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 9: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 9: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 15: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 0: train_loss=1.0453
Epoch 74: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 74: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.40it/s][A
                                                                      [AEpoch 74: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 74: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0464
Epoch 75: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 75: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.17it/s][A
                                                                      [AEpoch 75: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 75: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 7: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 11: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 0: train_loss=1.0465
Epoch 76: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 76: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.96it/s][A
                                                                      [AEpoch 76: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 76: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 9: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 10: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 22: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 23: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 0: train_loss=1.0460
Epoch 77: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 77: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.27it/s][A
                                                                      [AEpoch 77: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 77: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 1: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 10: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 16: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 18: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 0: train_loss=1.0456
Epoch 78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.07it/s][A
                                                                      [AEpoch 78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 11: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 14: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 0: train_loss=1.0469
Epoch 79: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 79: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.81it/s][A
                                                                      [AEpoch 79: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 79: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 0: train_loss=1.0455
Epoch 80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.15it/s][A
                                                                      [AEpoch 80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 9: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 10: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 11: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 12: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 13: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 0: train_loss=1.0457
Epoch 81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.68it/s][A
                                                                      [AEpoch 81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 1: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 4: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 17: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 21: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 22: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 23: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 0: train_loss=1.0462
Epoch 82: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 82: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.00it/s][A
                                                                      [AEpoch 82: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 82: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 10: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 19: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 0: train_loss=1.0457
Epoch 83: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 83: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.63it/s][A
                                                                      [AEpoch 83: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 83: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 1: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 6: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 8: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 9: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 14: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 15: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 18: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 19: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 23: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 0: train_loss=1.0461
Epoch 84: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 84: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.37it/s][A
                                                                      [AEpoch 84: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 84: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 7: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 17: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 18: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 20: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 21: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 22: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 23: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 0: train_loss=1.0464
Epoch 85: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 85: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.71it/s][A
                                                                      [AEpoch 85: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 85: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 12: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 13: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 14: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 19: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 0: train_loss=1.0462
Epoch 86: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 86: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.40it/s][A
                                                                      [AEpoch 86: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 86: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 4: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 12: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 16: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 19: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 0: train_loss=1.0467
Epoch 87: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 87: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.75it/s][A
                                                                      [AEpoch 87: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 87: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 10: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 10: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 2: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 7: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 8: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 9: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 10: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 14: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 15: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 18: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 21: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 22: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.0464
Epoch 88: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 88: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.25it/s][A
                                                                      [AEpoch 88: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 88: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 4: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 5: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 6: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 7: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 8: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 11: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 15: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 20: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 23: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 0: train_loss=1.0461
Epoch 89: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 89: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.66it/s][A
                                                                      [AEpoch 89: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 89: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 8: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 8: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 1: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 4: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 5: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 8: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 12: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 16: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 17: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 21: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 0: train_loss=1.0460
Epoch 90: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 90: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.67it/s][A
                                                                      [AEpoch 90: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 90: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 8: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 9: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 13: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 15: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 16: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 17: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 21: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 0: train_loss=1.0455
Epoch 91: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.57it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 91: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.56it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.02it/s][A
                                                                      [AEpoch 91: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 91: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 9: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 9: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 1: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 2: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 3: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 7: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 9: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 11: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 12: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 13: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 14: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 15: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 19: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 20: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 0: train_loss=1.0462
Epoch 92: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 92: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.14it/s][A
                                                                      [AEpoch 92: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 92: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 1: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 2: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 3: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 4: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 7: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 8: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 9: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 10: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 13: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 14: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 17: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 18: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 19: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 20: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 21: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 22: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 23: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 0: train_loss=1.0459
Epoch 93: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 93: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.15it/s][A
                                                                      [AEpoch 93: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 93: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 8: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 8: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 3: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 4: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 8: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 9: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 10: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 11: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 12: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 13: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 16: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 19: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 22: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 23: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 0: train_loss=1.0463
Epoch 94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
Batch 2: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 69.30it/s][A
                                                                      [AEpoch 94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 1: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 2: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 3: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 4: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 5: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 6: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 7: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 11: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 12: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 18: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 19: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 22: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 0: train_loss=1.0460
Epoch 95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.61it/s][A
                                                                      [AEpoch 95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 1: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 2: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 5: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 6: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 9: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 10: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 11: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 12: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 13: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 17: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 18: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 20: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 23: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 0: train_loss=1.0455
Epoch 96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.70it/s][A
                                                                      [AEpoch 96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 2: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 3: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 4: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 7: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 10: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 11: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 12: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 13: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 14: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 17: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 18: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 19: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 20: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 21: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 22: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 23: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 0: train_loss=1.0462
Epoch 97: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 97: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 1: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 2: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 68.98it/s][A
                                                                      [AEpoch 97: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 97: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 1: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 2: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 3: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 4: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 5: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 6: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 7: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 9: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 13: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 14: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 15: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 16: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 17: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 18: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 19: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 20: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 21: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 22: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 23: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 0: train_loss=1.0454
Epoch 98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.54it/s][A
                                                                      [AEpoch 98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050]        Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=sz4w, epoch_avg_train_loss=1.050] mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 20: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 20: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 24, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([24, 2421])
Batch 0: seq_len = 177, Q.shape = torch.Size([1, 177, 8, 16])
Batch 1: seq_len = 1176, Q.shape = torch.Size([1, 1176, 8, 16])
Batch 2: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 3: seq_len = 399, Q.shape = torch.Size([1, 399, 8, 16])
Batch 5: seq_len = 139, Q.shape = torch.Size([1, 139, 8, 16])
Batch 6: seq_len = 231, Q.shape = torch.Size([1, 231, 8, 16])
Batch 8: seq_len = 51, Q.shape = torch.Size([1, 51, 8, 16])
Batch 9: seq_len = 12, Q.shape = torch.Size([1, 12, 8, 16])
Batch 10: seq_len = 741, Q.shape = torch.Size([1, 741, 8, 16])
Batch 11: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 12: seq_len = 436, Q.shape = torch.Size([1, 436, 8, 16])
Batch 13: seq_len = 39, Q.shape = torch.Size([1, 39, 8, 16])
Batch 14: seq_len = 212, Q.shape = torch.Size([1, 212, 8, 16])
Batch 15: seq_len = 94, Q.shape = torch.Size([1, 94, 8, 16])
Batch 16: seq_len = 28, Q.shape = torch.Size([1, 28, 8, 16])
Batch 17: seq_len = 313, Q.shape = torch.Size([1, 313, 8, 16])
Batch 18: seq_len = 72, Q.shape = torch.Size([1, 72, 8, 16])
Batch 19: seq_len = 179, Q.shape = torch.Size([1, 179, 8, 16])
Batch 20: seq_len = 140, Q.shape = torch.Size([1, 140, 8, 16])
Batch 21: seq_len = 132, Q.shape = torch.Size([1, 132, 8, 16])
Batch 22: seq_len = 46, Q.shape = torch.Size([1, 46, 8, 16])
Batch 23: seq_len = 31, Q.shape = torch.Size([1, 31, 8, 16])
Batch 0: train_loss=1.0454
Epoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, v_num=sz4w, epoch_avg_train_loss=1.050]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])
 mask shape should be [batch_size, n_heads, seq_length, seq_length]
 batch_size: 3, n_heads: 8, seq_length: 2421, embed_dim: 128
 the shape of the mask is torch.Size([3, 2421])
Batch 0: seq_len = 161, Q.shape = torch.Size([1, 161, 8, 16])
Batch 1: seq_len = 182, Q.shape = torch.Size([1, 182, 8, 16])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.51it/s][A
                                                                      [AEpoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s, v_num=sz4w, epoch_avg_train_loss=1.050]Epoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s, v_num=sz4w, epoch_avg_train_loss=1.050]`Trainer.fit` stopped: `max_epochs=100` reached.
Epoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, v_num=sz4w, epoch_avg_train_loss=1.050]FIT Profiler Report
Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.configure_callbacks
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 module.py:936(configure_callbacks)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningDataModule]PMTfiedDataModule.setup
         2063 function calls (2047 primitive calls) in 0.014 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.014    0.014 PMTfiedDataModule.py:25(setup)
        1    0.000    0.000    0.011    0.011 PMTfiedDataModule.py:37(<listcomp>)
       25    0.000    0.000    0.010    0.000 dataset.py:409(__getitem__)
       24    0.000    0.000    0.010    0.000 DatasetMultiFlavourShard_Micro.py:50(__getitem__)
       24    0.002    0.000    0.010    0.000 DatasetMonoFlavourShard_Micro.py:55(__getitem__)
       24    0.002    0.000    0.005    0.000 DatasetMonoFlavourShard_Micro.py:153(_extract_features)
      121    0.002    0.000    0.002    0.000 {built-in method torch.tensor}
      4/1    0.000    0.000    0.002    0.002 _tensor.py:982(__format__)
        1    0.000    0.000    0.002    0.002 {function Tensor.__format__ at 0x7f7df09c7c10}
        1    0.000    0.000    0.002    0.002 _tensor.py:457(__repr__)
        1    0.000    0.000    0.002    0.002 _tensor_str.py:695(_str)
       24    0.002    0.000    0.002    0.000 DatasetMonoFlavourShard_Micro.py:166(<listcomp>)
        3    0.000    0.000    0.001    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.001    0.001 contextlib.py:114(__enter__)
        2    0.000    0.000    0.001    0.001 _python_dispatch.py:201(_disable_current_modes)
        1    0.000    0.000    0.001    0.001 <frozen importlib._bootstrap>:1002(_find_and_load)
        1    0.000    0.000    0.001    0.001 <frozen importlib._bootstrap>:967(_find_and_load_unlocked)
       24    0.001    0.000    0.001    0.000 shape_base.py:372(stack)
        1    0.000    0.000    0.001    0.001 <frozen importlib._bootstrap>:659(_load_unlocked)
        1    0.000    0.000    0.001    0.001 <frozen importlib._bootstrap_external>:844(exec_module)
       48    0.001    0.000    0.001    0.000 {built-in method numpy.zeros}
        1    0.000    0.000    0.001    0.001 _tensor_str.py:391(_str_intern)
        1    0.000    0.000    0.001    0.001 _tensor_str.py:303(_tensor_str)
        1    0.000    0.000    0.001    0.001 <frozen importlib._bootstrap_external>:916(get_code)
        1    0.000    0.000    0.001    0.001 _tensor_str.py:123(__init__)
        1    0.000    0.000    0.001    0.001 <frozen importlib._bootstrap_external>:1036(get_data)
        1    0.000    0.000    0.000    0.000 {built-in method io.open_code}
       24    0.000    0.000    0.000    0.000 {built-in method torch.argmax}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:220(_call_with_frames_removed)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.exec}
        1    0.000    0.000    0.000    0.000 schema_check_mode.py:3(<module>)
        1    0.000    0.000    0.000    0.000 dataset.py:426(random_split)
        2    0.000    0.000    0.000    0.000 {built-in method builtins.print}
        4    0.000    0.000    0.000    0.000 redirect.py:644(write)
        4    0.000    0.000    0.000    0.000 wandb_run.py:2304(<lambda>)
        4    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
        4    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
        2    0.000    0.000    0.000    0.000 __init__.py:345(namedtuple)
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}
        4    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
        1    0.000    0.000    0.000    0.000 {built-in method torch.randperm}
        1    0.000    0.000    0.000    0.000 {built-in method torch.bincount}
        1    0.000    0.000    0.000    0.000 {built-in method torch.isfinite}
       24    0.000    0.000    0.000    0.000 shape_base.py:443(<listcomp>)
       24    0.000    0.000    0.000    0.000 shape_base.py:455(<listcomp>)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:901(_find_spec)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1415(find_spec)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1383(_get_spec)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1514(find_spec)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:135(_path_stat)
        3    0.000    0.000    0.000    0.000 {built-in method posix.stat}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:645(_compile_bytecode)
        4    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
        1    0.000    0.000    0.000    0.000 {built-in method marshal.loads}
        1    0.000    0.000    0.000    0.000 _tensor.py:35(wrapped)
        1    0.000    0.000    0.000    0.000 {built-in method torch.masked_select}
       27    0.000    0.000    0.000    0.000 {method 'item' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 _tensor.py:964(__rdiv__)
        2    0.000    0.000    0.000    0.000 {built-in method builtins.eval}
        4    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
       24    0.000    0.000    0.000    0.000 shape_base.py:447(<setcomp>)
        4    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
        2    0.000    0.000    0.000    0.000 _tensor.py:1033(__iter__)
       24    0.000    0.000    0.000    0.000 DatasetMultiFlavourShard_Micro.py:72(_global_to_local_index)
        4    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        4    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
        2    0.000    0.000    0.000    0.000 {method 'unbind' of 'torch._C.TensorBase' objects}
      768    0.000    0.000    0.000    0.000 {built-in method numpy.asanyarray}
       24    0.000    0.000    0.000    0.000 DatasetMonoFlavourShard_Micro.py:160(<listcomp>)
        1    0.000    0.000    0.000    0.000 {built-in method torch.ceil}
        1    0.000    0.000    0.000    0.000 {method 'read' of '_io.BufferedReader' objects}
        1    0.000    0.000    0.000    0.000 {method 'reshape' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:154(_path_isfile)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:145(_path_is_mode_type)
        4    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
       24    0.000    0.000    0.000    0.000 shape_base.py:362(_stack_dispatcher)
    62/50    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        3    0.000    0.000    0.000    0.000 _tensor_str.py:117(tensor_totype)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:558(module_from_spec)
        1    0.000    0.000    0.000    0.000 _tensor_str.py:266(_tensor_str_with_formatter)
        4    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
        4    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
        1    0.000    0.000    0.000    0.000 {method 'max' of 'torch._C.TensorBase' objects}
        3    0.000    0.000    0.000    0.000 {method 'to' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:486(_init_module_attrs)
        1    0.000    0.000    0.000    0.000 _tensor_str.py:221(_vector_str)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1077(path_stats)
    59/58    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
       24    0.000    0.000    0.000    0.000 shape_base.py:207(_arrays_for_stack_dispatcher)
        2    0.000    0.000    0.000    0.000 DatasetMultiFlavourShard_Micro.py:47(__len__)
        1    0.000    0.000    0.000    0.000 {method 'min' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 {method 'reciprocal' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:361(cache_from_source)
        4    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
       24    0.000    0.000    0.000    0.000 __init__.py:819(is_tensor)
        2    0.000    0.000    0.000    0.000 {method 'tolist' of 'torch._C.TensorBase' objects}
        6    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}
        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:385(cached)
        5    0.000    0.000    0.000    0.000 {built-in method builtins.sum}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:491(_get_cached)
        1    0.000    0.000    0.000    0.000 {method 'float' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 {method 'ne' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 grad_mode.py:80(__enter__)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:156(__enter__)
       24    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}
        8    0.000    0.000    0.000    0.000 DatasetMultiFlavourShard_Micro.py:48(<genexpr>)
        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:121(_path_join)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.__build_class__}
        4    0.000    0.000    0.000    0.000 grad_mode.py:184(__init__)
        4    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
        4    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        2    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 _tensor_str.py:254(<listcomp>)
       29    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:166(_get_module_lock)
        4    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        1    0.000    0.000    0.000    0.000 _ops.py:544(_len_torch_dispatch_stack_pre_dispatch)
        1    0.000    0.000    0.000    0.000 dataset.py:486(<listcomp>)
        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:127(_path_split)
        3    0.000    0.000    0.000    0.000 _tensor_str.py:232(_val_formatter)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1509(_get_spec)
        2    0.000    0.000    0.000    0.000 grad_mode.py:75(__init__)
        1    0.000    0.000    0.000    0.000 {built-in method torch._C._functorch.is_functorch_wrapped_tensor}
        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:123(<listcomp>)
       15    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {built-in method torch._is_functional_tensor}
        6    0.000    0.000    0.000    0.000 DatasetMonoFlavourShard_Micro.py:52(__len__)
       16    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        1    0.000    0.000    0.000    0.000 {method 'abs' of 'torch._C.TensorBase' objects}
        4    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        3    0.000    0.000    0.000    0.000 _tensor_str.py:193(format)
        6    0.000    0.000    0.000    0.000 {built-in method builtins.max}
        1    0.000    0.000    0.000    0.000 contextlib.py:261(helper)
        2    0.000    0.000    0.000    0.000 grad_mode.py:84(__exit__)
        2    0.000    0.000    0.000    0.000 {built-in method torch._C._get_tracing_state}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:560(_classify_pyc)
        8    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        4    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
        1    0.000    0.000    0.000    0.000 forward_ad.py:144(unpack_dual)
        2    0.000    0.000    0.000    0.000 _contextlib.py:154(__new__)
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:696(spec_from_file_location)
        1    0.000    0.000    0.000    0.000 parameter.py:8(__instancecheck__)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:79(_unpack_uint32)
        4    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}
        4    0.000    0.000    0.000    0.000 {built-in method utcnow}
        1    0.000    0.000    0.000    0.000 _ops.py:441(count)
        1    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)
        1    0.000    0.000    0.000    0.000 _tensor_str.py:354(_add_suffixes)
       24    0.000    0.000    0.000    0.000 multiarray.py:153(concatenate)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 {method 'manual_seed' of 'torch._C.Generator' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:160(__exit__)
        3    0.000    0.000    0.000    0.000 dataset.py:405(__init__)
        3    0.000    0.000    0.000    0.000 {method '__format__' of 'float' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:87(acquire)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:593(_validate_timestamp_pyc)
        1    0.000    0.000    0.000    0.000 import_hooks.py:233(find_spec)
        1    0.000    0.000    0.000    0.000 {built-in method torch._C._get_default_device}
        7    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}
        4    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
        5    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:878(__exit__)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:112(release)
        4    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:129(<genexpr>)
        1    0.000    0.000    0.000    0.000 {method '_is_zerotensor' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:58(__init__)
        7    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:231(_verbose_message)
        4    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        4    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        5    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:874(__enter__)
        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1033(_handle_fromlist)
        1    0.000    0.000    0.000    0.000 {method 'is_neg' of 'torch._C.TensorBase' objects}
        7    0.000    0.000    0.000    0.000 {built-in method sys.intern}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:811(find_spec)
        1    0.000    0.000    0.000    0.000 typing.py:271(inner)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1346(_path_importer_cache)
        4    0.000    0.000    0.000    0.000 {method 'numel' of 'torch._C.TensorBase' objects}
        6    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}
        7    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 __init__.py:82(find_spec)
        6    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}
       14    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:185(cb)
        7    0.000    0.000    0.000    0.000 __init__.py:419(<genexpr>)
        7    0.000    0.000    0.000    0.000 {method '__contains__' of 'frozenset' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:523(_check_name_wrapper)
        6    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        3    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}
        6    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}
        1    0.000    0.000    0.000    0.000 <string>:1(<lambda>)
        4    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
        3    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f7dfbd377c0}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:398(parent)
        5    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:351(__init__)
        2    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
        7    0.000    0.000    0.000    0.000 {built-in method _imp.acquire_lock}
        1    0.000    0.000    0.000    0.000 {built-in method torch._C._len_torch_dispatch_stack}
        1    0.000    0.000    0.000    0.000 schema_check_mode.py:61(SchemaCheckMode)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:35(_new_module)
        4    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        5    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        7    0.000    0.000    0.000    0.000 {built-in method _imp.release_lock}
        1    0.000    0.000    0.000    0.000 {built-in method math.floor}
        1    0.000    0.000    0.000    0.000 {built-in method math.isclose}
        2    0.000    0.000    0.000    0.000 {built-in method torch.get_default_dtype}
        1    0.000    0.000    0.000    0.000 _tensor_str.py:259(<listcomp>)
        5    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
        2    0.000    0.000    0.000    0.000 {built-in method builtins.iter}
        3    0.000    0.000    0.000    0.000 {built-in method from_bytes}
        1    0.000    0.000    0.000    0.000 _tensor_str.py:256(<listcomp>)
        4    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
        1    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:152(__init__)
        2    0.000    0.000    0.000    0.000 {method 'has_names' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 {built-in method sys._getframe}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1006(__init__)
        1    0.000    0.000    0.000    0.000 _ops.py:575(mode_stack_state_for_pre_dispatch)
        1    0.000    0.000    0.000    0.000 {built-in method _imp.is_frozen}
        4    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
        1    0.000    0.000    0.000    0.000 _ops.py:442(<listcomp>)
        1    0.000    0.000    0.000    0.000 _tensor_str.py:190(width)
        3    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        1    0.000    0.000    0.000    0.000 typing.py:1375(cast)
        2    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}
        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 _python_dispatch.py:229(<listcomp>)
        2    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
        1    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:68(_relax_case)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:736(find_spec)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:841(create_module)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:406(has_location)
        1    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {built-in method _imp._fix_co_filename}
        1    0.000    0.000    0.000    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f7dee3ee040}
        1    0.000    0.000    0.000    0.000 _python_dispatch.py:212(<listcomp>)
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1031(get_filename)



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.setup
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 early_stopping.py:135(setup)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.setup
         13 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 progress_bar.py:171(setup)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 trainer.py:1241(is_global_zero)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 single_device.py:81(is_global_zero)
        2    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.setup
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:58(setup)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.setup
         1583 function calls in 0.003 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.003    0.003 model_checkpoint.py:267(setup)
        1    0.000    0.000    0.002    0.002 model_checkpoint.py:650(__warn_if_dir_not_empty)
        1    0.000    0.000    0.002    0.002 local.py:58(ls)
        1    0.001    0.001    0.001    0.001 local.py:63(<listcomp>)
        1    0.000    0.000    0.001    0.001 cloud_io.py:105(_is_dir)
        1    0.000    0.000    0.001    0.001 cloud_io.py:83(_is_object_storage)
        3    0.000    0.000    0.001    0.000 imports.py:45(module_available)
        3    0.000    0.000    0.001    0.000 imports.py:29(package_available)
        3    0.000    0.000    0.001    0.000 util.py:73(find_spec)
       17    0.000    0.000    0.001    0.000 local.py:71(info)
        3    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap>:901(_find_spec)
        3    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap_external>:1415(find_spec)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1383(_get_spec)
       21    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1514(find_spec)
       16    0.000    0.000    0.000    0.000 {method 'stat' of 'posix.DirEntry' objects}
        1    0.000    0.000    0.000    0.000 {built-in method posix.scandir}
       23    0.000    0.000    0.000    0.000 {built-in method posix.stat}
       21    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:135(_path_stat)
        1    0.000    0.000    0.000    0.000 rank_zero.py:36(wrapped_fn)
        1    0.000    0.000    0.000    0.000 rank_zero.py:76(rank_zero_warn)
        1    0.000    0.000    0.000    0.000 rank_zero.py:72(_warn)
      105    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:121(_path_join)
        1    0.000    0.000    0.000    0.000 {built-in method _warnings.warn}
        1    0.000    0.000    0.000    0.000 warnings.py:96(_showwarnmsg)
        1    0.000    0.000    0.000    0.000 warnings.py:20(_showwarnmsg_impl)
        1    0.000    0.000    0.000    0.000 cloud_io.py:60(get_filesystem)
        1    0.000    0.000    0.000    0.000 core.py:362(url_to_fs)
      105    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:123(<listcomp>)
       20    0.000    0.000    0.000    0.000 local.py:230(_strip_protocol)
        1    0.000    0.000    0.000    0.000 redirect.py:644(write)
        1    0.000    0.000    0.000    0.000 warnings.py:117(_formatwarnmsg)
        1    0.000    0.000    0.000    0.000 warnings.py:40(_custom_format_warning)
        1    0.000    0.000    0.000    0.000 wandb_run.py:2310(<lambda>)
        1    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
        1    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
        1    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
        1    0.000    0.000    0.000    0.000 pathlib.py:1079(__new__)
        1    0.000    0.000    0.000    0.000 registry.py:289(filesystem)
        1    0.000    0.000    0.000    0.000 core.py:331(_un_chain)
        1    0.000    0.000    0.000    0.000 spec.py:65(__call__)
        1    0.000    0.000    0.000    0.000 pathlib.py:702(_from_parts)
        1    0.000    0.000    0.000    0.000 local.py:133(isdir)
        1    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
        1    0.000    0.000    0.000    0.000 pathlib.py:682(_parse_args)
        1    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
      230    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}
        1    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
      105    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:231(_verbose_message)
      110    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        1    0.000    0.000    0.000    0.000 genericpath.py:39(isdir)
        1    0.000    0.000    0.000    0.000 utils.py:306(tokenize)
        1    0.000    0.000    0.000    0.000 pathlib.py:64(parse_parts)
        1    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        1    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
      106    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
       20    0.000    0.000    0.000    0.000 local.py:280(make_path_posix)
        1    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      104    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}
        3    0.000    0.000    0.000    0.000 __init__.py:82(find_spec)
       27    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:874(__enter__)
       27    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:878(__exit__)
        1    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
       88    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
       22    0.000    0.000    0.000    0.000 utils.py:327(stringify_path)
        1    0.000    0.000    0.000    0.000 {built-in method _hashlib.openssl_md5}
       24    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1346(_path_importer_cache)
        1    0.000    0.000    0.000    0.000 warnings.py:57(_is_path_in_lightning)
        1    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
        1    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
        1    0.000    0.000    0.000    0.000 enums.py:81(__eq__)
        3    0.000    0.000    0.000    0.000 __init__.py:57(find_spec)
       10    0.000    0.000    0.000    0.000 {built-in method sys.intern}
        2    0.000    0.000    0.000    0.000 registry.py:222(get_filesystem_class)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:736(find_spec)
        1    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
        3    0.000    0.000    0.000    0.000 __init__.py:66(find_spec)
       21    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        1    0.000    0.000    0.000    0.000 re.py:250(compile)
        3    0.000    0.000    0.000    0.000 import_hooks.py:233(find_spec)
        3    0.000    0.000    0.000    0.000 __init__.py:24(_module_matches_namespace)
        1    0.000    0.000    0.000    0.000 pathlib.py:742(__str__)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
       27    0.000    0.000    0.000    0.000 {built-in method _imp.release_lock}
       27    0.000    0.000    0.000    0.000 {built-in method _imp.acquire_lock}
        1    0.000    0.000    0.000    0.000 {method 'hexdigest' of '_hashlib.HASH' objects}
       24    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}
        1    0.000    0.000    0.000    0.000 re.py:289(_compile)
        1    0.000    0.000    0.000    0.000 core.py:541(split_protocol)
        1    0.000    0.000    0.000    0.000 pathlib.py:303(splitroot)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
       21    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:68(_relax_case)
        3    0.000    0.000    0.000    0.000 {built-in method _imp.is_builtin}
       16    0.000    0.000    0.000    0.000 {method 'is_file' of 'posix.DirEntry' objects}
        3    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        1    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:811(find_spec)
        1    0.000    0.000    0.000    0.000 config.py:99(apply_config)
        3    0.000    0.000    0.000    0.000 __init__.py:33(_module_matches_namespace)
        1    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
       16    0.000    0.000    0.000    0.000 {method 'is_dir' of 'posix.DirEntry' objects}
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.locals}
        1    0.000    0.000    0.000    0.000 types.py:171(__get__)
        1    0.000    0.000    0.000    0.000 warnings.py:403(__init__)
       16    0.000    0.000    0.000    0.000 {method 'is_symlink' of 'posix.DirEntry' objects}
        1    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 pathlib.py:725(_format_parsed_parts)
        4    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:351(__init__)
        1    0.000    0.000    0.000    0.000 pathlib.py:1193(absolute)
        2    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        3    0.000    0.000    0.000    0.000 six.py:194(find_spec)
        2    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
       13    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 trainer.py:1241(is_global_zero)
        6    0.000    0.000    0.000    0.000 {method 'partition' of 'str' objects}
        1    0.000    0.000    0.000    0.000 local.py:68(<listcomp>)
        1    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
        1    0.000    0.000    0.000    0.000 model_checkpoint.py:608(__resolve_ckpt_dir)
        6    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 pathlib.py:1089(_init)
        1    0.000    0.000    0.000    0.000 {built-in method utcnow}
        1    0.000    0.000    0.000    0.000 pathlib.py:1001(is_absolute)
        2    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
        3    0.000    0.000    0.000    0.000 {built-in method _imp.is_frozen}
        1    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        3    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        2    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        3    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 enum.py:792(value)
        3    0.000    0.000    0.000    0.000 _compat.py:52(find_spec)
        1    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f7dfbd377c0}
        1    0.000    0.000    0.000    0.000 core.py:395(<dictcomp>)
        3    0.000    0.000    0.000    0.000 __init__.py:89(<lambda>)
        1    0.000    0.000    0.000    0.000 {method 'lstrip' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'reverse' of 'list' objects}
        3    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 spec.py:67(<genexpr>)
        2    0.000    0.000    0.000    0.000 {built-in method _stat.S_ISDIR}
        1    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {method '__exit__' of 'posix.ScandirIterator' objects}
        1    0.000    0.000    0.000    0.000 spec.py:214(_get_kwargs_from_urls)
        1    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        1    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 {built-in method _stat.S_ISLNK}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        1    0.000    0.000    0.000    0.000 single_device.py:81(is_global_zero)
        1    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
        1    0.000    0.000    0.000    0.000 single_device.py:90(broadcast)



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.setup
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 hooks.py:420(setup)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.configure_optimizers
         1231 function calls (1036 primitive calls) in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.001    0.001 FlavourClassificationTransformerEncoder.py:66(configure_optimizers)
        1    0.000    0.000    0.001    0.001 lr_scheduler.py:1260(__init__)
        1    0.000    0.000    0.001    0.001 lr_scheduler.py:57(_check_verbose_deprecated_warning)
        1    0.000    0.000    0.001    0.001 {built-in method _warnings.warn}
        1    0.000    0.000    0.001    0.001 warnings.py:96(_showwarnmsg)
        1    0.000    0.000    0.001    0.001 warnings.py:20(_showwarnmsg_impl)
        1    0.000    0.000    0.001    0.001 warnings.py:117(_formatwarnmsg)
        1    0.000    0.000    0.001    0.001 warnings.py:40(_custom_format_warning)
        1    0.000    0.000    0.001    0.001 warnings.py:15(formatwarning)
        1    0.000    0.000    0.001    0.001 warnings.py:35(_formatwarnmsg_impl)
        1    0.000    0.000    0.001    0.001 linecache.py:26(getline)
        1    0.000    0.000    0.001    0.001 vararg_kernel.py:127(_monkey_patched_getlines)
        1    0.000    0.000    0.001    0.001 linecache.py:36(getlines)
        1    0.000    0.000    0.001    0.001 linecache.py:80(updatecache)
        1    0.000    0.000    0.000    0.000 tokenize.py:388(open)
        1    0.000    0.000    0.000    0.000 adam.py:31(__init__)
        1    0.000    0.000    0.000    0.000 optimizer.py:339(__init__)
        1    0.000    0.000    0.000    0.000 {built-in method io.open}
        1    0.000    0.000    0.000    0.000 {method 'readlines' of '_io._IOBase' objects}
       41    0.000    0.000    0.000    0.000 module.py:2233(parameters)
       41    0.000    0.000    0.000    0.000 module.py:2258(named_parameters)
       41    0.000    0.000    0.000    0.000 module.py:2219(_named_members)
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}
        1    0.000    0.000    0.000    0.000 _compile.py:21(inner)
   242/47    0.000    0.000    0.000    0.000 module.py:2395(named_modules)
        1    0.000    0.000    0.000    0.000 eval_frame.py:596(_fn)
        1    0.000    0.000    0.000    0.000 optimizer.py:987(add_param_group)
        1    0.000    0.000    0.000    0.000 redirect.py:644(write)
        1    0.000    0.000    0.000    0.000 wandb_run.py:2310(<lambda>)
        1    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
        1    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
        1    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
        1    0.000    0.000    0.000    0.000 {built-in method posix.stat}
       12    0.000    0.000    0.000    0.000 codecs.py:319(decode)
        1    0.000    0.000    0.000    0.000 tokenize.py:295(detect_encoding)
        1    0.000    0.000    0.000    0.000 decorators.py:38(disable)
      160    0.000    0.000    0.000    0.000 _tensor.py:1055(__hash__)
       12    0.000    0.000    0.000    0.000 {built-in method _codecs.utf_8_decode}
        1    0.000    0.000    0.000    0.000 optimizer.py:514(_patch_step_function)
        1    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
        2    0.000    0.000    0.000    0.000 tokenize.py:319(read_or_stop)
        1    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
       86    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
        1    0.000    0.000    0.000    0.000 optimizer.py:462(profile_hook_step)
        2    0.000    0.000    0.000    0.000 {method 'readline' of '_io.BufferedReader' objects}
        1    0.000    0.000    0.000    0.000 pathlib.py:1079(__new__)
       46    0.000    0.000    0.000    0.000 module.py:2286(<lambda>)
        1    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
        1    0.000    0.000    0.000    0.000 pathlib.py:702(_from_parts)
        1    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        1    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
        1    0.000    0.000    0.000    0.000 pathlib.py:682(_parse_args)
        1    0.000    0.000    0.000    0.000 eval_frame.py:562(__init__)
        1    0.000    0.000    0.000    0.000 pathlib.py:64(parse_parts)
        1    0.000    0.000    0.000    0.000 eval_frame.py:565(__call__)
        1    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
        2    0.000    0.000    0.000    0.000 functools.py:35(update_wrapper)
        1    0.000    0.000    0.000    0.000 eval_frame.py:265(__init__)
      161    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        1    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
       52    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        1    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
        1    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
       92    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
        1    0.000    0.000    0.000    0.000 warnings.py:57(_is_path_in_lightning)
        2    0.000    0.000    0.000    0.000 tokenize.py:325(find_cookie)
       18    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        1    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
        1    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 pathlib.py:742(__str__)
        1    0.000    0.000    0.000    0.000 typing_extensions.py:1710(args)
        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)
       41    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        3    0.000    0.000    0.000    0.000 {method 'match' of 're.Pattern' objects}
        3    0.000    0.000    0.000    0.000 eval_frame.py:240(innermost_fn)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
       11    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        1    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
       10    0.000    0.000    0.000    0.000 {built-in method sys.intern}
        1    0.000    0.000    0.000    0.000 pathlib.py:303(splitroot)
        1    0.000    0.000    0.000    0.000 pathlib.py:725(_format_parsed_parts)
        2    0.000    0.000    0.000    0.000 functools.py:65(wraps)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        2    0.000    0.000    0.000    0.000 warnings.py:403(__init__)
        1    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 typing_extensions.py:1569(__init__)
       10    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}
        9    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        1    0.000    0.000    0.000    0.000 typing_extensions.py:1714(kwargs)
        2    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 lr_scheduler.py:1368(_init_is_better)
       13    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
        1    0.000    0.000    0.000    0.000 lr_scheduler.py:1310(_reset)
        1    0.000    0.000    0.000    0.000 {built-in method utcnow}
        1    0.000    0.000    0.000    0.000 pathlib.py:1193(absolute)
        2    0.000    0.000    0.000    0.000 {built-in method torch._C._dynamo.eval_frame.set_eval_frame}
        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        1    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        1    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 {method 'seek' of '_io.BufferedReader' objects}
        1    0.000    0.000    0.000    0.000 {method 'startswith' of 'bytes' objects}
        1    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
        1    0.000    0.000    0.000    0.000 inspect.py:73(isclass)
        2    0.000    0.000    0.000    0.000 {method 'decode' of 'bytes' objects}
        2    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}
        1    0.000    0.000    0.000    0.000 pathlib.py:1001(is_absolute)
        1    0.000    0.000    0.000    0.000 lr_scheduler.py:1304(<listcomp>)
        1    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        1    0.000    0.000    0.000    0.000 typing_extensions.py:1592(__init__)
        1    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f7dfbd377c0}
        3    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
        1    0.000    0.000    0.000    0.000 pathlib.py:1089(_init)
        1    0.000    0.000    0.000    0.000 {method 'isdisjoint' of 'set' objects}
        1    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}
        1    0.000    0.000    0.000    0.000 eval_frame.py:232(nothing)
        1    0.000    0.000    0.000    0.000 {method 'lstrip' of 'str' objects}
        1    0.000    0.000    0.000    0.000 typing.py:1375(cast)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 {method 'reverse' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
        1    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.ord}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_fit_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:64(on_fit_start)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_fit_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 callback.py:64(on_fit_start)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_fit_start
         6711 function calls (5397 primitive calls) in 0.002 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.002    0.002 model_summary.py:59(on_fit_start)
       12    0.000    0.000    0.002    0.000 {built-in method builtins.sum}
      250    0.000    0.000    0.001    0.000 module.py:2233(parameters)
      250    0.000    0.000    0.001    0.000 module.py:2258(named_parameters)
      250    0.000    0.000    0.001    0.000 module.py:2219(_named_members)
        1    0.000    0.000    0.001    0.001 model_summary.py:312(_get_summary_data)
        3    0.000    0.000    0.001    0.000 model_summary.py:255(total_parameters)
      123    0.000    0.000    0.001    0.000 model_summary.py:257(<genexpr>)
        2    0.000    0.000    0.001    0.000 model_summary.py:247(param_nums)
        2    0.000    0.000    0.001    0.000 model_summary.py:249(<listcomp>)
        6    0.000    0.000    0.001    0.000 model_summary.py:135(num_parameters)
       86    0.000    0.000    0.001    0.000 model_summary.py:138(<genexpr>)
 1358/284    0.000    0.000    0.001    0.000 module.py:2395(named_modules)
      240    0.000    0.000    0.000    0.000 model_summary.py:457(_is_lazy_weight_tensor)
        1    0.000    0.000    0.000    0.000 model_summary.py:259(trainable_parameters)
       41    0.000    0.000    0.000    0.000 model_summary.py:261(<genexpr>)
  493/253    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        1    0.000    0.000    0.000    0.000 model_summary.py:265(total_layer_params)
        1    0.000    0.000    0.000    0.000 model_summary.py:269(model_size)
      240    0.000    0.000    0.000    0.000 parameter.py:8(__instancecheck__)
        1    0.000    0.000    0.000    0.000 model_summary.py:80(summarize)
      517    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
      480    0.000    0.000    0.000    0.000 _tensor.py:1055(__hash__)
        1    0.000    0.000    0.000    0.000 __init__.py:1436(info)
        1    0.000    0.000    0.000    0.000 __init__.py:1565(_log)
      274    0.000    0.000    0.000    0.000 module.py:2286(<lambda>)
        1    0.000    0.000    0.000    0.000 model_summary.py:73(_summary)
        1    0.000    0.000    0.000    0.000 model_summary.py:470(summarize)
        1    0.000    0.000    0.000    0.000 model_summary.py:204(__init__)
        1    0.000    0.000    0.000    0.000 __init__.py:1591(handle)
        1    0.000    0.000    0.000    0.000 __init__.py:1645(callHandlers)
        1    0.000    0.000    0.000    0.000 __init__.py:939(handle)
        1    0.000    0.000    0.000    0.000 __init__.py:1071(emit)
        1    0.000    0.000    0.000    0.000 model_summary.py:273(summarize)
        1    0.000    0.000    0.000    0.000 model_summary.py:371(_format_summary_table)
        1    0.000    0.000    0.000    0.000 redirect.py:644(write)
        1    0.000    0.000    0.000    0.000 wandb_run.py:2310(<lambda>)
        1    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
        1    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
        4    0.000    0.000    0.000    0.000 model_summary.py:274(<genexpr>)
      549    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
      480    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        1    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
        3    0.000    0.000    0.000    0.000 model_summary.py:71(__init__)
        3    0.000    0.000    0.000    0.000 model_summary.py:81(_register_hook)
      160    0.000    0.000    0.000    0.000 {method 'numel' of 'torch._C.TensorBase' objects}
        3    0.000    0.000    0.000    0.000 module.py:1463(register_forward_hook)
      240    0.000    0.000    0.000    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f7dee3ee040}
        6    0.000    0.000    0.000    0.000 model_summary.py:419(get_human_readable_count)
        1    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
        1    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
        1    0.000    0.000    0.000    0.000 __init__.py:1550(makeRecord)
      242    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        1    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
        1    0.000    0.000    0.000    0.000 __init__.py:282(__init__)
        1    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        1    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
       80    0.000    0.000    0.000    0.000 {built-in method math.prod}
        3    0.000    0.000    0.000    0.000 hooks.py:24(__init__)
       10    0.000    0.000    0.000    0.000 {built-in method builtins.max}
        1    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
       24    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
        6    0.000    0.000    0.000    0.000 model_summary.py:113(detach_hook)
        1    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
        1    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
        1    0.000    0.000    0.000    0.000 __init__.py:916(format)
       20    0.000    0.000    0.000    0.000 model_summary.py:385(<genexpr>)
        1    0.000    0.000    0.000    0.000 model_summary.py:218(named_modules)
        6    0.000    0.000    0.000    0.000 hooks.py:35(remove)
        1    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
        1    0.000    0.000    0.000    0.000 __init__.py:1514(findCaller)
        1    0.000    0.000    0.000    0.000 __init__.py:650(format)
        1    0.000    0.000    0.000    0.000 model_summary.py:392(<listcomp>)
        1    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
        3    0.000    0.000    0.000    0.000 model_summary.py:78(__del__)
        4    0.000    0.000    0.000    0.000 module.py:2348(named_children)
        1    0.000    0.000    0.000    0.000 model_summary.py:235(layer_types)
        1    0.000    0.000    0.000    0.000 __init__.py:1060(flush)
        1    0.000    0.000    0.000    0.000 posixpath.py:117(splitext)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 posixpath.py:140(basename)
        1    0.000    0.000    0.000    0.000 model_summary.py:251(training_modes)
        1    0.000    0.000    0.000    0.000 model_summary.py:282(<listcomp>)
        1    0.000    0.000    0.000    0.000 model_summary.py:237(<listcomp>)
       31    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        1    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
        5    0.000    0.000    0.000    0.000 {built-in method math.log10}
        1    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 __init__.py:628(usesTime)
        9    0.000    0.000    0.000    0.000 hooks.py:33(<genexpr>)
        1    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 __init__.py:634(formatMessage)
       20    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 model_summary.py:253(<listcomp>)
        1    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 posixpath.py:52(normcase)
        1    0.000    0.000    0.000    0.000 genericpath.py:121(_splitext)
        1    0.000    0.000    0.000    0.000 model_summary.py:231(layer_names)
        2    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        6    0.000    0.000    0.000    0.000 {built-in method builtins.min}
        1    0.000    0.000    0.000    0.000 __init__.py:218(_acquireLock)
        2    0.000    0.000    0.000    0.000 __init__.py:896(acquire)
        1    0.000    0.000    0.000    0.000 trainer.py:1195(precision)
        1    0.000    0.000    0.000    0.000 __init__.py:432(format)
        3    0.000    0.000    0.000    0.000 model_summary.py:130(layer_type)
        1    0.000    0.000    0.000    0.000 __init__.py:421(usesTime)
        2    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
        1    0.000    0.000    0.000    0.000 trainer.py:1241(is_global_zero)
        1    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
        1    0.000    0.000    0.000    0.000 __init__.py:160(<lambda>)
        2    0.000    0.000    0.000    0.000 __init__.py:903(release)
        1    0.000    0.000    0.000    0.000 model_summary.py:415(get_formatted_model_size)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        1    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        3    0.000    0.000    0.000    0.000 {method 'count' of 'str' objects}
        1    0.000    0.000    0.000    0.000 __init__.py:227(_releaseLock)
        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}
        4    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        1    0.000    0.000    0.000    0.000 __init__.py:119(getLevelName)
        3    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}
        2    0.000    0.000    0.000    0.000 module.py:243(example_input_array)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 __init__.py:429(_format)
        4    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        3    0.000    0.000    0.000    0.000 model_summary.py:140(training)
        1    0.000    0.000    0.000    0.000 threading.py:1358(current_thread)
        1    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)
        2    0.000    0.000    0.000    0.000 __init__.py:791(filter)
        6    0.000    0.000    0.000    0.000 {built-in method math.ceil}
        1    0.000    0.000    0.000    0.000 {built-in method utcnow}
        3    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        5    0.000    0.000    0.000    0.000 {built-in method math.floor}
        5    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}
        1    0.000    0.000    0.000    0.000 module.py:215(trainer)
        1    0.000    0.000    0.000    0.000 __init__.py:358(getMessage)
        1    0.000    0.000    0.000    0.000 __init__.py:1675(getEffectiveLevel)
        3    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        1    0.000    0.000    0.000    0.000 process.py:189(name)
        3    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
        1    0.000    0.000    0.000    0.000 model_summary.py:323(<listcomp>)
        1    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
        1    0.000    0.000    0.000    0.000 threading.py:1093(name)
        1    0.000    0.000    0.000    0.000 process.py:37(current_process)
        1    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        1    0.000    0.000    0.000    0.000 {built-in method sys._getframe}
        1    0.000    0.000    0.000    0.000 {built-in method time.time}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 __init__.py:1276(disable)
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:93(precision_plugin)
        1    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
        1    0.000    0.000    0.000    0.000 single_device.py:81(is_global_zero)
        1    0.000    0.000    0.000    0.000 {method 'keys' of 'collections.OrderedDict' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.ord}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 callback.py:64(on_fit_start)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_fit_start
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 hooks.py:30(on_fit_start)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_sanity_check_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:70(on_sanity_check_start)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)



Profile stats for: [Callback]TQDMProgressBar.on_sanity_check_start
         685 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.001    0.001 tqdm_progress.py:249(on_sanity_check_start)
        1    0.000    0.000    0.001    0.001 tqdm_progress.py:185(init_sanity_tqdm)
        2    0.000    0.000    0.001    0.000 std.py:663(__new__)
        2    0.000    0.000    0.001    0.000 std.py:760(get_lock)
        1    0.000    0.000    0.001    0.001 std.py:90(__init__)
        1    0.000    0.000    0.001    0.001 std.py:116(create_mp_lock)
        1    0.000    0.000    0.001    0.001 context.py:70(RLock)
        2    0.000    0.000    0.000    0.000 tqdm_progress.py:40(__init__)
        2    0.000    0.000    0.000    0.000 std.py:952(__init__)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1002(_find_and_load)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:967(_find_and_load_unlocked)
        1    0.000    0.000    0.000    0.000 std.py:1325(refresh)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:659(_load_unlocked)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:844(exec_module)
        1    0.000    0.000    0.000    0.000 std.py:1464(display)
        1    0.000    0.000    0.000    0.000 _monitor.py:30(__init__)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:916(get_code)
        1    0.000    0.000    0.000    0.000 threading.py:880(start)
        1    0.000    0.000    0.000    0.000 synchronize.py:186(__init__)
        1    0.000    0.000    0.000    0.000 synchronize.py:50(__init__)
        5    0.000    0.000    0.000    0.000 std.py:102(acquire)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:220(_call_with_frames_removed)
        1    0.000    0.000    0.000    0.000 std.py:457(print_status)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.exec}
        1    0.000    0.000    0.000    0.000 synchronize.py:10(<module>)
        1    0.000    0.000    0.000    0.000 threading.py:563(wait)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:645(_compile_bytecode)
        1    0.000    0.000    0.000    0.000 threading.py:280(wait)
        1    0.000    0.000    0.000    0.000 {built-in method marshal.loads}
        8    0.000    0.000    0.000    0.000 {built-in method builtins.__build_class__}
        1    0.000    0.000    0.000    0.000 std.py:1150(__str__)
        4    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 std.py:451(fp_write)
        2    0.000    0.000    0.000    0.000 utils.py:194(inner)
        1    0.000    0.000    0.000    0.000 synchronize.py:114(_make_name)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 std.py:464(format_meter)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:901(_find_spec)
        1    0.000    0.000    0.000    0.000 tempfile.py:149(__next__)
        1    0.000    0.000    0.000    0.000 redirect.py:644(write)
        1    0.000    0.000    0.000    0.000 wandb_run.py:2304(<lambda>)
        1    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
        2    0.000    0.000    0.000    0.000 utils.py:333(_screen_shape_linux)
        1    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
        1    0.000    0.000    0.000    0.000 {built-in method _thread.start_new_thread}
        1    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1415(find_spec)
        1    0.000    0.000    0.000    0.000 utils.py:378(disp_len)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1383(_get_spec)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1036(get_data)
        1    0.000    0.000    0.000    0.000 utils.py:374(_text_width)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.sum}
        1    0.000    0.000    0.000    0.000 tempfile.py:138(rng)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1514(find_spec)
       51    0.000    0.000    0.000    0.000 utils.py:375(<genexpr>)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:558(module_from_spec)
        1    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:486(_init_module_attrs)
        3    0.000    0.000    0.000    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
        1    0.000    0.000    0.000    0.000 threading.py:802(__init__)
        1    0.000    0.000    0.000    0.000 random.py:117(__init__)
        1    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
        2    0.000    0.000    0.000    0.000 utils.py:213(__init__)
        1    0.000    0.000    0.000    0.000 std.py:679(_get_free_pos)
        1    0.000    0.000    0.000    0.000 std.py:438(status_printer)
        2    0.000    0.000    0.000    0.000 {built-in method fcntl.ioctl}
        1    0.000    0.000    0.000    0.000 random.py:126(seed)
        1    0.000    0.000    0.000    0.000 std.py:1446(format_dict)
        1    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
       50    0.000    0.000    0.000    0.000 {built-in method unicodedata.east_asian_width}
        1    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        1    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
        1    0.000    0.000    0.000    0.000 {method 'read' of '_io.BufferedReader' objects}
        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:361(cache_from_source)
        1    0.000    0.000    0.000    0.000 tempfile.py:152(<listcomp>)
        1    0.000    0.000    0.000    0.000 {function Random.seed at 0x7f7dfa37ea60}
        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:385(cached)
        1    0.000    0.000    0.000    0.000 util.py:171(register_after_fork)
        1    0.000    0.000    0.000    0.000 std.py:682(<setcomp>)
        8    0.000    0.000    0.000    0.000 random.py:343(choice)
        2    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}
        1    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:135(_path_stat)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:491(_get_cached)
        1    0.000    0.000    0.000    0.000 {built-in method io.open_code}
        3    0.000    0.000    0.000    0.000 _weakrefset.py:63(__iter__)
        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:121(_path_join)
        3    0.000    0.000    0.000    0.000 {built-in method posix.stat}
        2    0.000    0.000    0.000    0.000 threading.py:528(__init__)
        1    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:156(__enter__)
        4    0.000    0.000    0.000    0.000 std.py:110(__enter__)
        3    0.000    0.000    0.000    0.000 _weakrefset.py:86(add)
        2    0.000    0.000    0.000    0.000 utils.py:347(<listcomp>)
        1    0.000    0.000    0.000    0.000 weakref.py:165(__setitem__)
        8    0.000    0.000    0.000    0.000 random.py:237(_randbelow_with_getrandbits)
       23    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        5    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:398(parent)
        4    0.000    0.000    0.000    0.000 std.py:113(__exit__)
        2    0.000    0.000    0.000    0.000 threading.py:228(__init__)
        1    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
        5    0.000    0.000    0.000    0.000 std.py:106(release)
        2    0.000    0.000    0.000    0.000 os.py:674(__getitem__)
        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:123(<listcomp>)
       17    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        4    0.000    0.000    0.000    0.000 utils.py:187(disable_on_exception)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:166(_get_module_lock)
        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:127(_path_split)
        4    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
        1    0.000    0.000    0.000    0.000 _weakrefset.py:111(remove)
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}
        1    0.000    0.000    0.000    0.000 std.py:186(__format__)
        2    0.000    0.000    0.000    0.000 functools.py:392(__get__)
        1    0.000    0.000    0.000    0.000 _weakrefset.py:27(__exit__)
        1    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1509(_get_spec)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:154(_path_isfile)
        2    0.000    0.000    0.000    0.000 {method 'remove' of 'set' objects}
        1    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
        2    0.000    0.000    0.000    0.000 utils.py:266(_supports_unicode)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:560(_classify_pyc)
        4    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1033(_handle_fromlist)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:145(_path_is_mode_type)
        1    0.000    0.000    0.000    0.000 synchronize.py:360(Barrier)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1077(path_stats)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.max}
        2    0.000    0.000    0.000    0.000 utils.py:156(__init__)
        3    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
        1    0.000    0.000    0.000    0.000 std.py:400(format_interval)
        6    0.000    0.000    0.000    0.000 utils.py:152(wrapper_setattr)
        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:79(_unpack_uint32)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:696(spec_from_file_location)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:87(acquire)
        1    0.000    0.000    0.000    0.000 threading.py:271(_is_owned)
        1    0.000    0.000    0.000    0.000 {built-in method fromtimestamp}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:593(_validate_timestamp_pyc)
        2    0.000    0.000    0.000    0.000 os.py:754(encode)
        5    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
        1    0.000    0.000    0.000    0.000 std.py:153(__init__)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:160(__exit__)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:35(_new_module)
        1    0.000    0.000    0.000    0.000 utils.py:125(__eq__)
        1    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        1    0.000    0.000    0.000    0.000 synchronize.py:46(SemLock)
        1    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:173(is_disabled)
        1    0.000    0.000    0.000    0.000 synchronize.py:142(BoundedSemaphore)
        1    0.000    0.000    0.000    0.000 _weakrefset.py:53(_commit_removals)
       10    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}
        4    0.000    0.000    0.000    0.000 utils.py:222(__eq__)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:112(release)
        7    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:231(_verbose_message)
        4    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:129(<genexpr>)
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
       14    0.000    0.000    0.000    0.000 {method 'getrandbits' of '_random.Random' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:58(__init__)
        1    0.000    0.000    0.000    0.000 context.py:233(get_context)
        1    0.000    0.000    0.000    0.000 utils.py:273(_is_ascii)
        5    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:878(__exit__)
        1    0.000    0.000    0.000    0.000 import_hooks.py:233(find_spec)
       12    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
       16    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        3    0.000    0.000    0.000    0.000 std.py:226(__init__)
        2    0.000    0.000    0.000    0.000 std.py:1153(_comparable)
        9    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:169(is_enabled)
        1    0.000    0.000    0.000    0.000 weakref.py:348(__new__)
        5    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:874(__enter__)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:185(cb)
        1    0.000    0.000    0.000    0.000 threading.py:256(__enter__)
        1    0.000    0.000    0.000    0.000 _weakrefset.py:17(__init__)
       14    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        3    0.000    0.000    0.000    0.000 std.py:1157(__hash__)
        6    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 threading.py:265(_release_save)
        1    0.000    0.000    0.000    0.000 threading.py:1162(daemon)
        2    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
        1    0.000    0.000    0.000    0.000 threading.py:268(_acquire_restore)
        1    0.000    0.000    0.000    0.000 _monitor.py:94(report)
        1    0.000    0.000    0.000    0.000 synchronize.py:210(Condition)
        2    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:523(_check_name_wrapper)
        1    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
        1    0.000    0.000    0.000    0.000 utils.py:252(_is_utf)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:811(find_spec)
        1    0.000    0.000    0.000    0.000 threading.py:1229(_make_invoke_excepthook)
        1    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
        4    0.000    0.000    0.000    0.000 {built-in method _weakref.proxy}
        1    0.000    0.000    0.000    0.000 __init__.py:82(find_spec)
        1    0.000    0.000    0.000    0.000 {built-in method atexit.register}
        1    0.000    0.000    0.000    0.000 _weakrefset.py:21(__enter__)
        1    0.000    0.000    0.000    0.000 weakref.py:353(__init__)
        1    0.000    0.000    0.000    0.000 threading.py:757(_newname)
        3    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
        1    0.000    0.000    0.000    0.000 utils.py:282(_screen_shape_wrapper)
        5    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
        1    0.000    0.000    0.000    0.000 threading.py:259(__exit__)
        3    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f7dfbd377c0}
        1    0.000    0.000    0.000    0.000 threading.py:1358(current_thread)
        1    0.000    0.000    0.000    0.000 synchronize.py:90(_make_methods)
        1    0.000    0.000    0.000    0.000 utils.py:108(__init__)
        1    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 threading.py:1147(daemon)
        7    0.000    0.000    0.000    0.000 {built-in method _imp.release_lock}
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        5    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}
        6    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
        4    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        2    0.000    0.000    0.000    0.000 {method 'setter' of 'property' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:351(__init__)
        7    0.000    0.000    0.000    0.000 {built-in method _imp.acquire_lock}
        1    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        8    0.000    0.000    0.000    0.000 {method 'bit_length' of 'int' objects}
        5    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 std.py:167(colour)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}
        1    0.000    0.000    0.000    0.000 std.py:231(__call__)
        3    0.000    0.000    0.000    0.000 threading.py:536(is_set)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1346(_path_importer_cache)
        1    0.000    0.000    0.000    0.000 {built-in method utcnow}
        2    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}
        3    0.000    0.000    0.000    0.000 {built-in method from_bytes}
        1    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:152(__init__)
        1    0.000    0.000    0.000    0.000 util.py:48(debug)
        1    0.000    0.000    0.000    0.000 {method 'difference' of 'set' objects}
        1    0.000    0.000    0.000    0.000 synchronize.py:321(Event)
        4    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
        1    0.000    0.000    0.000    0.000 synchronize.py:123(Semaphore)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:137(val_progress_bar)
        1    0.000    0.000    0.000    0.000 utils.py:112(__format__)
        1    0.000    0.000    0.000    0.000 std.py:98(<listcomp>)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:127(train_progress_bar)
        3    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.min}
        1    0.000    0.000    0.000    0.000 synchronize.py:159(Lock)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:165(process_position)
        1    0.000    0.000    0.000    0.000 context.py:197(get_start_method)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:406(has_location)
        2    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1006(__init__)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:161(refresh_rate)
        1    0.000    0.000    0.000    0.000 std.py:163(colour)
        3    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        1    0.000    0.000    0.000    0.000 {built-in method _imp.is_frozen}
        1    0.000    0.000    0.000    0.000 synchronize.py:184(RLock)
        1    0.000    0.000    0.000    0.000 progress_bar.py:60(sanity_check_description)
        1    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
        1    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {built-in method _imp._fix_co_filename}
        1    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        1    0.000    0.000    0.000    0.000 process.py:37(current_process)
        1    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:736(find_spec)
        1    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
        1    0.000    0.000    0.000    0.000 {built-in method time.time}
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:68(_relax_case)
        1    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1031(get_filename)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:841(create_module)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.ord}



Profile stats for: [Callback]ModelSummary.on_sanity_check_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:70(on_sanity_check_start)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 callback.py:70(on_sanity_check_start)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningDataModule]PMTfiedDataModule.val_dataloader
         605 function calls (516 primitive calls) in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 PMTfiedDataModule.py:77(val_dataloader)
      2/1    0.000    0.000    0.000    0.000 data.py:287(wrapper)
        1    0.000    0.000    0.000    0.000 dataloader.py:227(__init__)
        2    0.000    0.000    0.000    0.000 inspect.py:3111(signature)
        2    0.000    0.000    0.000    0.000 inspect.py:2859(from_callable)
        2    0.000    0.000    0.000    0.000 inspect.py:2246(_signature_from_callable)
       37    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        3    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)
        3    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}
        2    0.000    0.000    0.000    0.000 inspect.py:2152(_signature_from_function)
     45/3    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)
     45/3    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}
    23/22    0.000    0.000    0.000    0.000 data.py:334(wrapper)
       21    0.000    0.000    0.000    0.000 inspect.py:2498(__init__)
       21    0.000    0.000    0.000    0.000 data.py:295(<genexpr>)
    20/19    0.000    0.000    0.000    0.000 dataloader.py:418(__setattr__)
        2    0.000    0.000    0.000    0.000 inspect.py:2781(__init__)
        1    0.000    0.000    0.000    0.000 sampler.py:133(__init__)
       21    0.000    0.000    0.000    0.000 enum.py:358(__call__)
        1    0.000    0.000    0.000    0.000 dataloader.py:487(check_worker_number_rationality)
        1    0.000    0.000    0.000    0.000 sampler.py:262(__init__)
       23    0.000    0.000    0.000    0.000 inspect.py:2830(<genexpr>)
        2    0.000    0.000    0.000    0.000 inspect.py:494(unwrap)
        1    0.000    0.000    0.000    0.000 {built-in method posix.sched_getaffinity}
       50    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        2    0.000    0.000    0.000    0.000 data.py:303(<dictcomp>)
        2    0.000    0.000    0.000    0.000 sampler.py:146(num_samples)
       61    0.000    0.000    0.000    0.000 inspect.py:2548(name)
      8/6    0.000    0.000    0.000    0.000 {built-in method builtins.len}
       37    0.000    0.000    0.000    0.000 _collections_abc.py:262(__subclasshook__)
       27    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
       21    0.000    0.000    0.000    0.000 enum.py:670(__new__)
        1    0.000    0.000    0.000    0.000 dataloader.py:394(multiprocessing_context)
       21    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {built-in method torch.set_vital}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
       21    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        2    0.000    0.000    0.000    0.000 inspect.py:514(_is_wrapper)
       19    0.000    0.000    0.000    0.000 inspect.py:2560(kind)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        5    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
       19    0.000    0.000    0.000    0.000 inspect.py:2552(default)
        4    0.000    0.000    0.000    0.000 inspect.py:159(isfunction)
        2    0.000    0.000    0.000    0.000 dataset.py:422(__len__)
        2    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}
        1    0.000    0.000    0.000    0.000 dataloader.py:442(_auto_collation)
        2    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        2    0.000    0.000    0.000    0.000 {built-in method sys.getrecursionlimit}
        2    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
        2    0.000    0.000    0.000    0.000 inspect.py:2865(parameters)
        1    0.000    0.000    0.000    0.000 {method 'index' of 'tuple' objects}
        2    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_model_eval
         79891 function calls (70700 primitive calls) in 0.042 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.002    0.000    0.042    0.000 hooks.py:162(on_validation_model_eval)
      101    0.000    0.000    0.039    0.000 module.py:2459(eval)
 4646/101    0.007    0.000    0.039    0.000 module.py:2437(train)
     4646    0.016    0.000    0.023    0.000 module.py:1731(__setattr__)
     9191    0.004    0.000    0.009    0.000 module.py:2339(children)
18584/13938    0.003    0.000    0.006    0.000 {built-in method builtins.isinstance}
     9191    0.003    0.000    0.004    0.000 module.py:2348(named_children)
     4646    0.002    0.000    0.004    0.000 parameter.py:8(__instancecheck__)
    14039    0.001    0.000    0.001    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.001    0.000 trainer.py:1203(model)
      101    0.000    0.000    0.001    0.000 contextlib.py:123(__exit__)
     4646    0.001    0.000    0.001    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f7dee3ee040}
      101    0.000    0.000    0.000    0.000 module.py:215(trainer)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
     4545    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
     4646    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:351(model)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_validation_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:211(on_validation_start)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_validation_start
         36019 function calls in 0.166 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.002    0.000    0.166    0.002 tqdm_progress.py:287(on_validation_start)
      100    0.007    0.000    0.160    0.002 tqdm_progress.py:223(init_validation_tqdm)
      100    0.002    0.000    0.145    0.001 tqdm_progress.py:40(__init__)
      100    0.007    0.000    0.143    0.001 std.py:952(__init__)
      100    0.000    0.000    0.120    0.001 std.py:1325(refresh)
      100    0.001    0.000    0.119    0.001 std.py:1464(display)
      600    0.001    0.000    0.098    0.000 utils.py:194(inner)
      200    0.001    0.000    0.085    0.000 std.py:1441(moveto)
      300    0.002    0.000    0.079    0.000 redirect.py:644(write)
      300    0.001    0.000    0.077    0.000 wandb_run.py:2304(<lambda>)
      300    0.001    0.000    0.076    0.000 wandb_run.py:390(wrapper_fn)
      300    0.002    0.000    0.074    0.000 wandb_run.py:1429(_console_raw_callback)
      300    0.009    0.000    0.072    0.000 interface.py:749(publish_output_raw)
      300    0.003    0.000    0.053    0.000 interface_shared.py:76(_publish_output_raw)
      300    0.002    0.000    0.050    0.000 interface_sock.py:45(_publish)
      300    0.003    0.000    0.048    0.000 sock_client.py:219(send_record_publish)
      300    0.001    0.000    0.044    0.000 sock_client.py:153(send_server_request)
      300    0.002    0.000    0.044    0.000 sock_client.py:145(_send_message)
      300    0.001    0.000    0.040    0.000 sock_client.py:121(_sendall_with_error_handle)
      300    0.039    0.000    0.039    0.000 {method 'send' of '_socket.socket' objects}
      100    0.000    0.000    0.019    0.000 std.py:457(print_status)
      500    0.018    0.000    0.018    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      100    0.000    0.000    0.014    0.000 std.py:451(fp_write)
      100    0.002    0.000    0.014    0.000 std.py:1150(__str__)
      200    0.005    0.000    0.012    0.000 utils.py:333(_screen_shape_linux)
      100    0.006    0.000    0.009    0.000 std.py:464(format_meter)
      100    0.003    0.000    0.007    0.000 std.py:663(__new__)
      300    0.002    0.000    0.007    0.000 well_known_types.py:172(GetCurrentTime)
      300    0.002    0.000    0.005    0.000 well_known_types.py:242(FromDatetime)
      100    0.000    0.000    0.004    0.000 utils.py:378(disp_len)
      200    0.004    0.000    0.004    0.000 {built-in method fcntl.ioctl}
      100    0.000    0.000    0.003    0.000 utils.py:374(_text_width)
      100    0.001    0.000    0.003    0.000 {built-in method builtins.sum}
      200    0.001    0.000    0.003    0.000 utils.py:347(<listcomp>)
      100    0.001    0.000    0.003    0.000 std.py:1446(format_dict)
      300    0.003    0.000    0.003    0.000 enum_type_wrapper.py:92(__getattr__)
     4600    0.002    0.000    0.003    0.000 utils.py:375(<genexpr>)
      100    0.001    0.000    0.003    0.000 utils.py:213(__init__)
      100    0.002    0.000    0.002    0.000 tqdm_progress.py:137(val_progress_bar)
      200    0.000    0.000    0.002    0.000 std.py:110(__enter__)
      200    0.001    0.000    0.002    0.000 os.py:674(__getitem__)
      300    0.001    0.000    0.002    0.000 std.py:102(acquire)
      100    0.001    0.000    0.001    0.000 std.py:438(status_printer)
      300    0.001    0.000    0.001    0.000 calendar.py:655(timegm)
      100    0.000    0.000    0.001    0.000 _weakrefset.py:86(add)
      200    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}
      300    0.001    0.000    0.001    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
      101    0.000    0.000    0.001    0.000 trainer.py:1429(sanity_checking)
     1900    0.001    0.000    0.001    0.000 {built-in method builtins.getattr}
      100    0.000    0.000    0.001    0.000 tqdm_progress.py:173(is_disabled)
      300    0.001    0.000    0.001    0.000 std.py:226(__init__)
      300    0.001    0.000    0.001    0.000 interface_sock.py:41(_assign)
      100    0.001    0.000    0.001    0.000 functools.py:392(__get__)
      101    0.000    0.000    0.001    0.000 enums.py:81(__eq__)
      200    0.001    0.000    0.001    0.000 utils.py:266(_supports_unicode)
      200    0.001    0.000    0.001    0.000 utils.py:187(disable_on_exception)
      100    0.001    0.000    0.001    0.000 {method 'add' of 'set' objects}
      300    0.001    0.000    0.001    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
      600    0.001    0.000    0.001    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
      100    0.001    0.000    0.001    0.000 {built-in method fromtimestamp}
      500    0.001    0.000    0.001    0.000 {built-in method builtins.hasattr}
      300    0.001    0.000    0.001    0.000 {built-in method _struct.pack}
     4500    0.001    0.000    0.001    0.000 {built-in method unicodedata.east_asian_width}
      100    0.000    0.000    0.000    0.000 std.py:400(format_interval)
      300    0.000    0.000    0.000    0.000 utils.py:152(wrapper_setattr)
      100    0.000    0.000    0.000    0.000 utils.py:156(__init__)
      300    0.000    0.000    0.000    0.000 std.py:106(release)
      100    0.000    0.000    0.000    0.000 tqdm_progress.py:169(is_enabled)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 std.py:1147(__del__)
      200    0.000    0.000    0.000    0.000 os.py:754(encode)
      202    0.000    0.000    0.000    0.000 types.py:171(__get__)
      300    0.000    0.000    0.000    0.000 {built-in method utcnow}
      200    0.000    0.000    0.000    0.000 std.py:113(__exit__)
      300    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      100    0.000    0.000    0.000    0.000 _monitor.py:94(report)
      100    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 std.py:186(__format__)
      100    0.000    0.000    0.000    0.000 tqdm_progress.py:165(process_position)
      202    0.000    0.000    0.000    0.000 enum.py:792(value)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      400    0.000    0.000    0.000    0.000 utils.py:222(__eq__)
      100    0.000    0.000    0.000    0.000 std.py:153(__init__)
      300    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
      100    0.000    0.000    0.000    0.000 utils.py:273(_is_ascii)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      300    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
      300    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      300    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
      100    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
      100    0.000    0.000    0.000    0.000 std.py:760(get_lock)
      300    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      700    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      200    0.000    0.000    0.000    0.000 utils.py:370(_term_move_up)
      100    0.000    0.000    0.000    0.000 utils.py:252(_is_utf)
      401    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      100    0.000    0.000    0.000    0.000 utils.py:282(_screen_shape_wrapper)
      200    0.000    0.000    0.000    0.000 {built-in method _weakref.proxy}
      100    0.000    0.000    0.000    0.000 std.py:231(__call__)
      300    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      100    0.000    0.000    0.000    0.000 std.py:1157(__hash__)
      100    0.000    0.000    0.000    0.000 progress_bar.py:54(trainer)
      300    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      100    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f7dfbd377c0}
      300    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      100    0.000    0.000    0.000    0.000 tqdm_progress.py:161(refresh_rate)
      300    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
      100    0.000    0.000    0.000    0.000 utils.py:108(__init__)
      300    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
      100    0.000    0.000    0.000    0.000 std.py:167(colour)
      300    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
      300    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.max}
      100    0.000    0.000    0.000    0.000 utils.py:112(__format__)
      300    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
      100    0.000    0.000    0.000    0.000 threading.py:536(is_set)
      100    0.000    0.000    0.000    0.000 {built-in method time.time}
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      202    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      100    0.000    0.000    0.000    0.000 std.py:1265(close)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 progress_bar.py:68(validation_description)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.id}
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      100    0.000    0.000    0.000    0.000 std.py:163(colour)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_validation_start
         1010 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 callback.py:211(on_validation_start)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 callback.py:211(on_validation_start)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_start
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 hooks.py:50(on_validation_start)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Strategy]SingleDeviceStrategy.on_validation_start
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 strategy.py:549(on_validation_start)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_validation_epoch_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 callback.py:124(on_validation_epoch_start)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_validation_epoch_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:124(on_validation_epoch_start)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_validation_epoch_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 callback.py:124(on_validation_epoch_start)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:124(on_validation_epoch_start)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_epoch_start
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 hooks.py:241(on_validation_epoch_start)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [_EvaluationLoop].val_next
         9407 function calls (8599 primitive calls) in 0.056 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
  404/101    0.000    0.000    0.055    0.001 {built-in method builtins.next}
      101    0.000    0.000    0.055    0.001 combined_loader.py:339(__next__)
      101    0.001    0.000    0.055    0.001 combined_loader.py:128(__next__)
      101    0.004    0.000    0.054    0.001 dataloader.py:625(__next__)
      101    0.003    0.000    0.017    0.000 profiler.py:693(__exit__)
      101    0.001    0.000    0.016    0.000 dataloader.py:1297(_next_data)
      101    0.001    0.000    0.015    0.000 dataloader.py:1264(_get_data)
      101    0.000    0.000    0.014    0.000 dataloader.py:1118(_try_get_data)
      101    0.002    0.000    0.013    0.000 queue.py:154(get)
      101    0.001    0.000    0.013    0.000 _ops.py:887(__call__)
      101    0.003    0.000    0.013    0.000 profiler.py:687(__enter__)
      206    0.011    0.000    0.011    0.000 {method 'acquire' of '_thread.lock' objects}
        1    0.000    0.000    0.011    0.011 threading.py:280(wait)
      101    0.001    0.000    0.010    0.000 _ops.py:1047(__call__)
      101    0.001    0.000    0.010    0.000 _ops.py:943(_must_dispatch_in_python)
      101    0.009    0.000    0.009    0.000 {built-in method torch._ops.profiler._record_function_enter_new}
      101    0.001    0.000    0.008    0.000 _pytree.py:1181(tree_any)
      101    0.000    0.000    0.007    0.000 {built-in method builtins.any}
  707/202    0.003    0.000    0.006    0.000 _pytree.py:874(tree_iter)
      101    0.003    0.000    0.004    0.000 profiler.py:676(__init__)
      101    0.002    0.000    0.002    0.000 {built-in method torch._ops.profiler.}
      404    0.000    0.000    0.002    0.000 _pytree.py:656(_is_leaf)
      101    0.002    0.000    0.002    0.000 _ops.py:945(<lambda>)
      707    0.001    0.000    0.002    0.000 _pytree.py:649(_get_node_type)
      101    0.001    0.000    0.001    0.000 typing.py:271(inner)
      707    0.001    0.000    0.001    0.000 _pytree.py:638(_is_namedtuple_instance)
      101    0.000    0.000    0.000    0.000 threading.py:1133(is_alive)
      101    0.000    0.000    0.000    0.000 dataloader.py:1366(_process_data)
      101    0.000    0.000    0.000    0.000 evaluation_loop.py:346(_on_after_fetch)
      101    0.000    0.000    0.000    0.000 dataloader.py:1346(_try_put_index)
      101    0.000    0.000    0.000    0.000 _pytree.py:423(_dict_flatten)
      101    0.000    0.000    0.000    0.000 threading.py:351(notify)
      101    0.000    0.000    0.000    0.000 threading.py:1066(_wait_for_tstate_lock)
      101    0.000    0.000    0.000    0.000 queue.py:217(_get)
     1112    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      102    0.000    0.000    0.000    0.000 queue.py:209(_qsize)
      202    0.000    0.000    0.000    0.000 _pytree.py:395(_tuple_flatten)
      101    0.000    0.000    0.000    0.000 threading.py:256(__enter__)
      101    0.000    0.000    0.000    0.000 dataloader.py:619(_next_index)
      102    0.000    0.000    0.000    0.000 threading.py:271(_is_owned)
      303    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      101    0.000    0.000    0.000    0.000 threading.py:259(__exit__)
      101    0.000    0.000    0.000    0.000 threading.py:536(is_set)
      101    0.000    0.000    0.000    0.000 states.py:67(dataloader_prefix)
      101    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}
      101    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
      101    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}
      102    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      101    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 __init__.py:129(annotate)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method '__exit__' of 'torch._C.DisableTorchFunctionSubclass' objects}
      101    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 threading.py:265(_release_save)
        1    0.000    0.000    0.000    0.000 threading.py:268(_acquire_restore)
        1    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}
        1    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_before_batch_transfer
         1407 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      201    0.000    0.000    0.001    0.000 contextlib.py:123(__exit__)
      201    0.000    0.000    0.001    0.000 {built-in method builtins.next}
      201    0.000    0.000    0.001    0.000 profiler.py:55(profile)
      201    0.001    0.000    0.001    0.000 advanced.py:71(stop)
      201    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      201    0.000    0.000    0.000    0.000 hooks.py:613(on_before_batch_transfer)
      201    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Strategy]SingleDeviceStrategy.batch_to_device
         10251 function calls (10050 primitive calls) in 0.109 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      201    0.003    0.000    0.109    0.001 strategy.py:262(batch_to_device)
      201    0.000    0.000    0.106    0.001 module.py:354(_apply_batch_transfer_handler)
      201    0.000    0.000    0.105    0.001 module.py:336(_call_batch_hook)
      201    0.001    0.000    0.102    0.001 call.py:137(_call_lightning_module_hook)
      201    0.000    0.000    0.100    0.000 contextlib.py:114(__enter__)
      201    0.000    0.000    0.100    0.000 {built-in method builtins.next}
      201    0.000    0.000    0.100    0.000 profiler.py:55(profile)
      201    0.001    0.000    0.100    0.000 advanced.py:65(start)
      201    0.099    0.000    0.099    0.000 {method 'enable' of '_lsprof.Profiler' objects}
      201    0.000    0.000    0.003    0.000 data_connector.py:349(get_instance)
      402    0.001    0.000    0.003    0.000 model_helpers.py:29(is_overridden)
      402    0.002    0.000    0.002    0.000 overrides.py:10(is_overridden)
      201    0.000    0.000    0.001    0.000 module.py:1731(__setattr__)
      201    0.001    0.000    0.001    0.000 single_device.py:71(root_device)
2211/2010    0.000    0.000    0.001    0.000 {built-in method builtins.isinstance}
      201    0.000    0.000    0.000    0.000 contextlib.py:261(helper)
     1206    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      201    0.000    0.000    0.000    0.000 parameter.py:8(__instancecheck__)
      201    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)
      804    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      201    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      201    0.000    0.000    0.000    0.000 __init__.py:1424(debug)
      402    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      603    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      201    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
      201    0.000    0.000    0.000    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f7dee3ee040}
      201    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      201    0.000    0.000    0.000    0.000 {built-in method builtins.callable}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.transfer_batch_to_device
         10060 function calls in 0.093 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      201    0.001    0.000    0.092    0.000 hooks.py:564(transfer_batch_to_device)
      201    0.004    0.000    0.091    0.000 apply_func.py:71(move_data_to_device)
      201    0.002    0.000    0.086    0.000 apply_func.py:23(apply_to_collection)
      201    0.001    0.000    0.081    0.000 apply_func.py:70(<dictcomp>)
      804    0.007    0.000    0.080    0.000 apply_func.py:91(batch_to)
      804    0.074    0.000    0.074    0.000 {method 'to' of 'torch._C.TensorBase' objects}
     2814    0.002    0.000    0.004    0.000 {built-in method builtins.isinstance}
     1005    0.000    0.000    0.002    0.000 abc.py:117(__instancecheck__)
     1005    0.002    0.000    0.002    0.000 {built-in method _abc._abc_instancecheck}
      201    0.000    0.000    0.002    0.000 {built-in method builtins.all}
     1005    0.000    0.000    0.001    0.000 apply_func.py:69(<genexpr>)
      201    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      201    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      201    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      201    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      201    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      201    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
      201    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      201    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)
        2    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}
        2    0.000    0.000    0.000    0.000 apply_func.py:63(__subclasshook__)
        2    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        2    0.000    0.000    0.000    0.000 {built-in method builtins.callable}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_after_batch_transfer
         1407 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      201    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      201    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      201    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      201    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      201    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      201    0.000    0.000    0.000    0.000 hooks.py:641(on_after_batch_transfer)
      201    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_validation_batch_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 callback.py:142(on_validation_batch_start)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_validation_batch_start
         79555 function calls in 0.056 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.001    0.000    0.055    0.001 tqdm_progress.py:292(on_validation_batch_start)
      202    0.000    0.000    0.051    0.000 std.py:1325(refresh)
      202    0.001    0.000    0.050    0.000 std.py:1464(display)
      101    0.000    0.000    0.029    0.000 std.py:1360(reset)
     1204    0.001    0.000    0.028    0.000 utils.py:194(inner)
      602    0.001    0.000    0.023    0.000 redirect.py:644(write)
      602    0.000    0.000    0.022    0.000 wandb_run.py:2304(<lambda>)
      602    0.000    0.000    0.022    0.000 wandb_run.py:390(wrapper_fn)
      101    0.000    0.000    0.022    0.000 std.py:1382(set_description)
      602    0.001    0.000    0.021    0.000 wandb_run.py:1429(_console_raw_callback)
      602    0.002    0.000    0.020    0.000 interface.py:749(publish_output_raw)
      400    0.001    0.000    0.019    0.000 std.py:1441(moveto)
      202    0.001    0.000    0.016    0.000 std.py:457(print_status)
      202    0.001    0.000    0.015    0.000 std.py:1150(__str__)
      602    0.001    0.000    0.014    0.000 interface_shared.py:76(_publish_output_raw)
      602    0.001    0.000    0.013    0.000 interface_sock.py:45(_publish)
      602    0.001    0.000    0.011    0.000 sock_client.py:219(send_record_publish)
      602    0.000    0.000    0.010    0.000 sock_client.py:153(send_server_request)
      202    0.000    0.000    0.010    0.000 std.py:451(fp_write)
      602    0.002    0.000    0.010    0.000 sock_client.py:145(_send_message)
      202    0.004    0.000    0.010    0.000 std.py:464(format_meter)
      602    0.001    0.000    0.007    0.000 sock_client.py:121(_sendall_with_error_handle)
      602    0.006    0.000    0.006    0.000 {method 'send' of '_socket.socket' objects}
      202    0.000    0.000    0.005    0.000 utils.py:378(disp_len)
      202    0.000    0.000    0.004    0.000 utils.py:374(_text_width)
      202    0.001    0.000    0.004    0.000 {built-in method builtins.sum}
      202    0.001    0.000    0.004    0.000 std.py:1446(format_dict)
      602    0.004    0.000    0.004    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      602    0.000    0.000    0.004    0.000 well_known_types.py:172(GetCurrentTime)
      404    0.002    0.000    0.003    0.000 utils.py:273(_is_ascii)
    11423    0.002    0.000    0.003    0.000 utils.py:375(<genexpr>)
      202    0.001    0.000    0.003    0.000 utils.py:333(_screen_shape_linux)
      602    0.001    0.000    0.003    0.000 well_known_types.py:242(FromDatetime)
      101    0.001    0.000    0.002    0.000 progress_bar.py:90(total_val_batches_current_dataloader)
      404    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}
      202    0.001    0.000    0.001    0.000 {built-in method fcntl.ioctl}
      303    0.000    0.000    0.001    0.000 enums.py:81(__eq__)
    17974    0.001    0.000    0.001    0.000 {built-in method builtins.ord}
      203    0.000    0.000    0.001    0.000 trainer.py:1429(sanity_checking)
      602    0.001    0.000    0.001    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
    11221    0.001    0.000    0.001    0.000 {built-in method unicodedata.east_asian_width}
      602    0.001    0.000    0.001    0.000 enum_type_wrapper.py:92(__getattr__)
      602    0.001    0.000    0.001    0.000 calendar.py:655(timegm)
      602    0.001    0.000    0.001    0.000 interface_sock.py:41(_assign)
      202    0.000    0.000    0.001    0.000 utils.py:347(<listcomp>)
      602    0.001    0.000    0.001    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
      202    0.000    0.000    0.001    0.000 std.py:186(__format__)
      202    0.000    0.000    0.001    0.000 os.py:674(__getitem__)
      202    0.000    0.000    0.000    0.000 std.py:400(format_interval)
     1204    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1535(num_val_batches)
      606    0.000    0.000    0.000    0.000 types.py:171(__get__)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      602    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
     1806    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      202    0.000    0.000    0.000    0.000 std.py:102(acquire)
      202    0.000    0.000    0.000    0.000 std.py:106(release)
      602    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
      202    0.000    0.000    0.000    0.000 std.py:153(__init__)
      602    0.000    0.000    0.000    0.000 {built-in method utcnow}
      101    0.000    0.000    0.000    0.000 tqdm_progress.py:440(convert_inf)
      602    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
      202    0.000    0.000    0.000    0.000 {built-in method fromtimestamp}
      202    0.000    0.000    0.000    0.000 os.py:754(encode)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
     1010    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
     1406    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      202    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      400    0.000    0.000    0.000    0.000 utils.py:370(_term_move_up)
      404    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      202    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
      303    0.000    0.000    0.000    0.000 tqdm_progress.py:131(val_progress_bar)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      602    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      602    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      202    0.000    0.000    0.000    0.000 std.py:231(__call__)
      602    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      606    0.000    0.000    0.000    0.000 enum.py:792(value)
      303    0.000    0.000    0.000    0.000 std.py:226(__init__)
      606    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      101    0.000    0.000    0.000    0.000 progress_bar.py:145(has_dataloader_changed)
      606    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
      202    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
      202    0.000    0.000    0.000    0.000 {built-in method builtins.max}
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      202    0.000    0.000    0.000    0.000 utils.py:108(__init__)
      202    0.000    0.000    0.000    0.000 utils.py:112(__format__)
      602    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
      101    0.000    0.000    0.000    0.000 {built-in method math.isinf}
      404    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      202    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      303    0.000    0.000    0.000    0.000 {built-in method time.time}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      202    0.000    0.000    0.000    0.000 std.py:167(colour)
      202    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      202    0.000    0.000    0.000    0.000 progress_bar.py:54(trainer)
      202    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
      202    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      202    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      202    0.000    0.000    0.000    0.000 std.py:163(colour)
      101    0.000    0.000    0.000    0.000 {built-in method math.isnan}
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 progress_bar.py:68(validation_description)
        1    0.000    0.000    0.000    0.000 trainer.py:1528(num_sanity_val_batches)
        1    0.000    0.000    0.000    0.000 evaluation_loop.py:84(max_batches)
        1    0.000    0.000    0.000    0.000 evaluation_loop.py:90(<listcomp>)
        2    0.000    0.000    0.000    0.000 {built-in method builtins.min}
        1    0.000    0.000    0.000    0.000 trainer.py:1533(<listcomp>)
        1    0.000    0.000    0.000    0.000 progress_bar.py:60(sanity_check_description)



Profile stats for: [Callback]ModelSummary.on_validation_batch_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:142(on_validation_batch_start)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_start
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:142(on_validation_batch_start)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_batch_start
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 hooks.py:93(on_validation_batch_start)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Strategy]SingleDeviceStrategy.validation_step
         746707 function calls (724094 primitive calls) in 1.570 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.002    0.000    1.573    0.016 strategy.py:400(validation_step)
      101    0.007    0.000    1.568    0.016 FlavourClassificationTransformerEncoder.py:97(validation_step)
 4848/101    0.004    0.000    1.436    0.014 module.py:1549(_wrapped_call_impl)
 4848/101    0.022    0.000    1.435    0.014 module.py:1555(_call_impl)
      101    0.006    0.000    1.433    0.014 FlavourClassificationTransformerEncoder.py:55(forward)
      303    0.025    0.000    1.308    0.004 EncoderBlock.py:57(forward)
      303    0.093    0.000    0.590    0.002 XFormersAttention.py:33(forward)
      606    0.106    0.000    0.445    0.001 LayerNormalisation.py:17(forward)
     6363    0.010    0.000    0.390    0.000 __init__.py:1436(info)
     6363    0.009    0.000    0.378    0.000 __init__.py:1565(_log)
     6363    0.005    0.000    0.259    0.000 __init__.py:1591(handle)
     6363    0.009    0.000    0.252    0.000 __init__.py:1645(callHandlers)
     6363    0.007    0.000    0.244    0.000 __init__.py:939(handle)
     6363    0.004    0.000    0.229    0.000 __init__.py:1178(emit)
     6363    0.007    0.000    0.225    0.000 __init__.py:1071(emit)
     2020    0.005    0.000    0.169    0.000 linear.py:116(forward)
      303    0.017    0.000    0.163    0.001 FFN.py:16(forward)
     2020    0.161    0.000    0.161    0.000 {built-in method torch._C._nn.linear}
     6363    0.007    0.000    0.153    0.000 __init__.py:1060(flush)
     6363    0.139    0.000    0.139    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      606    0.003    0.000    0.126    0.000 __init__.py:189(memory_efficient_attention)
      606    0.001    0.000    0.121    0.000 __init__.py:457(_memory_efficient_attention)
      606    0.005    0.000    0.119    0.000 __init__.py:475(_memory_efficient_attention_forward)
     1515    0.002    0.000    0.112    0.000 {built-in method builtins.print}
     3030    0.003    0.000    0.110    0.000 redirect.py:644(write)
     3030    0.001    0.000    0.107    0.000 wandb_run.py:2304(<lambda>)
     3030    0.003    0.000    0.105    0.000 wandb_run.py:390(wrapper_fn)
     3030    0.004    0.000    0.102    0.000 wandb_run.py:1429(_console_raw_callback)
     3030    0.011    0.000    0.096    0.000 interface.py:749(publish_output_raw)
     6363    0.006    0.000    0.089    0.000 __init__.py:1550(makeRecord)
     6363    0.038    0.000    0.083    0.000 __init__.py:282(__init__)
     4646    0.082    0.000    0.082    0.000 {method 'any' of 'torch._C.TensorBase' objects}
      202    0.008    0.000    0.081    0.000 module.py:382(log)
     6363    0.003    0.000    0.065    0.000 __init__.py:916(format)
     4646    0.064    0.000    0.064    0.000 {built-in method torch.isnan}
     3030    0.004    0.000    0.064    0.000 interface_shared.py:76(_publish_output_raw)
     6363    0.009    0.000    0.062    0.000 __init__.py:650(format)
      606    0.001    0.000    0.060    0.000 dispatch.py:126(_dispatch_fw)
     3030    0.003    0.000    0.059    0.000 interface_sock.py:45(_publish)
      606    0.002    0.000    0.054    0.000 dispatch.py:55(_run_priority_list)
     3030    0.003    0.000    0.053    0.000 sock_client.py:219(send_record_publish)
     3030    0.001    0.000    0.049    0.000 sock_client.py:153(send_server_request)
     3030    0.007    0.000    0.048    0.000 sock_client.py:145(_send_message)
      202    0.005    0.000    0.044    0.000 result.py:355(log)
      606    0.002    0.000    0.038    0.000 cutlass.py:211(apply)
      606    0.003    0.000    0.038    0.000 attn_bias.py:707(from_seqlens)
      606    0.004    0.000    0.036    0.000 cutlass.py:275(apply_bmhk)
     1212    0.015    0.000    0.035    0.000 common.py:348(not_supported_reasons)
     3030    0.004    0.000    0.035    0.000 sock_client.py:121(_sendall_with_error_handle)
     6363    0.010    0.000    0.034    0.000 __init__.py:582(formatTime)
      707    0.001    0.000    0.032    0.000 _ops.py:1047(__call__)
     3030    0.030    0.000    0.030    0.000 {method 'send' of '_socket.socket' objects}
      606    0.003    0.000    0.029    0.000 flash.py:629(not_supported_reasons)
  818/810    0.002    0.000    0.029    0.000 apply_func.py:23(apply_to_collection)
      606    0.028    0.000    0.028    0.000 {built-in method torch._ops.aten._efficient_attention_forward}
      606    0.002    0.000    0.025    0.000 attn_bias.py:350(from_seqlens)
      606    0.001    0.000    0.023    0.000 cutlass.py:326(not_supported_reasons)
      606    0.002    0.000    0.022    0.000 attn_bias.py:329(_get_seqstart)
44087/41611    0.008    0.000    0.022    0.000 {built-in method builtins.isinstance}
     1212    0.021    0.000    0.021    0.000 {method 'reshape' of 'torch._C.TensorBase' objects}
     1818    0.003    0.000    0.021    0.000 __init__.py:438(get_device_capability)
     6363    0.013    0.000    0.021    0.000 __init__.py:1514(findCaller)
      202    0.000    0.000    0.020    0.000 result.py:424(update_metrics)
  544/406    0.002    0.000    0.020    0.000 apply_func.py:84(_apply_to_collection_slow)
      202    0.001    0.000    0.019    0.000 result.py:264(forward)
      610    0.019    0.000    0.019    0.000 {built-in method torch.tensor}
      808    0.019    0.000    0.019    0.000 {method 'mean' of 'torch._C.TensorBase' objects}
     1818    0.003    0.000    0.019    0.000 __init__.py:455(get_device_properties)
      101    0.002    0.000    0.019    0.000 functional.py:3014(cross_entropy)
      202    0.003    0.000    0.018    0.000 metric.py:476(wrapped_func)
      909    0.018    0.000    0.018    0.000 {method 'sum' of 'torch._C.TensorBase' objects}
     1414    0.017    0.000    0.017    0.000 {method 'item' of 'torch._C.TensorBase' objects}
     3030    0.002    0.000    0.017    0.000 well_known_types.py:172(GetCurrentTime)
      909    0.016    0.000    0.016    0.000 {built-in method torch.zeros}
      101    0.016    0.000    0.016    0.000 {built-in method torch._C._nn.cross_entropy_loss}
     6363    0.006    0.000    0.015    0.000 posixpath.py:117(splitext)
      606    0.015    0.000    0.015    0.000 {built-in method torch.sqrt}
      303    0.014    0.000    0.014    0.000 {built-in method torch.all}
      303    0.001    0.000    0.014    0.000 activation.py:103(forward)
      202    0.011    0.000    0.013    0.000 result.py:207(update)
     3030    0.004    0.000    0.013    0.000 well_known_types.py:242(FromDatetime)
      303    0.000    0.000    0.013    0.000 functional.py:1489(relu)
     6363    0.012    0.000    0.012    0.000 {built-in method time.strftime}
      303    0.012    0.000    0.012    0.000 {built-in method torch.relu}
      606    0.007    0.000    0.012    0.000 common.py:120(validate_inputs)
     6363    0.006    0.000    0.012    0.000 posixpath.py:140(basename)
     6363    0.011    0.000    0.011    0.000 {built-in method time.localtime}
      303    0.011    0.000    0.011    0.000 {built-in method torch.stack}
     6363    0.003    0.000    0.011    0.000 __init__.py:634(formatMessage)
      606    0.010    0.000    0.010    0.000 {method 'var' of 'torch._C.TensorBase' objects}
     1818    0.005    0.000    0.010    0.000 _utils.py:9(_get_device_index)
      909    0.002    0.000    0.009    0.000 dropout.py:58(forward)
     1212    0.003    0.000    0.009    0.000 attn_bias.py:90(_get_default_bias_device)
     6363    0.006    0.000    0.008    0.000 genericpath.py:121(_splitext)
5788/1255    0.001    0.000    0.008    0.000 abc.py:121(__subclasscheck__)
     1238    0.002    0.000    0.008    0.000 typing.py:719(__instancecheck__)
      303    0.008    0.000    0.008    0.000 {built-in method torch.argmax}
5788/1255    0.007    0.000    0.008    0.000 {built-in method _abc._abc_subclasscheck}
     6363    0.002    0.000    0.008    0.000 __init__.py:432(format)
      606    0.002    0.000    0.008    0.000 cutlass.py:49(_minimum_gemm_alignment)
      909    0.004    0.000    0.008    0.000 functional.py:1279(dropout)
    12726    0.004    0.000    0.007    0.000 __init__.py:896(acquire)
        2    0.000    0.000    0.007    0.003 result.py:415(register_key)
     9797    0.007    0.000    0.007    0.000 module.py:1716(__getattr__)
      750    0.000    0.000    0.006    0.000 abc.py:117(__instancecheck__)
      202    0.001    0.000    0.006    0.000 module.py:654(__to_tensor)
      606    0.001    0.000    0.006    0.000 __init__.py:115(is_available)
      750    0.001    0.000    0.006    0.000 {built-in method _abc._abc_instancecheck}
     6363    0.002    0.000    0.006    0.000 __init__.py:628(usesTime)
        2    0.000    0.000    0.006    0.003 result.py:306(to)
     1238    0.002    0.000    0.006    0.000 typing.py:848(__subclasscheck__)
     9393    0.006    0.000    0.006    0.000 {built-in method posix.getpid}
     6363    0.006    0.000    0.006    0.000 __init__.py:429(_format)
      303    0.001    0.000    0.006    0.000 _tensor.py:982(__format__)
     1919    0.001    0.000    0.005    0.000 {built-in method builtins.any}
    12726    0.004    0.000    0.005    0.000 __init__.py:903(release)
      606    0.005    0.000    0.005    0.000 {method 'transpose' of 'torch._C.TensorBase' objects}
     3030    0.005    0.000    0.005    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
    19089    0.004    0.000    0.004    0.000 {method 'rfind' of 'str' objects}
     3030    0.004    0.000    0.004    0.000 enum_type_wrapper.py:92(__getattr__)
      606    0.004    0.000    0.004    0.000 dispatch.py:79(_dispatch_fw_priority_list)
     6363    0.002    0.000    0.004    0.000 __init__.py:160(<lambda>)
    22629    0.004    0.000    0.004    0.000 {built-in method builtins.hasattr}
     3030    0.004    0.000    0.004    0.000 calendar.py:655(timegm)
      101    0.001    0.000    0.004    0.000 profiler.py:693(__exit__)
     4848    0.004    0.000    0.004    0.000 {built-in method torch._C._get_tracing_state}
     1252    0.000    0.000    0.004    0.000 {built-in method builtins.issubclass}
     1818    0.003    0.000    0.004    0.000 _utils.py:764(_get_device_index)
     6363    0.002    0.000    0.004    0.000 __init__.py:421(usesTime)
      606    0.001    0.000    0.004    0.000 __init__.py:111(_nvml_based_avail)
     1818    0.004    0.000    0.004    0.000 {method 'unsqueeze' of 'torch._C.TensorBase' objects}
      202    0.000    0.000    0.004    0.000 trainer.py:1642(_results)
      202    0.001    0.000    0.004    0.000 result.py:340(_extract_batch_size)
      202    0.000    0.000    0.003    0.000 trainer.py:1564(_active_loop)
      101    0.000    0.000    0.003    0.000 _ops.py:887(__call__)
      202    0.003    0.000    0.003    0.000 {method 'clone' of 'torch._C.TensorBase' objects}
      303    0.000    0.000    0.003    0.000 {built-in method builtins.next}
      101    0.001    0.000    0.003    0.000 profiler.py:687(__enter__)
      606    0.001    0.000    0.003    0.000 os.py:771(getenv)
     6363    0.003    0.000    0.003    0.000 threading.py:1358(current_thread)
    12726    0.003    0.000    0.003    0.000 __init__.py:791(filter)
     6363    0.002    0.000    0.003    0.000 __init__.py:119(getLevelName)
    12727    0.003    0.000    0.003    0.000 {method 'acquire' of '_thread.RLock' objects}
     1818    0.003    0.000    0.003    0.000 {built-in method torch.cuda._get_device_properties}
      101    0.001    0.000    0.003    0.000 data.py:61(extract_batch_size)
      909    0.003    0.000    0.003    0.000 {method 'view' of 'torch._C.TensorBase' objects}
     1818    0.002    0.000    0.003    0.000 common.py:482(check_lastdim_alignment_stride1)
     6363    0.002    0.000    0.003    0.000 posixpath.py:41(_get_sep)
      808    0.001    0.000    0.003    0.000 enums.py:81(__eq__)
      202    0.001    0.000    0.003    0.000 precision.py:173(val_step_context)
     6363    0.003    0.000    0.003    0.000 __init__.py:1689(isEnabledFor)
     6363    0.002    0.000    0.003    0.000 posixpath.py:52(normcase)
     8572    0.003    0.000    0.003    0.000 {built-in method builtins.getattr}
      101    0.003    0.000    0.003    0.000 {built-in method torch._ops.profiler._record_function_enter_new}
     3030    0.003    0.000    0.003    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
     6363    0.002    0.000    0.002    0.000 __init__.py:358(getMessage)
      101    0.000    0.000    0.002    0.000 _ops.py:943(_must_dispatch_in_python)
      101    0.000    0.000    0.002    0.000 contextlib.py:114(__enter__)
     1214    0.001    0.000    0.002    0.000 {built-in method builtins.all}
     1070    0.002    0.000    0.002    0.000 result.py:294(__setattr__)
 1313/505    0.001    0.000    0.002    0.000 data.py:42(_extract_batch_size)
      404    0.002    0.000    0.002    0.000 result.py:90(_generate_sync_fn)
      606    0.000    0.000    0.002    0.000 _collections_abc.py:760(get)
      202    0.001    0.000    0.002    0.000 <string>:2(__init__)
     6060    0.002    0.000    0.002    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
      404    0.000    0.000    0.002    0.000 trainer.py:1381(training)
      500    0.002    0.000    0.002    0.000 apply_func.py:17(is_dataclass_instance)
      101    0.000    0.000    0.002    0.000 _pytree.py:1181(tree_any)
      101    0.000    0.000    0.002    0.000 functional.py:1858(softmax)
      202    0.000    0.000    0.002    0.000 result.py:57(__post_init__)
     1212    0.001    0.000    0.002    0.000 common.py:32(is_available)
      909    0.002    0.000    0.002    0.000 {built-in method torch.dropout}
      202    0.001    0.000    0.002    0.000 fx_validator.py:191(check_logging_and_get_default_levels)
      101    0.002    0.000    0.002    0.000 {method 'float' of 'torch._C.TensorBase' objects}
     3030    0.002    0.000    0.002    0.000 interface_sock.py:41(_assign)
      606    0.001    0.000    0.002    0.000 os.py:674(__getitem__)
      101    0.002    0.000    0.002    0.000 {method 'softmax' of 'torch._C.TensorBase' objects}
      202    0.001    0.000    0.002    0.000 memory.py:24(recursive_detach)
      606    0.002    0.000    0.002    0.000 cutlass.py:136(_custom_mask_type)
    19089    0.002    0.000    0.002    0.000 {built-in method posix.fspath}
    13033    0.002    0.000    0.002    0.000 {method 'get' of 'dict' objects}
  707/202    0.001    0.000    0.002    0.000 _pytree.py:874(tree_iter)
     6363    0.002    0.000    0.002    0.000 {built-in method sys._getframe}
     2424    0.001    0.000    0.002    0.000 __init__.py:834(device_count)
     4246    0.002    0.000    0.002    0.000 {method 'stride' of 'torch._C.TensorBase' objects}
    12726    0.002    0.000    0.002    0.000 {built-in method _thread.get_ident}
      606    0.001    0.000    0.002    0.000 attn_bias.py:656(to)
     6363    0.002    0.000    0.002    0.000 threading.py:1093(name)
     6363    0.002    0.000    0.002    0.000 {method 'find' of 'str' objects}
     9393    0.002    0.000    0.002    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      210    0.001    0.000    0.001    0.000 grad_mode.py:184(__init__)
     1212    0.001    0.000    0.001    0.000 common.py:331(shape_not_supported_reasons)
     3030    0.001    0.000    0.001    0.000 __init__.py:106(_is_compiled)
     1818    0.001    0.000    0.001    0.000 __init__.py:284(_lazy_init)
     6363    0.001    0.000    0.001    0.000 {built-in method time.time}
      404    0.001    0.000    0.001    0.000 {method 'detach' of 'torch._C.TensorBase' objects}
     3030    0.001    0.000    0.001    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
     3030    0.001    0.000    0.001    0.000 {built-in method utcnow}
    12727    0.001    0.000    0.001    0.000 {method 'release' of '_thread.RLock' objects}
      101    0.001    0.000    0.001    0.000 precision.py:68(forward_context)
      101    0.000    0.000    0.001    0.000 module.py:262(current_epoch)
      909    0.001    0.000    0.001    0.000 _VF.py:26(__getattr__)
      202    0.000    0.000    0.001    0.000 contextlib.py:123(__exit__)
     3030    0.001    0.000    0.001    0.000 {built-in method _struct.pack}
      606    0.001    0.000    0.001    0.000 common.py:96(normalize_bmhk)
     6363    0.001    0.000    0.001    0.000 process.py:189(name)
     1616    0.001    0.000    0.001    0.000 types.py:171(__get__)
        2    0.000    0.000    0.001    0.000 result.py:187(__init__)
      101    0.001    0.000    0.001    0.000 trainer.py:1467(current_epoch)
     1212    0.000    0.000    0.001    0.000 cutlass.py:87(_get_tensor_bias)
      101    0.001    0.000    0.001    0.000 {built-in method torch._ops.profiler.}
      202    0.000    0.000    0.001    0.000 result.py:122(__post_init__)
     1818    0.000    0.000    0.001    0.000 __init__.py:237(is_initialized)
     2424    0.001    0.000    0.001    0.000 common.py:192(<genexpr>)
      606    0.001    0.000    0.001    0.000 cutlass.py:66(_get_seqlen_info)
     7381    0.001    0.000    0.001    0.000 {built-in method builtins.len}
      202    0.001    0.000    0.001    0.000 {method 'squeeze' of 'torch._C.TensorBase' objects}
      606    0.001    0.000    0.001    0.000 flash.py:511(_check_needs_no_topleft)
     6363    0.001    0.000    0.001    0.000 process.py:37(current_process)
     2424    0.001    0.000    0.001    0.000 common.py:131(<genexpr>)
      202    0.000    0.000    0.001    0.000 memory.py:40(detach_and_move)
      101    0.000    0.000    0.001    0.000 contextlib.py:261(helper)
      202    0.000    0.000    0.001    0.000 result.py:148(sync)
     2424    0.001    0.000    0.001    0.000 {built-in method builtins.max}
      606    0.000    0.000    0.001    0.000 cutlass.py:98(_check_bias_alignment)
    116/8    0.000    0.000    0.001    0.000 copy.py:128(deepcopy)
      202    0.001    0.000    0.001    0.000 result.py:127(_parse_reduce_fx)
      200    0.001    0.000    0.001    0.000 <string>:2(__eq__)
     2424    0.001    0.000    0.001    0.000 __init__.py:461(<genexpr>)
      202    0.000    0.000    0.001    0.000 trainer.py:1554(_evaluation_loop)
     2424    0.001    0.000    0.001    0.000 common.py:71(device)
      707    0.000    0.000    0.001    0.000 _pytree.py:649(_get_node_type)
      202    0.001    0.000    0.001    0.000 fx_validator.py:151(check_logging)
      404    0.000    0.000    0.001    0.000 _pytree.py:656(_is_leaf)
      500    0.000    0.000    0.001    0.000 dataclasses.py:1047(is_dataclass)
     2424    0.001    0.000    0.001    0.000 common.py:122(<genexpr>)
      101    0.001    0.000    0.001    0.000 contextlib.py:688(__init__)
      202    0.000    0.000    0.001    0.000 trainer.py:1429(sanity_checking)
      606    0.000    0.000    0.001    0.000 os.py:754(encode)
      101    0.000    0.000    0.001    0.000 contextlib.py:86(__init__)
     2424    0.001    0.000    0.001    0.000 common.py:152(<genexpr>)
     3030    0.001    0.000    0.001    0.000 {built-in method time.monotonic}
        4    0.000    0.000    0.001    0.000 metric.py:196(add_state)
     3030    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      606    0.000    0.000    0.000    0.000 os.py:758(decode)
     1620    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}
        4    0.000    0.000    0.000    0.000 _tensor.py:83(__deepcopy__)
      202    0.000    0.000    0.000    0.000 fx_validator.py:177(check_logging_levels)
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      707    0.000    0.000    0.000    0.000 _pytree.py:638(_is_namedtuple_instance)
     2855    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
      101    0.000    0.000    0.000    0.000 _reduction.py:7(get_enum)
     1212    0.000    0.000    0.000    0.000 attn_bias.py:316(to)
      202    0.000    0.000    0.000    0.000 fx_validator.py:166(get_default_logging_levels)
      101    0.000    0.000    0.000    0.000 profiler.py:676(__init__)
      202    0.000    0.000    0.000    0.000 logger_connector.py:213(should_reset_tensors)
     1818    0.000    0.000    0.000    0.000 flash.py:533(_check_strides_for_bmghk)
      202    0.000    0.000    0.000    0.000 result.py:74(op)
      614    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}
     1818    0.000    0.000    0.000    0.000 {built-in method torch._C._cuda_isInBadFork}
     3030    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        2    0.000    0.000    0.000    0.000 metric.py:101(__init__)
     3030    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
      204    0.000    0.000    0.000    0.000 result.py:536(_get_default_dtype)
     1923    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
      202    0.000    0.000    0.000    0.000 grad_mode.py:196(__exit__)
      200    0.000    0.000    0.000    0.000 trainer.py:1425(evaluating)
     1616    0.000    0.000    0.000    0.000 enum.py:792(value)
      101    0.000    0.000    0.000    0.000 container.py:317(__iter__)
     1212    0.000    0.000    0.000    0.000 {built-in method builtins.min}
        4    0.000    0.000    0.000    0.000 apply_func.py:69(<genexpr>)
      408    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C.TensorBase' objects}
      500    0.000    0.000    0.000    0.000 apply_func.py:11(is_namedtuple)
     1818    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      6/4    0.000    0.000    0.000    0.000 copy.py:258(_reconstruct)
      101    0.000    0.000    0.000    0.000 _pytree.py:423(_dict_flatten)
      202    0.000    0.000    0.000    0.000 {built-in method torch.numel}
      202    0.000    0.000    0.000    0.000 {built-in method torch.is_floating_point}
      101    0.000    0.000    0.000    0.000 _ops.py:945(<lambda>)
        8    0.000    0.000    0.000    0.000 apply_func.py:71(move_data_to_device)
      606    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      202    0.000    0.000    0.000    0.000 grad_mode.py:193(__enter__)
        4    0.000    0.000    0.000    0.000 storage.py:907(_deepcopy)
      606    0.000    0.000    0.000    0.000 {method 'decode' of 'bytes' objects}
     10/8    0.000    0.000    0.000    0.000 copy.py:226(_deepcopy_dict)
      303    0.000    0.000    0.000    0.000 {method '__format__' of 'int' objects}
        8    0.000    0.000    0.000    0.000 apply_func.py:91(batch_to)
      101    0.000    0.000    0.000    0.000 module.py:215(trainer)
        2    0.000    0.000    0.000    0.000 inspect.py:3111(signature)
        8    0.000    0.000    0.000    0.000 {method 'to' of 'torch._C.TensorBase' objects}
      303    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        2    0.000    0.000    0.000    0.000 inspect.py:2859(from_callable)
      4/2    0.000    0.000    0.000    0.000 inspect.py:2246(_signature_from_callable)
      305    0.000    0.000    0.000    0.000 result.py:163(is_mean_reduction)
        4    0.000    0.000    0.000    0.000 storage.py:140(__deepcopy__)
        4    0.000    0.000    0.000    0.000 storage.py:156(clone)
      606    0.000    0.000    0.000    0.000 cutlass.py:41(_uses_tensorcores)
      200    0.000    0.000    0.000    0.000 states.py:63(evaluating)
      202    0.000    0.000    0.000    0.000 strategy.py:351(model)
      103    0.000    0.000    0.000    0.000 typing.py:271(inner)
      206    0.000    0.000    0.000    0.000 {built-in method torch.get_default_dtype}
      303    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C.TensorBase' objects}
      214    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}
      606    0.000    0.000    0.000    0.000 result.py:70(op)
      202    0.000    0.000    0.000    0.000 _pytree.py:395(_tuple_flatten)
      202    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      404    0.000    0.000    0.000    0.000 result.py:60(should)
      606    0.000    0.000    0.000    0.000 dispatch.py:27(_get_use_fa3)
      404    0.000    0.000    0.000    0.000 result.py:80(group)
        4    0.000    0.000    0.000    0.000 {method 'copy_' of 'torch._C.StorageBase' objects}
        2    0.000    0.000    0.000    0.000 inspect.py:2152(_signature_from_function)
      101    0.000    0.000    0.000    0.000 strategy.py:93(precision_plugin)
      559    0.000    0.000    0.000    0.000 _collections_abc.py:409(__subclasshook__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.iter}
        2    0.000    0.000    0.000    0.000 module.py:429(__init__)
      209    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
      4/2    0.000    0.000    0.000    0.000 copy.py:209(_deepcopy_tuple)
      202    0.000    0.000    0.000    0.000 typing.py:1375(cast)
      101    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_variadic}
        4    0.000    0.000    0.000    0.000 dataclasses.py:1024(fields)
      4/2    0.000    0.000    0.000    0.000 copy.py:210(<listcomp>)
        2    0.000    0.000    0.000    0.000 metric.py:475(_wrap_update)
        6    0.000    0.000    0.000    0.000 {method '__reduce_ex__' of 'object' objects}
        4    0.000    0.000    0.000    0.000 _tensor.py:242(_typed_storage)
        4    0.000    0.000    0.000    0.000 {method 'new_empty' of 'torch._C.TensorBase' objects}
       58    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}
        4    0.000    0.000    0.000    0.000 grad_mode.py:80(__enter__)
      101    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}
        4    0.000    0.000    0.000    0.000 functools.py:35(update_wrapper)
        2    0.000    0.000    0.000    0.000 result.py:273(_wrap_compute)
      101    0.000    0.000    0.000    0.000 contextlib.py:691(__enter__)
      101    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 contextlib.py:694(__exit__)
       26    0.000    0.000    0.000    0.000 copy.py:242(_keep_alive)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      101    0.000    0.000    0.000    0.000 __init__.py:129(annotate)
        2    0.000    0.000    0.000    0.000 inspect.py:1840(_signature_bound_method)
      101    0.000    0.000    0.000    0.000 {method '__exit__' of 'torch._C.DisableTorchFunctionSubclass' objects}
        4    0.000    0.000    0.000    0.000 inspect.py:2781(__init__)
      214    0.000    0.000    0.000    0.000 {built-in method builtins.id}
      186    0.000    0.000    0.000    0.000 _collections_abc.py:315(__subclasshook__)
        6    0.000    0.000    0.000    0.000 inspect.py:2498(__init__)
        4    0.000    0.000    0.000    0.000 storage.py:712(_new_wrapped_storage)
       38    0.000    0.000    0.000    0.000 dataclasses.py:1039(<genexpr>)
        4    0.000    0.000    0.000    0.000 {method 'set_' of 'torch._C.TensorBase' objects}
       12    0.000    0.000    0.000    0.000 copy.py:263(<genexpr>)
        2    0.000    0.000    0.000    0.000 copyreg.py:103(_slotnames)
        8    0.000    0.000    0.000    0.000 storage.py:629(__init__)
        4    0.000    0.000    0.000    0.000 _contextlib.py:154(__new__)
        4    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}
        2    0.000    0.000    0.000    0.000 inspect.py:494(unwrap)
        2    0.000    0.000    0.000    0.000 inspect.py:2873(replace)
        8    0.000    0.000    0.000    0.000 storage.py:557(__new__)
        4    0.000    0.000    0.000    0.000 grad_mode.py:84(__exit__)
        4    0.000    0.000    0.000    0.000 grad_mode.py:75(__init__)
       10    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 {method 'untyped_storage' of 'torch._C.TensorBase' objects}
        4    0.000    0.000    0.000    0.000 {method 'contiguous' of 'torch._C.TensorBase' objects}
        6    0.000    0.000    0.000    0.000 enum.py:358(__call__)
        1    0.000    0.000    0.000    0.000 __init__.py:1276(disable)
        1    0.000    0.000    0.000    0.000 __init__.py:218(_acquireLock)
        8    0.000    0.000    0.000    0.000 inspect.py:2830(<genexpr>)
       48    0.000    0.000    0.000    0.000 copy.py:182(_deepcopy_atomic)
        2    0.000    0.000    0.000    0.000 result.py:171(is_max_reduction)
        4    0.000    0.000    0.000    0.000 {built-in method torch._C._has_storage}
        4    0.000    0.000    0.000    0.000 copyreg.py:94(__newobj__)
        4    0.000    0.000    0.000    0.000 {method 'nbytes' of 'torch._C.StorageBase' objects}
       16    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f7dfbd377c0}
       28    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
       22    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 functools.py:65(wraps)
        2    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}
        1    0.000    0.000    0.000    0.000 result.py:502(reset)
        2    0.000    0.000    0.000    0.000 inspect.py:514(_is_wrapper)
        1    0.000    0.000    0.000    0.000 __init__.py:227(_releaseLock)
        4    0.000    0.000    0.000    0.000 inspect.py:159(isfunction)
        4    0.000    0.000    0.000    0.000 {method 'is_conj' of 'torch._C.TensorBase' objects}
        6    0.000    0.000    0.000    0.000 enum.py:670(__new__)
        6    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}
        8    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
       10    0.000    0.000    0.000    0.000 inspect.py:2548(name)
        4    0.000    0.000    0.000    0.000 {method 'is_neg' of 'torch._C.TensorBase' objects}
        8    0.000    0.000    0.000    0.000 inspect.py:2560(kind)
        2    0.000    0.000    0.000    0.000 result.py:175(is_min_reduction)
        4    0.000    0.000    0.000    0.000 {method 'storage_offset' of 'torch._C.TensorBase' objects}
       14    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 __init__.py:1675(getEffectiveLevel)
        2    0.000    0.000    0.000    0.000 {method '__setstate__' of 'functools.partial' objects}
        4    0.000    0.000    0.000    0.000 storage.py:30(__init__)
        2    0.000    0.000    0.000    0.000 {method 'get' of 'mappingproxy' objects}
        4    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 inspect.py:2552(default)
        2    0.000    0.000    0.000    0.000 {built-in method sys.getrecursionlimit}
        2    0.000    0.000    0.000    0.000 inspect.py:2865(parameters)



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_validation_batch_end
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 callback.py:152(on_validation_batch_end)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_validation_batch_end
         41946 function calls in 0.033 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.001    0.000    0.032    0.000 tqdm_progress.py:309(on_validation_batch_end)
      101    0.000    0.000    0.031    0.000 tqdm_progress.py:451(_update_n)
      101    0.000    0.000    0.030    0.000 std.py:1325(refresh)
      101    0.000    0.000    0.030    0.000 std.py:1464(display)
      602    0.001    0.000    0.016    0.000 utils.py:194(inner)
      301    0.000    0.000    0.013    0.000 redirect.py:644(write)
      301    0.000    0.000    0.012    0.000 wandb_run.py:2304(<lambda>)
      301    0.000    0.000    0.012    0.000 wandb_run.py:390(wrapper_fn)
      200    0.001    0.000    0.012    0.000 std.py:1441(moveto)
      301    0.000    0.000    0.012    0.000 wandb_run.py:1429(_console_raw_callback)
      301    0.001    0.000    0.011    0.000 interface.py:749(publish_output_raw)
      101    0.000    0.000    0.009    0.000 std.py:457(print_status)
      101    0.000    0.000    0.009    0.000 std.py:1150(__str__)
      301    0.000    0.000    0.007    0.000 interface_shared.py:76(_publish_output_raw)
      301    0.000    0.000    0.007    0.000 interface_sock.py:45(_publish)
      301    0.000    0.000    0.006    0.000 sock_client.py:219(send_record_publish)
      301    0.000    0.000    0.006    0.000 sock_client.py:153(send_server_request)
      301    0.001    0.000    0.006    0.000 sock_client.py:145(_send_message)
      101    0.002    0.000    0.005    0.000 std.py:464(format_meter)
      101    0.000    0.000    0.005    0.000 std.py:451(fp_write)
      301    0.000    0.000    0.004    0.000 sock_client.py:121(_sendall_with_error_handle)
      301    0.004    0.000    0.004    0.000 {method 'send' of '_socket.socket' objects}
      101    0.000    0.000    0.003    0.000 utils.py:378(disp_len)
      301    0.003    0.000    0.003    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      101    0.000    0.000    0.003    0.000 utils.py:374(_text_width)
      101    0.001    0.000    0.003    0.000 std.py:1446(format_dict)
      101    0.001    0.000    0.003    0.000 {built-in method builtins.sum}
      101    0.001    0.000    0.002    0.000 utils.py:333(_screen_shape_linux)
     7176    0.002    0.000    0.002    0.000 utils.py:375(<genexpr>)
      301    0.000    0.000    0.002    0.000 well_known_types.py:172(GetCurrentTime)
      301    0.000    0.000    0.002    0.000 well_known_types.py:242(FromDatetime)
      202    0.001    0.000    0.001    0.000 utils.py:273(_is_ascii)
      101    0.001    0.000    0.001    0.000 {built-in method fcntl.ioctl}
      202    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}
     7075    0.001    0.000    0.001    0.000 {built-in method unicodedata.east_asian_width}
      301    0.001    0.000    0.001    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
      301    0.001    0.000    0.001    0.000 enum_type_wrapper.py:92(__getattr__)
      101    0.000    0.000    0.001    0.000 contextlib.py:123(__exit__)
      301    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
     8987    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      202    0.000    0.000    0.000    0.000 std.py:400(format_interval)
      101    0.000    0.000    0.000    0.000 utils.py:347(<listcomp>)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 tqdm_progress.py:425(_should_update)
      101    0.000    0.000    0.000    0.000 std.py:102(acquire)
      301    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
      101    0.000    0.000    0.000    0.000 os.py:674(__getitem__)
      602    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 std.py:186(__format__)
      301    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
      101    0.000    0.000    0.000    0.000 tqdm_progress.py:169(is_enabled)
      903    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      101    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
      301    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
      101    0.000    0.000    0.000    0.000 std.py:153(__init__)
      101    0.000    0.000    0.000    0.000 std.py:106(release)
      301    0.000    0.000    0.000    0.000 {built-in method utcnow}
      301    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
      301    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
      101    0.000    0.000    0.000    0.000 {built-in method now}
      301    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      101    0.000    0.000    0.000    0.000 os.py:754(encode)
      101    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
      202    0.000    0.000    0.000    0.000 tqdm_progress.py:161(refresh_rate)
      703    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      200    0.000    0.000    0.000    0.000 utils.py:370(_term_move_up)
      101    0.000    0.000    0.000    0.000 std.py:231(__call__)
      202    0.000    0.000    0.000    0.000 tqdm_progress.py:131(val_progress_bar)
      101    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
      202    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 utils.py:108(__init__)
      505    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
      301    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      301    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      202    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      101    0.000    0.000    0.000    0.000 {built-in method builtins.max}
      101    0.000    0.000    0.000    0.000 utils.py:112(__format__)
      101    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      101    0.000    0.000    0.000    0.000 std.py:167(colour)
      101    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      301    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      303    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
      101    0.000    0.000    0.000    0.000 {built-in method time.time}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      101    0.000    0.000    0.000    0.000 std.py:163(colour)
      101    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}



Profile stats for: [Callback]ModelSummary.on_validation_batch_end
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 callback.py:152(on_validation_batch_end)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_end
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:152(on_validation_batch_end)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_batch_end
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 hooks.py:103(on_validation_batch_end)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_validation_epoch_end
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 callback.py:127(on_validation_epoch_end)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)



Profile stats for: [Callback]TQDMProgressBar.on_validation_epoch_end
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:127(on_validation_epoch_end)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_validation_epoch_end
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 callback.py:127(on_validation_epoch_end)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_end
         1010 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 callback.py:127(on_validation_epoch_end)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_epoch_end
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 hooks.py:244(on_validation_epoch_end)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_validation_end
         1010 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 early_stopping.py:192(on_validation_end)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_validation_end
         75542 function calls (75528 primitive calls) in 0.057 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.001    0.000    0.057    0.001 tqdm_progress.py:323(on_validation_end)
      201    0.000    0.000    0.031    0.000 std.py:1464(display)
      101    0.001    0.000    0.024    0.000 std.py:1265(close)
      904    0.001    0.000    0.022    0.000 utils.py:194(inner)
      101    0.001    0.000    0.021    0.000 std.py:1402(set_postfix)
      503    0.000    0.000    0.019    0.000 redirect.py:644(write)
      503    0.000    0.000    0.018    0.000 wandb_run.py:2304(<lambda>)
      101    0.000    0.000    0.018    0.000 std.py:1325(refresh)
      503    0.000    0.000    0.018    0.000 wandb_run.py:390(wrapper_fn)
      503    0.001    0.000    0.017    0.000 wandb_run.py:1429(_console_raw_callback)
      503    0.002    0.000    0.016    0.000 interface.py:749(publish_output_raw)
      201    0.000    0.000    0.014    0.000 std.py:457(print_status)
      503    0.001    0.000    0.011    0.000 interface_shared.py:76(_publish_output_raw)
      101    0.001    0.000    0.010    0.000 progress_bar.py:177(get_metrics)
      503    0.000    0.000    0.010    0.000 interface_sock.py:45(_publish)
      201    0.000    0.000    0.010    0.000 std.py:451(fp_write)
      503    0.001    0.000    0.009    0.000 sock_client.py:219(send_record_publish)
      200    0.001    0.000    0.009    0.000 std.py:1441(moveto)
      503    0.000    0.000    0.008    0.000 sock_client.py:153(send_server_request)
      503    0.001    0.000    0.008    0.000 sock_client.py:145(_send_message)
      100    0.000    0.000    0.008    0.000 std.py:1150(__str__)
      101    0.000    0.000    0.006    0.000 trainer.py:1632(progress_bar_metrics)
      503    0.001    0.000    0.006    0.000 sock_client.py:121(_sendall_with_error_handle)
      101    0.000    0.000    0.006    0.000 logger_connector.py:250(progress_bar_metrics)
      503    0.005    0.000    0.005    0.000 {method 'send' of '_socket.socket' objects}
      100    0.002    0.000    0.005    0.000 std.py:464(format_meter)
      102    0.000    0.000    0.005    0.000 std.py:1286(fp_write)
      101    0.001    0.000    0.004    0.000 std.py:686(_decr_instances)
      201    0.000    0.000    0.004    0.000 utils.py:378(disp_len)
      101    0.000    0.000    0.004    0.000 logger_connector.py:229(metrics)
      201    0.000    0.000    0.004    0.000 utils.py:374(_text_width)
      303    0.000    0.000    0.003    0.000 trainer.py:1642(_results)
      201    0.001    0.000    0.003    0.000 {built-in method builtins.sum}
      101    0.002    0.000    0.003    0.000 progress_bar.py:210(get_standard_metrics)
      303    0.000    0.000    0.003    0.000 trainer.py:1564(_active_loop)
      503    0.000    0.000    0.003    0.000 well_known_types.py:172(GetCurrentTime)
      401    0.003    0.000    0.003    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
     9663    0.002    0.000    0.003    0.000 utils.py:375(<genexpr>)
      503    0.001    0.000    0.002    0.000 well_known_types.py:242(FromDatetime)
     1010    0.001    0.000    0.002    0.000 enums.py:81(__eq__)
      100    0.000    0.000    0.002    0.000 std.py:1446(format_dict)
      101    0.001    0.000    0.002    0.000 result.py:476(metrics)
      100    0.000    0.000    0.002    0.000 utils.py:333(_screen_shape_linux)
      201    0.001    0.000    0.002    0.000 _weakrefset.py:63(__iter__)
      200    0.001    0.000    0.001    0.000 utils.py:273(_is_ascii)
      101    0.000    0.000    0.001    0.000 utilities.py:25(_version)
      101    0.000    0.000    0.001    0.000 wandb.py:572(version)
       99    0.000    0.000    0.001    0.000 tqdm_progress.py:46(format_num)
      101    0.000    0.000    0.001    0.000 _weakrefset.py:111(remove)
      303    0.000    0.000    0.001    0.000 trainer.py:1381(training)
     2413    0.000    0.000    0.001    0.000 {built-in method builtins.isinstance}
      303    0.000    0.000    0.001    0.000 trainer.py:1554(_evaluation_loop)
      101    0.000    0.000    0.001    0.000 wandb_run.py:357(wrapper)
      202    0.000    0.000    0.001    0.000 result.py:430(_get_cache)
      503    0.001    0.000    0.001    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
      100    0.001    0.000    0.001    0.000 {built-in method fcntl.ioctl}
      200    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}
     9462    0.001    0.000    0.001    0.000 {built-in method unicodedata.east_asian_width}
      303    0.000    0.000    0.001    0.000 trainer.py:1429(sanity_checking)
     1919    0.001    0.000    0.001    0.000 types.py:171(__get__)
      503    0.001    0.000    0.001    0.000 calendar.py:655(timegm)
      503    0.001    0.000    0.001    0.000 enum_type_wrapper.py:92(__getattr__)
      202    0.000    0.000    0.001    0.000 {method 'remove' of 'set' objects}
      202    0.001    0.000    0.001    0.000 {method 'detach' of 'torch._C.TensorBase' objects}
       99    0.001    0.000    0.001    0.000 std.py:419(format_num)
      302    0.000    0.000    0.001    0.000 std.py:102(acquire)
      202    0.000    0.000    0.001    0.000 std.py:110(__enter__)
      200    0.000    0.000    0.001    0.000 abc.py:117(__instancecheck__)
     9103    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
      503    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
      200    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}
      302    0.000    0.000    0.000    0.000 std.py:106(release)
      503    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
      101    0.000    0.000    0.000    0.000 _weakrefset.py:27(__exit__)
     1006    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
      300    0.000    0.000    0.000    0.000 trainer.py:1425(evaluating)
      202    0.000    0.000    0.000    0.000 std.py:113(__exit__)
      200    0.000    0.000    0.000    0.000 std.py:400(format_interval)
      101    0.000    0.000    0.000    0.000 wandb_run.py:881(id)
      100    0.000    0.000    0.000    0.000 utils.py:347(<listcomp>)
      101    0.000    0.000    0.000    0.000 utils.py:125(__eq__)
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
      604    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
     1811    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      100    0.000    0.000    0.000    0.000 os.py:674(__getitem__)
      503    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
      202    0.000    0.000    0.000    0.000 result.py:465(_forked_name)
      101    0.000    0.000    0.000    0.000 _weakrefset.py:53(_commit_removals)
      100    0.000    0.000    0.000    0.000 std.py:186(__format__)
      503    0.000    0.000    0.000    0.000 {built-in method utcnow}
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      202    0.000    0.000    0.000    0.000 std.py:1153(_comparable)
     1919    0.000    0.000    0.000    0.000 enum.py:792(value)
      101    0.000    0.000    0.000    0.000 _weakrefset.py:17(__init__)
      303    0.000    0.000    0.000    0.000 result.py:463(<genexpr>)
     2020    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      503    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
      301    0.000    0.000    0.000    0.000 std.py:1428(<genexpr>)
      100    0.000    0.000    0.000    0.000 std.py:708(<lambda>)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
     1504    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      101    0.000    0.000    0.000    0.000 _weakrefset.py:21(__enter__)
      201    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
      302    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 result.py:461(valid_items)
      100    0.000    0.000    0.000    0.000 std.py:153(__init__)
      503    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      503    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      401    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 {built-in method now}
      100    0.000    0.000    0.000    0.000 os.py:754(encode)
      300    0.000    0.000    0.000    0.000 states.py:63(evaluating)
      101    0.000    0.000    0.000    0.000 std.py:1157(__hash__)
      202    0.000    0.000    0.000    0.000 result.py:158(forked_name)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      503    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      302    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      201    0.000    0.000    0.000    0.000 {built-in method builtins.max}
      301    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      200    0.000    0.000    0.000    0.000 utils.py:370(_term_move_up)
      198    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}
      403    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      100    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 tqdm_progress.py:131(val_progress_bar)
      500    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
      503    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
      100    0.000    0.000    0.000    0.000 std.py:231(__call__)
      302    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
      101    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}
      302    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      101    0.000    0.000    0.000    0.000 progress_bar.py:150(reset_dataloader_idx_tracker)
      101    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
      101    0.000    0.000    0.000    0.000 tqdm_progress.py:121(train_progress_bar)
      100    0.000    0.000    0.000    0.000 utils.py:108(__init__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.id}
      100    0.000    0.000    0.000    0.000 utils.py:112(__format__)
      202    0.000    0.000    0.000    0.000 trainer.py:1590(loggers)
      200    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}
      202    0.000    0.000    0.000    0.000 result.py:154(forked)
      303    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 std.py:167(colour)
      100    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      202    0.000    0.000    0.000    0.000 {method 'keys' of 'collections.OrderedDict' objects}
      100    0.000    0.000    0.000    0.000 {built-in method time.time}
      8/1    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)
      8/1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      101    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 std.py:163(colour)



Profile stats for: [Callback]ModelSummary.on_validation_end
         1010 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 callback.py:214(on_validation_end)
      101    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      101    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_end
         4820 function calls in 0.006 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.005    0.000 model_checkpoint.py:326(on_validation_end)
      101    0.002    0.000    0.004    0.000 model_checkpoint.py:413(_should_skip_saving_checkpoint)
      100    0.001    0.000    0.001    0.000 trainer.py:1458(global_step)
      100    0.001    0.000    0.001    0.000 model_checkpoint.py:423(_should_save_on_train_epoch_end)
      100    0.001    0.000    0.001    0.000 training_epoch_loop.py:99(global_step)
      301    0.000    0.000    0.001    0.000 enums.py:81(__eq__)
      200    0.000    0.000    0.001    0.000 trainer.py:1535(num_val_batches)
      101    0.000    0.000    0.000    0.000 trainer.py:1429(sanity_checking)
      602    0.000    0.000    0.000    0.000 types.py:171(__get__)
      100    0.000    0.000    0.000    0.000 progress.py:274(optimizer_steps)
      201    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      401    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      602    0.000    0.000    0.000    0.000 enum.py:792(value)
      602    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      201    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.sum}
      100    0.000    0.000    0.000    0.000 module.py:295(automatic_optimization)
      201    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_end
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 hooks.py:53(on_validation_end)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Strategy]SingleDeviceStrategy.on_validation_end
         707 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      101    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      101    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      101    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      101    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      101    0.000    0.000    0.000    0.000 strategy.py:565(on_validation_end)
      101    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_sanity_check_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:73(on_sanity_check_end)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_sanity_check_end
         14 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:254(on_sanity_check_end)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        2    0.000    0.000    0.000    0.000 std.py:1265(close)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:131(val_progress_bar)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:121(train_progress_bar)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_sanity_check_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:73(on_sanity_check_end)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:73(on_sanity_check_end)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningDataModule]PMTfiedDataModule.train_dataloader
         478 function calls (473 primitive calls) in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 PMTfiedDataModule.py:67(train_dataloader)
      2/1    0.000    0.000    0.000    0.000 data.py:287(wrapper)
        1    0.000    0.000    0.000    0.000 dataloader.py:227(__init__)
        2    0.000    0.000    0.000    0.000 inspect.py:3111(signature)
        2    0.000    0.000    0.000    0.000 inspect.py:2859(from_callable)
        2    0.000    0.000    0.000    0.000 inspect.py:2246(_signature_from_callable)
        2    0.000    0.000    0.000    0.000 inspect.py:2152(_signature_from_function)
    23/22    0.000    0.000    0.000    0.000 data.py:334(wrapper)
       21    0.000    0.000    0.000    0.000 inspect.py:2498(__init__)
       21    0.000    0.000    0.000    0.000 data.py:295(<genexpr>)
       37    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
    20/19    0.000    0.000    0.000    0.000 dataloader.py:418(__setattr__)
        2    0.000    0.000    0.000    0.000 inspect.py:2781(__init__)
        3    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)
        3    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}
       21    0.000    0.000    0.000    0.000 enum.py:358(__call__)
       27    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 dataloader.py:487(check_worker_number_rationality)
        1    0.000    0.000    0.000    0.000 sampler.py:262(__init__)
       23    0.000    0.000    0.000    0.000 inspect.py:2830(<genexpr>)
        1    0.000    0.000    0.000    0.000 sampler.py:133(__init__)
        2    0.000    0.000    0.000    0.000 inspect.py:494(unwrap)
        1    0.000    0.000    0.000    0.000 {built-in method posix.sched_getaffinity}
       61    0.000    0.000    0.000    0.000 inspect.py:2548(name)
       50    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        2    0.000    0.000    0.000    0.000 data.py:303(<dictcomp>)
        5    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        2    0.000    0.000    0.000    0.000 sampler.py:146(num_samples)
        1    0.000    0.000    0.000    0.000 {built-in method torch.set_vital}
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      8/6    0.000    0.000    0.000    0.000 {built-in method builtins.len}
       21    0.000    0.000    0.000    0.000 enum.py:670(__new__)
       21    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}
        1    0.000    0.000    0.000    0.000 dataloader.py:394(multiprocessing_context)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
       21    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
       19    0.000    0.000    0.000    0.000 inspect.py:2552(default)
       19    0.000    0.000    0.000    0.000 inspect.py:2560(kind)
        1    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        4    0.000    0.000    0.000    0.000 inspect.py:159(isfunction)
        2    0.000    0.000    0.000    0.000 inspect.py:514(_is_wrapper)
        2    0.000    0.000    0.000    0.000 dataset.py:422(__len__)
        2    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}
        2    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        2    0.000    0.000    0.000    0.000 {built-in method sys.getrecursionlimit}
        2    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
        1    0.000    0.000    0.000    0.000 dataloader.py:442(_auto_collation)
        2    0.000    0.000    0.000    0.000 inspect.py:2865(parameters)
        2    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'index' of 'tuple' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_train_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 callback.py:205(on_train_start)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)



Profile stats for: [Callback]TQDMProgressBar.on_train_start
         266 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.001    0.001 tqdm_progress.py:259(on_train_start)
        1    0.000    0.000    0.001    0.001 tqdm_progress.py:197(init_train_tqdm)
        1    0.000    0.000    0.001    0.001 tqdm_progress.py:40(__init__)
        1    0.000    0.000    0.001    0.001 std.py:952(__init__)
        1    0.000    0.000    0.001    0.001 std.py:1325(refresh)
        1    0.000    0.000    0.000    0.000 std.py:1464(display)
        1    0.000    0.000    0.000    0.000 std.py:457(print_status)
        1    0.000    0.000    0.000    0.000 std.py:451(fp_write)
        2    0.000    0.000    0.000    0.000 utils.py:194(inner)
        1    0.000    0.000    0.000    0.000 redirect.py:644(write)
        1    0.000    0.000    0.000    0.000 wandb_run.py:2304(<lambda>)
        1    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
        1    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
        1    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
        1    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
        2    0.000    0.000    0.000    0.000 utils.py:333(_screen_shape_linux)
        1    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
        1    0.000    0.000    0.000    0.000 std.py:1150(__str__)
        1    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
        1    0.000    0.000    0.000    0.000 std.py:663(__new__)
        1    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        1    0.000    0.000    0.000    0.000 std.py:464(format_meter)
        1    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
        1    0.000    0.000    0.000    0.000 utils.py:378(disp_len)
        1    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
        1    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
        1    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
        1    0.000    0.000    0.000    0.000 utils.py:374(_text_width)
        2    0.000    0.000    0.000    0.000 std.py:110(__enter__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.sum}
        3    0.000    0.000    0.000    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
        2    0.000    0.000    0.000    0.000 utils.py:347(<listcomp>)
        1    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
        2    0.000    0.000    0.000    0.000 {built-in method fcntl.ioctl}
       44    0.000    0.000    0.000    0.000 utils.py:375(<genexpr>)
        1    0.000    0.000    0.000    0.000 utils.py:213(__init__)
        1    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        1    0.000    0.000    0.000    0.000 std.py:1446(format_dict)
        2    0.000    0.000    0.000    0.000 os.py:674(__getitem__)
        3    0.000    0.000    0.000    0.000 std.py:102(acquire)
        1    0.000    0.000    0.000    0.000 functools.py:392(__get__)
        1    0.000    0.000    0.000    0.000 std.py:438(status_printer)
        2    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}
        1    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        1    0.000    0.000    0.000    0.000 _weakrefset.py:86(add)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:173(is_disabled)
       13    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        2    0.000    0.000    0.000    0.000 utils.py:187(disable_on_exception)
        1    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
        1    0.000    0.000    0.000    0.000 {built-in method fromtimestamp}
        2    0.000    0.000    0.000    0.000 utils.py:266(_supports_unicode)
        2    0.000    0.000    0.000    0.000 os.py:754(encode)
        1    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
        3    0.000    0.000    0.000    0.000 std.py:106(release)
        1    0.000    0.000    0.000    0.000 std.py:400(format_interval)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:169(is_enabled)
        1    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
       43    0.000    0.000    0.000    0.000 {built-in method unicodedata.east_asian_width}
        2    0.000    0.000    0.000    0.000 std.py:113(__exit__)
        1    0.000    0.000    0.000    0.000 utils.py:156(__init__)
        1    0.000    0.000    0.000    0.000 _monitor.py:94(report)
        5    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:127(train_progress_bar)
        1    0.000    0.000    0.000    0.000 std.py:153(__init__)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:165(process_position)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        3    0.000    0.000    0.000    0.000 std.py:226(__init__)
        1    0.000    0.000    0.000    0.000 std.py:186(__format__)
        2    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        3    0.000    0.000    0.000    0.000 utils.py:152(wrapper_setattr)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        3    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        4    0.000    0.000    0.000    0.000 utils.py:222(__eq__)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        3    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
        3    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
        3    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
        1    0.000    0.000    0.000    0.000 utils.py:273(_is_ascii)
        1    0.000    0.000    0.000    0.000 std.py:760(get_lock)
        1    0.000    0.000    0.000    0.000 utils.py:252(_is_utf)
        1    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
        1    0.000    0.000    0.000    0.000 std.py:1157(__hash__)
        1    0.000    0.000    0.000    0.000 std.py:1147(__del__)
        1    0.000    0.000    0.000    0.000 {built-in method utcnow}
        2    0.000    0.000    0.000    0.000 {built-in method _weakref.proxy}
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f7dfbd377c0}
        1    0.000    0.000    0.000    0.000 utils.py:282(_screen_shape_wrapper)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        1    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
        1    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 utils.py:112(__format__)
        1    0.000    0.000    0.000    0.000 std.py:167(colour)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
        3    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
        1    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
        1    0.000    0.000    0.000    0.000 std.py:231(__call__)
        3    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 utils.py:108(__init__)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        1    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.max}
        1    0.000    0.000    0.000    0.000 threading.py:536(is_set)
        1    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:161(refresh_rate)
        1    0.000    0.000    0.000    0.000 {built-in method time.time}
        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 progress_bar.py:64(train_description)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
        1    0.000    0.000    0.000    0.000 std.py:1265(close)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 std.py:163(colour)



Profile stats for: [Callback]ModelSummary.on_train_start
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 callback.py:205(on_train_start)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_start
         11 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 model_checkpoint.py:280(on_train_start)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_train_start
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 hooks.py:44(on_train_start)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Strategy]SingleDeviceStrategy.on_train_start
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 strategy.py:545(on_train_start)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_train_epoch_start
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 callback.py:92(on_train_epoch_start)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_train_epoch_start
         72288 function calls in 0.044 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000    0.044    0.000 tqdm_progress.py:263(on_train_epoch_start)
      200    0.000    0.000    0.042    0.000 std.py:1325(refresh)
      200    0.000    0.000    0.041    0.000 std.py:1464(display)
      100    0.000    0.000    0.028    0.000 std.py:1360(reset)
      200    0.001    0.000    0.022    0.000 std.py:1150(__str__)
      200    0.000    0.000    0.018    0.000 std.py:457(print_status)
      200    0.012    0.000    0.017    0.000 std.py:464(format_meter)
      100    0.000    0.000    0.015    0.000 std.py:1382(set_description)
      200    0.000    0.000    0.011    0.000 std.py:451(fp_write)
      400    0.000    0.000    0.011    0.000 utils.py:194(inner)
      200    0.000    0.000    0.009    0.000 redirect.py:644(write)
      200    0.000    0.000    0.009    0.000 wandb_run.py:2304(<lambda>)
      200    0.000    0.000    0.008    0.000 wandb_run.py:390(wrapper_fn)
      200    0.000    0.000    0.008    0.000 wandb_run.py:1429(_console_raw_callback)
      200    0.001    0.000    0.008    0.000 interface.py:749(publish_output_raw)
      200    0.000    0.000    0.006    0.000 utils.py:378(disp_len)
      200    0.000    0.000    0.006    0.000 utils.py:374(_text_width)
      200    0.001    0.000    0.006    0.000 {built-in method builtins.sum}
      200    0.000    0.000    0.005    0.000 interface_shared.py:76(_publish_output_raw)
      200    0.000    0.000    0.005    0.000 interface_sock.py:45(_publish)
      200    0.000    0.000    0.004    0.000 sock_client.py:219(send_record_publish)
      200    0.001    0.000    0.004    0.000 std.py:1446(format_dict)
    17444    0.003    0.000    0.004    0.000 utils.py:375(<genexpr>)
      200    0.000    0.000    0.004    0.000 sock_client.py:153(send_server_request)
      200    0.001    0.000    0.004    0.000 sock_client.py:145(_send_message)
      200    0.001    0.000    0.003    0.000 utils.py:333(_screen_shape_linux)
      200    0.000    0.000    0.003    0.000 sock_client.py:121(_sendall_with_error_handle)
      400    0.002    0.000    0.003    0.000 utils.py:273(_is_ascii)
      200    0.002    0.000    0.002    0.000 {method 'send' of '_socket.socket' objects}
      200    0.002    0.000    0.002    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      200    0.001    0.000    0.002    0.000 {built-in method fcntl.ioctl}
      400    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}
    17244    0.001    0.000    0.001    0.000 {built-in method unicodedata.east_asian_width}
      200    0.000    0.000    0.001    0.000 well_known_types.py:172(GetCurrentTime)
      200    0.000    0.000    0.001    0.000 well_known_types.py:242(FromDatetime)
    17400    0.001    0.000    0.001    0.000 {built-in method builtins.ord}
      200    0.000    0.000    0.001    0.000 utils.py:347(<listcomp>)
      200    0.000    0.000    0.001    0.000 os.py:674(__getitem__)
      200    0.000    0.000    0.001    0.000 std.py:186(__format__)
      200    0.000    0.000    0.000    0.000 std.py:400(format_interval)
      200    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
      200    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      200    0.000    0.000    0.000    0.000 std.py:102(acquire)
      100    0.000    0.000    0.000    0.000 tqdm_progress.py:440(convert_inf)
      200    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
      200    0.000    0.000    0.000    0.000 {built-in method fromtimestamp}
      200    0.000    0.000    0.000    0.000 std.py:106(release)
      200    0.000    0.000    0.000    0.000 std.py:153(__init__)
      200    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
      200    0.000    0.000    0.000    0.000 os.py:754(encode)
      100    0.000    0.000    0.000    0.000 progress_bar.py:80(total_train_batches)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      400    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
      200    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      200    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
      200    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      400    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      600    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      200    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
      200    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
      300    0.000    0.000    0.000    0.000 std.py:226(__init__)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      200    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
      100    0.000    0.000    0.000    0.000 {built-in method math.isinf}
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      200    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
      200    0.000    0.000    0.000    0.000 {built-in method utcnow}
      600    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
      200    0.000    0.000    0.000    0.000 std.py:231(__call__)
      600    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.max}
      300    0.000    0.000    0.000    0.000 tqdm_progress.py:121(train_progress_bar)
      200    0.000    0.000    0.000    0.000 utils.py:112(__format__)
      400    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      600    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      200    0.000    0.000    0.000    0.000 utils.py:108(__init__)
      200    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      200    0.000    0.000    0.000    0.000 std.py:167(colour)
      100    0.000    0.000    0.000    0.000 progress_bar.py:54(trainer)
      300    0.000    0.000    0.000    0.000 {built-in method time.time}
      200    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1467(current_epoch)
      200    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      200    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      200    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      200    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1523(num_training_batches)
      200    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      200    0.000    0.000    0.000    0.000 std.py:163(colour)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      100    0.000    0.000    0.000    0.000 {built-in method math.isnan}
      200    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}



Profile stats for: [Callback]ModelSummary.on_train_epoch_start
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:92(on_train_epoch_start)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_start
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:92(on_train_epoch_start)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_train_epoch_start
         14300 function calls in 0.013 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.013    0.000 FlavourClassificationTransformerEncoder.py:133(on_train_epoch_start)
      200    0.000    0.000    0.013    0.000 __init__.py:1436(info)
      200    0.000    0.000    0.012    0.000 __init__.py:1565(_log)
      200    0.000    0.000    0.008    0.000 __init__.py:1591(handle)
      200    0.000    0.000    0.008    0.000 __init__.py:1645(callHandlers)
      200    0.000    0.000    0.008    0.000 __init__.py:939(handle)
      200    0.000    0.000    0.007    0.000 __init__.py:1178(emit)
      200    0.000    0.000    0.007    0.000 __init__.py:1071(emit)
      200    0.000    0.000    0.005    0.000 __init__.py:1060(flush)
      200    0.004    0.000    0.004    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      200    0.000    0.000    0.003    0.000 __init__.py:1550(makeRecord)
      200    0.001    0.000    0.003    0.000 __init__.py:282(__init__)
      200    0.000    0.000    0.002    0.000 __init__.py:916(format)
      200    0.000    0.000    0.002    0.000 __init__.py:650(format)
      200    0.000    0.000    0.001    0.000 __init__.py:582(formatTime)
      200    0.000    0.000    0.001    0.000 __init__.py:1514(findCaller)
      200    0.000    0.000    0.000    0.000 posixpath.py:117(splitext)
      200    0.000    0.000    0.000    0.000 {built-in method time.strftime}
      200    0.000    0.000    0.000    0.000 posixpath.py:140(basename)
      200    0.000    0.000    0.000    0.000 __init__.py:634(formatMessage)
      200    0.000    0.000    0.000    0.000 {built-in method time.localtime}
      200    0.000    0.000    0.000    0.000 module.py:262(current_epoch)
      200    0.000    0.000    0.000    0.000 genericpath.py:121(_splitext)
      200    0.000    0.000    0.000    0.000 __init__.py:628(usesTime)
      200    0.000    0.000    0.000    0.000 __init__.py:432(format)
      400    0.000    0.000    0.000    0.000 __init__.py:896(acquire)
      200    0.000    0.000    0.000    0.000 __init__.py:119(getLevelName)
      200    0.000    0.000    0.000    0.000 __init__.py:429(_format)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      200    0.000    0.000    0.000    0.000 __init__.py:421(usesTime)
      400    0.000    0.000    0.000    0.000 __init__.py:903(release)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      200    0.000    0.000    0.000    0.000 __init__.py:160(<lambda>)
      600    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}
      400    0.000    0.000    0.000    0.000 __init__.py:791(filter)
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      200    0.000    0.000    0.000    0.000 threading.py:1358(current_thread)
      200    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
      600    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      200    0.000    0.000    0.000    0.000 posixpath.py:52(normcase)
      500    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      200    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)
      200    0.000    0.000    0.000    0.000 __init__.py:358(getMessage)
      200    0.000    0.000    0.000    0.000 module.py:215(trainer)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      400    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      200    0.000    0.000    0.000    0.000 trainer.py:1467(current_epoch)
      200    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
      600    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
      400    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      200    0.000    0.000    0.000    0.000 threading.py:1093(name)
      400    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
      200    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}
      200    0.000    0.000    0.000    0.000 {built-in method sys._getframe}
      400    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      200    0.000    0.000    0.000    0.000 process.py:189(name)
      200    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      200    0.000    0.000    0.000    0.000 {built-in method time.time}
      200    0.000    0.000    0.000    0.000 process.py:37(current_process)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: run_training_epoch
         460984 function calls (459442 primitive calls) in 53.271 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000   53.271    0.533 training_epoch_loop.py:135(run)
    11584    0.012    0.000   34.942    0.003 {built-in method builtins.next}
      100    0.002    0.000   34.807    0.348 training_epoch_loop.py:192(advance)
      100    0.000    0.000   34.798    0.348 fetchers.py:119(__next__)
      100    0.000    0.000   34.798    0.348 fetchers.py:55(__next__)
      100    0.002    0.000   34.798    0.348 training_epoch_loop.py:186(_on_before_fetch)
      100    0.001    0.000   34.796    0.348 advanced.py:65(start)
      100   34.794    0.348   34.794    0.348 {method 'enable' of '_lsprof.Profiler' objects}
      100    0.002    0.000   18.447    0.184 training_epoch_loop.py:176(on_run_start)
   594/99    0.004    0.000   18.445    0.186 {built-in method builtins.iter}
       99    0.000    0.000   18.444    0.186 fetchers.py:102(__iter__)
       99    0.001    0.000   18.443    0.186 fetchers.py:49(__iter__)
       99    0.033    0.000   18.432    0.186 combined_loader.py:347(__iter__)
       99    0.001    0.000   14.696    0.148 combined_loader.py:90(__iter__)
       99    0.002    0.000   14.695    0.148 combined_loader.py:41(__iter__)
       99    0.000    0.000   14.693    0.148 combined_loader.py:43(<listcomp>)
       99    0.001    0.000   14.692    0.148 dataloader.py:427(__iter__)
       99    0.001    0.000   14.692    0.148 dataloader.py:383(_get_iterator)
       99    0.068    0.001   14.690    0.148 dataloader.py:989(__init__)
      792    0.039    0.000   13.871    0.018 process.py:110(start)
      792    0.008    0.000   13.761    0.017 context.py:222(_Popen)
      792    0.015    0.000   13.753    0.017 context.py:274(_Popen)
      792    0.010    0.000   13.737    0.017 popen_fork.py:15(__init__)
      792    0.045    0.000   13.714    0.017 popen_fork.py:62(_launch)
      792   13.598    0.017   13.616    0.017 {built-in method posix.fork}
       99    0.000    0.000    3.688    0.037 dataloader.py:1476(__del__)
       99    0.009    0.000    3.688    0.037 dataloader.py:1399(_shutdown_workers)
      792    0.003    0.000    3.428    0.004 process.py:142(join)
      792    0.004    0.000    3.424    0.004 popen_fork.py:36(wait)
      792    0.003    0.000    3.402    0.004 connection.py:921(wait)
      792    0.003    0.000    3.390    0.004 selectors.py:403(select)
      792    3.380    0.004    3.385    0.004 {method 'poll' of 'select.poll' objects}
      891    0.046    0.000    0.541    0.001 context.py:100(Queue)
      891    0.040    0.000    0.484    0.001 queues.py:37(__init__)
     3168    0.167    0.000    0.348    0.000 synchronize.py:50(__init__)
     1881    0.014    0.000    0.313    0.000 context.py:65(Lock)
     1881    0.017    0.000    0.296    0.000 synchronize.py:161(__init__)
      990    0.004    0.000    0.227    0.000 threading.py:880(start)
      990    0.005    0.000    0.225    0.000 queues.py:86(put)
      891    0.013    0.000    0.209    0.000 queues.py:161(_start_thread)
     6039    0.190    0.000    0.190    0.000 {method 'acquire' of '_thread.lock' objects}
      990    0.003    0.000    0.180    0.000 threading.py:563(wait)
      990    0.004    0.000    0.175    0.000 threading.py:280(wait)
      792    0.001    0.000    0.164    0.000 dataloader.py:1373(_mark_worker_as_unavailable)
     3168    0.012    0.000    0.127    0.000 synchronize.py:114(_make_name)
     3168    0.031    0.000    0.106    0.000 tempfile.py:149(__next__)
      792    0.039    0.000    0.067    0.000 process.py:61(_cleanup)
     3168    0.012    0.000    0.065    0.000 tempfile.py:152(<listcomp>)
       99    0.003    0.000    0.061    0.001 dataloader.py:1085(_reset)
      891    0.004    0.000    0.058    0.000 context.py:85(BoundedSemaphore)
    25344    0.030    0.000    0.053    0.000 random.py:343(choice)
     4059    0.011    0.000    0.053    0.000 util.py:171(register_after_fork)
     1584    0.001    0.000    0.053    0.000 dataloader.py:1346(_try_put_index)
      891    0.001    0.000    0.052    0.000 synchronize.py:144(__init__)
    17028    0.013    0.000    0.046    0.000 popen_fork.py:24(poll)
      891    0.014    0.000    0.044    0.000 connection.py:520(Pipe)
      990    0.043    0.000    0.043    0.000 {built-in method _thread.start_new_thread}
     1782    0.038    0.000    0.042    0.000 util.py:186(__init__)
     4059    0.022    0.000    0.040    0.000 weakref.py:165(__setitem__)
    16236    0.031    0.000    0.031    0.000 {built-in method posix.waitpid}
      891    0.022    0.000    0.031    0.000 queues.py:71(_reset)
      792    0.018    0.000    0.029    0.000 process.py:80(__init__)
     1584    0.000    0.000    0.027    0.000 dataloader.py:619(_next_index)
      297    0.002    0.000    0.027    0.000 sampler.py:275(__iter__)
     2475    0.026    0.000    0.026    0.000 {built-in method posix.pipe}
      990    0.011    0.000    0.025    0.000 threading.py:802(__init__)
     2475    0.009    0.000    0.025    0.000 sampler.py:153(__iter__)
       99    0.000    0.000    0.024    0.000 context.py:90(Event)
       99    0.001    0.000    0.024    0.000 synchronize.py:323(__init__)
       99    0.000    0.000    0.022    0.000 threading.py:1028(join)
    25344    0.016    0.000    0.021    0.000 random.py:237(_randbelow_with_getrandbits)
       99    0.001    0.000    0.021    0.000 threading.py:1066(_wait_for_tstate_lock)
     1683    0.006    0.000    0.020    0.000 util.py:205(__call__)
     2277    0.019    0.000    0.019    0.000 threading.py:228(__init__)
      396    0.001    0.000    0.018    0.000 context.py:80(Semaphore)
      792    0.013    0.000    0.018    0.000 __init__.py:227(_releaseLock)
      396    0.000    0.000    0.017    0.000 synchronize.py:125(__init__)
     3168    0.015    0.000    0.015    0.000 {built-in method posix.close}
       99    0.000    0.000    0.014    0.000 context.py:75(Condition)
       99    0.000    0.000    0.014    0.000 synchronize.py:212(__init__)
      100    0.001    0.000    0.014    0.000 training_epoch_loop.py:116(done)
      100    0.003    0.000    0.013    0.000 training_epoch_loop.py:106(_is_training_done)
      792    0.007    0.000    0.013    0.000 util.py:433(_flush_std_streams)
     5940    0.009    0.000    0.013    0.000 <frozen importlib._bootstrap>:398(parent)
      891    0.001    0.000    0.013    0.000 queues.py:140(close)
     1089    0.003    0.000    0.012    0.000 threading.py:528(__init__)
     4059    0.008    0.000    0.012    0.000 weakref.py:348(__new__)
     1782    0.011    0.000    0.011    0.000 connection.py:121(__init__)
       99    0.001    0.000    0.010    0.000 fetchers.py:139(reset)
27324/26433    0.005    0.000    0.010    0.000 {built-in method builtins.len}
  633/576    0.001    0.000    0.009    0.000 signal_handling.py:64(handler)
      633    0.009    0.000    0.009    0.000 {built-in method torch._C._error_if_any_worker_fails}
     4077    0.006    0.000    0.009    0.000 {built-in method builtins.isinstance}
       99    0.003    0.000    0.009    0.000 fetchers.py:71(reset)
      100    0.005    0.000    0.008    0.000 training_epoch_loop.py:99(global_step)
      891    0.002    0.000    0.007    0.000 queues.py:204(_finalize_close)
       99    0.002    0.000    0.007    0.000 dataloader.py:564(__init__)
     3168    0.004    0.000    0.007    0.000 tempfile.py:138(rng)
       99    0.004    0.000    0.007    0.000 queue.py:34(__init__)
     4059    0.006    0.000    0.006    0.000 weakref.py:353(__init__)
      792    0.000    0.000    0.006    0.000 util.py:461(close_fds)
     1782    0.003    0.000    0.006    0.000 _weakrefset.py:86(add)
     1980    0.002    0.000    0.006    0.000 threading.py:351(notify)
    10692    0.006    0.000    0.006    0.000 {built-in method posix.getpid}
     1584    0.006    0.000    0.006    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
   198/99    0.000    0.000    0.006    0.000 data.py:47(sized_len)
     3960    0.004    0.000    0.006    0.000 {method 'join' of 'str' objects}
      100    0.002    0.000    0.005    0.000 loop.py:32(restarting)
      198    0.005    0.000    0.005    0.000 {built-in method torch.empty}
       99    0.002    0.000    0.005    0.000 dataloader.py:609(_reset)
     2574    0.005    0.000    0.005    0.000 {method 'add' of 'set' objects}
       99    0.000    0.000    0.005    0.000 combined_loader.py:355(__len__)
     2970    0.002    0.000    0.005    0.000 threading.py:256(__enter__)
     4851    0.005    0.000    0.005    0.000 {built-in method __new__ of type object at 0x7f7dfbd377c0}
       99    0.001    0.000    0.005    0.000 combined_loader.py:96(__len__)
      792    0.004    0.000    0.004    0.000 {method 'release' of '_thread.RLock' objects}
      198    0.004    0.000    0.004    0.000 {built-in method torch.randperm}
      792    0.001    0.000    0.004    0.000 selectors.py:352(register)
      792    0.001    0.000    0.004    0.000 synchronize.py:327(is_set)
     1683    0.004    0.000    0.004    0.000 queues.py:153(cancel_join_thread)
    43755    0.004    0.000    0.004    0.000 {method 'getrandbits' of '_random.Random' objects}
      198    0.004    0.000    0.004    0.000 {method 'random_' of 'torch._C.TensorBase' objects}
       99    0.001    0.000    0.004    0.000 synchronize.py:334(set)
      891    0.001    0.000    0.004    0.000 dataloader.py:1080(<genexpr>)
       99    0.001    0.000    0.003    0.000 __init__.py:876(current_device)
      496    0.000    0.000    0.003    0.000 abc.py:117(__instancecheck__)
     2178    0.003    0.000    0.003    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
     5940    0.003    0.000    0.003    0.000 {method 'rpartition' of 'str' objects}
       99    0.000    0.000    0.003    0.000 combined_loader.py:404(_get_iterables_lengths)
     3168    0.003    0.000    0.003    0.000 synchronize.py:90(_make_methods)
     2970    0.003    0.000    0.003    0.000 {method '__enter__' of '_thread.lock' objects}
      792    0.001    0.000    0.003    0.000 selectors.py:235(register)
     2871    0.003    0.000    0.003    0.000 {method 'append' of 'collections.deque' objects}
      891    0.001    0.000    0.003    0.000 synchronize.py:229(__enter__)
      496    0.003    0.000    0.003    0.000 {built-in method _abc._abc_instancecheck}
     2970    0.001    0.000    0.003    0.000 threading.py:259(__exit__)
       99    0.000    0.000    0.003    0.000 combined_loader.py:405(<listcomp>)
     3069    0.003    0.000    0.003    0.000 {built-in method _thread.allocate_lock}
       99    0.003    0.000    0.003    0.000 {built-in method torch._C._set_worker_pids}
      100    0.001    0.000    0.003    0.000 training_epoch_loop.py:147(reset)
      792    0.002    0.000    0.003    0.000 process.py:234(ident)
     2475    0.001    0.000    0.002    0.000 weakref.py:106(remove)
      100    0.002    0.000    0.002    0.000 trainer.py:1178(lightning_module)
      792    0.002    0.000    0.002    0.000 {method 'copy' of 'dict' objects}
      990    0.002    0.000    0.002    0.000 threading.py:1229(_make_invoke_excepthook)
     1782    0.002    0.000    0.002    0.000 context.py:233(get_context)
      792    0.001    0.000    0.002    0.000 selectors.py:348(__init__)
     2079    0.002    0.000    0.002    0.000 {method 'release' of '_thread.lock' objects}
       99    0.001    0.000    0.002    0.000 dataloader.py:458(__len__)
    25344    0.002    0.000    0.002    0.000 {method 'bit_length' of 'int' objects}
      891    0.000    0.000    0.002    0.000 synchronize.py:94(__enter__)
     2970    0.001    0.000    0.002    0.000 threading.py:271(_is_owned)
       99    0.002    0.000    0.002    0.000 {built-in method torch._C._remove_worker_pids}
       99    0.001    0.000    0.002    0.000 synchronize.py:296(notify_all)
      100    0.001    0.000    0.002    0.000 training_epoch_loop.py:331(_num_ready_batches_reached)
     1188    0.001    0.000    0.002    0.000 threading.py:1358(current_thread)
      792    0.001    0.000    0.002    0.000 process.py:153(is_alive)
     2970    0.001    0.000    0.001    0.000 {method '__exit__' of '_thread.lock' objects}
     2376    0.001    0.000    0.001    0.000 {method 'append' of 'list' objects}
      891    0.001    0.000    0.001    0.000 {method '__enter__' of '_multiprocessing.SemLock' objects}
     8415    0.001    0.000    0.001    0.000 util.py:48(debug)
     3168    0.001    0.000    0.001    0.000 {built-in method builtins.getattr}
      990    0.001    0.000    0.001    0.000 _weakrefset.py:39(_remove)
      990    0.001    0.000    0.001    0.000 threading.py:1162(daemon)
       99    0.000    0.000    0.001    0.000 __init__.py:115(is_available)
     4257    0.001    0.000    0.001    0.000 {built-in method builtins.id}
      198    0.001    0.000    0.001    0.000 {method 'item' of 'torch._C.TensorBase' objects}
       99    0.001    0.000    0.001    0.000 {built-in method torch._C._cuda_getDevice}
       99    0.001    0.000    0.001    0.000 {built-in method torch._C._get_privateuse1_backend_name}
      792    0.000    0.000    0.001    0.000 selectors.py:203(__exit__)
     2475    0.001    0.000    0.001    0.000 {built-in method _weakref._remove_dead_weakref}
     1584    0.001    0.000    0.001    0.000 process.py:94(<genexpr>)
      891    0.000    0.000    0.001    0.000 synchronize.py:232(__exit__)
      400    0.001    0.000    0.001    0.000 progress.py:172(reset_on_run)
      990    0.001    0.000    0.001    0.000 threading.py:268(_acquire_restore)
       99    0.000    0.000    0.001    0.000 sampler.py:298(__len__)
       99    0.001    0.000    0.001    0.000 dataloader.py:487(check_worker_number_rationality)
       99    0.001    0.000    0.001    0.000 __init__.py:284(_lazy_init)
      100    0.000    0.000    0.001    0.000 progress.py:204(reset_on_run)
      100    0.001    0.000    0.001    0.000 progress.py:274(optimizer_steps)
       99    0.000    0.000    0.001    0.000 __init__.py:111(_nvml_based_avail)
       99    0.000    0.000    0.001    0.000 threading.py:542(set)
      396    0.001    0.000    0.001    0.000 dataset.py:422(__len__)
      792    0.001    0.000    0.001    0.000 selectors.py:210(__init__)
       99    0.000    0.000    0.001    0.000 combined_loader.py:68(__init__)
      792    0.000    0.000    0.001    0.000 selectors.py:216(_fileobj_lookup)
       99    0.001    0.000    0.001    0.000 signal_handling.py:48(_set_SIGCHLD_handler)
       99    0.000    0.000    0.001    0.000 synchronize.py:270(notify)
     2574    0.001    0.000    0.001    0.000 {method 'discard' of 'set' objects}
       99    0.000    0.000    0.001    0.000 os.py:771(getenv)
       99    0.001    0.000    0.001    0.000 threading.py:757(_newname)
      792    0.001    0.000    0.001    0.000 selectors.py:269(close)
       99    0.000    0.000    0.001    0.000 _collections_abc.py:760(get)
       99    0.000    0.000    0.001    0.000 dataloader.py:94(_get_distributed_settings)
      297    0.000    0.000    0.001    0.000 sampler.py:146(num_samples)
      100    0.001    0.000    0.001    0.000 trainer.py:1125(strategy)
     3168    0.001    0.000    0.001    0.000 context.py:197(get_start_method)
       99    0.000    0.000    0.001    0.000 os.py:674(__getitem__)
      792    0.000    0.000    0.001    0.000 __init__.py:218(_acquireLock)
      891    0.000    0.000    0.001    0.000 synchronize.py:97(__exit__)
     3267    0.001    0.000    0.001    0.000 context.py:187(get_context)
      100    0.001    0.000    0.001    0.000 trainer.py:1523(num_training_batches)
      100    0.000    0.000    0.001    0.000 progress.py:282(reset_on_run)
      198    0.001    0.000    0.001    0.000 {method 'tolist' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.001    0.000 progress.py:249(reset_on_run)
       99    0.001    0.000    0.001    0.000 queue.py:206(_init)
      792    0.000    0.000    0.001    0.000 <string>:1(<lambda>)
      792    0.000    0.000    0.001    0.000 selectors.py:21(_fileobj_to_fd)
     3168    0.001    0.000    0.001    0.000 process.py:99(_check_closed)
       99    0.000    0.000    0.001    0.000 sampler.py:171(__len__)
       99    0.000    0.000    0.000    0.000 combined_loader.py:29(__init__)
       99    0.000    0.000    0.000    0.000 {built-in method posix.sched_getaffinity}
      792    0.000    0.000    0.000    0.000 selectors.py:276(_key_from_fd)
       99    0.000    0.000    0.000    0.000 __init__.py:237(is_initialized)
      990    0.000    0.000    0.000    0.000 threading.py:265(_release_save)
       99    0.000    0.000    0.000    0.000 os.py:754(encode)
     3168    0.000    0.000    0.000    0.000 process.py:37(current_process)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.vars}
       99    0.000    0.000    0.000    0.000 {method 'manual_seed' of 'torch._C.Generator' objects}
      299    0.000    0.000    0.000    0.000 loop.py:27(restarting)
     1089    0.000    0.000    0.000    0.000 threading.py:1147(daemon)
       99    0.000    0.000    0.000    0.000 threading.py:992(_stop)
      100    0.000    0.000    0.000    0.000 module.py:295(automatic_optimization)
      200    0.000    0.000    0.000    0.000 progress.py:114(reset)
       99    0.000    0.000    0.000    0.000 {built-in method builtins.max}
       99    0.000    0.000    0.000    0.000 util.py:229(cancel)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
       99    0.000    0.000    0.000    0.000 threading.py:381(notify_all)
      792    0.000    0.000    0.000    0.000 {built-in method select.poll}
      792    0.000    0.000    0.000    0.000 {built-in method math.ceil}
      792    0.000    0.000    0.000    0.000 connection.py:937(<listcomp>)
     2079    0.000    0.000    0.000    0.000 threading.py:536(is_set)
      792    0.000    0.000    0.000    0.000 selectors.py:64(__init__)
      300    0.000    0.000    0.000    0.000 progress.py:87(reset)
      792    0.000    0.000    0.000    0.000 process.py:205(daemon)
     1188    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
     1683    0.000    0.000    0.000    0.000 util.py:44(sub_debug)
      792    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      198    0.000    0.000    0.000    0.000 dataloader.py:446(_index_sampler)
      792    0.000    0.000    0.000    0.000 {method 'register' of 'select.poll' objects}
      792    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
       99    0.000    0.000    0.000    0.000 dataloader.py:1101(<listcomp>)
      495    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      891    0.000    0.000    0.000    0.000 {method 'clear' of 'collections.deque' objects}
      792    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      990    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}
      891    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
       99    0.000    0.000    0.000    0.000 distributed_c10d.py:996(is_initialized)
      198    0.000    0.000    0.000    0.000 fetchers.py:38(combined_loader)
      198    0.000    0.000    0.000    0.000 combined_loader.py:100(<genexpr>)
      891    0.000    0.000    0.000    0.000 {method '__exit__' of '_multiprocessing.SemLock' objects}
      792    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}
      792    0.000    0.000    0.000    0.000 process.py:189(name)
      198    0.000    0.000    0.000    0.000 __init__.py:106(_is_compiled)
      792    0.000    0.000    0.000    0.000 {built-in method posix.waitstatus_to_exitcode}
      990    0.000    0.000    0.000    0.000 {method 'remove' of 'collections.deque' objects}
      500    0.000    0.000    0.000    0.000 progress.py:56(reset)
      297    0.000    0.000    0.000    0.000 dataloader.py:442(_auto_collation)
      792    0.000    0.000    0.000    0.000 selectors.py:200(__enter__)
       99    0.000    0.000    0.000    0.000 {built-in method torch._C._cuda_isInBadFork}
       99    0.000    0.000    0.000    0.000 distributed_c10d.py:608(WORLD)
      198    0.000    0.000    0.000    0.000 connection.py:134(__del__)
       99    0.000    0.000    0.000    0.000 __init__.py:834(device_count)
       99    0.000    0.000    0.000    0.000 os.py:758(decode)
       99    0.000    0.000    0.000    0.000 __init__.py:10(is_available)
       99    0.000    0.000    0.000    0.000 synchronize.py:235(_make_methods)
      100    0.000    0.000    0.000    0.000 utilities.py:113(_is_max_limit_reached)
       99    0.000    0.000    0.000    0.000 {built-in method builtins.min}
      100    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
       99    0.000    0.000    0.000    0.000 distributed_c10d.py:480(default_pg)
       99    0.000    0.000    0.000    0.000 {method 'locked' of '_thread.lock' objects}
       99    0.000    0.000    0.000    0.000 dataloader.py:390(multiprocessing_context)
       99    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
       99    0.000    0.000    0.000    0.000 {method 'decode' of 'bytes' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1467(current_epoch)
       99    0.000    0.000    0.000    0.000 {method '_is_mine' of '_multiprocessing.SemLock' objects}
       99    0.000    0.000    0.000    0.000 combined_loader.py:308(flattened)



Profile stats for: [_TrainingEpochLoop].train_dataloader_next
         10017 function calls (9217 primitive calls) in 0.109 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
  400/100    0.000    0.000    0.108    0.001 {built-in method builtins.next}
      100    0.001    0.000    0.108    0.001 combined_loader.py:339(__next__)
      100    0.001    0.000    0.105    0.001 combined_loader.py:72(__next__)
      100    0.004    0.000    0.104    0.001 dataloader.py:625(__next__)
      100    0.003    0.000    0.046    0.000 profiler.py:687(__enter__)
      100    0.001    0.000    0.043    0.000 _ops.py:1047(__call__)
      100    0.042    0.000    0.042    0.000 {built-in method torch._ops.profiler._record_function_enter_new}
      100    0.001    0.000    0.031    0.000 dataloader.py:1297(_next_data)
      100    0.001    0.000    0.029    0.000 dataloader.py:1264(_get_data)
      100    0.000    0.000    0.028    0.000 dataloader.py:1118(_try_get_data)
      100    0.001    0.000    0.027    0.000 queue.py:154(get)
      204    0.025    0.000    0.025    0.000 {method 'acquire' of '_thread.lock' objects}
        1    0.000    0.000    0.025    0.025 threading.py:280(wait)
      100    0.004    0.000    0.018    0.000 profiler.py:693(__exit__)
      100    0.001    0.000    0.014    0.000 _ops.py:887(__call__)
      100    0.001    0.000    0.010    0.000 _ops.py:943(_must_dispatch_in_python)
      100    0.001    0.000    0.009    0.000 _pytree.py:1181(tree_any)
      100    0.000    0.000    0.008    0.000 {built-in method builtins.any}
  700/200    0.003    0.000    0.006    0.000 _pytree.py:874(tree_iter)
      100    0.004    0.000    0.005    0.000 profiler.py:676(__init__)
      100    0.003    0.000    0.003    0.000 {built-in method torch._ops.profiler.}
      400    0.000    0.000    0.002    0.000 _pytree.py:656(_is_leaf)
      700    0.001    0.000    0.002    0.000 _pytree.py:649(_get_node_type)
      100    0.002    0.000    0.002    0.000 _ops.py:945(<lambda>)
      700    0.001    0.000    0.002    0.000 _pytree.py:638(_is_namedtuple_instance)
      100    0.001    0.000    0.001    0.000 typing.py:271(inner)
      100    0.001    0.000    0.001    0.000 _pytree.py:862(tree_unflatten)
      500    0.000    0.000    0.001    0.000 {built-in method builtins.isinstance}
      100    0.001    0.000    0.001    0.000 training_epoch_loop.py:189(_on_after_fetch)
      100    0.000    0.000    0.001    0.000 abc.py:117(__instancecheck__)
      100    0.001    0.000    0.001    0.000 {built-in method _abc._abc_instancecheck}
      100    0.000    0.000    0.001    0.000 threading.py:1133(is_alive)
      100    0.000    0.000    0.000    0.000 dataloader.py:1366(_process_data)
      100    0.000    0.000    0.000    0.000 _pytree.py:785(unflatten)
      100    0.000    0.000    0.000    0.000 dataloader.py:1346(_try_put_index)
      100    0.000    0.000    0.000    0.000 _pytree.py:423(_dict_flatten)
      100    0.000    0.000    0.000    0.000 threading.py:1066(_wait_for_tstate_lock)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 threading.py:351(notify)
     1201    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      100    0.000    0.000    0.000    0.000 queue.py:217(_get)
      200    0.000    0.000    0.000    0.000 _pytree.py:395(_tuple_flatten)
      100    0.000    0.000    0.000    0.000 threading.py:256(__enter__)
      101    0.000    0.000    0.000    0.000 queue.py:209(_qsize)
      100    0.000    0.000    0.000    0.000 dataloader.py:619(_next_index)
      101    0.000    0.000    0.000    0.000 threading.py:271(_is_owned)
      100    0.000    0.000    0.000    0.000 threading.py:259(__exit__)
      100    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}
      100    0.000    0.000    0.000    0.000 threading.py:536(is_set)
      100    0.000    0.000    0.000    0.000 _pytree.py:701(is_leaf)
      100    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
      100    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 __init__.py:129(annotate)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      100    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}
      100    0.000    0.000    0.000    0.000 {method '__exit__' of 'torch._C.DisableTorchFunctionSubclass' objects}
        1    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)
      100    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}
      100    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 _collections_abc.py:283(__subclasshook__)
        1    0.000    0.000    0.000    0.000 threading.py:265(_release_save)
        1    0.000    0.000    0.000    0.000 threading.py:268(_acquire_restore)
        1    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}
        1    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_train_batch_start
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 callback.py:76(on_train_batch_start)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_train_batch_start
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 callback.py:76(on_train_batch_start)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_train_batch_start
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:76(on_train_batch_start)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_start
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 callback.py:76(on_train_batch_start)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_train_batch_start
         700 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 hooks.py:68(on_train_batch_start)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Strategy]SingleDeviceStrategy.on_train_batch_start
         700 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 strategy.py:577(on_train_batch_start)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: run_training_batch
         5900 function calls (5800 primitive calls) in 9.973 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.002    0.000    9.973    0.100 automatic.py:161(run)
      100    0.001    0.000    9.966    0.100 automatic.py:243(_optimizer_step)
      100    0.000    0.000    9.964    0.100 call.py:137(_call_lightning_module_hook)
      100    0.000    0.000    9.963    0.100 contextlib.py:114(__enter__)
      100    0.000    0.000    9.963    0.100 {built-in method builtins.next}
      100    0.000    0.000    9.963    0.100 profiler.py:55(profile)
      100    0.000    0.000    9.963    0.100 advanced.py:65(start)
      100    9.963    0.100    9.963    0.100 {method 'enable' of '_lsprof.Profiler' objects}
      100    0.001    0.000    0.005    0.000 automatic.py:197(_make_closure)
      100    0.001    0.000    0.002    0.000 automatic.py:115(__init__)
      100    0.001    0.000    0.001    0.000 automatic.py:228(_make_backward_fn)
      100    0.001    0.000    0.001    0.000 automatic.py:209(_make_zero_grad_fn)
      100    0.000    0.000    0.001    0.000 module.py:1731(__setattr__)
      200    0.000    0.000    0.001    0.000 fit_loop.py:427(_should_accumulate)
      100    0.000    0.000    0.000    0.000 closure.py:41(__init__)
      200    0.000    0.000    0.000    0.000 training_epoch_loop.py:336(_should_accumulate)
      100    0.000    0.000    0.000    0.000 trainer.py:1183(optimizers)
      100    0.000    0.000    0.000    0.000 progress.py:142(increment_ready)
      100    0.000    0.000    0.000    0.000 strategy.py:101(optimizers)
      100    0.000    0.000    0.000    0.000 automatic.py:205(_make_step_fn)
      200    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
  300/200    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      100    0.000    0.000    0.000    0.000 contextlib.py:261(helper)
      700    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 parameter.py:8(__instancecheck__)
      200    0.000    0.000    0.000    0.000 training_epoch_loop.py:331(_num_ready_batches_reached)
      100    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)
      100    0.000    0.000    0.000    0.000 __init__.py:1424(debug)
      200    0.000    0.000    0.000    0.000 training_epoch_loop.py:327(_accumulated_batches_reached)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      100    0.000    0.000    0.000    0.000 trainer.py:1467(current_epoch)
      300    0.000    0.000    0.000    0.000 strategy.py:468(handles_gradient_accumulation)
      100    0.000    0.000    0.000    0.000 module.py:295(automatic_optimization)
      100    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
      200    0.000    0.000    0.000    0.000 trainer.py:1523(num_training_batches)
      200    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      300    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f7dee3ee040}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.callable}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.optimizer_step
         9300 function calls (9200 primitive calls) in 9.960 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000    9.960    0.100 module.py:1277(optimizer_step)
      100    0.001    0.000    9.959    0.100 optimizer.py:84(step)
      100    0.001    0.000    9.958    0.100 strategy.py:219(optimizer_step)
      100    0.001    0.000    9.957    0.100 precision.py:112(optimizer_step)
      100    0.005    0.000    9.956    0.100 optimizer.py:464(wrapper)
      100    0.004    0.000    9.948    0.099 optimizer.py:70(_use_grad)
      100    0.002    0.000    9.943    0.099 adam.py:192(step)
      100    0.000    0.000    9.930    0.099 precision.py:95(_wrap_closure)
      100    0.000    0.000    9.930    0.099 automatic.py:142(__call__)
      100    0.000    0.000    9.929    0.099 _contextlib.py:113(decorate_context)
      100    0.000    0.000    9.929    0.099 automatic.py:126(closure)
      100    0.000    0.000    9.929    0.099 automatic.py:305(_training_step)
      100    0.000    0.000    9.928    0.099 call.py:294(_call_strategy_hook)
      100    0.000    0.000    9.927    0.099 contextlib.py:114(__enter__)
      100    0.000    0.000    9.927    0.099 {built-in method builtins.next}
      100    0.000    0.000    9.927    0.099 profiler.py:55(profile)
      100    0.000    0.000    9.927    0.099 advanced.py:65(start)
      100    9.926    0.099    9.926    0.099 {method 'enable' of '_lsprof.Profiler' objects}
      100    0.003    0.000    0.011    0.000 optimizer.py:410(_cuda_graph_capture_health_check)
      100    0.001    0.000    0.005    0.000 __init__.py:115(is_available)
      100    0.001    0.000    0.004    0.000 __init__.py:111(_nvml_based_avail)
      100    0.001    0.000    0.003    0.000 os.py:771(getenv)
      100    0.000    0.000    0.002    0.000 _collections_abc.py:760(get)
      100    0.000    0.000    0.002    0.000 profiler.py:687(__enter__)
      100    0.000    0.000    0.002    0.000 _ops.py:1047(__call__)
      100    0.002    0.000    0.002    0.000 {built-in method torch._ops.profiler._record_function_enter_new}
      100    0.001    0.000    0.002    0.000 os.py:674(__getitem__)
      100    0.001    0.000    0.001    0.000 _utils.py:856(is_compiling)
      100    0.001    0.000    0.001    0.000 grad_mode.py:184(__init__)
      200    0.001    0.000    0.001    0.000 _contextlib.py:154(__new__)
      300    0.001    0.000    0.001    0.000 {built-in method builtins.getattr}
      100    0.000    0.000    0.000    0.000 graphs.py:24(is_current_stream_capturing)
      100    0.000    0.000    0.000    0.000 module.py:1731(__setattr__)
      100    0.000    0.000    0.000    0.000 {built-in method torch._C._cuda_isCurrentStreamCapturing}
      100    0.000    0.000    0.000    0.000 __init__.py:34(is_built)
      100    0.000    0.000    0.000    0.000 profiler.py:676(__init__)
      200    0.000    0.000    0.000    0.000 grad_mode.py:135(__enter__)
      300    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}
      100    0.000    0.000    0.000    0.000 os.py:758(decode)
      100    0.000    0.000    0.000    0.000 os.py:754(encode)
  500/400    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      100    0.000    0.000    0.000    0.000 contextlib.py:261(helper)
      200    0.000    0.000    0.000    0.000 __init__.py:106(_is_compiled)
      100    0.000    0.000    0.000    0.000 parameter.py:8(__instancecheck__)
      400    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}
      100    0.000    0.000    0.000    0.000 _contextlib.py:146(clone)
      100    0.000    0.000    0.000    0.000 {method 'decode' of 'bytes' objects}
      100    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)
      100    0.000    0.000    0.000    0.000 __init__.py:213(is_compiling)
      100    0.000    0.000    0.000    0.000 __init__.py:834(device_count)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 typing.py:271(inner)
      100    0.000    0.000    0.000    0.000 __init__.py:1424(debug)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      100    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      300    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 decorators.py:140(graph_break)
      200    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f7dfbd377c0}
      300    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}
      200    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
      100    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
      100    0.000    0.000    0.000    0.000 strategy.py:93(precision_plugin)
      100    0.000    0.000    0.000    0.000 optimizer.py:34(do_nothing_closure)
      300    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 typing.py:1375(cast)
      100    0.000    0.000    0.000    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f7dee3ee040}
      100    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
      100    0.000    0.000    0.000    0.000 __init__.py:129(annotate)



Profile stats for: [Strategy]SingleDeviceStrategy.training_step
         3405481 function calls (3394321 primitive calls) in 5.260 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000    5.281    0.053 strategy.py:379(training_step)
      100    0.005    0.000    5.275    0.053 FlavourClassificationTransformerEncoder.py:81(training_step)
 4800/100    0.008    0.000    5.181    0.052 module.py:1549(_wrapped_call_impl)
 4800/100    0.030    0.000    5.181    0.052 module.py:1555(_call_impl)
      100    0.007    0.000    5.179    0.052 FlavourClassificationTransformerEncoder.py:55(forward)
      300    0.033    0.000    5.103    0.017 EncoderBlock.py:57(forward)
      300    0.507    0.002    4.009    0.013 XFormersAttention.py:33(forward)
     6600    0.013    0.000    1.849    0.000 __init__.py:189(memory_efficient_attention)
     6600    0.034    0.000    1.831    0.000 __init__.py:457(_memory_efficient_attention)
     6600    0.021    0.000    1.737    0.000 function.py:558(apply)
     6600    0.100    0.000    1.663    0.000 {built-in method apply}
     6600    0.050    0.000    1.563    0.000 __init__.py:76(forward)
    13200    0.024    0.000    0.938    0.000 dispatch.py:55(_run_priority_list)
     6600    0.020    0.000    0.929    0.000 __init__.py:489(_memory_efficient_attention_forward_requires_grad)
      600    0.377    0.001    0.695    0.001 LayerNormalisation.py:17(forward)
     7600    0.014    0.000    0.648    0.000 {built-in method builtins.print}
    15200    0.016    0.000    0.634    0.000 redirect.py:644(write)
    15200    0.009    0.000    0.614    0.000 wandb_run.py:2304(<lambda>)
    15200    0.015    0.000    0.605    0.000 wandb_run.py:390(wrapper_fn)
    15200    0.026    0.000    0.587    0.000 wandb_run.py:1429(_console_raw_callback)
    15200    0.069    0.000    0.550    0.000 interface.py:749(publish_output_raw)
    26400    0.222    0.000    0.519    0.000 common.py:348(not_supported_reasons)
     6600    0.013    0.000    0.509    0.000 dispatch.py:146(_dispatch_bw)
     6600    0.006    0.000    0.463    0.000 dispatch.py:126(_dispatch_fw)
     6300    0.010    0.000    0.409    0.000 __init__.py:1436(info)
     6300    0.009    0.000    0.396    0.000 __init__.py:1565(_log)
    46200    0.042    0.000    0.387    0.000 __init__.py:438(get_device_capability)
    15200    0.023    0.000    0.358    0.000 interface_shared.py:76(_publish_output_raw)
    46200    0.042    0.000    0.344    0.000 __init__.py:455(get_device_properties)
    15200    0.018    0.000    0.329    0.000 interface_sock.py:45(_publish)
     6600    0.017    0.000    0.306    0.000 cutlass.py:211(apply)
     6600    0.014    0.000    0.303    0.000 attn_bias.py:707(from_seqlens)
    15200    0.019    0.000    0.299    0.000 sock_client.py:219(send_record_publish)
     6600    0.038    0.000    0.289    0.000 cutlass.py:275(apply_bmhk)
     6300    0.005    0.000    0.275    0.000 __init__.py:1591(handle)
    15200    0.006    0.000    0.271    0.000 sock_client.py:153(send_server_request)
     6300    0.009    0.000    0.269    0.000 __init__.py:1645(callHandlers)
    15200    0.037    0.000    0.265    0.000 sock_client.py:145(_send_message)
     6300    0.007    0.000    0.260    0.000 __init__.py:939(handle)
     6600    0.029    0.000    0.258    0.000 flash.py:752(not_supported_reasons)
    13200    0.012    0.000    0.258    0.000 common.py:450(not_supported_reasons)
     6300    0.004    0.000    0.244    0.000 __init__.py:1178(emit)
      300    0.089    0.000    0.241    0.001 FFN.py:16(forward)
     6300    0.007    0.000    0.240    0.000 __init__.py:1071(emit)
     6600    0.014    0.000    0.228    0.000 attn_bias.py:350(from_seqlens)
     6600    0.015    0.000    0.225    0.000 cutlass.py:377(not_supported_reasons)
     6700    0.008    0.000    0.222    0.000 _ops.py:1047(__call__)
    19800    0.219    0.000    0.219    0.000 {method 'reshape' of 'torch._C.TensorBase' objects}
     6600    0.017    0.000    0.217    0.000 flash.py:629(not_supported_reasons)
     6600    0.012    0.000    0.213    0.000 cutlass.py:326(not_supported_reasons)
     6600    0.213    0.000    0.213    0.000 {built-in method torch._ops.aten._efficient_attention_forward}
     6600    0.014    0.000    0.210    0.000 attn_bias.py:329(_get_seqstart)
    46200    0.090    0.000    0.203    0.000 _utils.py:9(_get_device_index)
    15200    0.021    0.000    0.192    0.000 sock_client.py:121(_sendall_with_error_handle)
     6602    0.189    0.000    0.189    0.000 {built-in method torch.tensor}
    15200    0.167    0.000    0.167    0.000 {method 'send' of '_socket.socket' objects}
     6300    0.008    0.000    0.156    0.000 __init__.py:1060(flush)
    13200    0.043    0.000    0.151    0.000 cutlass.py:49(_minimum_gemm_alignment)
     6300    0.142    0.000    0.142    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
     2000    0.005    0.000    0.135    0.000 linear.py:116(forward)
     2000    0.127    0.000    0.127    0.000 {built-in method torch._C._nn.linear}
     7700    0.120    0.000    0.120    0.000 {method 'item' of 'torch._C.TensorBase' objects}
     6600    0.052    0.000    0.099    0.000 common.py:120(validate_inputs)
    15200    0.014    0.000    0.096    0.000 well_known_types.py:172(GetCurrentTime)
     7200    0.093    0.000    0.093    0.000 {method 'sum' of 'torch._C.TensorBase' objects}
    46200    0.082    0.000    0.092    0.000 _utils.py:764(_get_device_index)
     6300    0.006    0.000    0.089    0.000 __init__.py:1550(makeRecord)
     7200    0.087    0.000    0.087    0.000 {built-in method torch.zeros}
     6300    0.038    0.000    0.084    0.000 __init__.py:282(__init__)
     6300    0.003    0.000    0.076    0.000 __init__.py:916(format)
591279/590251    0.068    0.000    0.074    0.000 {built-in method builtins.isinstance}
    15200    0.024    0.000    0.073    0.000 well_known_types.py:242(FromDatetime)
     6300    0.009    0.000    0.073    0.000 __init__.py:650(format)
     4600    0.070    0.000    0.070    0.000 {method 'any' of 'torch._C.TensorBase' objects}
    46200    0.037    0.000    0.068    0.000 common.py:482(check_lastdim_alignment_stride1)
     4600    0.064    0.000    0.064    0.000 {built-in method torch.isnan}
     6600    0.061    0.000    0.061    0.000 __init__.py:503(_detect_lse_packed_or_raise)
    13200    0.021    0.000    0.060    0.000 attn_bias.py:90(_get_default_bias_device)
      100    0.008    0.000    0.058    0.001 module.py:382(log)
    19800    0.055    0.000    0.055    0.000 {method 'unsqueeze' of 'torch._C.TensorBase' objects}
     6300    0.011    0.000    0.045    0.000 __init__.py:582(formatTime)
     6600    0.012    0.000    0.045    0.000 utils.py:31(unwrap_dead_wrappers)
    46200    0.041    0.000    0.041    0.000 {built-in method torch.cuda._get_device_properties}
     6600    0.005    0.000    0.039    0.000 __init__.py:115(is_available)
    19900    0.013    0.000    0.036    0.000 {built-in method builtins.any}
    52800    0.014    0.000    0.035    0.000 __init__.py:834(device_count)
      900    0.002    0.000    0.034    0.000 dropout.py:58(forward)
    99002    0.033    0.000    0.033    0.000 {method 'stride' of 'torch._C.TensorBase' objects}
    59400    0.015    0.000    0.033    0.000 utils.py:33(<genexpr>)
      900    0.004    0.000    0.032    0.000 functional.py:1279(dropout)
     6600    0.032    0.000    0.032    0.000 {method 'transpose' of 'torch._C.TensorBase' objects}
    13200    0.029    0.000    0.029    0.000 common.py:96(normalize_bmhk)
    26400    0.017    0.000    0.029    0.000 common.py:331(shape_not_supported_reasons)
    46200    0.012    0.000    0.029    0.000 __init__.py:284(_lazy_init)
    15200    0.027    0.000    0.027    0.000 enum_type_wrapper.py:92(__getattr__)
    15200    0.026    0.000    0.026    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
      900    0.026    0.000    0.026    0.000 {built-in method torch.dropout}
     6600    0.003    0.000    0.025    0.000 __init__.py:111(_nvml_based_avail)
    59400    0.015    0.000    0.024    0.000 __init__.py:106(_is_compiled)
     6300    0.023    0.000    0.023    0.000 {built-in method time.strftime}
    15200    0.021    0.000    0.023    0.000 calendar.py:655(timegm)
     6300    0.014    0.000    0.022    0.000 __init__.py:1514(findCaller)
     6600    0.003    0.000    0.022    0.000 os.py:771(getenv)
     6600    0.003    0.000    0.019    0.000 _collections_abc.py:760(get)
    26400    0.010    0.000    0.017    0.000 common.py:32(is_available)
    46200    0.010    0.000    0.017    0.000 __init__.py:237(is_initialized)
    26400    0.009    0.000    0.017    0.000 cutlass.py:87(_get_tensor_bias)
  405/401    0.001    0.000    0.017    0.000 apply_func.py:23(apply_to_collection)
      100    0.004    0.000    0.016    0.000 result.py:355(log)
    66000    0.016    0.000    0.016    0.000 common.py:71(device)
     6600    0.006    0.000    0.016    0.000 os.py:674(__getitem__)
     6300    0.006    0.000    0.015    0.000 posixpath.py:117(splitext)
    15200    0.015    0.000    0.015    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
    13201    0.006    0.000    0.015    0.000 {built-in method builtins.all}
    30400    0.015    0.000    0.015    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
    52800    0.014    0.000    0.014    0.000 {built-in method builtins.max}
    13200    0.004    0.000    0.014    0.000 cutlass.py:98(_check_bias_alignment)
      700    0.014    0.000    0.014    0.000 {method 'mean' of 'torch._C.TensorBase' objects}
    21500    0.014    0.000    0.014    0.000 {built-in method posix.getpid}
     6600    0.013    0.000    0.013    0.000 dispatch.py:79(_dispatch_fw_priority_list)
     6600    0.008    0.000    0.013    0.000 attn_bias.py:656(to)
    78652    0.013    0.000    0.013    0.000 {built-in method builtins.hasattr}
    15200    0.013    0.000    0.013    0.000 interface_sock.py:41(_assign)
    58242    0.012    0.000    0.012    0.000 {built-in method builtins.getattr}
      300    0.001    0.000    0.012    0.000 _tensor.py:982(__format__)
     6300    0.006    0.000    0.012    0.000 posixpath.py:140(basename)
     6600    0.009    0.000    0.012    0.000 cutlass.py:136(_custom_mask_type)
     6300    0.011    0.000    0.011    0.000 {built-in method time.localtime}
      300    0.011    0.000    0.011    0.000 {built-in method torch.stack}
  270/201    0.001    0.000    0.011    0.000 apply_func.py:84(_apply_to_collection_slow)
      600    0.011    0.000    0.011    0.000 {method 'var' of 'torch._C.TensorBase' objects}
     6300    0.003    0.000    0.010    0.000 __init__.py:634(formatMessage)
    19800    0.010    0.000    0.010    0.000 {built-in method torch._C._functorch.unwrap_if_dead}
    13200    0.006    0.000    0.009    0.000 flash.py:511(_check_needs_no_topleft)
     9700    0.009    0.000    0.009    0.000 module.py:1716(__getattr__)
      100    0.002    0.000    0.009    0.000 result.py:502(reset)
     6300    0.006    0.000    0.008    0.000 genericpath.py:121(_splitext)
      300    0.001    0.000    0.008    0.000 activation.py:103(forward)
     6600    0.008    0.000    0.008    0.000 cutlass.py:66(_get_seqlen_info)
    26400    0.008    0.000    0.008    0.000 common.py:131(<genexpr>)
    15200    0.008    0.000    0.008    0.000 {built-in method utcnow}
      100    0.002    0.000    0.008    0.000 functional.py:3014(cross_entropy)
     6300    0.002    0.000    0.008    0.000 __init__.py:432(format)
    12600    0.004    0.000    0.007    0.000 __init__.py:896(acquire)
    15200    0.007    0.000    0.007    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
      600    0.007    0.000    0.007    0.000 {built-in method torch.sqrt}
    26400    0.007    0.000    0.007    0.000 common.py:192(<genexpr>)
    15200    0.007    0.000    0.007    0.000 {built-in method _struct.pack}
      300    0.000    0.000    0.007    0.000 functional.py:1489(relu)
       99    0.002    0.000    0.007    0.000 result.py:256(reset)
      300    0.007    0.000    0.007    0.000 {built-in method torch.relu}
    46200    0.007    0.000    0.007    0.000 {built-in method torch._C._cuda_isInBadFork}
    26400    0.006    0.000    0.006    0.000 common.py:122(<genexpr>)
     6300    0.002    0.000    0.006    0.000 __init__.py:628(usesTime)
      100    0.001    0.000    0.006    0.000 signature_utils.py:18(is_param_in_hook_signature)
     6600    0.004    0.000    0.006    0.000 os.py:754(encode)
      398    0.006    0.000    0.006    0.000 {method 'clone' of 'torch._C.TensorBase' objects}
    26400    0.006    0.000    0.006    0.000 common.py:152(<genexpr>)
      100    0.001    0.000    0.006    0.000 profiler.py:693(__exit__)
     6300    0.006    0.000    0.006    0.000 __init__.py:429(_format)
     6600    0.005    0.000    0.005    0.000 {built-in method torch._C._are_functorch_transforms_active}
      300    0.005    0.000    0.005    0.000 {built-in method torch.all}
      100    0.001    0.000    0.005    0.000 inspect.py:1129(getfullargspec)
      100    0.005    0.000    0.005    0.000 {built-in method torch._C._nn.cross_entropy_loss}
       99    0.001    0.000    0.005    0.000 metric.py:689(reset)
      100    0.000    0.000    0.005    0.000 result.py:424(update_metrics)
    44304    0.005    0.000    0.005    0.000 {built-in method builtins.len}
      100    0.000    0.000    0.005    0.000 result.py:264(forward)
    40964    0.005    0.000    0.005    0.000 {method 'append' of 'list' objects}
    46302    0.005    0.000    0.005    0.000 _jit_internal.py:1130(is_scripting)
    12600    0.004    0.000    0.005    0.000 __init__.py:903(release)
      900    0.005    0.000    0.005    0.000 {method 'view' of 'torch._C.TensorBase' objects}
    26400    0.005    0.000    0.005    0.000 {built-in method builtins.min}
    13200    0.005    0.000    0.005    0.000 __init__.py:461(<genexpr>)
      100    0.001    0.000    0.005    0.000 metric.py:476(wrapped_func)
      100    0.001    0.000    0.004    0.000 module.py:654(__to_tensor)
    18900    0.004    0.000    0.004    0.000 {method 'rfind' of 'str' objects}
     6300    0.003    0.000    0.004    0.000 __init__.py:160(<lambda>)
    21500    0.004    0.000    0.004    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      100    0.000    0.000    0.004    0.000 _ops.py:887(__call__)
     4800    0.004    0.000    0.004    0.000 {built-in method torch._C._get_tracing_state}
      514    0.002    0.000    0.004    0.000 typing.py:719(__instancecheck__)
     6600    0.002    0.000    0.004    0.000 __init__.py:1148(are_deterministic_algorithms_enabled)
    13200    0.004    0.000    0.004    0.000 attn_bias.py:316(to)
     6300    0.003    0.000    0.004    0.000 __init__.py:421(usesTime)
    13200    0.003    0.000    0.004    0.000 __init__.py:69(_unserialize_op)
  202/101    0.001    0.000    0.004    0.000 inspect.py:2246(_signature_from_callable)
    19800    0.004    0.000    0.004    0.000 flash.py:533(_check_strides_for_bmghk)
     6600    0.002    0.000    0.004    0.000 os.py:758(decode)
    12600    0.004    0.000    0.004    0.000 __init__.py:791(filter)
      100    0.001    0.000    0.003    0.000 result.py:207(update)
    12600    0.003    0.000    0.003    0.000 {method 'acquire' of '_thread.RLock' objects}
      300    0.000    0.000    0.003    0.000 {built-in method builtins.next}
     6300    0.003    0.000    0.003    0.000 threading.py:1358(current_thread)
      100    0.000    0.000    0.003    0.000 _ops.py:943(_must_dispatch_in_python)
     6300    0.002    0.000    0.003    0.000 __init__.py:119(getLevelName)
      100    0.000    0.000    0.003    0.000 _pytree.py:1181(tree_any)
      101    0.001    0.000    0.003    0.000 inspect.py:2152(_signature_from_function)
     6300    0.002    0.000    0.003    0.000 posixpath.py:41(_get_sep)
     6300    0.002    0.000    0.003    0.000 posixpath.py:52(normcase)
    15200    0.003    0.000    0.003    0.000 {method '__exit__' of '_thread.lock' objects}
     6300    0.003    0.000    0.003    0.000 __init__.py:1689(isEnabledFor)
    15200    0.003    0.000    0.003    0.000 {built-in method time.monotonic}
      200    0.001    0.000    0.003    0.000 precision.py:167(train_step_context)
      100    0.003    0.000    0.003    0.000 {built-in method torch.argmax}
      514    0.001    0.000    0.003    0.000 typing.py:848(__subclasscheck__)
     6300    0.003    0.000    0.003    0.000 __init__.py:358(getMessage)
     6600    0.002    0.000    0.002    0.000 function.py:34(save_for_backward)
      100    0.000    0.000    0.002    0.000 contextlib.py:114(__enter__)
  700/200    0.001    0.000    0.002    0.000 _pytree.py:874(tree_iter)
     6600    0.002    0.000    0.002    0.000 function.py:591(_is_setup_context_defined)
    13203    0.002    0.000    0.002    0.000 {method 'get' of 'dict' objects}
    13200    0.002    0.000    0.002    0.000 cutlass.py:41(_uses_tensorcores)
      248    0.002    0.000    0.002    0.000 apply_func.py:17(is_dataclass_instance)
      100    0.001    0.000    0.002    0.000 fx_validator.py:191(check_logging_and_get_default_levels)
    18900    0.002    0.000    0.002    0.000 {built-in method posix.fspath}
     6300    0.002    0.000    0.002    0.000 {built-in method sys._getframe}
      100    0.001    0.000    0.002    0.000 <string>:2(__init__)
    15200    0.002    0.000    0.002    0.000 {method 'toordinal' of 'datetime.date' objects}
     6600    0.002    0.000    0.002    0.000 {method 'encode' of 'str' objects}
      100    0.000    0.000    0.002    0.000 profiler.py:687(__enter__)
      300    0.000    0.000    0.002    0.000 trainer.py:1381(training)
    12600    0.002    0.000    0.002    0.000 {built-in method _thread.get_ident}
      100    0.000    0.000    0.002    0.000 trainer.py:1642(_results)
      372    0.000    0.000    0.002    0.000 abc.py:117(__instancecheck__)
     6600    0.002    0.000    0.002    0.000 {method 'decode' of 'bytes' objects}
      200    0.001    0.000    0.002    0.000 result.py:90(_generate_sync_fn)
     6600    0.002    0.000    0.002    0.000 {built-in method torch._C._get_deterministic_algorithms}
     6300    0.001    0.000    0.001    0.000 {method 'find' of 'str' objects}
     6300    0.001    0.000    0.001    0.000 {built-in method time.time}
     6300    0.001    0.000    0.001    0.000 threading.py:1093(name)
      372    0.001    0.000    0.001    0.000 {built-in method _abc._abc_instancecheck}
      100    0.000    0.000    0.001    0.000 result.py:57(__post_init__)
    15200    0.001    0.000    0.001    0.000 {built-in method builtins.ord}
    13200    0.001    0.000    0.001    0.000 __init__.py:63(_serialize_op)
        1    0.000    0.000    0.001    0.001 result.py:415(register_key)
      100    0.001    0.000    0.001    0.000 memory.py:24(recursive_detach)
      100    0.001    0.000    0.001    0.000 {built-in method torch._ops.profiler._record_function_enter_new}
      900    0.001    0.000    0.001    0.000 _VF.py:26(__getattr__)
    12600    0.001    0.000    0.001    0.000 {method 'release' of '_thread.RLock' objects}
      100    0.001    0.000    0.001    0.000 precision.py:68(forward_context)
      300    0.001    0.000    0.001    0.000 enums.py:81(__eq__)
      100    0.001    0.000    0.001    0.000 module.py:262(current_epoch)
      398    0.001    0.000    0.001    0.000 {method 'detach' of 'torch._C.TensorBase' objects}
      200    0.000    0.000    0.001    0.000 contextlib.py:123(__exit__)
      521    0.000    0.000    0.001    0.000 {built-in method builtins.issubclass}
      100    0.000    0.000    0.001    0.000 trainer.py:1564(_active_loop)
      100    0.001    0.000    0.001    0.000 {built-in method torch._ops.profiler.}
     6300    0.001    0.000    0.001    0.000 process.py:189(name)
      303    0.001    0.000    0.001    0.000 inspect.py:2498(__init__)
      100    0.001    0.000    0.001    0.000 container.py:317(__iter__)
        1    0.000    0.000    0.001    0.001 result.py:306(to)
      514    0.000    0.000    0.001    0.000 abc.py:121(__subclasscheck__)
     1322    0.001    0.000    0.001    0.000 result.py:294(__setattr__)
      400    0.000    0.000    0.001    0.000 _pytree.py:656(_is_leaf)
     6300    0.001    0.000    0.001    0.000 process.py:37(current_process)
     6600    0.001    0.000    0.001    0.000 dispatch.py:142(_is_cutlassB_faster_than_flash)
      700    0.000    0.000    0.001    0.000 _pytree.py:649(_get_node_type)
      514    0.001    0.000    0.001    0.000 {built-in method _abc._abc_subclasscheck}
     6600    0.001    0.000    0.001    0.000 dispatch.py:27(_get_use_fa3)
      100    0.000    0.000    0.001    0.000 contextlib.py:261(helper)
      102    0.000    0.000    0.001    0.000 inspect.py:2781(__init__)
      100    0.001    0.000    0.001    0.000 fx_validator.py:151(check_logging)
      100    0.000    0.000    0.001    0.000 result.py:122(__post_init__)
      100    0.001    0.000    0.001    0.000 contextlib.py:688(__init__)
      100    0.000    0.000    0.001    0.000 result.py:127(_parse_reduce_fx)
      227    0.000    0.000    0.001    0.000 {built-in method builtins.setattr}
      100    0.001    0.000    0.001    0.000 trainer.py:1467(current_epoch)
      100    0.001    0.000    0.001    0.000 {method 'squeeze' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)
        1    0.000    0.000    0.000    0.000 result.py:187(__init__)
      700    0.000    0.000    0.000    0.000 _pytree.py:638(_is_namedtuple_instance)
     1502    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}
      104    0.000    0.000    0.000    0.000 grad_mode.py:184(__init__)
      100    0.000    0.000    0.000    0.000 _reduction.py:7(get_enum)
      600    0.000    0.000    0.000    0.000 types.py:171(__get__)
      200    0.000    0.000    0.000    0.000 strategy.py:351(model)
      100    0.000    0.000    0.000    0.000 fit_loop.py:140(_results)
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 fx_validator.py:177(check_logging_levels)
      100    0.000    0.000    0.000    0.000 memory.py:40(detach_and_move)
      202    0.000    0.000    0.000    0.000 {method 'to' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.000    0.000 logger_connector.py:213(should_reset_tensors)
      100    0.000    0.000    0.000    0.000 result.py:148(sync)
     58/4    0.000    0.000    0.000    0.000 copy.py:128(deepcopy)
      100    0.000    0.000    0.000    0.000 profiler.py:676(__init__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.iter}
       99    0.000    0.000    0.000    0.000 <string>:2(__eq__)
      248    0.000    0.000    0.000    0.000 dataclasses.py:1047(is_dataclass)
      303    0.000    0.000    0.000    0.000 enum.py:358(__call__)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 _pytree.py:423(_dict_flatten)
      304    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}
        2    0.000    0.000    0.000    0.000 metric.py:196(add_state)
      100    0.000    0.000    0.000    0.000 _ops.py:945(<lambda>)
      404    0.000    0.000    0.000    0.000 inspect.py:2830(<genexpr>)
        2    0.000    0.000    0.000    0.000 _tensor.py:83(__deepcopy__)
        1    0.000    0.000    0.000    0.000 metric.py:101(__init__)
      100    0.000    0.000    0.000    0.000 result.py:74(op)
      101    0.000    0.000    0.000    0.000 result.py:536(_get_default_dtype)
      300    0.000    0.000    0.000    0.000 {method '__format__' of 'int' objects}
      100    0.000    0.000    0.000    0.000 module.py:215(trainer)
      300    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 <string>:1(<lambda>)
      100    0.000    0.000    0.000    0.000 {built-in method torch.numel}
      100    0.000    0.000    0.000    0.000 grad_mode.py:196(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method torch.is_floating_point}
      600    0.000    0.000    0.000    0.000 enum.py:792(value)
      248    0.000    0.000    0.000    0.000 apply_func.py:11(is_namedtuple)
      300    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C.TensorBase' objects}
      700    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      200    0.000    0.000    0.000    0.000 _pytree.py:395(_tuple_flatten)
      3/2    0.000    0.000    0.000    0.000 copy.py:258(_reconstruct)
      100    0.000    0.000    0.000    0.000 grad_mode.py:193(__enter__)
      100    0.000    0.000    0.000    0.000 fx_validator.py:166(get_default_logging_levels)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      303    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}
      202    0.000    0.000    0.000    0.000 inspect.py:159(isfunction)
      101    0.000    0.000    0.000    0.000 typing.py:271(inner)
        4    0.000    0.000    0.000    0.000 apply_func.py:71(move_data_to_device)
      303    0.000    0.000    0.000    0.000 enum.py:670(__new__)
      101    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}
      5/4    0.000    0.000    0.000    0.000 copy.py:226(_deepcopy_dict)
        2    0.000    0.000    0.000    0.000 storage.py:907(_deepcopy)
      605    0.000    0.000    0.000    0.000 inspect.py:2548(name)
      100    0.000    0.000    0.000    0.000 result.py:340(_extract_batch_size)
        4    0.000    0.000    0.000    0.000 apply_func.py:91(batch_to)
        1    0.000    0.000    0.000    0.000 inspect.py:3111(signature)
      102    0.000    0.000    0.000    0.000 {built-in method torch.get_default_dtype}
        1    0.000    0.000    0.000    0.000 inspect.py:2859(from_callable)
      100    0.000    0.000    0.000    0.000 strategy.py:93(precision_plugin)
      203    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
      106    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}
      300    0.000    0.000    0.000    0.000 result.py:70(op)
      108    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f7dfbd377c0}
        2    0.000    0.000    0.000    0.000 storage.py:140(__deepcopy__)
      200    0.000    0.000    0.000    0.000 result.py:60(should)
      200    0.000    0.000    0.000    0.000 result.py:80(group)
      304    0.000    0.000    0.000    0.000 inspect.py:2560(kind)
        2    0.000    0.000    0.000    0.000 storage.py:156(clone)
      100    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_variadic}
      100    0.000    0.000    0.000    0.000 result.py:97(__call__)
      300    0.000    0.000    0.000    0.000 inspect.py:2556(annotation)
      302    0.000    0.000    0.000    0.000 inspect.py:2552(default)
      100    0.000    0.000    0.000    0.000 result.py:143(sync)
      100    0.000    0.000    0.000    0.000 contextlib.py:694(__exit__)
        1    0.000    0.000    0.000    0.000 metric.py:475(_wrap_update)
      100    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 __init__.py:129(annotate)
      100    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}
      100    0.000    0.000    0.000    0.000 {method '__exit__' of 'torch._C.DisableTorchFunctionSubclass' objects}
      100    0.000    0.000    0.000    0.000 contextlib.py:691(__enter__)
        2    0.000    0.000    0.000    0.000 dataclasses.py:1024(fields)
      204    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
        2    0.000    0.000    0.000    0.000 {method 'copy_' of 'torch._C.StorageBase' objects}
      100    0.000    0.000    0.000    0.000 typing.py:1375(cast)
      101    0.000    0.000    0.000    0.000 inspect.py:2865(parameters)
      100    0.000    0.000    0.000    0.000 inspect.py:2869(return_annotation)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      2/1    0.000    0.000    0.000    0.000 copy.py:209(_deepcopy_tuple)
        2    0.000    0.000    0.000    0.000 functools.py:35(update_wrapper)
        1    0.000    0.000    0.000    0.000 module.py:429(__init__)
        2    0.000    0.000    0.000    0.000 _tensor.py:242(_typed_storage)
      100    0.000    0.000    0.000    0.000 result.py:101(no_op)
      110    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 inspect.py:1840(_signature_bound_method)
      2/1    0.000    0.000    0.000    0.000 copy.py:210(<listcomp>)
        1    0.000    0.000    0.000    0.000 result.py:273(_wrap_compute)
       13    0.000    0.000    0.000    0.000 copy.py:242(_keep_alive)
       19    0.000    0.000    0.000    0.000 dataclasses.py:1039(<genexpr>)
        2    0.000    0.000    0.000    0.000 {method 'set_' of 'torch._C.TensorBase' objects}
      107    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        2    0.000    0.000    0.000    0.000 storage.py:712(_new_wrapped_storage)
        2    0.000    0.000    0.000    0.000 grad_mode.py:80(__enter__)
        1    0.000    0.000    0.000    0.000 result.py:171(is_max_reduction)
        6    0.000    0.000    0.000    0.000 copy.py:263(<genexpr>)
        3    0.000    0.000    0.000    0.000 {method '__reduce_ex__' of 'object' objects}
        2    0.000    0.000    0.000    0.000 {method 'new_empty' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 inspect.py:494(unwrap)
        4    0.000    0.000    0.000    0.000 storage.py:629(__init__)
        5    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 inspect.py:2873(replace)
        2    0.000    0.000    0.000    0.000 grad_mode.py:84(__exit__)
        4    0.000    0.000    0.000    0.000 storage.py:557(__new__)
        2    0.000    0.000    0.000    0.000 grad_mode.py:75(__init__)
        2    0.000    0.000    0.000    0.000 apply_func.py:69(<genexpr>)
        2    0.000    0.000    0.000    0.000 functools.py:65(wraps)
        2    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}
        2    0.000    0.000    0.000    0.000 _contextlib.py:154(__new__)
       24    0.000    0.000    0.000    0.000 copy.py:182(_deepcopy_atomic)
        2    0.000    0.000    0.000    0.000 {method 'untyped_storage' of 'torch._C.TensorBase' objects}
       14    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
        2    0.000    0.000    0.000    0.000 {built-in method torch._C._has_storage}
        2    0.000    0.000    0.000    0.000 copyreg.py:94(__newobj__)
        2    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 inspect.py:514(_is_wrapper)
        3    0.000    0.000    0.000    0.000 _collections_abc.py:315(__subclasshook__)
        2    0.000    0.000    0.000    0.000 {method 'contiguous' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 {method 'nbytes' of 'torch._C.StorageBase' objects}
        2    0.000    0.000    0.000    0.000 {method 'is_conj' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 {method 'is_neg' of 'torch._C.TensorBase' objects}
        7    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 {method 'storage_offset' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 result.py:175(is_min_reduction)
        1    0.000    0.000    0.000    0.000 result.py:163(is_mean_reduction)
        2    0.000    0.000    0.000    0.000 storage.py:30(__init__)
        1    0.000    0.000    0.000    0.000 {method '__setstate__' of 'functools.partial' objects}
        2    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {built-in method sys.getrecursionlimit}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_before_zero_grad
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 callback.py:285(on_before_zero_grad)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_before_zero_grad
         1000 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:285(on_before_zero_grad)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_before_zero_grad
         1000 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 callback.py:285(on_before_zero_grad)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_zero_grad
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:285(on_before_zero_grad)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_before_zero_grad
         700 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 hooks.py:259(on_before_zero_grad)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.optimizer_zero_grad
         6934 function calls (6434 primitive calls) in 0.011 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000    0.011    0.000 module.py:1310(optimizer_zero_grad)
      100    0.000    0.000    0.010    0.000 _compile.py:21(inner)
      100    0.001    0.000    0.009    0.000 eval_frame.py:596(_fn)
      100    0.002    0.000    0.008    0.000 optimizer.py:911(zero_grad)
      100    0.000    0.000    0.003    0.000 profiler.py:693(__exit__)
      100    0.000    0.000    0.002    0.000 profiler.py:687(__enter__)
      100    0.000    0.000    0.002    0.000 _ops.py:887(__call__)
      100    0.000    0.000    0.002    0.000 _ops.py:1047(__call__)
      100    0.002    0.000    0.002    0.000 {built-in method torch._ops.profiler._record_function_enter_new}
      100    0.000    0.000    0.002    0.000 _ops.py:943(_must_dispatch_in_python)
      100    0.000    0.000    0.002    0.000 _pytree.py:1181(tree_any)
      100    0.000    0.000    0.002    0.000 {built-in method builtins.any}
  700/200    0.001    0.000    0.001    0.000 _pytree.py:874(tree_iter)
      700    0.000    0.000    0.001    0.000 _pytree.py:649(_get_node_type)
      100    0.000    0.000    0.001    0.000 profiler.py:676(__init__)
      400    0.000    0.000    0.001    0.000 _pytree.py:656(_is_leaf)
      700    0.000    0.000    0.000    0.000 _pytree.py:638(_is_namedtuple_instance)
      100    0.000    0.000    0.000    0.000 {built-in method torch._ops.profiler.}
      103    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 typing.py:271(inner)
      100    0.000    0.000    0.000    0.000 _pytree.py:423(_dict_flatten)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      107    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 _ops.py:945(<lambda>)
      200    0.000    0.000    0.000    0.000 {built-in method torch._C._dynamo.eval_frame.set_eval_frame}
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 decorators.py:38(disable)
      300    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      200    0.000    0.000    0.000    0.000 _pytree.py:395(_tuple_flatten)
      700    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      102    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        1    0.000    0.000    0.000    0.000 eval_frame.py:565(__call__)
        1    0.000    0.000    0.000    0.000 eval_frame.py:562(__init__)
      100    0.000    0.000    0.000    0.000 {method '__exit__' of 'torch._C.DisableTorchFunctionSubclass' objects}
      100    0.000    0.000    0.000    0.000 __init__.py:129(annotate)
        1    0.000    0.000    0.000    0.000 eval_frame.py:265(__init__)
      100    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 functools.py:35(update_wrapper)
      100    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        3    0.000    0.000    0.000    0.000 eval_frame.py:240(innermost_fn)
        5    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}
        1    0.000    0.000    0.000    0.000 functools.py:65(wraps)
        1    0.000    0.000    0.000    0.000 inspect.py:73(isclass)
        1    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}
        3    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
        1    0.000    0.000    0.000    0.000 eval_frame.py:232(nothing)
        1    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.id}



Profile stats for: [Strategy]SingleDeviceStrategy.backward
         3500 function calls (3400 primitive calls) in 4.366 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000    4.366    0.044 strategy.py:191(backward)
      100    0.000    0.000    4.366    0.044 precision.py:45(pre_backward)
      100    0.000    0.000    4.365    0.044 call.py:185(_call_callback_hooks)
      100    0.000    0.000    4.364    0.044 contextlib.py:114(__enter__)
      100    0.000    0.000    4.364    0.044 {built-in method builtins.next}
      100    0.000    0.000    4.364    0.044 profiler.py:55(profile)
      100    0.000    0.000    4.364    0.044 advanced.py:65(start)
      100    4.364    0.044    4.364    0.044 {method 'enable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 module.py:1731(__setattr__)
      100    0.000    0.000    0.000    0.000 early_stopping.py:130(state_key)
  300/200    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      100    0.000    0.000    0.000    0.000 callback.py:48(_generate_state_key)
      100    0.000    0.000    0.000    0.000 parameter.py:8(__instancecheck__)
      100    0.000    0.000    0.000    0.000 contextlib.py:261(helper)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.repr}
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)
      100    0.000    0.000    0.000    0.000 __init__.py:1424(debug)
      100    0.000    0.000    0.000    0.000 strategy.py:93(precision_plugin)
      100    0.000    0.000    0.000    0.000 module.py:215(trainer)
      300    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      100    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
      100    0.000    0.000    0.000    0.000 strategy.py:345(pre_backward)
      300    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f7dee3ee040}
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.callable}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_before_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 callback.py:274(on_before_backward)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_before_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:274(on_before_backward)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_before_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:274(on_before_backward)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 callback.py:274(on_before_backward)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_before_backward
         700 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 hooks.py:280(on_before_backward)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_after_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 callback.py:277(on_after_backward)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)



Profile stats for: [Callback]TQDMProgressBar.on_after_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:277(on_after_backward)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_after_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:277(on_after_backward)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_after_backward
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:277(on_after_backward)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_after_backward
         700 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 hooks.py:289(on_after_backward)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_before_optimizer_step
         1000 function calls in 0.001 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:280(on_before_optimizer_step)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)



Profile stats for: [Callback]TQDMProgressBar.on_before_optimizer_step
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:280(on_before_optimizer_step)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_before_optimizer_step
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:280(on_before_optimizer_step)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_optimizer_step
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 callback.py:280(on_before_optimizer_step)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_before_optimizer_step
         700 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 hooks.py:298(on_before_optimizer_step)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.configure_gradient_clipping
         27500 function calls in 0.137 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000    0.137    0.001 module.py:1218(configure_gradient_clipping)
      100    0.002    0.000    0.136    0.001 module.py:1150(clip_gradients)
      100    0.000    0.000    0.131    0.001 precision.py:143(clip_gradients)
      100    0.003    0.000    0.130    0.001 precision.py:162(clip_grad_by_norm)
      100    0.004    0.000    0.127    0.001 clip_grad.py:19(_no_grad_wrapper)
      100    0.007    0.000    0.122    0.001 clip_grad.py:25(clip_grad_norm_)
      100    0.064    0.001    0.064    0.001 {built-in method torch._foreach_norm}
      100    0.014    0.000    0.014    0.000 {built-in method torch._C._linalg.linalg_vector_norm}
      100    0.000    0.000    0.009    0.000 _tensor.py:35(wrapped)
      100    0.002    0.000    0.009    0.000 _tensor.py:964(__rdiv__)
      100    0.007    0.000    0.007    0.000 {method 'reciprocal' of 'torch._C.TensorBase' objects}
      200    0.001    0.000    0.007    0.000 _foreach_utils.py:43(_has_foreach_support)
      100    0.005    0.000    0.005    0.000 {built-in method torch.stack}
      100    0.004    0.000    0.004    0.000 {built-in method torch._foreach_mul_}
      200    0.004    0.000    0.004    0.000 _foreach_utils.py:39(_device_has_foreach_support)
      100    0.003    0.000    0.004    0.000 clip_grad.py:53(<listcomp>)
      100    0.000    0.000    0.003    0.000 _contextlib.py:113(decorate_context)
      100    0.001    0.000    0.003    0.000 clip_grad.py:74(<listcomp>)
      100    0.001    0.000    0.003    0.000 enums.py:34(supported_type)
     4100    0.002    0.000    0.002    0.000 {method 'to' of 'torch._C.TensorBase' objects}
      100    0.002    0.000    0.002    0.000 {built-in method torch.clamp}
      200    0.001    0.000    0.002    0.000 {built-in method builtins.all}
      100    0.001    0.000    0.002    0.000 _foreach_utils.py:32(_group_tensors_by_device_and_dtype)
      100    0.001    0.000    0.001    0.000 {built-in method torch._C._group_tensors_by_device_and_dtype}
     8200    0.001    0.000    0.001    0.000 _foreach_utils.py:44(<genexpr>)
      100    0.001    0.000    0.001    0.000 enum.py:434(__iter__)
      100    0.000    0.000    0.001    0.000 {built-in method builtins.any}
      300    0.000    0.000    0.001    0.000 enums.py:36(<genexpr>)
      200    0.000    0.000    0.001    0.000 grad_mode.py:80(__enter__)
      400    0.000    0.000    0.001    0.000 grad_mode.py:184(__init__)
      200    0.000    0.000    0.001    0.000 enums.py:81(__eq__)
      200    0.000    0.000    0.000    0.000 grad_mode.py:84(__exit__)
      300    0.000    0.000    0.000    0.000 enum.py:438(<genexpr>)
     4100    0.000    0.000    0.000    0.000 precision.py:126(main_params)
      600    0.000    0.000    0.000    0.000 types.py:171(__get__)
      100    0.000    0.000    0.000    0.000 _contextlib.py:146(clone)
      200    0.000    0.000    0.000    0.000 grad_mode.py:75(__init__)
      200    0.000    0.000    0.000    0.000 _foreach_utils.py:8(_get_foreach_kernels_supported_devices)
      100    0.000    0.000    0.000    0.000 enum.py:358(__call__)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      400    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}
      200    0.000    0.000    0.000    0.000 _contextlib.py:154(__new__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 enum.py:670(__new__)
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      400    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      200    0.000    0.000    0.000    0.000 {built-in method torch._C._get_privateuse1_backend_name}
      600    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}
      400    0.000    0.000    0.000    0.000 module.py:215(trainer)
      600    0.000    0.000    0.000    0.000 enum.py:792(value)
      100    0.000    0.000    0.000    0.000 trainer.py:1129(precision_plugin)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      500    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      400    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
      100    0.000    0.000    0.000    0.000 module.py:230(fabric)
      200    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f7dfbd377c0}
      100    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}
      200    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function}
      100    0.000    0.000    0.000    0.000 strategy.py:93(precision_plugin)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_train_batch_end
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 callback.py:81(on_train_batch_end)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_train_batch_end
         87559 function calls (87555 primitive calls) in 0.089 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.003    0.000    0.089    0.001 tqdm_progress.py:269(on_train_batch_end)
      200    0.002    0.000    0.064    0.000 std.py:1325(refresh)
      200    0.001    0.000    0.061    0.000 std.py:1464(display)
      100    0.001    0.000    0.049    0.000 tqdm_progress.py:451(_update_n)
      200    0.001    0.000    0.031    0.000 std.py:457(print_status)
      200    0.002    0.000    0.029    0.000 std.py:1150(__str__)
      200    0.000    0.000    0.022    0.000 std.py:451(fp_write)
      400    0.001    0.000    0.021    0.000 utils.py:194(inner)
      100    0.002    0.000    0.021    0.000 std.py:1402(set_postfix)
      100    0.001    0.000    0.015    0.000 progress_bar.py:177(get_metrics)
      200    0.007    0.000    0.014    0.000 std.py:464(format_meter)
      200    0.004    0.000    0.014    0.000 std.py:1446(format_dict)
      200    0.000    0.000    0.012    0.000 redirect.py:644(write)
      200    0.000    0.000    0.012    0.000 wandb_run.py:2304(<lambda>)
      200    0.000    0.000    0.012    0.000 wandb_run.py:390(wrapper_fn)
      200    0.000    0.000    0.011    0.000 wandb_run.py:1429(_console_raw_callback)
      200    0.001    0.000    0.011    0.000 interface.py:749(publish_output_raw)
      200    0.006    0.000    0.009    0.000 utils.py:333(_screen_shape_linux)
      200    0.008    0.000    0.008    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      200    0.001    0.000    0.008    0.000 utils.py:378(disp_len)
      100    0.000    0.000    0.007    0.000 trainer.py:1632(progress_bar_metrics)
      200    0.000    0.000    0.007    0.000 utils.py:374(_text_width)
      200    0.002    0.000    0.007    0.000 {built-in method builtins.sum}
      200    0.000    0.000    0.007    0.000 interface_shared.py:76(_publish_output_raw)
      100    0.001    0.000    0.007    0.000 logger_connector.py:250(progress_bar_metrics)
      100    0.002    0.000    0.007    0.000 progress_bar.py:210(get_standard_metrics)
      200    0.000    0.000    0.006    0.000 interface_sock.py:45(_publish)
      200    0.000    0.000    0.006    0.000 sock_client.py:219(send_record_publish)
    19084    0.004    0.000    0.006    0.000 utils.py:375(<genexpr>)
      200    0.000    0.000    0.006    0.000 sock_client.py:153(send_server_request)
      200    0.001    0.000    0.005    0.000 sock_client.py:145(_send_message)
      100    0.000    0.000    0.005    0.000 logger_connector.py:229(metrics)
      100    0.001    0.000    0.004    0.000 utilities.py:25(_version)
      200    0.000    0.000    0.004    0.000 sock_client.py:121(_sendall_with_error_handle)
      100    0.001    0.000    0.004    0.000 result.py:476(metrics)
      200    0.004    0.000    0.004    0.000 {method 'send' of '_socket.socket' objects}
      100    0.001    0.000    0.003    0.000 wandb.py:572(version)
      400    0.002    0.000    0.003    0.000 utils.py:273(_is_ascii)
      200    0.003    0.000    0.003    0.000 {built-in method fcntl.ioctl}
      100    0.001    0.000    0.002    0.000 wandb_run.py:357(wrapper)
      300    0.000    0.000    0.002    0.000 trainer.py:1642(_results)
      400    0.001    0.000    0.002    0.000 {method 'format' of 'str' objects}
      600    0.000    0.000    0.002    0.000 trainer.py:1381(training)
      200    0.000    0.000    0.002    0.000 well_known_types.py:172(GetCurrentTime)
    18884    0.002    0.000    0.002    0.000 {built-in method unicodedata.east_asian_width}
      200    0.001    0.000    0.002    0.000 std.py:102(acquire)
      600    0.001    0.000    0.001    0.000 enums.py:81(__eq__)
      200    0.000    0.000    0.001    0.000 well_known_types.py:242(FromDatetime)
      100    0.001    0.000    0.001    0.000 wandb_run.py:881(id)
       99    0.001    0.000    0.001    0.000 tqdm_progress.py:46(format_num)
      100    0.001    0.000    0.001    0.000 tqdm_progress.py:425(_should_update)
     2096    0.000    0.000    0.001    0.000 {built-in method builtins.isinstance}
      300    0.000    0.000    0.001    0.000 trainer.py:1564(_active_loop)
      200    0.000    0.000    0.001    0.000 utils.py:347(<listcomp>)
    17400    0.001    0.000    0.001    0.000 {built-in method builtins.ord}
      400    0.001    0.000    0.001    0.000 std.py:400(format_interval)
      100    0.001    0.000    0.001    0.000 result.py:461(valid_items)
      300    0.000    0.000    0.001    0.000 fit_loop.py:140(_results)
      199    0.000    0.000    0.001    0.000 abc.py:117(__instancecheck__)
      100    0.000    0.000    0.001    0.000 result.py:430(_get_cache)
      199    0.001    0.000    0.001    0.000 {built-in method _abc._abc_instancecheck}
      200    0.000    0.000    0.001    0.000 os.py:674(__getitem__)
      200    0.001    0.000    0.001    0.000 enum_type_wrapper.py:92(__getattr__)
      100    0.001    0.000    0.001    0.000 {method 'detach' of 'torch._C.TensorBase' objects}
      200    0.001    0.000    0.001    0.000 std.py:153(__init__)
       99    0.000    0.000    0.001    0.000 std.py:419(format_num)
      200    0.001    0.000    0.001    0.000 result.py:463(<genexpr>)
      100    0.000    0.000    0.001    0.000 result.py:465(_forked_name)
      200    0.001    0.000    0.001    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
      100    0.000    0.000    0.001    0.000 tqdm_progress.py:169(is_enabled)
     1200    0.000    0.000    0.000    0.000 types.py:171(__get__)
      200    0.000    0.000    0.000    0.000 std.py:231(__call__)
      200    0.000    0.000    0.000    0.000 std.py:186(__format__)
      500    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 result.py:158(forked_name)
      200    0.000    0.000    0.000    0.000 {built-in method now}
      200    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
      200    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      300    0.000    0.000    0.000    0.000 tqdm_progress.py:121(train_progress_bar)
      200    0.000    0.000    0.000    0.000 std.py:106(release)
      100    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
      200    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
      200    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      200    0.000    0.000    0.000    0.000 os.py:754(encode)
      400    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
      200    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
      200    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
      200    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
      299    0.000    0.000    0.000    0.000 std.py:1428(<genexpr>)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      300    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
      800    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      200    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      200    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
     1200    0.000    0.000    0.000    0.000 enum.py:792(value)
     1000    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
      997    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      200    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
     1200    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      198    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}
      200    0.000    0.000    0.000    0.000 tqdm_progress.py:161(refresh_rate)
      400    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      200    0.000    0.000    0.000    0.000 utils.py:108(__init__)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.max}
      200    0.000    0.000    0.000    0.000 trainer.py:1590(loggers)
      200    0.000    0.000    0.000    0.000 {built-in method utcnow}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}
      200    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      200    0.000    0.000    0.000    0.000 utils.py:112(__format__)
      200    0.000    0.000    0.000    0.000 std.py:167(colour)
      200    0.000    0.000    0.000    0.000 {built-in method time.time}
      200    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      200    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      199    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}
      200    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      100    0.000    0.000    0.000    0.000 result.py:154(forked)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      3/1    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)
      3/1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}
      200    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      300    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      200    0.000    0.000    0.000    0.000 std.py:163(colour)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      200    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
      200    0.000    0.000    0.000    0.000 {method 'keys' of 'collections.OrderedDict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}



Profile stats for: [Callback]ModelSummary.on_train_batch_end
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 callback.py:81(on_train_batch_end)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_end
         2800 function calls in 0.003 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.003    0.000 model_checkpoint.py:284(on_train_batch_end)
      100    0.001    0.000    0.003    0.000 model_checkpoint.py:413(_should_skip_saving_checkpoint)
      100    0.000    0.000    0.001    0.000 trainer.py:1429(sanity_checking)
      100    0.000    0.000    0.001    0.000 enums.py:81(__eq__)
      100    0.000    0.000    0.000    0.000 trainer.py:1458(global_step)
      200    0.000    0.000    0.000    0.000 types.py:171(__get__)
      100    0.000    0.000    0.000    0.000 training_epoch_loop.py:99(global_step)
      200    0.000    0.000    0.000    0.000 enum.py:792(value)
      200    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 progress.py:274(optimizer_steps)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      200    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      200    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      100    0.000    0.000    0.000    0.000 module.py:295(automatic_optimization)
      200    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_train_batch_end
         700 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 hooks.py:79(on_train_batch_end)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_validation_model_zero_grad
         75900 function calls (56400 primitive calls) in 0.034 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000    0.033    0.000 hooks.py:158(on_validation_model_zero_grad)
      100    0.008    0.000    0.032    0.000 module.py:2500(zero_grad)
     4100    0.002    0.000    0.024    0.000 module.py:2233(parameters)
     4100    0.001    0.000    0.022    0.000 module.py:2258(named_parameters)
     4100    0.005    0.000    0.021    0.000 module.py:2219(_named_members)
24200/4700    0.010    0.000    0.011    0.000 module.py:2395(named_modules)
     8600    0.002    0.000    0.002    0.000 {method 'add' of 'set' objects}
     4600    0.002    0.000    0.002    0.000 module.py:2286(<lambda>)
     8000    0.001    0.000    0.002    0.000 _tensor.py:1055(__hash__)
     9200    0.001    0.000    0.001    0.000 {method 'items' of 'collections.OrderedDict' objects}
     8000    0.001    0.000    0.001    0.000 {built-in method builtins.id}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      100    0.000    0.000    0.000    0.000 module.py:1716(__getattr__)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_train_epoch_end
         49709 function calls in 0.026 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.025    0.000 tqdm_progress.py:278(on_train_epoch_end)
      100    0.001    0.000    0.019    0.000 std.py:1402(set_postfix)
      100    0.000    0.000    0.017    0.000 std.py:1325(refresh)
      100    0.000    0.000    0.016    0.000 std.py:1464(display)
      100    0.000    0.000    0.010    0.000 std.py:457(print_status)
      100    0.000    0.000    0.007    0.000 std.py:1150(__str__)
      100    0.000    0.000    0.006    0.000 std.py:451(fp_write)
      200    0.000    0.000    0.006    0.000 utils.py:194(inner)
      100    0.000    0.000    0.005    0.000 progress_bar.py:177(get_metrics)
      100    0.000    0.000    0.005    0.000 redirect.py:644(write)
      100    0.000    0.000    0.005    0.000 wandb_run.py:2304(<lambda>)
      100    0.000    0.000    0.005    0.000 wandb_run.py:390(wrapper_fn)
      100    0.000    0.000    0.005    0.000 wandb_run.py:1429(_console_raw_callback)
      100    0.000    0.000    0.004    0.000 interface.py:749(publish_output_raw)
      100    0.001    0.000    0.004    0.000 std.py:464(format_meter)
      100    0.000    0.000    0.004    0.000 trainer.py:1632(progress_bar_metrics)
      100    0.000    0.000    0.004    0.000 logger_connector.py:250(progress_bar_metrics)
      100    0.000    0.000    0.003    0.000 utils.py:378(disp_len)
      100    0.000    0.000    0.003    0.000 utils.py:374(_text_width)
      100    0.001    0.000    0.003    0.000 {built-in method builtins.sum}
      100    0.000    0.000    0.003    0.000 interface_shared.py:76(_publish_output_raw)
      100    0.000    0.000    0.003    0.000 interface_sock.py:45(_publish)
      100    0.000    0.000    0.002    0.000 logger_connector.py:229(metrics)
      300    0.000    0.000    0.002    0.000 trainer.py:1642(_results)
      100    0.000    0.000    0.002    0.000 sock_client.py:219(send_record_publish)
     9562    0.002    0.000    0.002    0.000 utils.py:375(<genexpr>)
      100    0.000    0.000    0.002    0.000 sock_client.py:153(send_server_request)
      100    0.000    0.000    0.002    0.000 sock_client.py:145(_send_message)
      100    0.000    0.000    0.002    0.000 std.py:1446(format_dict)
      100    0.000    0.000    0.002    0.000 utils.py:333(_screen_shape_linux)
      600    0.000    0.000    0.002    0.000 trainer.py:1381(training)
      100    0.000    0.000    0.002    0.000 sock_client.py:121(_sendall_with_error_handle)
      100    0.001    0.000    0.001    0.000 {method 'send' of '_socket.socket' objects}
      600    0.001    0.000    0.001    0.000 enums.py:81(__eq__)
      200    0.001    0.000    0.001    0.000 utils.py:273(_is_ascii)
      100    0.000    0.000    0.001    0.000 result.py:476(metrics)
      300    0.000    0.000    0.001    0.000 trainer.py:1564(_active_loop)
      100    0.000    0.000    0.001    0.000 progress_bar.py:210(get_standard_metrics)
      100    0.001    0.000    0.001    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
      300    0.000    0.000    0.001    0.000 fit_loop.py:140(_results)
      100    0.000    0.000    0.001    0.000 well_known_types.py:172(GetCurrentTime)
      100    0.001    0.000    0.001    0.000 {built-in method fcntl.ioctl}
      200    0.001    0.000    0.001    0.000 result.py:463(<genexpr>)
     9462    0.001    0.000    0.001    0.000 {built-in method unicodedata.east_asian_width}
      200    0.000    0.000    0.001    0.000 {method 'format' of 'str' objects}
       99    0.000    0.000    0.001    0.000 tqdm_progress.py:46(format_num)
      100    0.000    0.000    0.001    0.000 utilities.py:25(_version)
      100    0.000    0.000    0.001    0.000 utils.py:347(<listcomp>)
      100    0.000    0.000    0.001    0.000 well_known_types.py:242(FromDatetime)
     1200    0.000    0.000    0.001    0.000 types.py:171(__get__)
      100    0.000    0.000    0.000    0.000 std.py:102(acquire)
      100    0.000    0.000    0.000    0.000 wandb.py:572(version)
     8700    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
     1696    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
       99    0.000    0.000    0.000    0.000 std.py:419(format_num)
      200    0.000    0.000    0.000    0.000 std.py:400(format_interval)
      100    0.000    0.000    0.000    0.000 wandb_run.py:357(wrapper)
      100    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
      100    0.000    0.000    0.000    0.000 os.py:674(__getitem__)
      100    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
      100    0.000    0.000    0.000    0.000 std.py:186(__format__)
      100    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 std.py:106(release)
      100    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      299    0.000    0.000    0.000    0.000 std.py:1428(<genexpr>)
      199    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)
      200    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
     1200    0.000    0.000    0.000    0.000 enum.py:792(value)
      100    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 std.py:153(__init__)
      100    0.000    0.000    0.000    0.000 {built-in method now}
     1200    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      100    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
      200    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 os.py:754(encode)
      697    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      500    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      100    0.000    0.000    0.000    0.000 result.py:461(valid_items)
      199    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}
      100    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
      300    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
      500    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
      100    0.000    0.000    0.000    0.000 utils.py:112(__format__)
      100    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
      100    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
      100    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      100    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}
      100    0.000    0.000    0.000    0.000 std.py:231(__call__)
      100    0.000    0.000    0.000    0.000 wandb_run.py:881(id)
      200    0.000    0.000    0.000    0.000 tqdm_progress.py:121(train_progress_bar)
      100    0.000    0.000    0.000    0.000 {built-in method utcnow}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.max}
      200    0.000    0.000    0.000    0.000 trainer.py:1590(loggers)
      100    0.000    0.000    0.000    0.000 result.py:430(_get_cache)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 std.py:167(colour)
      300    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 utils.py:108(__init__)
      200    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      100    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
      199    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}
      198    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}
      100    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      100    0.000    0.000    0.000    0.000 {built-in method time.time}
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
      200    0.000    0.000    0.000    0.000 {method 'keys' of 'collections.OrderedDict' objects}
      100    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
      100    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      100    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 std.py:163(colour)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      100    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}



Profile stats for: [Callback]ModelSummary.on_train_epoch_end
         1000 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 callback.py:95(on_train_epoch_end)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_train_epoch_end
         34381 function calls (33121 primitive calls) in 0.052 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.002    0.000    0.051    0.001 FlavourClassificationTransformerEncoder.py:137(on_train_epoch_end)
      100    0.001    0.000    0.032    0.000 module.py:382(log)
      100    0.001    0.000    0.010    0.000 result.py:355(log)
      100    0.000    0.000    0.009    0.000 __init__.py:1436(info)
      100    0.000    0.000    0.008    0.000 __init__.py:1565(_log)
  405/401    0.001    0.000    0.008    0.000 apply_func.py:23(apply_to_collection)
      100    0.000    0.000    0.006    0.000 result.py:502(reset)
       99    0.001    0.000    0.006    0.000 result.py:256(reset)
      100    0.000    0.000    0.006    0.000 result.py:424(update_metrics)
      100    0.000    0.000    0.006    0.000 signature_utils.py:18(is_param_in_hook_signature)
      100    0.000    0.000    0.005    0.000 __init__.py:1591(handle)
      100    0.000    0.000    0.005    0.000 __init__.py:1645(callHandlers)
      100    0.000    0.000    0.005    0.000 result.py:264(forward)
      100    0.000    0.000    0.005    0.000 __init__.py:939(handle)
      100    0.000    0.000    0.005    0.000 metric.py:476(wrapped_func)
      100    0.001    0.000    0.005    0.000 inspect.py:1129(getfullargspec)
       99    0.001    0.000    0.005    0.000 metric.py:689(reset)
      100    0.000    0.000    0.005    0.000 __init__.py:1178(emit)
      100    0.000    0.000    0.005    0.000 __init__.py:1071(emit)
      100    0.005    0.000    0.005    0.000 {built-in method torch.stack}
      100    0.004    0.000    0.004    0.000 result.py:207(update)
      298    0.004    0.000    0.004    0.000 {method 'clone' of 'torch._C.TensorBase' objects}
  202/101    0.001    0.000    0.004    0.000 inspect.py:2246(_signature_from_callable)
  270/201    0.001    0.000    0.003    0.000 apply_func.py:84(_apply_to_collection_slow)
      100    0.001    0.000    0.003    0.000 module.py:654(__to_tensor)
      101    0.002    0.000    0.003    0.000 inspect.py:2152(_signature_from_function)
      100    0.000    0.000    0.003    0.000 __init__.py:1060(flush)
      100    0.003    0.000    0.003    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
4579/3551    0.001    0.000    0.002    0.000 {built-in method builtins.isinstance}
      100    0.000    0.000    0.002    0.000 __init__.py:1550(makeRecord)
      100    0.002    0.000    0.002    0.000 {method 'mean' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.002    0.000 __init__.py:916(format)
      100    0.002    0.000    0.002    0.000 {method 'item' of 'torch._C.TensorBase' objects}
      100    0.001    0.000    0.002    0.000 __init__.py:282(__init__)
      100    0.000    0.000    0.002    0.000 __init__.py:650(format)
      398    0.001    0.000    0.001    0.000 {method 'detach' of 'torch._C.TensorBase' objects}
      514    0.000    0.000    0.001    0.000 typing.py:719(__instancecheck__)
        1    0.000    0.000    0.001    0.001 result.py:415(register_key)
      514    0.001    0.000    0.001    0.000 typing.py:848(__subclasscheck__)
      300    0.000    0.000    0.001    0.000 trainer.py:1381(training)
      100    0.001    0.000    0.001    0.000 __init__.py:582(formatTime)
      100    0.000    0.000    0.001    0.000 trainer.py:1642(_results)
        1    0.000    0.000    0.001    0.001 result.py:306(to)
      300    0.000    0.000    0.001    0.000 enums.py:81(__eq__)
      303    0.000    0.000    0.001    0.000 inspect.py:2498(__init__)
      100    0.001    0.000    0.001    0.000 __init__.py:1514(findCaller)
      100    0.000    0.000    0.001    0.000 contextlib.py:123(__exit__)
     1322    0.001    0.000    0.001    0.000 result.py:294(__setattr__)
      100    0.000    0.000    0.001    0.000 memory.py:24(recursive_detach)
      100    0.000    0.000    0.001    0.000 fx_validator.py:191(check_logging_and_get_default_levels)
      102    0.000    0.000    0.001    0.000 inspect.py:2781(__init__)
       99    0.000    0.000    0.001    0.000 <string>:2(__eq__)
      521    0.000    0.000    0.001    0.000 {built-in method builtins.issubclass}
      100    0.000    0.000    0.000    0.000 trainer.py:1564(_active_loop)
      100    0.000    0.000    0.000    0.000 {method 'clear' of 'list' objects}
        1    0.000    0.000    0.000    0.000 result.py:187(__init__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      227    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}
      248    0.000    0.000    0.000    0.000 apply_func.py:17(is_dataclass_instance)
      100    0.000    0.000    0.000    0.000 module.py:262(current_epoch)
      100    0.000    0.000    0.000    0.000 <string>:2(__init__)
     58/4    0.000    0.000    0.000    0.000 copy.py:128(deepcopy)
      100    0.000    0.000    0.000    0.000 fx_validator.py:177(check_logging_levels)
      100    0.000    0.000    0.000    0.000 {method 'squeeze' of 'torch._C.TensorBase' objects}
      514    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)
      100    0.000    0.000    0.000    0.000 fit_loop.py:140(_results)
      200    0.000    0.000    0.000    0.000 result.py:90(_generate_sync_fn)
      652    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
      100    0.000    0.000    0.000    0.000 memory.py:40(detach_and_move)
      202    0.000    0.000    0.000    0.000 {method 'to' of 'torch._C.TensorBase' objects}
      248    0.000    0.000    0.000    0.000 dataclasses.py:1047(is_dataclass)
      100    0.000    0.000    0.000    0.000 posixpath.py:117(splitext)
      600    0.000    0.000    0.000    0.000 types.py:171(__get__)
      100    0.000    0.000    0.000    0.000 {built-in method time.strftime}
      100    0.000    0.000    0.000    0.000 result.py:148(sync)
      303    0.000    0.000    0.000    0.000 enum.py:358(__call__)
      372    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)
      100    0.000    0.000    0.000    0.000 result.py:122(__post_init__)
      104    0.000    0.000    0.000    0.000 grad_mode.py:184(__init__)
      100    0.000    0.000    0.000    0.000 posixpath.py:140(basename)
        2    0.000    0.000    0.000    0.000 metric.py:196(add_state)
      100    0.000    0.000    0.000    0.000 result.py:57(__post_init__)
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
      100    0.000    0.000    0.000    0.000 {built-in method time.localtime}
      514    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}
        2    0.000    0.000    0.000    0.000 _tensor.py:83(__deepcopy__)
      100    0.000    0.000    0.000    0.000 result.py:127(_parse_reduce_fx)
      404    0.000    0.000    0.000    0.000 inspect.py:2830(<genexpr>)
      100    0.000    0.000    0.000    0.000 __init__.py:634(formatMessage)
      304    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}
      100    0.000    0.000    0.000    0.000 __init__.py:628(usesTime)
      372    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}
      100    0.000    0.000    0.000    0.000 __init__.py:160(<lambda>)
      100    0.000    0.000    0.000    0.000 result.py:74(op)
      100    0.000    0.000    0.000    0.000 genericpath.py:121(_splitext)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 __init__.py:432(format)
        1    0.000    0.000    0.000    0.000 metric.py:101(__init__)
      200    0.000    0.000    0.000    0.000 __init__.py:896(acquire)
      101    0.000    0.000    0.000    0.000 result.py:536(_get_default_dtype)
      100    0.000    0.000    0.000    0.000 <string>:1(<lambda>)
      803    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 grad_mode.py:196(__exit__)
      442    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
      248    0.000    0.000    0.000    0.000 apply_func.py:11(is_namedtuple)
      3/2    0.000    0.000    0.000    0.000 copy.py:258(_reconstruct)
      100    0.000    0.000    0.000    0.000 posixpath.py:52(normcase)
      100    0.000    0.000    0.000    0.000 __init__.py:429(_format)
      100    0.000    0.000    0.000    0.000 __init__.py:421(usesTime)
      200    0.000    0.000    0.000    0.000 __init__.py:791(filter)
      202    0.000    0.000    0.000    0.000 inspect.py:159(isfunction)
      100    0.000    0.000    0.000    0.000 module.py:215(trainer)
      200    0.000    0.000    0.000    0.000 __init__.py:903(release)
      300    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}
      100    0.000    0.000    0.000    0.000 {built-in method torch.numel}
        4    0.000    0.000    0.000    0.000 apply_func.py:71(move_data_to_device)
      5/4    0.000    0.000    0.000    0.000 copy.py:226(_deepcopy_dict)
        2    0.000    0.000    0.000    0.000 storage.py:907(_deepcopy)
      100    0.000    0.000    0.000    0.000 trainer.py:1467(current_epoch)
      303    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}
      700    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      303    0.000    0.000    0.000    0.000 enum.py:670(__new__)
      100    0.000    0.000    0.000    0.000 grad_mode.py:193(__enter__)
        4    0.000    0.000    0.000    0.000 apply_func.py:91(batch_to)
      600    0.000    0.000    0.000    0.000 enum.py:792(value)
      605    0.000    0.000    0.000    0.000 inspect.py:2548(name)
      100    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
      100    0.000    0.000    0.000    0.000 {built-in method sys._getframe}
      664    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
      100    0.000    0.000    0.000    0.000 {built-in method torch.is_floating_point}
      100    0.000    0.000    0.000    0.000 threading.py:1358(current_thread)
      100    0.000    0.000    0.000    0.000 __init__.py:119(getLevelName)
      100    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
      100    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)
      100    0.000    0.000    0.000    0.000 __init__.py:358(getMessage)
      100    0.000    0.000    0.000    0.000 fx_validator.py:151(check_logging)
      101    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}
      100    0.000    0.000    0.000    0.000 fx_validator.py:166(get_default_logging_levels)
      100    0.000    0.000    0.000    0.000 logger_connector.py:213(should_reset_tensors)
      108    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f7dfbd377c0}
      200    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
      300    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        2    0.000    0.000    0.000    0.000 storage.py:140(__deepcopy__)
      102    0.000    0.000    0.000    0.000 {built-in method torch.get_default_dtype}
        2    0.000    0.000    0.000    0.000 storage.py:156(clone)
        1    0.000    0.000    0.000    0.000 inspect.py:3111(signature)
        1    0.000    0.000    0.000    0.000 inspect.py:2859(from_callable)
      106    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}
      304    0.000    0.000    0.000    0.000 inspect.py:2560(kind)
      100    0.000    0.000    0.000    0.000 result.py:340(_extract_batch_size)
      100    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      300    0.000    0.000    0.000    0.000 result.py:70(op)
      101    0.000    0.000    0.000    0.000 result.py:163(is_mean_reduction)
      100    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}
      300    0.000    0.000    0.000    0.000 inspect.py:2556(annotation)
      200    0.000    0.000    0.000    0.000 result.py:60(should)
      302    0.000    0.000    0.000    0.000 inspect.py:2552(default)
      200    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
      100    0.000    0.000    0.000    0.000 process.py:189(name)
        1    0.000    0.000    0.000    0.000 metric.py:475(_wrap_update)
      100    0.000    0.000    0.000    0.000 threading.py:1093(name)
      100    0.000    0.000    0.000    0.000 process.py:37(current_process)
      100    0.000    0.000    0.000    0.000 {built-in method time.time}
      200    0.000    0.000    0.000    0.000 result.py:80(group)
      204    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
      200    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
      2/1    0.000    0.000    0.000    0.000 copy.py:209(_deepcopy_tuple)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        2    0.000    0.000    0.000    0.000 {method 'copy_' of 'torch._C.StorageBase' objects}
        2    0.000    0.000    0.000    0.000 dataclasses.py:1024(fields)
      100    0.000    0.000    0.000    0.000 inspect.py:2869(return_annotation)
        2    0.000    0.000    0.000    0.000 {built-in method torch.tensor}
      100    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      103    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
      101    0.000    0.000    0.000    0.000 inspect.py:2865(parameters)
        2    0.000    0.000    0.000    0.000 functools.py:35(update_wrapper)
        2    0.000    0.000    0.000    0.000 _tensor.py:242(_typed_storage)
        1    0.000    0.000    0.000    0.000 module.py:429(__init__)
      2/1    0.000    0.000    0.000    0.000 copy.py:210(<listcomp>)
      100    0.000    0.000    0.000    0.000 typing.py:1375(cast)
      110    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 result.py:273(_wrap_compute)
       13    0.000    0.000    0.000    0.000 copy.py:242(_keep_alive)
      107    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        2    0.000    0.000    0.000    0.000 storage.py:712(_new_wrapped_storage)
       19    0.000    0.000    0.000    0.000 dataclasses.py:1039(<genexpr>)
        6    0.000    0.000    0.000    0.000 copy.py:263(<genexpr>)
        1    0.000    0.000    0.000    0.000 inspect.py:1840(_signature_bound_method)
        3    0.000    0.000    0.000    0.000 {method '__reduce_ex__' of 'object' objects}
        2    0.000    0.000    0.000    0.000 {method 'set_' of 'torch._C.TensorBase' objects}
        4    0.000    0.000    0.000    0.000 storage.py:629(__init__)
        2    0.000    0.000    0.000    0.000 grad_mode.py:80(__enter__)
        2    0.000    0.000    0.000    0.000 {method 'new_empty' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.all}
        2    0.000    0.000    0.000    0.000 apply_func.py:69(<genexpr>)
        3    0.000    0.000    0.000    0.000 _collections_abc.py:315(__subclasshook__)
        5    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 storage.py:557(__new__)
        2    0.000    0.000    0.000    0.000 grad_mode.py:84(__exit__)
        1    0.000    0.000    0.000    0.000 inspect.py:2873(replace)
        2    0.000    0.000    0.000    0.000 _contextlib.py:154(__new__)
        1    0.000    0.000    0.000    0.000 inspect.py:494(unwrap)
        1    0.000    0.000    0.000    0.000 result.py:171(is_max_reduction)
        2    0.000    0.000    0.000    0.000 grad_mode.py:75(__init__)
        1    0.000    0.000    0.000    0.000 typing.py:271(inner)
        2    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}
       24    0.000    0.000    0.000    0.000 copy.py:182(_deepcopy_atomic)
        2    0.000    0.000    0.000    0.000 {method 'untyped_storage' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 {built-in method torch._C._has_storage}
        2    0.000    0.000    0.000    0.000 copyreg.py:94(__newobj__)
       14    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
        2    0.000    0.000    0.000    0.000 functools.py:65(wraps)
        2    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.000    0.000 inspect.py:514(_is_wrapper)
        2    0.000    0.000    0.000    0.000 {method 'contiguous' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 {method 'nbytes' of 'torch._C.StorageBase' objects}
        2    0.000    0.000    0.000    0.000 {method 'is_conj' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 {method 'storage_offset' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 storage.py:30(__init__)
        2    0.000    0.000    0.000    0.000 {method 'stride' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 {method 'is_neg' of 'torch._C.TensorBase' objects}
        4    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        7    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 result.py:175(is_min_reduction)
        1    0.000    0.000    0.000    0.000 {method '__setstate__' of 'functools.partial' objects}
        2    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}
        2    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
        1    0.000    0.000    0.000    0.000 {built-in method sys.getrecursionlimit}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_train_epoch_end
         15325 function calls in 0.037 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.000    0.000    0.036    0.000 early_stopping.py:186(on_train_epoch_end)
      100    0.001    0.000    0.034    0.000 early_stopping.py:198(_run_early_stopping_check)
      100    0.007    0.000    0.019    0.000 early_stopping.py:218(_evaluate_stopping_criteria)
      100    0.000    0.000    0.011    0.000 trainer.py:1606(callback_metrics)
      100    0.000    0.000    0.011    0.000 logger_connector.py:236(callback_metrics)
      102    0.010    0.000    0.010    0.000 {built-in method torch.isfinite}
      100    0.000    0.000    0.010    0.000 logger_connector.py:229(metrics)
      100    0.001    0.000    0.009    0.000 result.py:476(metrics)
      200    0.000    0.000    0.006    0.000 result.py:430(_get_cache)
      100    0.000    0.000    0.004    0.000 result.py:276(wrapped_func)
      100    0.002    0.000    0.004    0.000 result.py:246(compute)
      700    0.001    0.000    0.002    0.000 enums.py:81(__eq__)
      100    0.002    0.000    0.002    0.000 {method 'clone' of 'torch._C.TensorBase' objects}
      300    0.000    0.000    0.002    0.000 trainer.py:1642(_results)
      100    0.000    0.000    0.002    0.000 apply_func.py:113(convert_tensors_to_scalars)
      100    0.002    0.000    0.002    0.000 {built-in method torch.lt}
      100    0.000    0.000    0.002    0.000 apply_func.py:23(apply_to_collection)
      600    0.000    0.000    0.001    0.000 trainer.py:1381(training)
      100    0.000    0.000    0.001    0.000 apply_func.py:122(to_item)
      100    0.000    0.000    0.001    0.000 early_stopping.py:181(_should_skip_check)
      103    0.001    0.000    0.001    0.000 {method 'item' of 'torch._C.TensorBase' objects}
      100    0.001    0.000    0.001    0.000 early_stopping.py:142(_validate_condition_metric)
      100    0.000    0.000    0.001    0.000 trainer.py:1429(sanity_checking)
      300    0.000    0.000    0.001    0.000 trainer.py:1564(_active_loop)
      300    0.000    0.000    0.001    0.000 fit_loop.py:140(_results)
      100    0.001    0.000    0.001    0.000 early_stopping.py:161(monitor_op)
     1400    0.000    0.000    0.001    0.000 types.py:171(__get__)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 result.py:64(should)
      100    0.000    0.000    0.000    0.000 {method 'squeeze' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 early_stopping.py:268(_log_info)
      100    0.000    0.000    0.000    0.000 distributed.py:392(_distributed_is_initialized)
      100    0.000    0.000    0.000    0.000 result.py:90(_generate_sync_fn)
        2    0.000    0.000    0.000    0.000 __init__.py:1436(info)
        2    0.000    0.000    0.000    0.000 __init__.py:1565(_log)
      100    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
      100    0.000    0.000    0.000    0.000 {method 'detach' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        2    0.000    0.000    0.000    0.000 __init__.py:1591(handle)
        2    0.000    0.000    0.000    0.000 __init__.py:1645(callHandlers)
        2    0.000    0.000    0.000    0.000 __init__.py:939(handle)
      100    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        2    0.000    0.000    0.000    0.000 __init__.py:1071(emit)
      100    0.000    0.000    0.000    0.000 {method 'to' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 early_stopping.py:257(_improvement_message)
     1104    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        2    0.000    0.000    0.000    0.000 redirect.py:644(write)
      100    0.000    0.000    0.000    0.000 result.py:465(_forked_name)
      300    0.000    0.000    0.000    0.000 result.py:463(<genexpr>)
     1400    0.000    0.000    0.000    0.000 enum.py:792(value)
      100    0.000    0.000    0.000    0.000 distributed_c10d.py:996(is_initialized)
        2    0.000    0.000    0.000    0.000 wandb_run.py:2310(<lambda>)
        2    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
        2    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
     1400    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 imports.py:162(__bool__)
      100    0.000    0.000    0.000    0.000 result.py:461(valid_items)
      202    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      100    0.000    0.000    0.000    0.000 distributed_c10d.py:608(WORLD)
      100    0.000    0.000    0.000    0.000 typing.py:271(inner)
        2    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
      400    0.000    0.000    0.000    0.000 result.py:143(sync)
      100    0.000    0.000    0.000    0.000 result.py:158(forked_name)
        2    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
      100    0.000    0.000    0.000    0.000 __init__.py:10(is_available)
      100    0.000    0.000    0.000    0.000 result.py:294(__setattr__)
        2    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
        2    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        2    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
      100    0.000    0.000    0.000    0.000 {method 'numel' of 'torch._C.TensorBase' objects}
      100    0.000    0.000    0.000    0.000 imports.py:154(_check_available)
        3    0.000    0.000    0.000    0.000 _tensor.py:982(__format__)
      100    0.000    0.000    0.000    0.000 distributed_c10d.py:480(default_pg)
        2    0.000    0.000    0.000    0.000 __init__.py:1550(makeRecord)
        2    0.000    0.000    0.000    0.000 __init__.py:282(__init__)
      100    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
      204    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
      100    0.000    0.000    0.000    0.000 result.py:163(is_mean_reduction)
      200    0.000    0.000    0.000    0.000 result.py:97(__call__)
      206    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        2    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
      100    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
      200    0.000    0.000    0.000    0.000 result.py:60(should)
      100    0.000    0.000    0.000    0.000 metric.py:181(update_called)
      100    0.000    0.000    0.000    0.000 result.py:80(group)
        2    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
        2    0.000    0.000    0.000    0.000 __init__.py:916(format)
      100    0.000    0.000    0.000    0.000 strategy.py:341(reduce_boolean_decision)
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      200    0.000    0.000    0.000    0.000 result.py:101(no_op)
        2    0.000    0.000    0.000    0.000 __init__.py:650(format)
      100    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
      100    0.000    0.000    0.000    0.000 result.py:154(forked)
        2    0.000    0.000    0.000    0.000 __init__.py:1514(findCaller)
      100    0.000    0.000    0.000    0.000 result.py:70(op)
        2    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
      100    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 __init__.py:791(filter)
        2    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
        2    0.000    0.000    0.000    0.000 posixpath.py:117(splitext)
        2    0.000    0.000    0.000    0.000 __init__.py:628(usesTime)
        4    0.000    0.000    0.000    0.000 __init__.py:896(acquire)
        2    0.000    0.000    0.000    0.000 __init__.py:1060(flush)
        2    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        5    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
        2    0.000    0.000    0.000    0.000 genericpath.py:121(_splitext)
        2    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
        2    0.000    0.000    0.000    0.000 posixpath.py:140(basename)
        2    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        2    0.000    0.000    0.000    0.000 __init__.py:634(formatMessage)
        2    0.000    0.000    0.000    0.000 trainer.py:1147(world_size)
        3    0.000    0.000    0.000    0.000 {method '__format__' of 'float' objects}
        2    0.000    0.000    0.000    0.000 __init__.py:432(format)
        2    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        2    0.000    0.000    0.000    0.000 __init__.py:421(usesTime)
        4    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        6    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        2    0.000    0.000    0.000    0.000 __init__.py:160(<lambda>)
        2    0.000    0.000    0.000    0.000 {built-in method utcnow}
        4    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
        2    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
        1    0.000    0.000    0.000    0.000 __init__.py:218(_acquireLock)
        2    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        2    0.000    0.000    0.000    0.000 __init__.py:429(_format)
        4    0.000    0.000    0.000    0.000 __init__.py:903(release)
        2    0.000    0.000    0.000    0.000 posixpath.py:52(normcase)
        6    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}
        2    0.000    0.000    0.000    0.000 threading.py:1093(name)
        2    0.000    0.000    0.000    0.000 threading.py:1358(current_thread)
        2    0.000    0.000    0.000    0.000 __init__.py:119(getLevelName)
        2    0.000    0.000    0.000    0.000 __init__.py:358(getMessage)
        2    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)
        1    0.000    0.000    0.000    0.000 __init__.py:227(_releaseLock)
        2    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        2    0.000    0.000    0.000    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
        1    0.000    0.000    0.000    0.000 __init__.py:1675(getEffectiveLevel)
        2    0.000    0.000    0.000    0.000 rank_zero.py:91(rank_prefixed_message)
        3    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C.TensorBase' objects}
        4    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        5    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
        2    0.000    0.000    0.000    0.000 {built-in method sys._getframe}
        6    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        2    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {built-in method time.time}
        4    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}
        3    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}
        2    0.000    0.000    0.000    0.000 process.py:189(name)
        2    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        2    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        1    0.000    0.000    0.000    0.000 __init__.py:1276(disable)
        2    0.000    0.000    0.000    0.000 process.py:37(current_process)
        2    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
        2    0.000    0.000    0.000    0.000 {built-in method builtins.ord}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_end
         59225 function calls (56643 primitive calls) in 0.138 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.002    0.000    0.137    0.001 model_checkpoint.py:317(on_train_epoch_end)
      100    0.000    0.000    0.089    0.001 model_checkpoint.py:368(_save_topk_checkpoint)
      100    0.000    0.000    0.089    0.001 model_checkpoint.py:698(_save_monitor_checkpoint)
        4    0.000    0.000    0.084    0.021 model_checkpoint.py:718(_update_best_and_save)
        4    0.000    0.000    0.081    0.020 model_checkpoint.py:387(_save_checkpoint)
        4    0.000    0.000    0.081    0.020 trainer.py:1346(save_checkpoint)
        4    0.000    0.000    0.081    0.020 checkpoint_connector.py:404(dump_checkpoint)
      101    0.000    0.000    0.076    0.001 {built-in method builtins.next}
      100    0.000    0.000    0.076    0.001 profiler.py:55(profile)
        4    0.000    0.000    0.076    0.019 call.py:167(_call_lightning_datamodule_hook)
        4    0.000    0.000    0.076    0.019 contextlib.py:114(__enter__)
        4    0.000    0.000    0.076    0.019 advanced.py:65(start)
        4    0.076    0.019    0.076    0.019 {method 'enable' of '_lsprof.Profiler' objects}
      100    0.001    0.000    0.044    0.000 model_checkpoint.py:667(_monitor_candidates)
 1980/380    0.004    0.000    0.037    0.000 copy.py:128(deepcopy)
  500/100    0.001    0.000    0.036    0.000 copy.py:226(_deepcopy_dict)
      400    0.008    0.000    0.033    0.000 _tensor.py:83(__deepcopy__)
      400    0.001    0.000    0.014    0.000 storage.py:907(_deepcopy)
      400    0.001    0.000    0.009    0.000 storage.py:140(__deepcopy__)
      400    0.002    0.000    0.008    0.000 storage.py:156(clone)
      400    0.006    0.000    0.006    0.000 {method 'copy_' of 'torch._C.StorageBase' objects}
      100    0.000    0.000    0.005    0.000 trainer.py:1606(callback_metrics)
      100    0.000    0.000    0.005    0.000 logger_connector.py:236(callback_metrics)
      100    0.003    0.000    0.005    0.000 model_checkpoint.py:512(check_monitor_top_k)
      100    0.000    0.000    0.004    0.000 logger_connector.py:229(metrics)
      100    0.000    0.000    0.003    0.000 result.py:476(metrics)
        4    0.000    0.000    0.003    0.001 checkpoint_connector.py:499(_get_loops_state_dict)
    32/16    0.000    0.000    0.003    0.000 loop.py:52(state_dict)
        4    0.000    0.000    0.002    0.001 model_checkpoint.py:654(_get_metric_interpolated_filepath_name)
      400    0.001    0.000    0.002    0.000 _tensor.py:242(_typed_storage)
      900    0.001    0.000    0.002    0.000 enums.py:81(__eq__)
      300    0.000    0.000    0.002    0.000 trainer.py:1642(_results)
       36    0.000    0.000    0.002    0.000 progress.py:24(state_dict)
      400    0.001    0.000    0.002    0.000 storage.py:712(_new_wrapped_storage)
       36    0.000    0.000    0.002    0.000 dataclasses.py:1054(asdict)
      100    0.000    0.000    0.002    0.000 apply_func.py:113(convert_tensors_to_scalars)
      400    0.002    0.000    0.002    0.000 {method 'new_empty' of 'torch._C.TensorBase' objects}
   408/36    0.001    0.000    0.002    0.000 dataclasses.py:1078(_asdict_inner)
      400    0.001    0.000    0.001    0.000 {method 'set_' of 'torch._C.TensorBase' objects}
      200    0.001    0.000    0.001    0.000 {built-in method torch.tensor}
      600    0.000    0.000    0.001    0.000 trainer.py:1381(training)
        4    0.000    0.000    0.001    0.000 model_checkpoint.py:770(file_exists)
        4    0.000    0.000    0.001    0.000 checkpoint_connector.py:496(_get_lightning_module_state_dict)
        4    0.000    0.000    0.001    0.000 strategy.py:473(lightning_module_state_dict)
    184/4    0.001    0.000    0.001    0.000 module.py:1865(state_dict)
      100    0.000    0.000    0.001    0.000 apply_func.py:23(apply_to_collection)
        4    0.000    0.000    0.001    0.000 spec.py:633(exists)
       97    0.001    0.000    0.001    0.000 {built-in method torch.lt}
        4    0.000    0.000    0.001    0.000 local.py:71(info)
      100    0.000    0.000    0.001    0.000 model_checkpoint.py:413(_should_skip_saving_checkpoint)
      100    0.000    0.000    0.001    0.000 apply_func.py:122(to_item)
      108    0.001    0.000    0.001    0.000 {method 'item' of 'torch._C.TensorBase' objects}
        4    0.001    0.000    0.001    0.000 {built-in method posix.stat}
      800    0.001    0.000    0.001    0.000 storage.py:629(__init__)
     1300    0.001    0.000    0.001    0.000 copy.py:242(_keep_alive)
5006/5002    0.001    0.000    0.001    0.000 {built-in method builtins.isinstance}
      800    0.001    0.000    0.001    0.000 grad_mode.py:184(__init__)
     1531    0.001    0.000    0.001    0.000 {built-in method builtins.getattr}
        4    0.000    0.000    0.001    0.000 model_checkpoint.py:567(format_checkpoint_name)
      400    0.000    0.000    0.001    0.000 grad_mode.py:80(__enter__)
      300    0.000    0.000    0.001    0.000 trainer.py:1564(_active_loop)
      100    0.000    0.000    0.001    0.000 model_checkpoint.py:423(_should_save_on_train_epoch_end)
        4    0.000    0.000    0.001    0.000 model_checkpoint.py:531(_format_checkpoint_name)
      300    0.000    0.000    0.001    0.000 fit_loop.py:140(_results)
      400    0.000    0.000    0.001    0.000 grad_mode.py:84(__exit__)
     1800    0.001    0.000    0.001    0.000 types.py:171(__get__)
      800    0.001    0.000    0.001    0.000 storage.py:557(__new__)
      204    0.000    0.000    0.001    0.000 trainer.py:1458(global_step)
        4    0.000    0.000    0.001    0.000 strategy.py:173(optimizer_state)
      184    0.000    0.000    0.001    0.000 module.py:1826(_save_to_state_dict)
      260    0.001    0.000    0.001    0.000 {method 'detach' of 'torch._C.TensorBase' objects}
       96    0.000    0.000    0.001    0.000 contextlib.py:123(__exit__)
        4    0.000    0.000    0.001    0.000 re.py:233(findall)
      200    0.000    0.000    0.001    0.000 trainer.py:1535(num_val_batches)
      400    0.001    0.000    0.001    0.000 _contextlib.py:154(__new__)
      400    0.001    0.000    0.001    0.000 grad_mode.py:75(__init__)
        4    0.000    0.000    0.001    0.000 _compile.py:21(inner)
      204    0.000    0.000    0.001    0.000 training_epoch_loop.py:99(global_step)
     5321    0.001    0.000    0.001    0.000 {built-in method builtins.id}
      100    0.000    0.000    0.001    0.000 trainer.py:1429(sanity_checking)
        4    0.000    0.000    0.001    0.000 eval_frame.py:596(_fn)
     4360    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      200    0.000    0.000    0.000    0.000 result.py:430(_get_cache)
        4    0.000    0.000    0.000    0.000 optimizer.py:631(state_dict)
        4    0.000    0.000    0.000    0.000 fit_loop.py:415(on_save_checkpoint)
        4    0.000    0.000    0.000    0.000 combined_loader.py:378(_state_dicts)
        4    0.000    0.000    0.000    0.000 combined_loader.py:380(<listcomp>)
      400    0.000    0.000    0.000    0.000 {built-in method torch._C._has_storage}
        4    0.000    0.000    0.000    0.000 typing.py:1141(__instancecheck__)
      800    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}
      308    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        4    0.000    0.000    0.000    0.000 re.py:289(_compile)
      400    0.000    0.000    0.000    0.000 {method 'untyped_storage' of 'torch._C.TensorBase' objects}
        4    0.000    0.000    0.000    0.000 {method 'findall' of 're.Pattern' objects}
      128    0.000    0.000    0.000    0.000 dataclasses.py:1024(fields)
      444    0.000    0.000    0.000    0.000 dataclasses.py:1042(_is_dataclass_instance)
        4    0.000    0.000    0.000    0.000 optimizer.py:703(<listcomp>)
        1    0.000    0.000    0.000    0.000 sre_compile.py:783(compile)
        4    0.000    0.000    0.000    0.000 optimizer.py:689(pack_group)
       10    0.000    0.000    0.000    0.000 typing.py:1065(_get_protocol_attrs)
     1800    0.000    0.000    0.000    0.000 enum.py:792(value)
        5    0.000    0.000    0.000    0.000 typing.py:1082(_is_callable_members_only)
     1200    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x7f7dfbd377c0}
     1200    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}
      400    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C.TensorBase' objects}
     1800    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
      300    0.000    0.000    0.000    0.000 result.py:463(<genexpr>)
     1621    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
      804    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}
        1    0.000    0.000    0.000    0.000 sre_parse.py:944(parse)
      647    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
1141/1138    0.000    0.000    0.000    0.000 {built-in method builtins.len}
      100    0.000    0.000    0.000    0.000 result.py:465(_forked_name)
      400    0.000    0.000    0.000    0.000 {method 'is_conj' of 'torch._C.TensorBase' objects}
        2    0.000    0.000    0.000    0.000 {built-in method builtins.max}
      400    0.000    0.000    0.000    0.000 {method 'stride' of 'torch._C.TensorBase' objects}
       96    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      108    0.000    0.000    0.000    0.000 typing.py:271(inner)
      400    0.000    0.000    0.000    0.000 {method 'nbytes' of 'torch._C.StorageBase' objects}
       11    0.000    0.000    0.000    0.000 {built-in method builtins.min}
      400    0.000    0.000    0.000    0.000 {method 'storage_offset' of 'torch._C.TensorBase' objects}
        4    0.000    0.000    0.000    0.000 optimizer.py:691(<dictcomp>)
      400    0.000    0.000    0.000    0.000 {method 'is_neg' of 'torch._C.TensorBase' objects}
      2/1    0.000    0.000    0.000    0.000 sre_parse.py:436(_parse_sub)
        4    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}
      433    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
      500    0.000    0.000    0.000    0.000 dataclasses.py:1039(<genexpr>)
        4    0.000    0.000    0.000    0.000 optimizer.py:705(<dictcomp>)
      2/1    0.000    0.000    0.000    0.000 sre_parse.py:494(_parse)
      100    0.000    0.000    0.000    0.000 result.py:461(valid_items)
      204    0.000    0.000    0.000    0.000 trainer.py:1467(current_epoch)
      204    0.000    0.000    0.000    0.000 progress.py:274(optimizer_steps)
        8    0.000    0.000    0.000    0.000 _tensor.py:982(__format__)
      105    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
      645    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 sre_compile.py:622(_code)
        4    0.000    0.000    0.000    0.000 optimizer.py:693(<dictcomp>)
      408    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}
        4    0.000    0.000    0.000    0.000 lr_scheduler.py:1383(state_dict)
      400    0.000    0.000    0.000    0.000 storage.py:30(__init__)
       96    0.000    0.000    0.000    0.000 model_checkpoint.py:677(_save_last_checkpoint)
      680    0.000    0.000    0.000    0.000 copy.py:182(_deepcopy_atomic)
        1    0.000    0.000    0.000    0.000 decorators.py:38(disable)
        4    0.000    0.000    0.000    0.000 call.py:217(_call_callbacks_state_dict)
      100    0.000    0.000    0.000    0.000 result.py:158(forked_name)
        4    0.000    0.000    0.000    0.000 local.py:230(_strip_protocol)
      400    0.000    0.000    0.000    0.000 _jit_internal.py:1130(is_scripting)
        4    0.000    0.000    0.000    0.000 lr_scheduler.py:1384(<dictcomp>)
      401    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 {built-in method torch.isnan}
        4    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)
      316    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        4    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}
      204    0.000    0.000    0.000    0.000 module.py:295(automatic_optimization)
      100    0.000    0.000    0.000    0.000 {method 'numel' of 'torch._C.TensorBase' objects}
      552    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}
      100    0.000    0.000    0.000    0.000 {built-in method builtins.sum}
        4    0.000    0.000    0.000    0.000 optimizer.py:699(<listcomp>)
      3/1    0.000    0.000    0.000    0.000 sre_compile.py:87(_compile)
        1    0.000    0.000    0.000    0.000 typing.py:1200(_proto_hook)
        1    0.000    0.000    0.000    0.000 eval_frame.py:565(__call__)
      376    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}
        1    0.000    0.000    0.000    0.000 sre_compile.py:560(_compile_info)
       96    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      100    0.000    0.000    0.000    0.000 result.py:154(forked)
      164    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}
        4    0.000    0.000    0.000    0.000 posixpath.py:71(join)
        8    0.000    0.000    0.000    0.000 callback.py:48(_generate_state_key)
        4    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}
       97    0.000    0.000    0.000    0.000 strategy.py:341(reduce_boolean_decision)
        9    0.000    0.000    0.000    0.000 {built-in method builtins.all}
        1    0.000    0.000    0.000    0.000 eval_frame.py:562(__init__)
        4    0.000    0.000    0.000    0.000 early_stopping.py:130(state_key)
        1    0.000    0.000    0.000    0.000 functools.py:35(update_wrapper)
        1    0.000    0.000    0.000    0.000 eval_frame.py:265(__init__)
        1    0.000    0.000    0.000    0.000 sre_compile.py:292(_optimize_charset)
        4    0.000    0.000    0.000    0.000 model_checkpoint.py:256(state_key)
        1    0.000    0.000    0.000    0.000 sre_parse.py:356(_escape)
        8    0.000    0.000    0.000    0.000 {built-in method builtins.repr}
      128    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}
       15    0.000    0.000    0.000    0.000 typing.py:1084(<genexpr>)
        1    0.000    0.000    0.000    0.000 sre_parse.py:225(__init__)
        8    0.000    0.000    0.000    0.000 hparams_mixin.py:152(hparams)
        1    0.000    0.000    0.000    0.000 sre_parse.py:97(closegroup)
        4    0.000    0.000    0.000    0.000 model_checkpoint.py:335(state_dict)
      2/1    0.000    0.000    0.000    0.000 sre_compile.py:485(_get_literal_prefix)
        4    0.000    0.000    0.000    0.000 trainer.py:1183(optimizers)
       16    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}
      4/2    0.000    0.000    0.000    0.000 sre_parse.py:175(getwidth)
        4    0.000    0.000    0.000    0.000 contextlib.py:261(helper)
       14    0.000    0.000    0.000    0.000 sre_parse.py:165(__getitem__)
        4    0.000    0.000    0.000    0.000 trainer.py:1203(model)
        8    0.000    0.000    0.000    0.000 sre_parse.py:255(get)
        4    0.000    0.000    0.000    0.000 early_stopping.py:165(state_dict)
        1    0.000    0.000    0.000    0.000 enum.py:977(__and__)
        4    0.000    0.000    0.000    0.000 trainer.py:1129(precision_plugin)
        1    0.000    0.000    0.000    0.000 sre_parse.py:76(__init__)
        4    0.000    0.000    0.000    0.000 local.py:280(make_path_posix)
       11    0.000    0.000    0.000    0.000 sre_parse.py:234(__next)
        7    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)
        4    0.000    0.000    0.000    0.000 training_epoch_loop.py:317(on_save_checkpoint)
        4    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)
        1    0.000    0.000    0.000    0.000 sre_parse.py:85(opengroup)
       32    0.000    0.000    0.000    0.000 loop.py:40(on_save_checkpoint)
        8    0.000    0.000    0.000    0.000 typing.py:1149(<genexpr>)
        8    0.000    0.000    0.000    0.000 {built-in method torch._C._dynamo.eval_frame.set_eval_frame}
        4    0.000    0.000    0.000    0.000 {method '__format__' of 'int' objects}
        4    0.000    0.000    0.000    0.000 {method '__format__' of 'float' objects}
        4    0.000    0.000    0.000    0.000 __init__.py:1424(debug)
        4    0.000    0.000    0.000    0.000 trainer.py:1191(lr_scheduler_configs)
        8    0.000    0.000    0.000    0.000 model_checkpoint.py:547(<lambda>)
        1    0.000    0.000    0.000    0.000 sre_parse.py:296(_class_escape)
        5    0.000    0.000    0.000    0.000 sre_parse.py:287(tell)
        4    0.000    0.000    0.000    0.000 strategy.py:101(optimizers)
        2    0.000    0.000    0.000    0.000 enum.py:358(__call__)
        4    0.000    0.000    0.000    0.000 utils.py:327(stringify_path)
       10    0.000    0.000    0.000    0.000 {method 'keys' of 'mappingproxy' objects}
        1    0.000    0.000    0.000    0.000 sre_parse.py:433(_uniq)
        7    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}
        3    0.000    0.000    0.000    0.000 eval_frame.py:240(innermost_fn)
       20    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
        8    0.000    0.000    0.000    0.000 sre_parse.py:250(match)
        8    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C.TensorBase' objects}
       17    0.000    0.000    0.000    0.000 {built-in method builtins.callable}
        4    0.000    0.000    0.000    0.000 strategy.py:93(precision_plugin)
        1    0.000    0.000    0.000    0.000 sre_compile.py:265(_compile_charset)
        4    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)
        5    0.000    0.000    0.000    0.000 sre_parse.py:161(__len__)
        4    0.000    0.000    0.000    0.000 strategy.py:351(model)
        1    0.000    0.000    0.000    0.000 sre_compile.py:447(_simple)
        4    0.000    0.000    0.000    0.000 combined_loader.py:308(flattened)
        1    0.000    0.000    0.000    0.000 sre_parse.py:928(fix_flags)
        5    0.000    0.000    0.000    0.000 {method 'find' of 'bytearray' objects}
        3    0.000    0.000    0.000    0.000 sre_parse.py:112(__init__)
        4    0.000    0.000    0.000    0.000 sre_parse.py:173(append)
        4    0.000    0.000    0.000    0.000 sre_parse.py:82(groups)
        1    0.000    0.000    0.000    0.000 sre_compile.py:456(_generate_overlap_table)
        8    0.000    0.000    0.000    0.000 callback.py:232(state_dict)
        2    0.000    0.000    0.000    0.000 enum.py:670(__new__)
        4    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}
        4    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)
       10    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 sre_compile.py:619(isstring)
        1    0.000    0.000    0.000    0.000 functools.py:65(wraps)
        4    0.000    0.000    0.000    0.000 hooks.py:692(on_save_checkpoint)
        1    0.000    0.000    0.000    0.000 {built-in method _sre.compile}
        4    0.000    0.000    0.000    0.000 precision.py:138(state_dict)
        2    0.000    0.000    0.000    0.000 sre_compile.py:81(_combine_flags)
        4    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}
        1    0.000    0.000    0.000    0.000 eval_frame.py:232(nothing)
        5    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}
        4    0.000    0.000    0.000    0.000 single_device.py:90(broadcast)
        2    0.000    0.000    0.000    0.000 sre_compile.py:477(_get_iscased)
        1    0.000    0.000    0.000    0.000 {built-in method fromkeys}
        4    0.000    0.000    0.000    0.000 {built-in method posix.fspath}
        1    0.000    0.000    0.000    0.000 inspect.py:73(isclass)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
        3    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}
        1    0.000    0.000    0.000    0.000 sre_parse.py:169(__setitem__)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'mappingproxy' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.iter}
        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}



Profile stats for: [LightningDataModule]PMTfiedDataModule.state_dict
         28 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        4    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        4    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        4    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        4    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        4    0.000    0.000    0.000    0.000 datamodule.py:150(state_dict)
        4    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_save_checkpoint
         40 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        4    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        4    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        4    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        4    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        4    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        4    0.000    0.000    0.000    0.000 callback.py:250(on_save_checkpoint)
        4    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        4    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        4    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_save_checkpoint
         40 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        4    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        4    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        4    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        4    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        4    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        4    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        4    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        4    0.000    0.000    0.000    0.000 callback.py:250(on_save_checkpoint)
        4    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_save_checkpoint
         40 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        4    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        4    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        4    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        4    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        4    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        4    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        4    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        4    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 callback.py:250(on_save_checkpoint)
        4    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_save_checkpoint
         40 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        4    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        4    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        4    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        4    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        4    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        4    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        4    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        4    0.000    0.000    0.000    0.000 callback.py:250(on_save_checkpoint)
        4    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_save_checkpoint
         28 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        4    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        4    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        4    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        4    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        4    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        4    0.000    0.000    0.000    0.000 hooks.py:692(on_save_checkpoint)
        4    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.lr_scheduler_step
         1132 function calls in 0.006 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.001    0.000    0.006    0.000 module.py:1249(lr_scheduler_step)
      100    0.003    0.000    0.005    0.000 lr_scheduler.py:1316(step)
      100    0.001    0.000    0.001    0.000 lr_scheduler.py:1353(is_better)
      100    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
      100    0.000    0.000    0.000    0.000 {built-in method builtins.next}
      100    0.000    0.000    0.000    0.000 lr_scheduler.py:1349(in_cooldown)
      100    0.000    0.000    0.000    0.000 profiler.py:55(profile)
       16    0.000    0.000    0.000    0.000 lr_scheduler.py:1342(_reduce_lr)
      100    0.000    0.000    0.000    0.000 advanced.py:71(stop)
      100    0.000    0.000    0.000    0.000 lr_scheduler.py:1340(<listcomp>)
      100    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
      100    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
       16    0.000    0.000    0.000    0.000 {built-in method builtins.max}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_train_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 callback.py:208(on_train_end)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}



Profile stats for: [Callback]TQDMProgressBar.on_train_end
         484 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:283(on_train_end)
        1    0.000    0.000    0.000    0.000 std.py:1265(close)
        1    0.000    0.000    0.000    0.000 std.py:1464(display)
        1    0.000    0.000    0.000    0.000 std.py:1150(__str__)
        1    0.000    0.000    0.000    0.000 std.py:464(format_meter)
        4    0.000    0.000    0.000    0.000 utils.py:194(inner)
        3    0.000    0.000    0.000    0.000 redirect.py:644(write)
        3    0.000    0.000    0.000    0.000 wandb_run.py:2304(<lambda>)
        3    0.000    0.000    0.000    0.000 wandb_run.py:390(wrapper_fn)
        3    0.000    0.000    0.000    0.000 wandb_run.py:1429(_console_raw_callback)
        3    0.000    0.000    0.000    0.000 interface.py:749(publish_output_raw)
        1    0.000    0.000    0.000    0.000 std.py:457(print_status)
        2    0.000    0.000    0.000    0.000 std.py:1286(fp_write)
        3    0.000    0.000    0.000    0.000 interface_shared.py:76(_publish_output_raw)
        3    0.000    0.000    0.000    0.000 interface_sock.py:45(_publish)
        3    0.000    0.000    0.000    0.000 sock_client.py:219(send_record_publish)
        3    0.000    0.000    0.000    0.000 sock_client.py:153(send_server_request)
        3    0.000    0.000    0.000    0.000 sock_client.py:145(_send_message)
        1    0.000    0.000    0.000    0.000 std.py:451(fp_write)
        1    0.000    0.000    0.000    0.000 utils.py:378(disp_len)
        3    0.000    0.000    0.000    0.000 sock_client.py:121(_sendall_with_error_handle)
        1    0.000    0.000    0.000    0.000 utils.py:374(_text_width)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.sum}
        3    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}
       96    0.000    0.000    0.000    0.000 utils.py:375(<genexpr>)
        1    0.000    0.000    0.000    0.000 std.py:686(_decr_instances)
        1    0.000    0.000    0.000    0.000 std.py:1446(format_dict)
        3    0.000    0.000    0.000    0.000 well_known_types.py:172(GetCurrentTime)
        1    0.000    0.000    0.000    0.000 utils.py:333(_screen_shape_linux)
        3    0.000    0.000    0.000    0.000 well_known_types.py:242(FromDatetime)
        2    0.000    0.000    0.000    0.000 utils.py:273(_is_ascii)
        2    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}
       95    0.000    0.000    0.000    0.000 {built-in method unicodedata.east_asian_width}
        1    0.000    0.000    0.000    0.000 {built-in method fcntl.ioctl}
        1    0.000    0.000    0.000    0.000 _weakrefset.py:63(__iter__)
        1    0.000    0.000    0.000    0.000 {method 'flush' of '_io.TextIOWrapper' objects}
        1    0.000    0.000    0.000    0.000 _weakrefset.py:111(remove)
        3    0.000    0.000    0.000    0.000 calendar.py:655(timegm)
        2    0.000    0.000    0.000    0.000 std.py:110(__enter__)
        2    0.000    0.000    0.000    0.000 std.py:102(acquire)
        2    0.000    0.000    0.000    0.000 {method 'remove' of 'set' objects}
        1    0.000    0.000    0.000    0.000 utils.py:347(<listcomp>)
        2    0.000    0.000    0.000    0.000 std.py:113(__exit__)
       89    0.000    0.000    0.000    0.000 {built-in method builtins.ord}
        3    0.000    0.000    0.000    0.000 enum_type_wrapper.py:92(__getattr__)
        3    0.000    0.000    0.000    0.000 {method 'utctimetuple' of 'datetime.datetime' objects}
        2    0.000    0.000    0.000    0.000 std.py:400(format_interval)
        1    0.000    0.000    0.000    0.000 os.py:674(__getitem__)
        2    0.000    0.000    0.000    0.000 std.py:106(release)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 _weakrefset.py:27(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        3    0.000    0.000    0.000    0.000 {method 'ByteSize' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 std.py:186(__format__)
        1    0.000    0.000    0.000    0.000 utils.py:125(__eq__)
        6    0.000    0.000    0.000    0.000 {method 'CopyFrom' of 'google._upb._message.Message' objects}
        1    0.000    0.000    0.000    0.000 _weakrefset.py:53(_commit_removals)
        1    0.000    0.000    0.000    0.000 os.py:754(encode)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        3    0.000    0.000    0.000    0.000 interface_sock.py:41(_assign)
        2    0.000    0.000    0.000    0.000 std.py:1153(_comparable)
        1    0.000    0.000    0.000    0.000 _weakrefset.py:21(__enter__)
        1    0.000    0.000    0.000    0.000 std.py:153(__init__)
       10    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}
        3    0.000    0.000    0.000    0.000 {built-in method posix.getpid}
        3    0.000    0.000    0.000    0.000 {method 'SerializeToString' of 'google._upb._message.Message' objects}
        3    0.000    0.000    0.000    0.000 {built-in method utcnow}
        3    0.000    0.000    0.000    0.000 {built-in method _struct.pack}
        1    0.000    0.000    0.000    0.000 std.py:1157(__hash__)
        3    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}
        1    0.000    0.000    0.000    0.000 {built-in method now}
        1    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}
        7    0.000    0.000    0.000    0.000 {built-in method builtins.len}
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)
        2    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}
        5    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}
        3    0.000    0.000    0.000    0.000 {built-in method time.monotonic}
        2    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}
        3    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}
        3    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}
        2    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 {built-in method builtins.max}
        1    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}
        1    0.000    0.000    0.000    0.000 _weakrefset.py:17(__init__)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}
        1    0.000    0.000    0.000    0.000 utils.py:108(__init__)
        1    0.000    0.000    0.000    0.000 utils.py:112(__format__)
        1    0.000    0.000    0.000    0.000 std.py:167(colour)
        3    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}
        3    0.000    0.000    0.000    0.000 {built-in method builtins.abs}
        1    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}
        1    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}
        1    0.000    0.000    0.000    0.000 tqdm_progress.py:121(train_progress_bar)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {built-in method time.time}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 std.py:163(colour)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.id}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 std.py:1301(<lambda>)



Profile stats for: [Callback]ModelSummary.on_train_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:208(on_train_end)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 callback.py:208(on_train_end)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_train_end
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 hooks.py:47(on_train_end)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Strategy]SingleDeviceStrategy.on_train_end
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 strategy.py:561(on_train_end)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.on_fit_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:67(on_fit_end)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.on_fit_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:67(on_fit_end)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.on_fit_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 callback.py:67(on_fit_end)



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_end
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:67(on_fit_end)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.on_fit_end
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 hooks.py:37(on_fit_end)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningDataModule]PMTfiedDataModule.teardown
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 hooks.py:447(teardown)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}



Profile stats for: [Callback]EarlyStopping{'monitor': 'val_acc', 'mode': 'min'}.teardown
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:61(teardown)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]TQDMProgressBar.teardown
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 callback.py:61(teardown)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelSummary.teardown
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 callback.py:61(teardown)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.teardown
         10 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 trainer.py:1178(lightning_module)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 trainer.py:1125(strategy)
        1    0.000    0.000    0.000    0.000 strategy.py:360(lightning_module)
        1    0.000    0.000    0.000    0.000 callback.py:61(teardown)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



Profile stats for: [LightningModule]FlavourClassificationTransformerEncoder.teardown
         7 function calls in 0.000 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)
        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}
        1    0.000    0.000    0.000    0.000 profiler.py:55(profile)
        1    0.000    0.000    0.000    0.000 advanced.py:71(stop)
        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}
        1    0.000    0.000    0.000    0.000 hooks.py:447(teardown)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}



wandb: uploading output.log
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: epoch_avg_train_loss ‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:           train_loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:              val_acc ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:             val_loss ‚ñà‚ñÅ‚ñÖ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb:                epoch 99
wandb: epoch_avg_train_loss 1.04535
wandb:           train_loss 1.04535
wandb:  trainer/global_step 99
wandb:              val_acc 0.33333
wandb:             val_loss 1.04599
wandb: 
wandb: üöÄ View run glamorous-water-1 at: https://wandb.ai/cyans-k-benhavns-universitet/%5B20250204_163721%5D%20Flavour%20Classification/runs/p1l9sz4w
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/cyans-k-benhavns-universitet/%5B20250204_163721%5D%20Flavour%20Classification
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250204_163730-p1l9sz4w/logs

| Parameter       | Value               |
|-----------------|---------------------|
| attention       | XFormers       |
| d_model         | 128            |
| n_heads         | 8              |
| d_f             | 128            |
| num_layers      | 3              |
| d_input         | 32             |
| num_classes     | 3              |
| dropout         | 0.1            |
| learning_rate   | 0.001          |
| epochs          | 100            |
| batch_size      | 32             |

######## Execution time: 00:00:59 ########
