2025-02-04 16:50:32,828 - INFO - Starting training with the following parameters:
2025-02-04 16:50:32,829 - INFO - | Parameter       | Value               |
|-----------------|---------------------|
| attention       | XFormers       |
| d_model         | 128            |
| n_heads         | 8              |
| d_f             | 128            |
| num_layers      | 3              |
| d_input         | 32             |
| num_classes     | 3              |
| dropout         | 0.1            |
| learning_rate   | 0.005          |
| epochs          | 200            |
| batch_size      | 32             |

2025-02-04 16:50:33,361 - INFO - Epoch 0: val_loss=1.2206, val_acc=0.00%
2025-02-04 16:50:33,479 - INFO - ####################Training epoch 0####################
2025-02-04 16:50:33,557 - INFO - Epoch 0: train_loss=1.1120
2025-02-04 16:50:33,859 - INFO - Epoch 0: val_loss=1.1526, val_acc=33.33%
2025-02-04 16:50:33,863 - INFO - Epoch 0: EPOCH_AVG_TRAIN_LOSS=1.1120
2025-02-04 16:50:33,891 - INFO - ####################Training epoch 1####################
2025-02-04 16:50:34,087 - INFO - Epoch 1: train_loss=2.5280
2025-02-04 16:50:34,343 - INFO - Epoch 1: val_loss=0.7063, val_acc=66.67%
2025-02-04 16:50:34,347 - INFO - Epoch 1: EPOCH_AVG_TRAIN_LOSS=2.5280
2025-02-04 16:50:34,370 - INFO - ####################Training epoch 2####################
2025-02-04 16:50:34,569 - INFO - Epoch 2: train_loss=2.4689
2025-02-04 16:50:34,829 - INFO - Epoch 2: val_loss=0.6274, val_acc=66.67%
2025-02-04 16:50:34,833 - INFO - Epoch 2: EPOCH_AVG_TRAIN_LOSS=2.4689
2025-02-04 16:50:34,880 - INFO - ####################Training epoch 3####################
2025-02-04 16:50:35,080 - INFO - Epoch 3: train_loss=1.9903
2025-02-04 16:50:35,334 - INFO - Epoch 3: val_loss=0.6106, val_acc=66.67%
2025-02-04 16:50:35,337 - INFO - Epoch 3: EPOCH_AVG_TRAIN_LOSS=1.9903
2025-02-04 16:50:35,382 - INFO - ####################Training epoch 4####################
2025-02-04 16:50:35,583 - INFO - Epoch 4: train_loss=1.7150
2025-02-04 16:50:35,840 - INFO - Epoch 4: val_loss=0.6256, val_acc=66.67%
2025-02-04 16:50:35,843 - INFO - Epoch 4: EPOCH_AVG_TRAIN_LOSS=1.7150
2025-02-04 16:50:35,870 - INFO - ####################Training epoch 5####################
2025-02-04 16:50:36,069 - INFO - Epoch 5: train_loss=1.5331
2025-02-04 16:50:36,325 - INFO - Epoch 5: val_loss=0.6402, val_acc=66.67%
2025-02-04 16:50:36,329 - INFO - Epoch 5: EPOCH_AVG_TRAIN_LOSS=1.5331
2025-02-04 16:50:36,332 - INFO - ####################Training epoch 6####################
2025-02-04 16:50:36,533 - INFO - Epoch 6: train_loss=1.4666
2025-02-04 16:50:36,789 - INFO - Epoch 6: val_loss=0.6571, val_acc=66.67%
2025-02-04 16:50:36,793 - INFO - Epoch 6: EPOCH_AVG_TRAIN_LOSS=1.4666
2025-02-04 16:50:36,796 - INFO - ####################Training epoch 7####################
2025-02-04 16:50:36,998 - INFO - Epoch 7: train_loss=1.4148
2025-02-04 16:50:37,256 - INFO - Epoch 7: val_loss=0.6752, val_acc=66.67%
2025-02-04 16:50:37,260 - INFO - Epoch 7: EPOCH_AVG_TRAIN_LOSS=1.4148
2025-02-04 16:50:37,263 - INFO - ####################Training epoch 8####################
2025-02-04 16:50:37,466 - INFO - Epoch 8: train_loss=1.3732
2025-02-04 16:50:37,722 - INFO - Epoch 8: val_loss=0.6936, val_acc=66.67%
2025-02-04 16:50:37,726 - INFO - Epoch 8: EPOCH_AVG_TRAIN_LOSS=1.3732
2025-02-04 16:50:37,729 - INFO - ####################Training epoch 9####################
2025-02-04 16:50:37,933 - INFO - Epoch 9: train_loss=1.3407
2025-02-04 16:50:38,194 - INFO - Epoch 9: val_loss=0.7025, val_acc=100.00%
2025-02-04 16:50:38,197 - INFO - Epoch 9: EPOCH_AVG_TRAIN_LOSS=1.3407
2025-02-04 16:50:38,201 - INFO - ####################Training epoch 10####################
2025-02-04 16:50:38,403 - INFO - Epoch 10: train_loss=1.3265
2025-02-04 16:50:38,663 - INFO - Epoch 10: val_loss=0.7108, val_acc=100.00%
2025-02-04 16:50:38,666 - INFO - Epoch 10: EPOCH_AVG_TRAIN_LOSS=1.3265
2025-02-04 16:50:38,670 - INFO - ####################Training epoch 11####################
2025-02-04 16:50:38,872 - INFO - Epoch 11: train_loss=1.3145
2025-02-04 16:50:39,133 - INFO - Epoch 11: val_loss=0.7186, val_acc=66.67%
2025-02-04 16:50:39,137 - INFO - Epoch 11: EPOCH_AVG_TRAIN_LOSS=1.3145
2025-02-04 16:50:39,140 - INFO - ####################Training epoch 12####################
2025-02-04 16:50:39,342 - INFO - Epoch 12: train_loss=1.3041
2025-02-04 16:50:39,601 - INFO - Epoch 12: val_loss=0.7258, val_acc=66.67%
2025-02-04 16:50:39,605 - INFO - Epoch 12: EPOCH_AVG_TRAIN_LOSS=1.3041
2025-02-04 16:50:39,608 - INFO - ####################Training epoch 13####################
2025-02-04 16:50:39,812 - INFO - Epoch 13: train_loss=1.2949
2025-02-04 16:50:40,070 - INFO - Epoch 13: val_loss=0.7292, val_acc=66.67%
2025-02-04 16:50:40,074 - INFO - Epoch 13: EPOCH_AVG_TRAIN_LOSS=1.2949
2025-02-04 16:50:40,077 - INFO - ####################Training epoch 14####################
2025-02-04 16:50:40,282 - INFO - Epoch 14: train_loss=1.2913
2025-02-04 16:50:40,541 - INFO - Epoch 14: val_loss=0.7323, val_acc=66.67%
2025-02-04 16:50:40,545 - INFO - Epoch 14: EPOCH_AVG_TRAIN_LOSS=1.2913
2025-02-04 16:50:40,548 - INFO - ####################Training epoch 15####################
2025-02-04 16:50:40,748 - INFO - Epoch 15: train_loss=1.2882
2025-02-04 16:50:41,006 - INFO - Epoch 15: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:41,010 - INFO - Epoch 15: EPOCH_AVG_TRAIN_LOSS=1.2882
2025-02-04 16:50:41,013 - INFO - ####################Training epoch 16####################
2025-02-04 16:50:41,218 - INFO - Epoch 16: train_loss=nan
2025-02-04 16:50:41,477 - INFO - Epoch 16: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:41,480 - INFO - Epoch 16: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:41,484 - INFO - ####################Training epoch 17####################
2025-02-04 16:50:41,688 - INFO - Epoch 17: train_loss=nan
2025-02-04 16:50:41,947 - INFO - Epoch 17: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:41,951 - INFO - Epoch 17: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:41,954 - INFO - ####################Training epoch 18####################
2025-02-04 16:50:42,157 - INFO - Epoch 18: train_loss=nan
2025-02-04 16:50:42,416 - INFO - Epoch 18: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:42,420 - INFO - Epoch 18: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:42,423 - INFO - ####################Training epoch 19####################
2025-02-04 16:50:42,628 - INFO - Epoch 19: train_loss=nan
2025-02-04 16:50:42,883 - INFO - Epoch 19: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:42,887 - INFO - Epoch 19: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:42,891 - INFO - ####################Training epoch 20####################
2025-02-04 16:50:43,093 - INFO - Epoch 20: train_loss=nan
2025-02-04 16:50:43,353 - INFO - Epoch 20: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:43,356 - INFO - Epoch 20: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:43,360 - INFO - ####################Training epoch 21####################
2025-02-04 16:50:43,562 - INFO - Epoch 21: train_loss=nan
2025-02-04 16:50:43,820 - INFO - Epoch 21: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:43,823 - INFO - Epoch 21: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:43,826 - INFO - ####################Training epoch 22####################
2025-02-04 16:50:44,030 - INFO - Epoch 22: train_loss=nan
2025-02-04 16:50:44,289 - INFO - Epoch 22: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:44,292 - INFO - Epoch 22: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:44,296 - INFO - ####################Training epoch 23####################
2025-02-04 16:50:44,498 - INFO - Epoch 23: train_loss=nan
2025-02-04 16:50:44,759 - INFO - Epoch 23: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:44,763 - INFO - Epoch 23: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:44,766 - INFO - ####################Training epoch 24####################
2025-02-04 16:50:44,973 - INFO - Epoch 24: train_loss=nan
2025-02-04 16:50:45,231 - INFO - Epoch 24: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:45,235 - INFO - Epoch 24: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:45,238 - INFO - ####################Training epoch 25####################
2025-02-04 16:50:45,440 - INFO - Epoch 25: train_loss=nan
2025-02-04 16:50:45,700 - INFO - Epoch 25: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:45,703 - INFO - Epoch 25: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:45,707 - INFO - ####################Training epoch 26####################
2025-02-04 16:50:45,909 - INFO - Epoch 26: train_loss=nan
2025-02-04 16:50:46,169 - INFO - Epoch 26: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:46,172 - INFO - Epoch 26: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:46,176 - INFO - ####################Training epoch 27####################
2025-02-04 16:50:46,380 - INFO - Epoch 27: train_loss=nan
2025-02-04 16:50:46,639 - INFO - Epoch 27: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:46,643 - INFO - Epoch 27: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:46,646 - INFO - ####################Training epoch 28####################
2025-02-04 16:50:46,848 - INFO - Epoch 28: train_loss=nan
2025-02-04 16:50:47,107 - INFO - Epoch 28: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:47,111 - INFO - Epoch 28: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:47,114 - INFO - ####################Training epoch 29####################
2025-02-04 16:50:47,319 - INFO - Epoch 29: train_loss=nan
2025-02-04 16:50:47,578 - INFO - Epoch 29: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:47,582 - INFO - Epoch 29: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:47,585 - INFO - ####################Training epoch 30####################
2025-02-04 16:50:47,787 - INFO - Epoch 30: train_loss=nan
2025-02-04 16:50:48,044 - INFO - Epoch 30: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:48,047 - INFO - Epoch 30: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:48,051 - INFO - ####################Training epoch 31####################
2025-02-04 16:50:48,253 - INFO - Epoch 31: train_loss=nan
2025-02-04 16:50:48,512 - INFO - Epoch 31: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:48,515 - INFO - Epoch 31: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:48,519 - INFO - ####################Training epoch 32####################
2025-02-04 16:50:48,723 - INFO - Epoch 32: train_loss=nan
2025-02-04 16:50:48,981 - INFO - Epoch 32: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:48,987 - INFO - Epoch 32: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:48,990 - INFO - ####################Training epoch 33####################
2025-02-04 16:50:49,191 - INFO - Epoch 33: train_loss=nan
2025-02-04 16:50:49,450 - INFO - Epoch 33: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:49,454 - INFO - Epoch 33: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:49,457 - INFO - ####################Training epoch 34####################
2025-02-04 16:50:49,660 - INFO - Epoch 34: train_loss=nan
2025-02-04 16:50:49,919 - INFO - Epoch 34: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:49,922 - INFO - Epoch 34: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:49,925 - INFO - ####################Training epoch 35####################
2025-02-04 16:50:50,127 - INFO - Epoch 35: train_loss=nan
2025-02-04 16:50:50,385 - INFO - Epoch 35: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:50,389 - INFO - Epoch 35: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:50,392 - INFO - ####################Training epoch 36####################
2025-02-04 16:50:50,595 - INFO - Epoch 36: train_loss=nan
2025-02-04 16:50:50,853 - INFO - Epoch 36: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:50,857 - INFO - Epoch 36: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:50,860 - INFO - ####################Training epoch 37####################
2025-02-04 16:50:51,063 - INFO - Epoch 37: train_loss=nan
2025-02-04 16:50:51,321 - INFO - Epoch 37: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:51,325 - INFO - Epoch 37: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:51,328 - INFO - ####################Training epoch 38####################
2025-02-04 16:50:51,529 - INFO - Epoch 38: train_loss=nan
2025-02-04 16:50:51,789 - INFO - Epoch 38: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:51,792 - INFO - Epoch 38: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:51,796 - INFO - ####################Training epoch 39####################
2025-02-04 16:50:51,997 - INFO - Epoch 39: train_loss=nan
2025-02-04 16:50:52,255 - INFO - Epoch 39: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:52,258 - INFO - Epoch 39: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:52,262 - INFO - ####################Training epoch 40####################
2025-02-04 16:50:52,462 - INFO - Epoch 40: train_loss=nan
2025-02-04 16:50:52,721 - INFO - Epoch 40: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:52,725 - INFO - Epoch 40: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:52,728 - INFO - ####################Training epoch 41####################
2025-02-04 16:50:52,931 - INFO - Epoch 41: train_loss=nan
2025-02-04 16:50:53,191 - INFO - Epoch 41: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:53,194 - INFO - Epoch 41: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:53,198 - INFO - ####################Training epoch 42####################
2025-02-04 16:50:53,401 - INFO - Epoch 42: train_loss=nan
2025-02-04 16:50:53,658 - INFO - Epoch 42: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:53,662 - INFO - Epoch 42: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:53,665 - INFO - ####################Training epoch 43####################
2025-02-04 16:50:53,867 - INFO - Epoch 43: train_loss=nan
2025-02-04 16:50:54,128 - INFO - Epoch 43: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:54,132 - INFO - Epoch 43: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:54,135 - INFO - ####################Training epoch 44####################
2025-02-04 16:50:54,338 - INFO - Epoch 44: train_loss=nan
2025-02-04 16:50:54,597 - INFO - Epoch 44: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:54,600 - INFO - Epoch 44: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:54,604 - INFO - ####################Training epoch 45####################
2025-02-04 16:50:54,803 - INFO - Epoch 45: train_loss=nan
2025-02-04 16:50:55,061 - INFO - Epoch 45: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:55,065 - INFO - Epoch 45: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:55,068 - INFO - ####################Training epoch 46####################
2025-02-04 16:50:55,273 - INFO - Epoch 46: train_loss=nan
2025-02-04 16:50:55,529 - INFO - Epoch 46: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:55,533 - INFO - Epoch 46: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:55,536 - INFO - ####################Training epoch 47####################
2025-02-04 16:50:55,737 - INFO - Epoch 47: train_loss=nan
2025-02-04 16:50:55,993 - INFO - Epoch 47: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:55,997 - INFO - Epoch 47: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:56,000 - INFO - ####################Training epoch 48####################
2025-02-04 16:50:56,203 - INFO - Epoch 48: train_loss=nan
2025-02-04 16:50:56,462 - INFO - Epoch 48: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:56,465 - INFO - Epoch 48: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:56,469 - INFO - ####################Training epoch 49####################
2025-02-04 16:50:56,671 - INFO - Epoch 49: train_loss=nan
2025-02-04 16:50:56,928 - INFO - Epoch 49: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:56,932 - INFO - Epoch 49: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:56,935 - INFO - ####################Training epoch 50####################
2025-02-04 16:50:57,140 - INFO - Epoch 50: train_loss=nan
2025-02-04 16:50:57,399 - INFO - Epoch 50: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:57,403 - INFO - Epoch 50: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:57,406 - INFO - ####################Training epoch 51####################
2025-02-04 16:50:57,610 - INFO - Epoch 51: train_loss=nan
2025-02-04 16:50:57,869 - INFO - Epoch 51: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:57,872 - INFO - Epoch 51: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:57,876 - INFO - ####################Training epoch 52####################
2025-02-04 16:50:58,077 - INFO - Epoch 52: train_loss=nan
2025-02-04 16:50:58,335 - INFO - Epoch 52: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:58,339 - INFO - Epoch 52: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:58,342 - INFO - ####################Training epoch 53####################
2025-02-04 16:50:58,544 - INFO - Epoch 53: train_loss=nan
2025-02-04 16:50:58,804 - INFO - Epoch 53: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:58,807 - INFO - Epoch 53: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:58,810 - INFO - ####################Training epoch 54####################
2025-02-04 16:50:59,012 - INFO - Epoch 54: train_loss=nan
2025-02-04 16:50:59,272 - INFO - Epoch 54: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:59,276 - INFO - Epoch 54: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:59,279 - INFO - ####################Training epoch 55####################
2025-02-04 16:50:59,483 - INFO - Epoch 55: train_loss=nan
2025-02-04 16:50:59,740 - INFO - Epoch 55: val_loss=nan, val_acc=66.67%
2025-02-04 16:50:59,744 - INFO - Epoch 55: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:50:59,747 - INFO - ####################Training epoch 56####################
2025-02-04 16:50:59,951 - INFO - Epoch 56: train_loss=nan
2025-02-04 16:51:00,213 - INFO - Epoch 56: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:00,216 - INFO - Epoch 56: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:00,219 - INFO - ####################Training epoch 57####################
2025-02-04 16:51:00,421 - INFO - Epoch 57: train_loss=nan
2025-02-04 16:51:00,677 - INFO - Epoch 57: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:00,681 - INFO - Epoch 57: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:00,684 - INFO - ####################Training epoch 58####################
2025-02-04 16:51:00,885 - INFO - Epoch 58: train_loss=nan
2025-02-04 16:51:01,143 - INFO - Epoch 58: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:01,146 - INFO - Epoch 58: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:01,150 - INFO - ####################Training epoch 59####################
2025-02-04 16:51:01,352 - INFO - Epoch 59: train_loss=nan
2025-02-04 16:51:01,611 - INFO - Epoch 59: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:01,614 - INFO - Epoch 59: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:01,618 - INFO - ####################Training epoch 60####################
2025-02-04 16:51:01,819 - INFO - Epoch 60: train_loss=nan
2025-02-04 16:51:02,077 - INFO - Epoch 60: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:02,080 - INFO - Epoch 60: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:02,084 - INFO - ####################Training epoch 61####################
2025-02-04 16:51:02,288 - INFO - Epoch 61: train_loss=nan
2025-02-04 16:51:02,547 - INFO - Epoch 61: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:02,550 - INFO - Epoch 61: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:02,553 - INFO - ####################Training epoch 62####################
2025-02-04 16:51:02,754 - INFO - Epoch 62: train_loss=nan
2025-02-04 16:51:03,009 - INFO - Epoch 62: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:03,013 - INFO - Epoch 62: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:03,016 - INFO - ####################Training epoch 63####################
2025-02-04 16:51:03,219 - INFO - Epoch 63: train_loss=nan
2025-02-04 16:51:03,477 - INFO - Epoch 63: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:03,481 - INFO - Epoch 63: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:03,484 - INFO - ####################Training epoch 64####################
2025-02-04 16:51:03,687 - INFO - Epoch 64: train_loss=nan
2025-02-04 16:51:03,951 - INFO - Epoch 64: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:03,955 - INFO - Epoch 64: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:03,958 - INFO - ####################Training epoch 65####################
2025-02-04 16:51:04,160 - INFO - Epoch 65: train_loss=nan
2025-02-04 16:51:04,417 - INFO - Epoch 65: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:04,421 - INFO - Epoch 65: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:04,424 - INFO - ####################Training epoch 66####################
2025-02-04 16:51:04,627 - INFO - Epoch 66: train_loss=nan
2025-02-04 16:51:04,885 - INFO - Epoch 66: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:04,889 - INFO - Epoch 66: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:04,892 - INFO - ####################Training epoch 67####################
2025-02-04 16:51:05,096 - INFO - Epoch 67: train_loss=nan
2025-02-04 16:51:05,353 - INFO - Epoch 67: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:05,357 - INFO - Epoch 67: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:05,361 - INFO - ####################Training epoch 68####################
2025-02-04 16:51:05,564 - INFO - Epoch 68: train_loss=nan
2025-02-04 16:51:05,821 - INFO - Epoch 68: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:05,824 - INFO - Epoch 68: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:05,828 - INFO - ####################Training epoch 69####################
2025-02-04 16:51:06,028 - INFO - Epoch 69: train_loss=nan
2025-02-04 16:51:06,283 - INFO - Epoch 69: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:06,287 - INFO - Epoch 69: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:06,290 - INFO - ####################Training epoch 70####################
2025-02-04 16:51:06,494 - INFO - Epoch 70: train_loss=nan
2025-02-04 16:51:06,751 - INFO - Epoch 70: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:06,754 - INFO - Epoch 70: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:06,758 - INFO - ####################Training epoch 71####################
2025-02-04 16:51:06,959 - INFO - Epoch 71: train_loss=nan
2025-02-04 16:51:07,217 - INFO - Epoch 71: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:07,220 - INFO - Epoch 71: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:07,224 - INFO - ####################Training epoch 72####################
2025-02-04 16:51:07,426 - INFO - Epoch 72: train_loss=nan
2025-02-04 16:51:07,685 - INFO - Epoch 72: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:07,688 - INFO - Epoch 72: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:07,692 - INFO - ####################Training epoch 73####################
2025-02-04 16:51:07,895 - INFO - Epoch 73: train_loss=nan
2025-02-04 16:51:08,154 - INFO - Epoch 73: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:08,158 - INFO - Epoch 73: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:08,161 - INFO - ####################Training epoch 74####################
2025-02-04 16:51:08,363 - INFO - Epoch 74: train_loss=nan
2025-02-04 16:51:08,621 - INFO - Epoch 74: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:08,625 - INFO - Epoch 74: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:08,628 - INFO - ####################Training epoch 75####################
2025-02-04 16:51:08,833 - INFO - Epoch 75: train_loss=nan
2025-02-04 16:51:09,091 - INFO - Epoch 75: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:09,095 - INFO - Epoch 75: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:09,098 - INFO - ####################Training epoch 76####################
2025-02-04 16:51:09,301 - INFO - Epoch 76: train_loss=nan
2025-02-04 16:51:09,559 - INFO - Epoch 76: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:09,563 - INFO - Epoch 76: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:09,566 - INFO - ####################Training epoch 77####################
2025-02-04 16:51:09,769 - INFO - Epoch 77: train_loss=nan
2025-02-04 16:51:10,028 - INFO - Epoch 77: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:10,032 - INFO - Epoch 77: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:10,035 - INFO - ####################Training epoch 78####################
2025-02-04 16:51:10,238 - INFO - Epoch 78: train_loss=nan
2025-02-04 16:51:10,492 - INFO - Epoch 78: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:10,496 - INFO - Epoch 78: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:10,499 - INFO - ####################Training epoch 79####################
2025-02-04 16:51:10,702 - INFO - Epoch 79: train_loss=nan
2025-02-04 16:51:10,961 - INFO - Epoch 79: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:10,964 - INFO - Epoch 79: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:10,968 - INFO - ####################Training epoch 80####################
2025-02-04 16:51:11,168 - INFO - Epoch 80: train_loss=nan
2025-02-04 16:51:11,427 - INFO - Epoch 80: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:11,430 - INFO - Epoch 80: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:11,434 - INFO - ####################Training epoch 81####################
2025-02-04 16:51:11,637 - INFO - Epoch 81: train_loss=nan
2025-02-04 16:51:11,894 - INFO - Epoch 81: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:11,898 - INFO - Epoch 81: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:11,901 - INFO - ####################Training epoch 82####################
2025-02-04 16:51:12,104 - INFO - Epoch 82: train_loss=nan
2025-02-04 16:51:12,362 - INFO - Epoch 82: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:12,366 - INFO - Epoch 82: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:12,369 - INFO - ####################Training epoch 83####################
2025-02-04 16:51:12,571 - INFO - Epoch 83: train_loss=nan
2025-02-04 16:51:12,833 - INFO - Epoch 83: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:12,837 - INFO - Epoch 83: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:12,840 - INFO - ####################Training epoch 84####################
2025-02-04 16:51:13,043 - INFO - Epoch 84: train_loss=nan
2025-02-04 16:51:13,302 - INFO - Epoch 84: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:13,306 - INFO - Epoch 84: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:13,309 - INFO - ####################Training epoch 85####################
2025-02-04 16:51:13,514 - INFO - Epoch 85: train_loss=nan
2025-02-04 16:51:13,773 - INFO - Epoch 85: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:13,777 - INFO - Epoch 85: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:13,780 - INFO - ####################Training epoch 86####################
2025-02-04 16:51:13,983 - INFO - Epoch 86: train_loss=nan
2025-02-04 16:51:14,241 - INFO - Epoch 86: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:14,244 - INFO - Epoch 86: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:14,248 - INFO - ####################Training epoch 87####################
2025-02-04 16:51:14,451 - INFO - Epoch 87: train_loss=nan
2025-02-04 16:51:14,708 - INFO - Epoch 87: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:14,712 - INFO - Epoch 87: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:14,715 - INFO - ####################Training epoch 88####################
2025-02-04 16:51:14,917 - INFO - Epoch 88: train_loss=nan
2025-02-04 16:51:15,175 - INFO - Epoch 88: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:15,179 - INFO - Epoch 88: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:15,182 - INFO - ####################Training epoch 89####################
2025-02-04 16:51:15,382 - INFO - Epoch 89: train_loss=nan
2025-02-04 16:51:15,640 - INFO - Epoch 89: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:15,644 - INFO - Epoch 89: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:15,647 - INFO - ####################Training epoch 90####################
2025-02-04 16:51:15,848 - INFO - Epoch 90: train_loss=nan
2025-02-04 16:51:16,107 - INFO - Epoch 90: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:16,110 - INFO - Epoch 90: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:16,114 - INFO - ####################Training epoch 91####################
2025-02-04 16:51:16,316 - INFO - Epoch 91: train_loss=nan
2025-02-04 16:51:16,574 - INFO - Epoch 91: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:16,577 - INFO - Epoch 91: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:16,580 - INFO - ####################Training epoch 92####################
2025-02-04 16:51:16,783 - INFO - Epoch 92: train_loss=nan
2025-02-04 16:51:17,042 - INFO - Epoch 92: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:17,045 - INFO - Epoch 92: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:17,048 - INFO - ####################Training epoch 93####################
2025-02-04 16:51:17,249 - INFO - Epoch 93: train_loss=nan
2025-02-04 16:51:17,508 - INFO - Epoch 93: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:17,512 - INFO - Epoch 93: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:17,515 - INFO - ####################Training epoch 94####################
2025-02-04 16:51:17,716 - INFO - Epoch 94: train_loss=nan
2025-02-04 16:51:17,973 - INFO - Epoch 94: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:17,977 - INFO - Epoch 94: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:17,980 - INFO - ####################Training epoch 95####################
2025-02-04 16:51:18,182 - INFO - Epoch 95: train_loss=nan
2025-02-04 16:51:18,438 - INFO - Epoch 95: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:18,442 - INFO - Epoch 95: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:18,445 - INFO - ####################Training epoch 96####################
2025-02-04 16:51:18,647 - INFO - Epoch 96: train_loss=nan
2025-02-04 16:51:18,905 - INFO - Epoch 96: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:18,908 - INFO - Epoch 96: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:18,912 - INFO - ####################Training epoch 97####################
2025-02-04 16:51:19,114 - INFO - Epoch 97: train_loss=nan
2025-02-04 16:51:19,375 - INFO - Epoch 97: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:19,379 - INFO - Epoch 97: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:19,382 - INFO - ####################Training epoch 98####################
2025-02-04 16:51:19,584 - INFO - Epoch 98: train_loss=nan
2025-02-04 16:51:19,843 - INFO - Epoch 98: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:19,847 - INFO - Epoch 98: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:19,851 - INFO - ####################Training epoch 99####################
2025-02-04 16:51:20,053 - INFO - Epoch 99: train_loss=nan
2025-02-04 16:51:20,312 - INFO - Epoch 99: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:20,316 - INFO - Epoch 99: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:20,319 - INFO - ####################Training epoch 100####################
2025-02-04 16:51:20,520 - INFO - Epoch 100: train_loss=nan
2025-02-04 16:51:20,780 - INFO - Epoch 100: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:20,783 - INFO - Epoch 100: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:20,787 - INFO - ####################Training epoch 101####################
2025-02-04 16:51:20,990 - INFO - Epoch 101: train_loss=nan
2025-02-04 16:51:21,247 - INFO - Epoch 101: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:21,251 - INFO - Epoch 101: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:21,254 - INFO - ####################Training epoch 102####################
2025-02-04 16:51:21,457 - INFO - Epoch 102: train_loss=nan
2025-02-04 16:51:21,715 - INFO - Epoch 102: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:21,719 - INFO - Epoch 102: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:21,722 - INFO - ####################Training epoch 103####################
2025-02-04 16:51:21,927 - INFO - Epoch 103: train_loss=nan
2025-02-04 16:51:22,191 - INFO - Epoch 103: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:22,195 - INFO - Epoch 103: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:22,198 - INFO - ####################Training epoch 104####################
2025-02-04 16:51:22,400 - INFO - Epoch 104: train_loss=nan
2025-02-04 16:51:22,658 - INFO - Epoch 104: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:22,662 - INFO - Epoch 104: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:22,665 - INFO - ####################Training epoch 105####################
2025-02-04 16:51:22,870 - INFO - Epoch 105: train_loss=nan
2025-02-04 16:51:23,128 - INFO - Epoch 105: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:23,131 - INFO - Epoch 105: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:23,134 - INFO - ####################Training epoch 106####################
2025-02-04 16:51:23,337 - INFO - Epoch 106: train_loss=nan
2025-02-04 16:51:23,596 - INFO - Epoch 106: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:23,600 - INFO - Epoch 106: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:23,603 - INFO - ####################Training epoch 107####################
2025-02-04 16:51:23,805 - INFO - Epoch 107: train_loss=nan
2025-02-04 16:51:24,066 - INFO - Epoch 107: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:24,070 - INFO - Epoch 107: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:24,074 - INFO - ####################Training epoch 108####################
2025-02-04 16:51:24,277 - INFO - Epoch 108: train_loss=nan
2025-02-04 16:51:24,535 - INFO - Epoch 108: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:24,539 - INFO - Epoch 108: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:24,542 - INFO - ####################Training epoch 109####################
2025-02-04 16:51:24,747 - INFO - Epoch 109: train_loss=nan
2025-02-04 16:51:25,005 - INFO - Epoch 109: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:25,009 - INFO - Epoch 109: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:25,012 - INFO - ####################Training epoch 110####################
2025-02-04 16:51:25,214 - INFO - Epoch 110: train_loss=nan
2025-02-04 16:51:25,472 - INFO - Epoch 110: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:25,475 - INFO - Epoch 110: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:25,479 - INFO - ####################Training epoch 111####################
2025-02-04 16:51:25,695 - INFO - Epoch 111: train_loss=nan
2025-02-04 16:51:25,954 - INFO - Epoch 111: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:25,958 - INFO - Epoch 111: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:25,961 - INFO - ####################Training epoch 112####################
2025-02-04 16:51:26,162 - INFO - Epoch 112: train_loss=nan
2025-02-04 16:51:26,419 - INFO - Epoch 112: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:26,423 - INFO - Epoch 112: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:26,426 - INFO - ####################Training epoch 113####################
2025-02-04 16:51:26,628 - INFO - Epoch 113: train_loss=nan
2025-02-04 16:51:26,885 - INFO - Epoch 113: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:26,889 - INFO - Epoch 113: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:26,892 - INFO - ####################Training epoch 114####################
2025-02-04 16:51:27,094 - INFO - Epoch 114: train_loss=nan
2025-02-04 16:51:27,349 - INFO - Epoch 114: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:27,352 - INFO - Epoch 114: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:27,356 - INFO - ####################Training epoch 115####################
2025-02-04 16:51:27,558 - INFO - Epoch 115: train_loss=nan
2025-02-04 16:51:27,817 - INFO - Epoch 115: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:27,820 - INFO - Epoch 115: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:27,824 - INFO - ####################Training epoch 116####################
2025-02-04 16:51:28,026 - INFO - Epoch 116: train_loss=nan
2025-02-04 16:51:28,285 - INFO - Epoch 116: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:28,289 - INFO - Epoch 116: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:28,292 - INFO - ####################Training epoch 117####################
2025-02-04 16:51:28,496 - INFO - Epoch 117: train_loss=nan
2025-02-04 16:51:28,760 - INFO - Epoch 117: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:28,764 - INFO - Epoch 117: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:28,767 - INFO - ####################Training epoch 118####################
2025-02-04 16:51:28,972 - INFO - Epoch 118: train_loss=nan
2025-02-04 16:51:29,229 - INFO - Epoch 118: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:29,232 - INFO - Epoch 118: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:29,236 - INFO - ####################Training epoch 119####################
2025-02-04 16:51:29,440 - INFO - Epoch 119: train_loss=nan
2025-02-04 16:51:29,700 - INFO - Epoch 119: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:29,703 - INFO - Epoch 119: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:29,707 - INFO - ####################Training epoch 120####################
2025-02-04 16:51:29,909 - INFO - Epoch 120: train_loss=nan
2025-02-04 16:51:30,167 - INFO - Epoch 120: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:30,171 - INFO - Epoch 120: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:30,174 - INFO - ####################Training epoch 121####################
2025-02-04 16:51:30,376 - INFO - Epoch 121: train_loss=nan
2025-02-04 16:51:30,633 - INFO - Epoch 121: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:30,636 - INFO - Epoch 121: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:30,640 - INFO - ####################Training epoch 122####################
2025-02-04 16:51:30,843 - INFO - Epoch 122: train_loss=nan
2025-02-04 16:51:31,100 - INFO - Epoch 122: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:31,104 - INFO - Epoch 122: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:31,107 - INFO - ####################Training epoch 123####################
2025-02-04 16:51:31,310 - INFO - Epoch 123: train_loss=nan
2025-02-04 16:51:31,570 - INFO - Epoch 123: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:31,574 - INFO - Epoch 123: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:31,577 - INFO - ####################Training epoch 124####################
2025-02-04 16:51:31,779 - INFO - Epoch 124: train_loss=nan
2025-02-04 16:51:32,037 - INFO - Epoch 124: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:32,040 - INFO - Epoch 124: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:32,044 - INFO - ####################Training epoch 125####################
2025-02-04 16:51:32,243 - INFO - Epoch 125: train_loss=nan
2025-02-04 16:51:32,499 - INFO - Epoch 125: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:32,502 - INFO - Epoch 125: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:32,506 - INFO - ####################Training epoch 126####################
2025-02-04 16:51:32,709 - INFO - Epoch 126: train_loss=nan
2025-02-04 16:51:32,964 - INFO - Epoch 126: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:32,967 - INFO - Epoch 126: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:32,970 - INFO - ####################Training epoch 127####################
2025-02-04 16:51:33,172 - INFO - Epoch 127: train_loss=nan
2025-02-04 16:51:33,427 - INFO - Epoch 127: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:33,431 - INFO - Epoch 127: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:33,434 - INFO - ####################Training epoch 128####################
2025-02-04 16:51:33,634 - INFO - Epoch 128: train_loss=nan
2025-02-04 16:51:33,892 - INFO - Epoch 128: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:33,896 - INFO - Epoch 128: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:33,899 - INFO - ####################Training epoch 129####################
2025-02-04 16:51:34,103 - INFO - Epoch 129: train_loss=nan
2025-02-04 16:51:34,362 - INFO - Epoch 129: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:34,366 - INFO - Epoch 129: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:34,369 - INFO - ####################Training epoch 130####################
2025-02-04 16:51:34,571 - INFO - Epoch 130: train_loss=nan
2025-02-04 16:51:34,828 - INFO - Epoch 130: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:34,831 - INFO - Epoch 130: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:34,835 - INFO - ####################Training epoch 131####################
2025-02-04 16:51:35,038 - INFO - Epoch 131: train_loss=nan
2025-02-04 16:51:35,296 - INFO - Epoch 131: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:35,300 - INFO - Epoch 131: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:35,303 - INFO - ####################Training epoch 132####################
2025-02-04 16:51:35,505 - INFO - Epoch 132: train_loss=nan
2025-02-04 16:51:35,761 - INFO - Epoch 132: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:35,764 - INFO - Epoch 132: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:35,768 - INFO - ####################Training epoch 133####################
2025-02-04 16:51:35,969 - INFO - Epoch 133: train_loss=nan
2025-02-04 16:51:36,227 - INFO - Epoch 133: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:36,230 - INFO - Epoch 133: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:36,234 - INFO - ####################Training epoch 134####################
2025-02-04 16:51:36,435 - INFO - Epoch 134: train_loss=nan
2025-02-04 16:51:36,693 - INFO - Epoch 134: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:36,697 - INFO - Epoch 134: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:36,700 - INFO - ####################Training epoch 135####################
2025-02-04 16:51:36,902 - INFO - Epoch 135: train_loss=nan
2025-02-04 16:51:37,157 - INFO - Epoch 135: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:37,160 - INFO - Epoch 135: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:37,164 - INFO - ####################Training epoch 136####################
2025-02-04 16:51:37,373 - INFO - Epoch 136: train_loss=nan
2025-02-04 16:51:37,630 - INFO - Epoch 136: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:37,634 - INFO - Epoch 136: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:37,637 - INFO - ####################Training epoch 137####################
2025-02-04 16:51:37,844 - INFO - Epoch 137: train_loss=nan
2025-02-04 16:51:38,101 - INFO - Epoch 137: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:38,105 - INFO - Epoch 137: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:38,108 - INFO - ####################Training epoch 138####################
2025-02-04 16:51:38,311 - INFO - Epoch 138: train_loss=nan
2025-02-04 16:51:38,569 - INFO - Epoch 138: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:38,573 - INFO - Epoch 138: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:38,576 - INFO - ####################Training epoch 139####################
2025-02-04 16:51:38,779 - INFO - Epoch 139: train_loss=nan
2025-02-04 16:51:39,036 - INFO - Epoch 139: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:39,040 - INFO - Epoch 139: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:39,043 - INFO - ####################Training epoch 140####################
2025-02-04 16:51:39,245 - INFO - Epoch 140: train_loss=nan
2025-02-04 16:51:39,500 - INFO - Epoch 140: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:39,504 - INFO - Epoch 140: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:39,507 - INFO - ####################Training epoch 141####################
2025-02-04 16:51:39,710 - INFO - Epoch 141: train_loss=nan
2025-02-04 16:51:39,969 - INFO - Epoch 141: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:39,973 - INFO - Epoch 141: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:39,976 - INFO - ####################Training epoch 142####################
2025-02-04 16:51:40,179 - INFO - Epoch 142: train_loss=nan
2025-02-04 16:51:40,436 - INFO - Epoch 142: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:40,440 - INFO - Epoch 142: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:40,443 - INFO - ####################Training epoch 143####################
2025-02-04 16:51:40,644 - INFO - Epoch 143: train_loss=nan
2025-02-04 16:51:40,899 - INFO - Epoch 143: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:40,903 - INFO - Epoch 143: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:40,906 - INFO - ####################Training epoch 144####################
2025-02-04 16:51:41,109 - INFO - Epoch 144: train_loss=nan
2025-02-04 16:51:41,366 - INFO - Epoch 144: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:41,370 - INFO - Epoch 144: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:41,373 - INFO - ####################Training epoch 145####################
2025-02-04 16:51:41,576 - INFO - Epoch 145: train_loss=nan
2025-02-04 16:51:41,835 - INFO - Epoch 145: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:41,838 - INFO - Epoch 145: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:41,841 - INFO - ####################Training epoch 146####################
2025-02-04 16:51:42,045 - INFO - Epoch 146: train_loss=nan
2025-02-04 16:51:42,306 - INFO - Epoch 146: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:42,309 - INFO - Epoch 146: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:42,313 - INFO - ####################Training epoch 147####################
2025-02-04 16:51:42,515 - INFO - Epoch 147: train_loss=nan
2025-02-04 16:51:42,776 - INFO - Epoch 147: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:42,779 - INFO - Epoch 147: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:42,783 - INFO - ####################Training epoch 148####################
2025-02-04 16:51:42,985 - INFO - Epoch 148: train_loss=nan
2025-02-04 16:51:43,245 - INFO - Epoch 148: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:43,248 - INFO - Epoch 148: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:43,251 - INFO - ####################Training epoch 149####################
2025-02-04 16:51:43,453 - INFO - Epoch 149: train_loss=nan
2025-02-04 16:51:43,709 - INFO - Epoch 149: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:43,713 - INFO - Epoch 149: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:43,716 - INFO - ####################Training epoch 150####################
2025-02-04 16:51:43,917 - INFO - Epoch 150: train_loss=nan
2025-02-04 16:51:44,175 - INFO - Epoch 150: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:44,178 - INFO - Epoch 150: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:44,182 - INFO - ####################Training epoch 151####################
2025-02-04 16:51:44,386 - INFO - Epoch 151: train_loss=nan
2025-02-04 16:51:44,652 - INFO - Epoch 151: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:44,655 - INFO - Epoch 151: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:44,658 - INFO - ####################Training epoch 152####################
2025-02-04 16:51:44,863 - INFO - Epoch 152: train_loss=nan
2025-02-04 16:51:45,120 - INFO - Epoch 152: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:45,123 - INFO - Epoch 152: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:45,127 - INFO - ####################Training epoch 153####################
2025-02-04 16:51:45,329 - INFO - Epoch 153: train_loss=nan
2025-02-04 16:51:45,584 - INFO - Epoch 153: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:45,588 - INFO - Epoch 153: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:45,591 - INFO - ####################Training epoch 154####################
2025-02-04 16:51:45,796 - INFO - Epoch 154: train_loss=nan
2025-02-04 16:51:46,055 - INFO - Epoch 154: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:46,059 - INFO - Epoch 154: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:46,062 - INFO - ####################Training epoch 155####################
2025-02-04 16:51:46,264 - INFO - Epoch 155: train_loss=nan
2025-02-04 16:51:46,521 - INFO - Epoch 155: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:46,524 - INFO - Epoch 155: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:46,528 - INFO - ####################Training epoch 156####################
2025-02-04 16:51:46,731 - INFO - Epoch 156: train_loss=nan
2025-02-04 16:51:46,990 - INFO - Epoch 156: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:46,993 - INFO - Epoch 156: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:46,996 - INFO - ####################Training epoch 157####################
2025-02-04 16:51:47,196 - INFO - Epoch 157: train_loss=nan
2025-02-04 16:51:47,456 - INFO - Epoch 157: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:47,460 - INFO - Epoch 157: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:47,463 - INFO - ####################Training epoch 158####################
2025-02-04 16:51:47,667 - INFO - Epoch 158: train_loss=nan
2025-02-04 16:51:47,923 - INFO - Epoch 158: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:47,927 - INFO - Epoch 158: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:47,930 - INFO - ####################Training epoch 159####################
2025-02-04 16:51:48,132 - INFO - Epoch 159: train_loss=nan
2025-02-04 16:51:48,391 - INFO - Epoch 159: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:48,395 - INFO - Epoch 159: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:48,398 - INFO - ####################Training epoch 160####################
2025-02-04 16:51:48,598 - INFO - Epoch 160: train_loss=nan
2025-02-04 16:51:48,855 - INFO - Epoch 160: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:48,858 - INFO - Epoch 160: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:48,862 - INFO - ####################Training epoch 161####################
2025-02-04 16:51:49,067 - INFO - Epoch 161: train_loss=nan
2025-02-04 16:51:49,324 - INFO - Epoch 161: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:49,327 - INFO - Epoch 161: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:49,331 - INFO - ####################Training epoch 162####################
2025-02-04 16:51:49,531 - INFO - Epoch 162: train_loss=nan
2025-02-04 16:51:49,787 - INFO - Epoch 162: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:49,791 - INFO - Epoch 162: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:49,794 - INFO - ####################Training epoch 163####################
2025-02-04 16:51:49,997 - INFO - Epoch 163: train_loss=nan
2025-02-04 16:51:50,255 - INFO - Epoch 163: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:50,259 - INFO - Epoch 163: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:50,262 - INFO - ####################Training epoch 164####################
2025-02-04 16:51:50,463 - INFO - Epoch 164: train_loss=nan
2025-02-04 16:51:50,721 - INFO - Epoch 164: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:50,725 - INFO - Epoch 164: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:50,728 - INFO - ####################Training epoch 165####################
2025-02-04 16:51:50,933 - INFO - Epoch 165: train_loss=nan
2025-02-04 16:51:51,191 - INFO - Epoch 165: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:51,195 - INFO - Epoch 165: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:51,198 - INFO - ####################Training epoch 166####################
2025-02-04 16:51:51,400 - INFO - Epoch 166: train_loss=nan
2025-02-04 16:51:51,659 - INFO - Epoch 166: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:51,663 - INFO - Epoch 166: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:51,666 - INFO - ####################Training epoch 167####################
2025-02-04 16:51:51,868 - INFO - Epoch 167: train_loss=nan
2025-02-04 16:51:52,127 - INFO - Epoch 167: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:52,131 - INFO - Epoch 167: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:52,134 - INFO - ####################Training epoch 168####################
2025-02-04 16:51:52,336 - INFO - Epoch 168: train_loss=nan
2025-02-04 16:51:52,593 - INFO - Epoch 168: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:52,597 - INFO - Epoch 168: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:52,600 - INFO - ####################Training epoch 169####################
2025-02-04 16:51:52,803 - INFO - Epoch 169: train_loss=nan
2025-02-04 16:51:53,058 - INFO - Epoch 169: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:53,061 - INFO - Epoch 169: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:53,065 - INFO - ####################Training epoch 170####################
2025-02-04 16:51:53,266 - INFO - Epoch 170: train_loss=nan
2025-02-04 16:51:53,525 - INFO - Epoch 170: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:53,529 - INFO - Epoch 170: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:53,532 - INFO - ####################Training epoch 171####################
2025-02-04 16:51:53,883 - INFO - Epoch 171: train_loss=nan
2025-02-04 16:51:54,145 - INFO - Epoch 171: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:54,148 - INFO - Epoch 171: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:54,151 - INFO - ####################Training epoch 172####################
2025-02-04 16:51:54,364 - INFO - Epoch 172: train_loss=nan
2025-02-04 16:51:54,629 - INFO - Epoch 172: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:54,633 - INFO - Epoch 172: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:54,636 - INFO - ####################Training epoch 173####################
2025-02-04 16:51:54,846 - INFO - Epoch 173: train_loss=nan
2025-02-04 16:51:55,111 - INFO - Epoch 173: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:55,114 - INFO - Epoch 173: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:55,117 - INFO - ####################Training epoch 174####################
2025-02-04 16:51:55,329 - INFO - Epoch 174: train_loss=nan
2025-02-04 16:51:55,592 - INFO - Epoch 174: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:55,595 - INFO - Epoch 174: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:55,599 - INFO - ####################Training epoch 175####################
2025-02-04 16:51:55,806 - INFO - Epoch 175: train_loss=nan
2025-02-04 16:51:56,070 - INFO - Epoch 175: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:56,074 - INFO - Epoch 175: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:56,077 - INFO - ####################Training epoch 176####################
2025-02-04 16:51:56,287 - INFO - Epoch 176: train_loss=nan
2025-02-04 16:51:56,557 - INFO - Epoch 176: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:56,560 - INFO - Epoch 176: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:56,563 - INFO - ####################Training epoch 177####################
2025-02-04 16:51:56,773 - INFO - Epoch 177: train_loss=nan
2025-02-04 16:51:57,036 - INFO - Epoch 177: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:57,040 - INFO - Epoch 177: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:57,043 - INFO - ####################Training epoch 178####################
2025-02-04 16:51:57,259 - INFO - Epoch 178: train_loss=nan
2025-02-04 16:51:57,524 - INFO - Epoch 178: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:57,528 - INFO - Epoch 178: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:57,531 - INFO - ####################Training epoch 179####################
2025-02-04 16:51:57,743 - INFO - Epoch 179: train_loss=nan
2025-02-04 16:51:58,010 - INFO - Epoch 179: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:58,013 - INFO - Epoch 179: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:58,017 - INFO - ####################Training epoch 180####################
2025-02-04 16:51:58,228 - INFO - Epoch 180: train_loss=nan
2025-02-04 16:51:58,494 - INFO - Epoch 180: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:58,498 - INFO - Epoch 180: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:58,501 - INFO - ####################Training epoch 181####################
2025-02-04 16:51:58,710 - INFO - Epoch 181: train_loss=nan
2025-02-04 16:51:58,977 - INFO - Epoch 181: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:58,981 - INFO - Epoch 181: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:58,984 - INFO - ####################Training epoch 182####################
2025-02-04 16:51:59,196 - INFO - Epoch 182: train_loss=nan
2025-02-04 16:51:59,463 - INFO - Epoch 182: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:59,466 - INFO - Epoch 182: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:59,470 - INFO - ####################Training epoch 183####################
2025-02-04 16:51:59,680 - INFO - Epoch 183: train_loss=nan
2025-02-04 16:51:59,947 - INFO - Epoch 183: val_loss=nan, val_acc=66.67%
2025-02-04 16:51:59,951 - INFO - Epoch 183: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:51:59,954 - INFO - ####################Training epoch 184####################
2025-02-04 16:52:00,166 - INFO - Epoch 184: train_loss=nan
2025-02-04 16:52:00,431 - INFO - Epoch 184: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:00,435 - INFO - Epoch 184: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:00,438 - INFO - ####################Training epoch 185####################
2025-02-04 16:52:00,649 - INFO - Epoch 185: train_loss=nan
2025-02-04 16:52:00,914 - INFO - Epoch 185: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:00,918 - INFO - Epoch 185: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:00,921 - INFO - ####################Training epoch 186####################
2025-02-04 16:52:01,132 - INFO - Epoch 186: train_loss=nan
2025-02-04 16:52:01,399 - INFO - Epoch 186: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:01,402 - INFO - Epoch 186: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:01,405 - INFO - ####################Training epoch 187####################
2025-02-04 16:52:01,617 - INFO - Epoch 187: train_loss=nan
2025-02-04 16:52:01,885 - INFO - Epoch 187: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:01,888 - INFO - Epoch 187: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:01,892 - INFO - ####################Training epoch 188####################
2025-02-04 16:52:02,099 - INFO - Epoch 188: train_loss=nan
2025-02-04 16:52:02,364 - INFO - Epoch 188: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:02,367 - INFO - Epoch 188: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:02,371 - INFO - ####################Training epoch 189####################
2025-02-04 16:52:02,583 - INFO - Epoch 189: train_loss=nan
2025-02-04 16:52:02,851 - INFO - Epoch 189: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:02,854 - INFO - Epoch 189: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:02,857 - INFO - ####################Training epoch 190####################
2025-02-04 16:52:03,074 - INFO - Epoch 190: train_loss=nan
2025-02-04 16:52:03,339 - INFO - Epoch 190: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:03,342 - INFO - Epoch 190: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:03,345 - INFO - ####################Training epoch 191####################
2025-02-04 16:52:03,555 - INFO - Epoch 191: train_loss=nan
2025-02-04 16:52:03,822 - INFO - Epoch 191: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:03,825 - INFO - Epoch 191: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:03,828 - INFO - ####################Training epoch 192####################
2025-02-04 16:52:04,039 - INFO - Epoch 192: train_loss=nan
2025-02-04 16:52:04,305 - INFO - Epoch 192: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:04,309 - INFO - Epoch 192: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:04,312 - INFO - ####################Training epoch 193####################
2025-02-04 16:52:04,521 - INFO - Epoch 193: train_loss=nan
2025-02-04 16:52:04,786 - INFO - Epoch 193: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:04,790 - INFO - Epoch 193: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:04,793 - INFO - ####################Training epoch 194####################
2025-02-04 16:52:05,004 - INFO - Epoch 194: train_loss=nan
2025-02-04 16:52:05,269 - INFO - Epoch 194: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:05,272 - INFO - Epoch 194: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:05,275 - INFO - ####################Training epoch 195####################
2025-02-04 16:52:05,488 - INFO - Epoch 195: train_loss=nan
2025-02-04 16:52:05,751 - INFO - Epoch 195: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:05,754 - INFO - Epoch 195: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:05,758 - INFO - ####################Training epoch 196####################
2025-02-04 16:52:05,967 - INFO - Epoch 196: train_loss=nan
2025-02-04 16:52:06,231 - INFO - Epoch 196: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:06,234 - INFO - Epoch 196: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:06,238 - INFO - ####################Training epoch 197####################
2025-02-04 16:52:06,447 - INFO - Epoch 197: train_loss=nan
2025-02-04 16:52:06,712 - INFO - Epoch 197: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:06,715 - INFO - Epoch 197: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:06,719 - INFO - ####################Training epoch 198####################
2025-02-04 16:52:06,929 - INFO - Epoch 198: train_loss=nan
2025-02-04 16:52:07,193 - INFO - Epoch 198: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:07,196 - INFO - Epoch 198: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:07,199 - INFO - ####################Training epoch 199####################
2025-02-04 16:52:07,408 - INFO - Epoch 199: train_loss=nan
2025-02-04 16:52:07,676 - INFO - Epoch 199: val_loss=nan, val_acc=66.67%
2025-02-04 16:52:07,680 - INFO - Epoch 199: EPOCH_AVG_TRAIN_LOSS=nan
2025-02-04 16:52:07,867 - INFO - Model saved.
